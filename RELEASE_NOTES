$Id$
===============================
Release Notes for PEGASUS 2.1.0
===============================

NEW FEATURES
--------------

1) Support for Second Level Staging

   Normally, Pegasus transfers the data to and from a directory on the
   shared filesystem on the head node of a compute site. The directory
   needs to be visible to both the head node and the worker nodes for
   the compute jobs to execute correctly.

   In the case, where the worker nodes cannot see the filesystem of
   the head node there needs to be a Second Level Staging (SLS)
   process that transfers the data from the head node to a directory
   on the worker node tmp. To achieve this, Pegasus uses the pre-job
   and post-job feature of kickstart to pull the input data from the
   head  node and push back the output data of a job to the head
   node. 

   Even though we do SLS, Pegasus still relies on the existence of a
   shared file system due to the following two reasons

   a) for the transfer executable to pick up the proxy, that we
   transfer from the submit host to the head node.

   b) to access sls input and output files that contain the file
   transfer urls to manage the transfer of data to worker node and
   back to headnode. 


   Additionally, if you are running your workflows on a Condor pool,
   one can bypass the use of kickstart to do the SLS. Please contact
   pegasus@isi.edu for more details of this scenario. In this case,
   the workflows generated by Pegasus have been shown to run in total
   non shared filesystem environment.

   To use this feature, user needs to set 

   pegasus.execute.*.filesystem.local  true

   The above change was tracked via bugzilla
   http://vtcpc.isi.edu/bugzilla/show_bug.cgi?id=21


2) New DAX schema
   The new has release moved to the new DAX schema version 2.1. Schema
   is  available online http://pegasus.isi.edu/schema/dax-2.1.xsd

   The main change in it is that the dontTransfer and dontRegister
   flags have been replaced by transfer and register flags. Changes
   were made both to the Java DAX Generator and Pegasus to conform to
   the new schema.  

   Additionally, the DAX parser in Pegasus looks at the schema version to
   determine whether to pick up dontTransfer and dontRegister flags (
   to support backward compatibility with the older daxes).

   Also with the filename type added a type attribute. It defaults to
   data. Additionally user can have the values
   executable|pattern. Users can use type=executable to specify any 
   dependant executables that their jobs required. All executable
   files are tracked in the transformation catalog.

   The above change was tracked via bugzilla
   http://vtcpc.isi.edu/bugzilla/show_bug.cgi?id=6 


3) Workflow and Planner Metrics Logging
   Workflow and Planning metrics are now logged for each workflow that
   is planned by Pegasus. By default, they are logged to
   $PEGASUS_HOME/var/pegasus.log  

   To turn metrics logging off, set pegasus.log.metrics to false

   To change the file to which the metrics are logged set
   pegasus.log.metrics.file  path/to/log/file

   Here is a snippet from the log file that shows what is logged
   {
   user = vahi
   vogroup = pegasus-ligo
   submitdir.base = /nfs/asd2/vahi/jbproject/Pegasus/dags
   submitdir.relative = /vahi/pegasus-ligo/blackdiamond/run0064
   planning.start = 2007-09-24T18:14:23-07:00
   planning.end = 2007-09-24T18:14:29-07:00
   properties
                 =/nfs/asd2/vahi/jbproject/Pegasus/dags/vahi/pegasus-ligo/blackdiamond/run0064/pegasus.6766.properties
   dax = /nfs/asd2/vahi/jbproject/Pegasus/blackdiamond_dax.xml
   dax-label = blackdiamond
   compute-jobs.count = 3
   si-jobs.count = 1
   so-jobs.count = 3
   inter-jobs.count = 0
   reg-jobs.count = 3
   cleanup-jobs.count = 2
   total-jobs.count = 14
   }


4) Support for querying multiple replica catalogs

   Pegasus now allows the users to query multiple replica catalogs at
   the same time to discover the locations of input data sets.

   For this a new Replica Catalog implmentation was developed. 
  
   The users need to do the following to use it.

   Set the replica catalog to MRC in the properties file.

   pegasus.catalog.replica MRC
 
   Each associated replica catalog can be configured via properties as	
   follows. The user associates a variable name referred to as [value]
   for each of the catalogs, where [value] is any legal identifier
   (concretely [A-Za-z][_A-Za-z0-9]*) 

   For each associated replica catalogs the user needs to specify the following properties.

   pegasus.catalog.replica.mrc.[value]      to specify the type of  replica catalog
   pegasus.catalog.replica.mrc.[value].key  to specify a property name key for a
                                            particular catalog
 

   For example, if a user wants to query two lrc's at the same time he/she can specify as follows

    pegasus.catalog.replica.mrc.lrc1 LRC
    pegasus.catalog.replica.mrc.lrc2.url rls://sukhna

    pegasus.catalog.replica.mrc.lrc2 LRC
    pegasus.catalog.replica.mrc.lrc2.url rls://smarty

   In the above example, lrc1, lrc2 are any valid identifier names and
   url is the property key that needed to be specified.  


5) Local Replica Selector
 
   Pegasus has a new local replica selector that only prefers replicas
   from the local host and that start with a file: URL scheme.  It is
   useful, when users want to stagin files to a remote site from your
   submit host using the Condor file transfer mechanism. 

   In order to use this, set the replica selector to Local in the
   properties.

        - pegasus.selector.replica  Local


6) Heft Based Site Selector

   Added a new site selector that is based on the HEFT processor
   scheduling algorithm. 

   The implementation assumes default data communication costs when jobs
   are  not scheduled on to the same site. Later on this may be made more
   configurable. 

   The runtime for the jobs is specified in the transformation catalog
   by associating the pegasus profile key runtime with the entries. 

   The number of processors in a site is picked up from the attribute
   idle-nodes associated with the vanilla jobmanager of the site in
   the site catalog.  


   To use this site selector, users need to set the following property 

   pegasus.selector.site Heft
 

7) Using multiple grid ftp servers for stageout
   If a user specifies multiple grid ftp servers for the output site
   in the site catalog, the stageout jobs will be distributed over all
   of them.

   More info can be found at
   http://vtcpc.isi.edu/bugzilla/show_bug.cgi?id=3 


8) Scalable Directory structure on the stageout site
   Users can now distribute their output files in a directory
   structure on the output site. On setting the Boolean property
   pegasus.dir.storage.deep to true, the relative submit directory
   structure is replicated on the output site. Additionally, within
   this directory the files are distributed into sub directories with
   each subdirectory having 256 files.

   The subdirectories are named in decimal format.
   

9) Specifying the jobmanager universe for the compute jobs in the DAX 
   Users can know specify the jobmanager type for the compute jobs in the
   DAX. This is achieved by specifying the jobmanager.universe profile
   key in the hints namespace. 

   Valid values for this are transfer|vanilla.

   This is useful for users who are running on a grid site, with the
   worker nodes behind a firewall and want a subset of their jobs to
   run on the head node.

   More info can be found at
   http://vtcpc.isi.edu/bugzilla/show_bug.cgi?id=5
 

10) Stork Support for doing transfers
   
   The internal transfer interfaces of Pegasus were updated to use the
   latest version of Stork for managing data transfers. 
   
   To use Stork implementations set the following
   pegasus.transfer.refiner = SDefault
   pegasus.transfer.*.impl=Stork

11) nogrid option for pegasus-run
  
 pegasus-run has now a --nogrid option. This bypasses the checks for
   proxy existence that are done before submitting the workflow for
   execution. It disables all globus checks like check for environment
   variables GLOBUS_lOCATION and LD_LIBRARY_PATH.

   This is useful for running workflows in native Condor environments. 

12) Submitting workflows directly using pegauss-plan

   A new option --submit|-S option was added to pegasus-plan. This
   allows users to submit workflows directly, after they have been
   planned. 

13) Specifying relative submit directory
   Since pegasus 2.0 , pegasus-plan creates a directory structure in
   the base submit directory. The base submit directory is specified
   by --dir option to pegasus-plan. If a user, want to specify a
   relative submit directory, he can use the --relative-dir option to
   pegasus-plan. 

   The above change was tracked via bugzilla
   http://vtcpc.isi.edu/bugzilla/show_bug.cgi?id=14 


BUGS FIXED
----------
1) Specifying Relative Path to the DAX

   An incorrect path to the dax was generated internally when a user
   specified a relative path to the dax to pegasus-plan 

   This is fixed now, and was tracked via bugzilla
   http://vtcpc.isi.edu/bugzilla/show_bug.cgi?id=13


2) RLS java api bug fix 4114 (globus bugzilla number )

   globus_rls_client.jar updated with the bug fix 4114. Also added the
   jar for java 1.4 in lib/java1.4  

3) Passing of DAGMan parameters via properties in case of deferred
   planning 

   In case of deferred planning, the properties that control DAGMan
   execution were not being passed as options to DAGMan. This is now
   fixed. 

   The following properties are being handled correctly now.

   pegasus.dagman.maxjobs
   pegasus.dagman.maxpre
   pegasus.dagman.maxidle
   pegasus.dagman.maxpost


===============================
Release Notes for PEGASUS 2.0.1
================================

There is new documentation in the form of a quick start guide and glossary
in the docs directory. More documentation will be coming soon and will be available
in the release as well as on the pegasus website under documentation.

NEW FEATURES
------------
1) Pegasus now can store provenance data into PASOA. The actions taken
   by the various refiners are logged into the store. It is still an 
   experimental feature. To turn it on, set the property 

   pegasus.catalog.provenance.refinement  pasoa

   The PASOA store needs to run on localhost on port 8080
   https://localhost:8080/preserv-1.0

2) You can also use Pegasus to store execution provenance in PASOA.
   To use set the properties
   pegasus.exitcode.impl=pasoa
   pegasus.exitcode.path.pasoa=${pegasus.home}/bin/pasoa-client
   pegasus.exitcode.arguments=<dax file> <dag file>

BUGS FIXED
----------
1) sitecatalog-converter 
   patch to fix pegasus profile conversion

2) pegasus-submit-dag
   added --maxidle option to allow setting number of idle jobs on
   the remote site. 

3) VORS.pm 
   Fixed a small typo in there, that lead to perl compilation
   errors. 

4) pegasus-get-sites
   Removed local tc entries and added environments 
   for PEGASUS_HOME, GLOBUS_LOCATION and LD_LIBRARY_PATH to local
   site.

5) mpiexec 
   The execution of clustered jobs via mpiexec was broken in 2.0
   release. That is now fixed.

6) exitcode/exitpost
   Fixed a bug in exitcode that caused a call to the DB PTC even though
   the property pegasus.catalog.provenance was not set.

KNOWN BUGS
-----------

===============================
Release Notes for PEGASUS 2.0.0
===============================

NEW FEATURES
--------------

pegasus-plan.
	This is the main client for invoking pegasus. The earlier gencdag command is now called pegasus-plan

pegasus-run
	This is the client that submits the planned workflow to Condor and starts a monitoring tailstatd daemon

pegasus-status
	This client lets you monitor a particular workflow. Its a wrapper around condor-q

pegasus-remove.
	This client lets you remove a running workflow from the condor queue.
        A rescue dag will be generated which can be submitted by just running pegasus-run on the dag directory.


