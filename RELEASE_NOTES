Release Notes for VDS 1.4.7
$Id: RELEASE_NOTES,v 1.11 2006/10/11 00:16:03 vahi Exp $
-------------------------------
- Start using the new properties outlined in the release notes, in
  lieu of the properties deprecated in this release. Though, the
  properties are backward compatible,it will save pain later on.

  Detailed description of the new properties can be found in the
  properties file available at $VDS_HOME/etc/properties.pdf.

- The developers changelog can be found at
  http://vds.isi.edu/blog

New Features
--------------------------------
- Horizontal Partitioner
  A new partitioner, that allows you to configure the number of
  partitions for each level of the workflow. It builds upon the level
  based partitioning that you get out of BFS partitioner. Instead of
  having only one partition for all jobs in a level of the workflow,
  you can control how many partitions are created at a level of a
  workflow corresponding to an underlying logical transformation.
  All the jobs in a partition refer to the same logical
  transformation. 
  
  You can specify for each transformation name the following
  parameters :

  1) bundle value
  the number of partitions at a level referring to the same	
  transformation.
  This is done by setting the properties
  vds.partitioner.horziontal.bundle.[txname] to specify the bundle
  parameters, where [txname] is replaced by the logical name of the
  transformation

  For example, if in a workflow you have 10 jobs referring to a
  transformation with a logical name txA at a particular level(l) of the
  workflow, and the properties set as follows
  vds.partitioner.horizontal.bundle.txA 4

  The above will result in in 4 partitions being created, 2 having 2
  jobs each and 2 having 3 jobs for that level (l).


  2) collapse value
  the number of jobs on a level of the workflow referring to a
  transformation that make a single partition.

  This is done by setting the properties
  vds.partitioner.horziontal.collapse.[txname] to specify the collapse
  parameters, where [txname] is replaced by the logical name of the
  transformation.

  For example, if in a workflow you 10 jobs referring to a
  transformation with a logical name txA at a particular level(l)
  of the workflow, and properties set as follows
  vds.partitioner.horizontal.collapse.txA 4

  The above will result in 3 partitions being created, 2 having 4 jobs
  each and one having 2 jobs for that level (l).


  Note that the bundle and collapse parameters are orthogonal. If both
  are specified, then bundle parameter takes precedence.

  To use this partitioning scheme, set the --type option to partitiondax
  to horizontal


- POSTScript and Exitcode Handling
  Each of the kickstarted jobs can have a postscript associated with
  them. This postscript is executed on the submit host, after a job
  has finished execution. By default, each of the GridStart
  implementations have their own postscripts, that are able to parse
  their output and determine the exit status. For example, for
  kickstart VDS provides an exitpost utility that parses the kickstart
  record and can populate the data into a provenance tracking
  catalog. However, you can specify your own postscript to be executed
  if desired.

  To set the postscript to be used for the whole workflow, one can
  specify the property vds.exitcode.impl .This property applies to all
  the jobs in the workflow. The value can be overriden selectively for
  jobs by associating Dagman profile key POST with the jobs either in
  DAX, site in the Site Catalog or the transformation in the
  transformation catalog.

  The path to the postscript can be specified by the property
  vds.exitcode.path.[value] and arguments byvds.exitcode.arguments,
  where [value] is the value of this property (vds.exitcode.impl).

  The following types of postscript are supported.

  exitpost
  This is the postscript that can be found at $VDS_HOME/bin/exitpost,
  and is by default invoked on all jobs that are launched via
  kickstart. It is a wrapper around exitcode, that makes backups of
  the processed files. It assumes that the error file ends in ".err"
  (to be fixed), and looks into it for certain PBS failures like
  wall-time exceeded, which may fail a job without producing good
  kickstart output. This thin wrapper is the preferred way now to
  invoke exitcode from within a DAG. 

  exitcode
  This is the postscript that can be found at
  $VDS_HOME/bin/exitcode. It can be invoked on jobs that are launched
  via kickstart. This is a legacy postscript, and has now been
  replaced byexitpost.

  none
  This means that no postscript is generated for the jobs. This is
  useful for MPI jobs that are not launched via kickstart currently.

  any legal identifier
  Any other identifier of the form ([_A-Za-z][_A-Za-z0-9]*), than one
  of the 3 reserved keywords above, signifies a user postscript. This
  allows you to specify your own postscript for the jobs in the
  workflow. The path to the postscript can be specified by the
  property vds.exitcode.path.[value] 
  where [value] is this legal identifier that you have specified. The
  arguments to be passed to your postscript can be set by the property
  vds.exitcode.arguments.  
  If the path is not set for the user postscript, then the default
  path $VDS_HOME/bin/exitcode is picked up.
  The user postscript is passed the name of the .out file of the job as
  the last argument on the command line.
  For e.g. with the following property settings	
   vds.exitcode.default user_script
   vds.exitcode.path.user_script /bin/user_postscript
   vds.exitcode.arguments -verbose
  for a job with submit file a.sub a user postscript invocation will
  appear in the .dag file as
  /bin/user_postscript -verbose a.out


- Label Based Clustering
  Earlier for job clustering you could only cluster independant jobs
  that were at the same level. Now you can cluster a group of jobs
  that have the same label associated with them. To label the
  workflow, you need to associate VDS profiles with the jobs in the
  DAX. The profile key to use for labelling the workflow can be set by
  the property vds.clusterer.label.key.  It defaults to label, meaning
  if you have a VDS profile key label with jobs, the jobs with the
  same label will go into the same clustered job.
  
  In addition, you can now apply recursively different clustering
  techniques to your workflows.

  The clustering guide in the userguide has been updated, that
  explains clustering in more detail.
  A PDF version can be found online at 
  http://www.isi.edu/~vahi/vds/doc/VDSUG_PegasusJobClustering.pdf

- Disabling of chmod jobs for executable staging
  During staging of executables to remote sites, chmod jobs are added
  to the workflow. These jobs run on the remote sites and do a chmod
  on the staged executable. For some sites, this maynot be
  required. The permissions might be preserved, or there maybe an
  automatic mechanism that does it.

  The setting of the property vds.transfer.disable.chmod.sites, allows
  you to specify the list of sites, where the chmod jobs are not to
  be executed. For those sites, the chmod jobs are replaced by NoOP
  jobs. The NoOP jobs are executed by Condor, and instead will
  immediately have a terminate event written to the job log file and
  removed from the queue.


- Turning on the DEBUG for the exitcode invocation
  Introduced a new property vds.exitcode.debug that turns on the debug
  for the exitcode. By default the debug is turned off.
  On setting the property to true, an exitcode log file being created 
  for each job in the submit directory. The log file created is of the
  format [JOBNAME].exit.log .

Bugs Fixed
-------------------------------
- Fixed argument passing to exitcode
  There was a bug whereby in certain cases, the arguments and
  properties were passed incorrectly to the exitcode invocation.
  Details can be found at
  http://bugzilla.globus.org/vds/show_bug.cgi?id=144

- Abortion of deferred planning workflows
  On doing a condor_rm on a job in the outer level workflow, the sub
  workflow for the corresponding partition was not aborted. This is
  now fixed.
  Details can be found at
  http://bugzilla.globus.org/vds/show_bug.cgi?id=143


- Fixed stdout handling during job clustering
  The stdout for the clustered job used to be clobbered in the case
  where stdout was being explicilty tracked for the constituent jobs. 
  Details can be found at
  http://bugzilla.globus.org/vds/show_bug.cgi?id=142


- Fixed exit condition if LRC's are inaccessible when planning
  Planning on the workflow used to abort in the case when all the
  LRC's were inaccessible on the premise that you cannot select
  replicas. However, this exit condition did not take into
  consideration any mappings that user might pass in the cache files. 
  Details can be found at
  http://bugzilla.globus.org/vds/show_bug.cgi?id=141


- Fixed bug in prescript generation for partitions in deferred
  planning
  The --cache option was incorrectly generated for prescripts of
  certain partitions.
  Details can be found at
  http://bugzilla.globus.org/vds/show_bug.cgi?id=140


- Fixed Kickstart backslash escape problems
  Certain argument strings were incorrectly parsed. 
  Details can be found at 
  http://bugzilla.globus.org/vds/show_bug.cgi?id=139


Release Notes for VDS 1.4.6
-------------------------------
A new release that fixes some nagging bugs, that were discovered after
1.4.5 release. This is to ensure that the correct version of VDS goes
into VDT 1.3.13.


Bugs Fixed
-------------------------------
- Fixed the property vds.scheduler.condor.arguments.quote
  The property vds.scheduler.condor.arguments.quote was incorrectly
  implemented which lead to the inability of setting this property to
  false. As a result,the quoting of arguments in the condor submit
  files, always happened irrespective of the property value. 
  Hence, all workflows generated from a VDS 1.4.5 release fail on 
  the existing OSG 0.4.x and earlier OSG sites.
  This is now fixed, and the property can be set to false, to turn off
  quoting of arguments.

- Fixed incorrect dependency creation in tentacles mode 
  A bug was discovered while running the SCEC workflows for creating
  remote working directories. Incorrect dependencies were being
  created in the case where the workflow is reduced, and the reduced
  jobs need to be staged to an output site.  
  The fix has been shown to work on a smaller workflow that exhibited
  same behaviour. However, still needs to be verified on the large
  SCEC workflows.

  Details can be found at
  http://bugzilla.globus.org/vds/show_bug.cgi?id=137

- Fixed random directory creation on local site.
  Fixed random directory creation on local site with Tentacles mode,
  in the case where the root compute job has no input files.

  Details can be found at
  http://bugzilla.globus.org/vds/show_bug.cgi?id=138

Release Notes for VDS 1.4.5 
-------------------------------
- Start using the new properties outlined in the release notes, in
  lieu of the properties deprecated in this release. Though, the
  properties are backward compatible,it will save pain later on.
  
  Detailed description of the new properties can be found in the
  properties file available at $VDS_HOME/etc/properties.pdf.

- The developers changelog can be found at 
  http://vds.isi.edu/blog

New Features 
--------------------------------
- Argument Quoting according to Condor Quoting Rules
  Condor came up with a consistent quoting mechanism for quoting
  arguments. These appear in Condor versions 6.7.19 onwards. 
  Pegasus was modified, to generate submit files that follow these
  quoting mechanisms. For a more detailed description on the rules 
  (or how they were interpreted in VDS), look at the javadoc for 
  CondorQuoteParser in the VDS documentation.

  By default, all argument string are now quoted according to these 
  rules. For users who are still using older Condor versions on their 
  submit hosts, set the property 
  vds.scheduler.condor.arguments.quote to false.

- Transfer executable now handles symlinking in case of deep lfns
  Earlier while creating symbolic links against deep lfns on remote sites,
  transfer used to fail. The reason for this was that, transfer did
  not check for the existence of the parent directory where the 
  symbolic link had to be created. Transfer was modified to check for 
  the existence of the parent directory, and tries to create the
  parent directory if the directory does not exist.

  for e.g. for the following
  src: file:///nfs/shared/f.a and
  dst: file:///tmp/karan/shared/f.a
  the directory karan/shared is created
  in the /tmp directory before creating the symbolic link.

  The above is only possible, if transfers are not being run in third 
  party mode.


- Changed Deferred Planning to use HashedFileFactory
  For deferred planning, the partitions and all the jobs of the megadag are
  now organized using the HashedFileFactory. Each partition has its own submit
  directory. The HashedFileFactory is used to organize these partition submit
  directories in a sensible way from the base submit directory.

  The jobs that submit condor dagman job for each partition reside in the parent 
  directory of the partitions submit directory. However, the job itself is run 
  in partition submit directory which ensures that all the dagman files are 
  generated in the correct directory (the partition submit directory).
  At present the prescript used is gencdag. The prescript log also appears 
  in the parent directory of the partition submit directory i.e the same directory
  that contains the condor_dagman job for the partition.

- New Label Based Partitioner
  There is a new partitioner to partition your workflow for use in deferred planning.
  It partitions the DAX on the basis of the labels associated with the jobs in the 
  DAX. Jobs with the same labels are assumed to be in the same partition.
  
  A VDS profile key needs to be associated with the jobs in the DAX. By default, a
  key named label is expected to be   associated. However, this can be changed by 
  setting the property vds.label.key.

  A job that is not associated with any label profile key, is put in a default partition.


- Added new options to gencdag (basename and monitor)
  The gencdag tool now has an addition option called basename. Can be specified by 
  either specifying -b prefix or --basename prefix. The prefix is the prefix that
  is to be applied while creating all the workflow specific files like the .cache 
  file, the condor log file in root submit directory, the .dag file etc. This 
  overrides the prefix that is determined by looking up the label information
  in the DAX file.

  Added a monitor option to gencdag, that results in the tailstatd daemon being 
  invoked on the concrete workflow.
  This is still an experimental feature, that is being used for monitoring of 
  workflows in case of deferred planning. The tailstatd daemon is always invoked 
  if using vds-plan and vds-run. It is recommended, that users use that to run 
  their workflows.

- Logging to an output stream
  Earlier, all the logging message generated by Pegasus were logged to stdout and
  stderr. However, now users can choose to log these messages directly to a file.
  
  The file to which to log all the messages can be specified by
  vds.log.* property.

- Extensions to the Restricted Replica Selector
  Earlier, the user could only specify for a particular site X, the list of sites 
  from where not to stage in data. The Replica Selector was extended to allow the 
  user to specify preffered sites for staging in data.

  A good site for a compute site X, is a preferred site from which
  replicas should be staged to site X. If there are more than one good sites
  having a particular replica, then a random site is selected amongst these
  preferred sites.

  A bad site for a compute site X, is a site from which replica's should
  not be staged. The reason of not accessing replica from a bad site can vary
  from the link being down, to the user not having permissions on that site's
  data.

  The good | bad sites are specified by the properties
  vds.replica.*.prefer.stagein.sites|vds.replica.*.ignore.stagein.sites,
  where the * in the property name denotes the name of the compute site. A *
  in the property key is taken to mean all sites.

  The vds.replica.*.prefer.stagein.sites property takes precedence over
  vds.replica.*.ignore.stagein.sites property i.e. if for a site X, a site Y
  is specified both in the ignored and the preferred set, then site Y is
  taken to mean as only a preferred site for a site X.

- Extensions to the rc-client
  Bulk insert and bulk delete functionality were added in RLS implementation of
  the Replica Catalog. Two new options to rc-client (--insert and --delete) 
  were added to make use of this functionality.
  
  Both of the options take a filename as an argument. The file is supposed to
  contain the mappings that are to be inserted or deleted. Each line in the 
  file refers to one mapping. The format of each line is as follows:
  LFN PFN [k=v [..]]

  The input file is read in chunks, with the property vds.rc.chunk.size 
  specifying the number of lines that are read in at a time. The chunk of 
  file that is read in is passed to the underlying replica implementation as
  one. Thus taking advantage of any bulk facilities that may exist in the 
  underlying replica implementations.

- Timeouts for replica catalog
  For LRC and RLI implementations, the user can set the timeout by setting the
  foloowing properties
    		       - vds.rc.lrc.timeout 
                       - vds.rc.rli.timeout properties.

  Alternatively user can set vds.rc.rls.timeout to apply the same timeout to 
  both the LRC and the RLI.
 
- Changes to seqexec
  The program invocations fed to seqexec now require shell-style  
  quoting, if any white-spaces occur inside a single argument. The  
  quoting rules of double-quote, single-quote and back-slash apply  
  exactly as in Bourne shell.

  Added a "-R fn" option to report progress into a file fn.  
  Multiple seqexec may write to the same file, and care is taken  
  (append mode and fcntl try-locks) that concurrent log entries are not  
  garbled (though no guarantee on Linux-NFS). Alternatively, an  
  environment variable SEQEXEC_PROGRESS_REPORT can point to this file.  
  [tbd: log format]

  Added a "-f" option to seqexec for vertical clustering. In this  
  case, seqexec behaves like "bash -e", meaning, it will fail and exit  
  on the first program that failed. Failure is defined as any program  
  that does not return a raw exit code of 0.
  The raw exit status, see wait(2), incorporates any signals in  
  the lower portion, and the exit code of the program in the higher  
  portion. Thus, if a program dies upon a signal, has still failed.



  Fixed a bug with the continuation lines of seqexec. Very large  
  lines were read in pieces, and merged. However, the merge operation  
  inserted a whitespace between the buffers, thus breaking arguments in  
  the middle.

  Updated seqexec man page.
 
- Changes to exitcode (Saving backups of processed files)
  Added a wrapper around "exitcode" called "exitmode", which will  
  make backups of the processed files. It will also assume that the  
  error file ends in ".err" (to be fixed), and look into it for certain  
  PBS failures like wall-time exceeded, which may fail a job without  
  producing good kickstart output. This thin wrapper is the preferred  
  way now to invoke exitcode from within a DAG. It prevents DAGMAN from
  overwriting a jobs output and error while when it retries a job in case
  of failure.

  Modified exitcode to faster and more easily skip over data  
  sections that it does not include into the PTC. These skips yield a  
  faster turn-around.

- Kickstart captures remote environment seen by the job in it's output
  This has proven extremely helpful for grid debugging. However, the  
  environment information is not added to the PTC. The exitcode XML  
  parser was adjusted to accommodate reading the new XML. Created a new  
  XML schema file

- Backups in kickstart
  Added a feature to kickstart, by which it will back-up the file  
  pointed to for stdout and stderr instead of overwriting. This is  
  ensured by prefixing the filename with a roof. If you redirect both  
  to the same file, only specify the roof for stdout.

- Other changes to kickstart
  Deferred the stat() call and file truncation for stdio  
  filehandles until after changing into the work directory (-w and -W  
  options). This applies mostly to relative path specifications for  
  stdout, stderr and log.

  [pending 1] If stdout and stderr both are redirected to the same  
  file, kickstart should use dup() to duplicate stdout onto stderr,  
  instead of open(). See Stevens's APUE, chapter 4 and 5 for reasons why.

  [pending 2] There is code reporting the remote site's resource  
  limits, as far as they can be figured out. Like environment, these  
  donnot go into the PTC, but lead to many "a-ha" experiences  
  while grid debugging.

  [pending 3] Request for a "production" switch, so that the rather  
  verbose environment and resource limits are omitted from the output.  
  

Bugs Fixed
--------------------------------
- rc-client list command with constraints specified
  Earlier a list command with a constraint through the rc-client, used to incorrectly 
  list the entries. It did not return the complete set, as the result set was
  being overwritten. This bug was present when using the rc-client to talk to RLS.

- bug with continuation lines of seqexec
  Fixed a bug with the continuation lines of seqexec. Very large  
  lines were read in pieces, and merged. However, the merge operation  
  inserted a whitespace between the buffers, thus breaking arguments in  
  the middle.

  Details can be found at 
  http://bugzilla.globus.org/vds/show_bug.cgi?id=136

Known Bugs
--------------------------------

- Transfer executable
  There are problems, if the OS permits and provides an  
  "unlimited" hard limit of concurrent open file handles. This is  
  actually an OS bug, but we need to deal with it. The work-around code  
  has not been shown to do the right thing (tm) yet.

- The known/open bugs can be found at http://vds.isi.edu/bugzilla.


Release Notes for VDS 1.4.4
-------------------------------
- Start using the new properties outlined in the release notes, in
  lieu of the properties deprecated in this release. Though, the
  properties are backward compatible,it will save pain later on.
  
  Detailed description of the new properties can be found in the
  properties file available at $VDS_HOME/etc/properties.pdf.

- The developers changelog can be found at 
  http://vds.isi.edu/blog

New Features 
--------------------------------
- Finer grained control over transfers within the workflow.
  Earlier the vds.transfer property, was used to determine both the
  transfer implementation and the refining strategy. 
  The transfer implementation refers to what transfer tools to use, to
  run the transfer jobs on the grid. 
  The refiner strategy indicates how the workflow is refined while
  adding in the various types of transfer jobs to the workflow, like
  whether the stage in nodes need to be chained together or bundled
  together. 

  To set implementation
  vds.transfer.*.impl   Transfer|T2|Stork|OldGUC|RFT|CRFT|SRM
  The * in the property name can be replaced by *|stagein|stageout|inter.

  To set the refiner
  vds.transfer.refiner  Default|SDefault|Bundle|Chain|SChain

  These are backward compatible. On using vds.transfer the appropriate
  implementations and refiners are set. 

- Finer grained control over third party transfers
  Earlier the user could specify a set of sites for which third party
  transfers had to be used to stage in and out the data. Now, there is
  support for specifying what type of transfer need to be third party
  for a particular site.
  
  The vds.transfer.thirdparty.sites is now deprecated and has been
  replaced by vds.transfer.*.thirdparty.sites. The * in the property
  name can be replaced by *|stagein|stageout|inter.
  The value is a comma separated list of sites, and * can be used to
  denote all sites.

- The users can now specify where to run the third party
  transfers. The third party transfers by default are run on the
  submit host. However, the user can now optionally run them on the
  remote grid sites by specifying the property
  vds.transfer.*.thirdparty.sites. 

  The * in the property  name can be replaced by
  *|stagein|stageout|inter.  
  The value is a comma separated list of sites, and * can be used to
  denote all sites.

- Added preliminary support for SRM using srmcp
  Added in a new transfer implementation that uses srmcp (a SRM
  client) to transfer files to and from a SRM server. 
  To use this set vds.transfer.*.impl to SRM.
  It looks for a transformation with the fully qualified name as
  srm::srmcp in the transformation catalog. It works with the single
  refiner, as the client can currently handle only one file transfer
  per job. 

- State machine inside seqexec
  seqexec distributed as part of VDS has been modified to incorporate
  a state machine. This allows it to escape quotes that are provided
  as arguments to the jobs. Standard shell/quoting/escaping rules
  apply. 

Bugs Fixed
--------------------------------
- The x bit on the executables staged to a remote site as part of the
  workflow is is now correctly set. Earlier, kickstart was used to set
  the X bit. However, that did not work for standard universe jobs
  that preclude a job being launched via kickstart.

  Details can be found at 
  http://bugzilla.globus.org/vds/show_bug.cgi?id=102

- In case of using third party transfer for staging of executables the
  URL's were not correctly resolved. This is now fixed.

  Details can be found at 
  http://bugzilla.globus.org/vds/show_bug.cgi?id=129

- The format of the DAX mTime was changed so that the exitcode parser
  does not trip over, while parsing the invocation record generated
  by kickstart.

- Fixed the clustering bug, whereby the parallelism of the workflow
  being clustered was lost.

  Details can be found at 
  http://bugzilla.globus.org/vds/show_bug.cgi?id=132   

- Fixed white space issues in vds-get-sites

Known Bugs
--------------------------------
- The known/open bugs can be found at http://vds.isi.edu/bugzilla.



Release Notes for VDS 1.4.3
-------------------------------


New Features 
--------------------------------
- Support for local condor pool execution
  Users can now submit directly to the underlying condor pool of which
  the submit host is a part of. This results in CondorG being
  bypassed, as jobs are submitted directly to the condor daemons
  running. This is different from the glide in case, where the nodes
  are glided in from a remote pool to the local pool.

  To activate a particular style of dag generation for a particular
  site, associate with it the VDS profile key style in the site
  catalog.
  
  Currently the following 3 styles are supported
  1) globus: default mode for generating the workflows, that are
     submitted to a remote site using CondorG  
  2) condor: when the submit host is the part of a condor pool, and
     you want to execute the workflow on that condor pool.
  3) glidein: the case, when nodes have been glided in from a remote
     site to the local site.
  
- Label and mTime of the DAX passed to kickstart
  On setting vds.gridstart.label to true, the label and mTime of the
  DAX are passed ahead to kickstart for population into the PTC.

Bugs Fixed
--------------------------------
- Clustering and postscript.
  In the case where the clustered jobs were not being launched via
  kickstart, the postscript was still generated. This now only happens
  if either the clustered job is being launched via kickstart or an
  explicit vds.post key is associated with the jobs being clustered.

Known Bugs
--------------------------------
- The known/open bugs can be found at http://vds.isi.edu/bugzilla.



Release Notes for VDS 1.4.2
-------------------------------
- Start using the new properties outlined in the release notes, in
  lieu of the properties deprecated in this release. Though, the
  properties are backward compatible,it will save pain later on.
  
  Detailed description of the new properties can be found in the
  properties file available at $VDS_HOME/etc/properties.pdf.

- The developers changelog can be found at 
  http://vds.isi.edu/blog


New Features 
--------------------------------
- Support for invoke with kickstart
  VDS now has a new tool named invoke, that can be found at
  $VDS_HOME/bin/invoke. It is used to circumvent the Condor limit of
  restricting the argument string to 4K. In the case invoke is used,
  then the arguments are actually written out in a text file, that is
  then transported to remote site using condor file transfer, or other
  transfer mechanisms.
  The invoke behaviour can be fine tuned using the following two
  properties 
	     1) vds.gridstart.invoke.always (boolean indicating
		whether to always use invoke or not within kickstart)
	     2) vds.gridstart.invoke.length (the trigger limit for
	        using invoke in kickstart. default is 4000)
	     
- Single Kickstart record for clustered jobs
  Seqexec is used to launch a cluster of jobs as one job. 
  Changed the way how seqexec is invoked for the clustered jobs. All
  jobs that are launched by seqexec are kickstart enabled. This leads
  to a single kickstart record being written out to the stdout, that
  is transported back to the submit host. Exitcode is inovked only
  once when the whole stdout has been streamed back.

- Expansion of Chain transfer mode
  The Chain transfer mode has been expanded to create chains of
  stagein transfer nodes, instead of a single chain as earlier. The
  number of chains can be specified by association the vds profile
  "chain.stagein" The profile can be associated either with the site
  in the site catalog, or with the transfer executable in the
  transformation catalog.
	
- Specifying Job Priorities
  Added property vds.job.priority to allow the user to specify a
  priority for his/her jobs. The value given to the property is
  translated to the priority condor command in the submit
  files. Priority can also be specified by associating the condor
  profile key priority with the job.
  
  For transfer jobs, the priorities are picked up from the
  vds.transfer.*.priority properties. The * in the property name can
  be replaced by *|stagein|inter|stageout. The * means all types of
  transfer jobs.

- Transfer of Proxies
  Added a property vds.transfer.proxy to designate whether the proxy
  needs to be transferred explicitly or not to the remote end for the
  jobs. By default, CondorG transfers limited proxies to a remote
  end. In certain cases that is not sufficient like the GRidFTP DSI
  implementation for SRB To apply selectively the transfer of proxies
  for certain jobs, the user associate the vds profile transfer.proxy
  with the jobs. 

  Details of the feature can be found at
  http://bugzilla.globus.org/vds/show_bug.cgi?id=116 
  

Bugs Fixed
--------------------------------
- Fixed the generation of jobnames for the clustered job, in job
  clustering. 

  Details can be found at 
  http://bugzilla.globus.org/vds/show_bug.cgi?id=124

Known Bugs
--------------------------------
- The known/open bugs can be found at http://vds.isi.edu/bugzilla.



Release Notes for VDS 1.4.1
-------------------------------
- Start using the new properties outlined in the release notes, in
  lieu of the properties deprecated in this release. Though, the
  properties are backward compatible,it will save pain later on.
  
  Detailed description of the new properties can be found in the
  properties file available at $VDS_HOME/etc/properties.pdf.

- The developers changelog can be found at 
  http://vds.isi.edu/blog


New Features 
--------------------------------
- Finer grained control over transfers within the workflow.
  Earlier the vds.transfer property, was used to determine both the
  transfer implementation and the refining strategy.


Bugs Fixed
--------------------------------
- The known/open bugs can be found at http://vds.isi.edu/bugzilla.

Known Bugs
--------------------------------
- The known/open bugs can be found at http://vds.isi.edu/bugzilla.



Release Notes for VDS 1.4.0
-------------------------------
- Start using the new properties outlined in the release notes, in
  lieu of the properties deprecated in this release. Though, the
  properties are backward compatible,it will save pain later on.
  
  Detailed description of the new properties can be found in the
  properties file available at $VDS_HOME/etc/properties.pdf.

- The developers changelog can be found at 
  http://vds.isi.edu/blog


New Features 
--------------------------------


Bugs Fixed
--------------------------------
- The known/open bugs can be found at http://vds.isi.edu/bugzilla.

Known Bugs
--------------------------------
- The known/open bugs can be found at http://vds.isi.edu/bugzilla.


OLDER RELEASE NOTES FROM $VDS_HOME/CHANGELOG
--------------------------------
IMPORTANT: Starting with 1.3.6, the rDBMS schemas have changed!
IMPORTANT: You MUST set VDS_HOME before source'ing any setup scripts!

1.3.5 -> 1.3.6
==============

- If you have an existing VDC, you will need to run a couple of simple
  ALTER TABLE commands, as bundled in $VDS_HOME/vds/update-*.sql where *
  is my for MySQL and pg for Postgres. The other distinction is whether
  your run the ChunkSchema (default) or AnnotationSchema ("-anno"). 
  Since these are mutual exclusive (old), you must only run one. You
  can discover what is installed by querying the table "chimera" (old)
  or "vds_schema" (recent) for "catalog='vdc'".

- To support our own shipped version of globus-url-copy as "guc" from GT3,
  we now set the environment variable GLOBUS_TCP_SOURCE_RANGE to the
  value of GLOBUS_TCP_PORT_RANGE, if the first is unset, but the latter
  is set. 

1.2.5 -> 1.3.*
==============

- gencdag changed the following option: The previous option --p has to be
  either spelled -p (single hyphen) or --pools.

- setup scripts now REQUIRE that VDS_HOME is previously set. They WILL FAIL
  if the variable was not set. We had to remove the auto-setup feature,
  because there is no reliable, portable way to obtain a scripts invocation
  path when being sourced. 

1.2.4 -> 1.2.5
==============

- The vds.db.driver* properties have changed! All of them! We now
  instantiate one driver per schema, which allows different catalogs to
  go to different databases.You need to adjust to the new driver
  property schema in order to continue using your database.

  If you previous database driver properties looked like this:

   vds.db.vdc.schema       AnnotationSchema
   vds.db.ptc.schema       InvocationSchema
   vds.db.driver           Postgres
   vds.db.driver.url       jdbc:postgresql:${user.name}
   vds.db.driver.user      ${user.name}
   vds.db.driver.password  ${user.name}

  you need to insert an asterisk before the driver:

   vds.db.*.driver          Postgres
   vds.db.*.driver.url      jdbc:postgresql:${user.name}
   vds.db.*.driver.user     ${user.name}
   vds.db.*.driver.password ${user.name}

  For the future, if you prefer to put your PTC into a different
  database, you can either copy the section with the *, replace each
  asterisk with the word 'ptc', and adjust the values, for example:

   vds.db.*.driver          Postgres
   vds.db.*.driver.url      jdbc:postgresql:vdc
   vds.db.*.driver.user     vdc
   vds.db.*.driver.password geheim
   vds.db.ptc.driver           MySQL
   vds.db.ptc.driver.url       jdbc:mysql://localhost/${user.name}
   vds.db.ptc.driver.user      ${user.name}
   vds.db.ptc.driver.password  ${user.name}

  The section above would still use the "vdc" database in Postgres by
  default, but now stores the PTC subsection into a catalog of its own
  inside a different database.

- Eventually, we will switch to the AnnotationSchema instead of the
  ChunkSchema for the rDBMS-based VDCs. You can easily create and drop
  the complete VDC for the AnnotationSchema by using the
  $VDS_HOME/sql/(create|delete)-(pg|my)-anno.sql files.

- Introduction of optional transfer markers on logical filenames. The
  capitol "T" marking, which is mutually exclusive with the "t" marking,
  denotes that the transfer of such a marked file is optional, and the
  transfer must not fail the workflow, if the source does not exist.


