<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN"
"http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<chapter id="reference">
  <title>Reference Manual</title>

  <section>
    <title>Introduction</title>

    <para>Introduction Text  </para>

  </section>
  <section>
  <title>Properties</title>



   </section>

  <section>
   <title>Profiles</title>


    <para>The Pegasus Workflow Mapper uses the concept of profiles to
    encapsulate configurations for various aspects of dealing with the Grid
    infrastructure. Profiles provide an abstract yet uniform interface to
    specify configuration options for various layers from planner/mapper
    behavior to remote environment settings. At various stages during the
    mapping process, profiles may be added associated with the job.</para>

    <para>This document describes various types of profiles, levels of
    priorities for intersecting profiles, and how to specify profiles in
    different contexts.</para>

  
      <section>

      <title>Profile Structure Heading</title>
       <para>All profiles are triples comprised of a namespace, a name or key,
    and a value. The namespace is a simple identifier. The key has only
    meaning within its namespace, and it’s yet another identifier. There are
    no constraints on the contents of a value</para>

       <para>Profiles may be represented with different syntaxes in different
    context. However, each syntax will describe the underlying triple.</para>
        </section>


            <section>
             <title>Profile Namespaces</title>

        <para>Each namespace refers to a different aspect of a job’s runtime
    settings. A profile’s representation in the concrete plan (e.g. the Condor
    submit files) depends its namespace. Pegasus supports the following
    Namespaces for profiles:</para>

    <itemizedlist>
      <listitem>
        <para><emphasis role="bold">env</emphasis> permits remote environment
        variables to be set.</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">globus</emphasis> sets Globus RSL
        parameters.</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">condor</emphasis> sets Condor
        configuration parameters for the submit file.</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">dagman</emphasis> introduces Condor DAGMan
        configuration parameters.</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">pegasus</emphasis> configures the
        behaviour of various planner/mapper components.</para>
      </listitem>
    </itemizedlist>

       <section>
        <title>The env Profile Namespace</title>

      <para>The <emphasis>env</emphasis> namespace allows users to specify
      environment variables of remote jobs. Globus transports the environment
      variables, and ensure that they are set before the job starts.</para>

      <para>The key used in conjunction with an <emphasis>env</emphasis>
      profile denotes the name of the environment variable. The value of the
      profile becomes the value of the remote environment variable.</para>

      <para>Grid jobs usually only set a minimum of environment variables by
      virtue of Globus. You cannot compare the environment variables visible
      from an interactive login with those visible to a grid job. Thus, it
      often becomes necessary to set environment variables like
      LD_LIBRARY_PATH for remote jobs.</para>

      <para>If you use any of the Pegasus worker package tools like transfer
      or the rc-client, it becomes necessary to set PEGASUS_HOME and
      GLOBUS_LOCATION even for jobs that run locally</para>

      <table>
        <title>Table 1: Useful Environment Settings</title>

        <tgroup cols="2">
          <tbody>
            <row>
              <entry><emphasis role="bold">Environment
              Variable</emphasis></entry>

              <entry><emphasis role="bold">Description</emphasis></entry>
            </row>

            <row>
              <entry>PEGASUS_HOME</entry>

              <entry>Used by auxillary jobs created by Pegasus both on remote
              site and local site. Should be set usually set in the Site
              Catalog for the sites</entry>
            </row>

            <row>
              <entry>GLOBUS_LOCATION</entry>

              <entry>Used by auxillary jobs created by Pegasus both on remote
              site and local site. Should be set usually set in the Site
              Catalog for the sites</entry>
            </row>

            <row>
              <entry>LD_LIBRARY_PATH</entry>

              <entry>Point this to $GLOBUS_LOCATION/lib, except you cannot use
              the dollar variable. You must use the full path. Applies to
              both, local and remote jobs that use Globus components and
              should be usually set in the site catalog for the sites</entry>
            </row>
          </tbody>
        </tgroup>
      </table>

      <para>Even though Condor and Globus both permit environment variable
      settings through their profiles, all remote environment variables must
      be set through the means of <emphasis>env</emphasis> profiles.</para>
    </section>

      <section>
      <title>The globus Profile Namespace</title>

      <para>The <emphasis>globus</emphasis> profile namespace encapsulates
      Globus resource specification language (RSL) instructions. The RSL
      configures settings and behavior of the remote scheduling system. Some
      systems require queue name to schedule jobs, a project name for
      accounting purposes, or a run-time estimate to schedule jobs. The Globus
      RSL addresses all these issues.</para>

      <para>A key in the <emphasis>globus</emphasis> namespace denotes the
      command name of an RLS instruction. The profile value becomes the RSL
      value. Even though Globus RSL is typically shown using parentheses
      around the instruction, the out pair of parentheses is not necessary in
      globus profile specifications</para>

      <para>Table 2 shows some commonly used RSL instructions. For an
      authoritative list of all possible RSL instructions refer to the Globus
      RSL specification.</para>

      <table>
        <title>Table 2: Useful Globus RSL Instructions</title>

        <tgroup cols="2">
          <tbody>
            <row>
              <entry><emphasis role="bold">Key</emphasis></entry>

              <entry><emphasis role="bold">Description</emphasis></entry>
            </row>

            <row>
              <entry>count</entry>

              <entry>the number of times an executable is started.</entry>
            </row>

            <row>
              <entry>jobtype</entry>

              <entry>specifies how the job manager should start the remote
              job. While Pegasus defaults to single, use mpi when running MPI
              jobs.</entry>
            </row>

            <row>
              <entry>maxcputime</entry>

              <entry>the max cpu time for a single execution of a job.</entry>
            </row>

            <row>
              <entry>maxmemory</entry>

              <entry>the maximum memory in MB required for the job</entry>
            </row>

            <row>
              <entry>maxtime</entry>

              <entry>the maximum time or walltime for a single execution of a
              job.</entry>
            </row>

            <row>
              <entry>maxwalltime</entry>

              <entry>the maximum walltime for a single execution of a
              job.</entry>
            </row>

            <row>
              <entry>minmemory</entry>

              <entry>the minumum amount of memory required for this
              job</entry>
            </row>

            <row>
              <entry>project</entry>

              <entry>associates an account with a job at the remote
              end.</entry>
            </row>

            <row>
              <entry>queue</entry>

              <entry>the remote queue in which the job should be run. Used
              when remote scheduler is PBS that supports queues.</entry>
            </row>
          </tbody>
        </tgroup>
      </table>

      <para>Pegasus prevents the user from specifying certain RSL instructions
      as globus profiles, because they are either automatically generated or
      can be overridden through some different means. For instance, if you
      need to specify remote environment settings, do not use the environment
      key in the globus profiles. Use one or more env profiles instead.</para>

      <table>
        <title>Table 3: RSL Instructions that are not permissible</title>

        <tgroup cols="2">
          <tbody>
            <row>
              <entry><emphasis role="bold">Key</emphasis></entry>

              <entry><emphasis role="bold">Reason for
              Prohibition</emphasis></entry>
            </row>

            <row>
              <entry>arguments</entry>

              <entry>you specify arguments in the arguments section for a job
              in the DAX</entry>
            </row>

            <row>
              <entry>directory</entry>

              <entry>the site catalog and properties determine which directory
              a job will run in.</entry>
            </row>

            <row>
              <entry>environment</entry>

              <entry>use multiple env profiles instead</entry>
            </row>

            <row>
              <entry>executable</entry>

              <entry>the physical executable to be used is specified in the
              transformation catalog and is also dependant on the gridstart
              module being used. If you are launching jobs via kickstart then
              the executable created is the path to kickstart and the
              application executable path appears in the arguments for
              kickstart</entry>
            </row>

            <row>
              <entry>stdin</entry>

              <entry>you specify in the DAX for the job</entry>
            </row>

            <row>
              <entry>stdout</entry>

              <entry>you specify in the DAX for the job</entry>
            </row>

            <row>
              <entry>stderr</entry>

              <entry>you specify in the DAX for the job</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>

      <section>
      <title>The condor Profile Namespace</title>

      <para>The Condor submit file controls every detail how and where a job
      is run. The <emphasis>condor</emphasis> profiles permit to add or
      overwrite instructions in the Condor submit file.</para>

      <para>The <emphasis>condor</emphasis> namespace directly sets commands
      in the Condor submit file for a job the profile applies to. Keys in the
      <emphasis>condor</emphasis> profile namespace denote the name of the
      Condor command. The profile value becomes the command's argument. All
      <emphasis>condor</emphasis> profiles are translated into key=value lines
      in the Condor submit file</para>

      <para>Some of the common condor commands that a user may need to specify
      are listed below. For an authoritative list refer to the online condor
      documentation. Note: Pegasus Workflow Planner/Mapper by default specify
      a lot of condor commands in the submit files depending upon the job, and
      where it is being run.</para>

      <table>
        <title>Table 4: Useful Condor Commands</title>

        <tgroup cols="2">
          <tbody>
            <row>
              <entry><emphasis role="bold">Key</emphasis></entry>

              <entry><emphasis role="bold">Description</emphasis></entry>
            </row>

            <row>
              <entry>universe</entry>

              <entry>Pegasus defaults to either globus or scheduler universes.
              Set to standard for compute jobs that require standard universe.
              Set to vanilla to run natively in a condor pool, or to run on
              resources grabbed via condor glidein.</entry>
            </row>

            <row>
              <entry>periodic_release</entry>

              <entry>is the number of times job is released back to the queue
              if it goes to HOLD, e.g. due to Globus errors. Pegasus defaults
              to 3.</entry>
            </row>

            <row>
              <entry>periodic_remove</entry>

              <entry>is the number of times a job is allowed to get into HOLD
              state before being removed from the queue. Pegasus defaults to
              3.</entry>
            </row>

            <row>
              <entry>filesystemdomain</entry>

              <entry>Useful for Condor glide-ins to pin a job to a remote
              site.</entry>
            </row>

            <row>
              <entry>stream_error</entry>

              <entry>boolean to turn on the streaming of the stderr of the
              remote job back to submit host.</entry>
            </row>

            <row>
              <entry>stream_output</entry>

              <entry>boolean to turn on the streaming of the stdout of the
              remote job back to submit host.</entry>
            </row>

            <row>
              <entry>priority</entry>

              <entry>integer value to assign the priority of a job. Higher
              value means higher priority. The priorities are only applied for
              vanilla / standard/ local universe jobs. Determines the order in
              which a users own jobs are executed.</entry>
            </row>
          </tbody>
        </tgroup>
      </table>

      <para>Other useful condor keys, that advanced users may find useful and
      can be set by profiles are</para>

      <orderedlist>
        <listitem>
          <para>should_transfer_files</para>
        </listitem>

        <listitem>
          <para>transfer_output</para>
        </listitem>

        <listitem>
          <para>transfer_error</para>
        </listitem>

        <listitem>
          <para>whentotransferoutput</para>
        </listitem>

        <listitem>
          <para>requirements</para>
        </listitem>

        <listitem>
          <para>rank</para>
        </listitem>
      </orderedlist>

      <para>Pegasus prevents the user from specifying certain Condor commands
      in condor profiles, because they are automatically generated or can be
      overridden through some different means. Table 5 shows prohibited Condor
      commands.</para>

      <table>
        <title>Table 5: Condor commands prohibited in condor profiles</title>

        <tgroup cols="2">
          <tbody>
            <row>
              <entry><emphasis role="bold">Key</emphasis></entry>

              <entry><emphasis role="bold">Reason for
              Prohibition</emphasis></entry>
            </row>

            <row>
              <entry>arguments</entry>

              <entry>you specify arguments in the arguments section for a job
              in the DAX</entry>
            </row>

            <row>
              <entry>environment</entry>

              <entry>use multiple env profiles instead</entry>
            </row>

            <row>
              <entry>executable</entry>

              <entry>the physical executable to be used is specified in the
              transformation catalog and is also dependant on the gridstart
              module being used. If you are launching jobs via kickstart then
              the executable created is the path to kickstart and the
              application executable path appears in the arguments for
              kickstart</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>

      <section>
      <title>The dagman Profile Namespace</title>

      <para>DAGMan is Condor's workflow manager. While planners generate most
      of DAGMan's configuration, it is possible to tweak certain job-related
      characteristics using dagman profiles. A dagman profile can be used to
      specify a DAGMan pre- or post-script.</para>

      <para>Pre- and post-scripts execute on the submit machine. Both inherit
      the environment settings from the submit host when pegasus-submit-dag or
      pegasus-run is invoked.</para>

      <para>By default, kickstart launches all jobs except standard universe
      and MPI jobs. Kickstart tracks the execution of the job, and returns
      usage statistics for the job. A DAGMan post-script starts the Pegasus
      application exitcode to determine, if the job succeeded. DAGMan receives
      the success indication as exit status from exitcode.</para>

      <para>If you need to run your own post-script, you have to take over the
      job success parsing. The planner is set up to pass the file name of the
      remote job's stdout, usually the output from kickstart, as sole argument
      to the post-script.</para>

      <para>Table 6 shows the keys in the dagman profile domain that are
      understood by Pegasus and can be associated at a per job basis.</para>

      <para><table>
          <title>Table 6: Useful dagman Commands that can be associated at a
          per job basis</title>

          <tgroup cols="2">
            <tbody>
              <row>
                <entry><emphasis role="bold">Key</emphasis></entry>

                <entry><emphasis role="bold">Description</emphasis></entry>
              </row>

              <row>
                <entry>PRE</entry>

                <entry>is the path to the pre-script. DAGMan executes the
                pre-script before it runs the job.</entry>
              </row>

              <row>
                <entry>PRE.ARGUMENTS</entry>

                <entry>are command-line arguments for the pre-script, if
                any.</entry>
              </row>

              <row>
                <entry>POST</entry>

                <entry>is the postscript type/mode that a user wants to
                associate with a job. <orderedlist>
                    <listitem>
                      <para><emphasis role="bold">pegasus-exitcode</emphasis>
                      - pegasus will by default associate this postscript with
                      all jobs launched via kickstart, as long the POST.SCOPE
                      value is not set to NONE.</para>
                    </listitem>

                    <listitem>
                      <para><emphasis role="bold">none</emphasis> -means that
                      no postscript is generated for the jobs. This is useful
                      for MPI jobs that are not launched via kickstart
                      currently.</para>
                    </listitem>

                    <listitem>
                      <para><emphasis role="bold">any legal
                      identifier</emphasis> - Any other identifier of the form
                      ([_A-Za-z][_A-Za-z0-9]*), than one of the 2 reserved
                      keywords above, signifies a user postscript. This allows
                      the user to specify their own postscript for the jobs in
                      the workflow. The path to the postscript can be
                      specified by the dagman profile <emphasis
                      role="bold">POST.PATH.[value</emphasis>] where [value]
                      is this legal identifier specified. The user postscript
                      is passed the name of the .out file of the job as the
                      last argument on the command line.</para>

                      <para>For e.g. if the following dagman profiles were
                      associated with a job X</para>

                      <orderedlist>
                        <listitem>
                          <para>POST with value user_script
                          /bin/user_postscript</para>
                        </listitem>

                        <listitem>
                          <para>POST.PATH.user_script with value
                          /path/to/user/script</para>
                        </listitem>

                        <listitem>
                          <para>POST.ARGUMENTS with value -verbose</para>
                        </listitem>
                      </orderedlist>

                      <para>then the following postscript will be associated
                      with the job X in the .dag file</para>

                      <para>/path/to/user/script -verbose X.out where X.out
                      contains the stdout of the job X</para>
                    </listitem>
                  </orderedlist></entry>
              </row>

              <row>
                <entry>POST.PATH.* ( where * is replaced by the value of the
                POST Profile )</entry>

                <entry>the path to the post script on the submit host.</entry>
              </row>

              <row>
                <entry>POST.ARGUMENTS</entry>

                <entry>are the command line arguments for the post script, if
                any.</entry>
              </row>

              <row>
                <entry>RETRY</entry>

                <entry>is the number of times DAGMan retries the full job
                cycle from pre-script through post-script, if failure was
                detected.</entry>
              </row>

              <row>
                <entry>CATEGORY</entry>

                <entry>the DAGMan category the job belongs to.</entry>
              </row>

              <row>
                <entry>PRIORITY</entry>

                <entry>the priority to apply to a job. DAGMan uses this to
                select what jobs to release when MAXJOBS is enforced for the
                DAG.</entry>
              </row>
            </tbody>
          </tgroup>
        </table></para>

      <para/>

      <para>Table 7 shows the keys in the dagman profile domain that are
      understood by Pegasus and can be used to apply to the whole workflow.
      These are used to control DAGMan's behavior at the workflow level, and
      are recommended to be specified in the properties file.</para>

      <table>
        <title>Table 7: Useful dagman Commands that can be specified in the
        properties file.</title>

        <tgroup cols="2">
          <tbody>
            <row>
              <entry><emphasis role="bold">Key</emphasis></entry>

              <entry><emphasis role="bold">Description</emphasis></entry>
            </row>

            <row>
              <entry>MAXPRE</entry>

              <entry>sets the maximum number of PRE scripts within the DAG
              that may be running at one time</entry>
            </row>

            <row>
              <entry>MAXPOST</entry>

              <entry>sets the maximum number of PRE scripts within the DAG
              that may be running at one time</entry>
            </row>

            <row>
              <entry>MAXJOBS</entry>

              <entry>sets the maximum number of jobs within the DAG that will
              be submitted to Condor at one time.</entry>
            </row>

            <row>
              <entry>MAXIDLE</entry>

              <entry>sets the maximum number of idle jobs within the DAG that
              will be submitted to Condor at one time.</entry>
            </row>

            <row>
              <entry>[CATEGORY-NAME].MAXJOBS</entry>

              <entry>is the value of maxjobs for a particular category. Users
              can associate different categories to the jobs at a per job
              basis. However, the value of a dagman knob for a category can
              only be specified at a per workflow basis in the
              properties.</entry>
            </row>

            <row>
              <entry>POST.SCOPE</entry>

              <entry>scope for the postscripts. <orderedlist>
                  <listitem>
                    <para>If set to <emphasis role="bold">all</emphasis> ,
                    means each job in the workflow will have a postscript
                    associated with it.</para>
                  </listitem>

                  <listitem>
                    <para>If set to <emphasis role="bold">none</emphasis> ,
                    means no job has postscript associated with it. None mode
                    should be used if you are running vanilla / standard/
                    local universe jobs, as in those cases Condor traps the
                    remote exitcode correctly. None scope is not recommended
                    for grid universe jobs.</para>
                  </listitem>

                  <listitem>
                    <para>If set to <emphasis
                    role="bold">essential</emphasis>, means only essential
                    jobs have post scripts associated with them. At present
                    the only non essential job is the replica registration
                    job.</para>
                  </listitem>
                </orderedlist></entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>

      <section>
      <title>The pegasus Profile Namespace</title>

      <para>The <emphasis>pegasus</emphasis> profiles allow users to configure
      extra options to the Pegasus Workflow Planner that can be applied
      selectively to a job or a group of jobs. Site selectors may use a
      sub-set of <emphasis>pegasus</emphasis> profiles for their
      decision-making.</para>

      <para>Table 8 shows some of the useful configuration option Pegasus
      understands.</para>

      <table>
        <title>Table 8: Useful pegasus Profiles.</title>

        <tgroup cols="2">
          <tbody>
            <row>
              <entry><emphasis role="bold">Key</emphasis></entry>

              <entry><emphasis role="bold">Description</emphasis></entry>
            </row>

            <row>
              <entry>workdir</entry>

              <entry>Sets the remote initial dir for a Condor-G job. Overrides
              the work directory algorithm that uses the site catalog and
              properties.</entry>
            </row>

            <row>
              <entry>clusters.num</entry>

              <entry>Please refer to the Pegasus Clustering Guide for detailed
              description. This option determines the total number of clusters
              per level. Jobs are evenly spread across clusters.</entry>
            </row>

            <row>
              <entry>clusters.size</entry>

              <entry>Please refer to the Pegasus Clustering Guide for detailed
              description. This profile determines the number of jobs in each
              cluster. The number of clusters depends on the total number of
              jobs on the level.</entry>
            </row>

            <row>
              <entry>collapser</entry>

              <entry>Indicates the clustering executable that is used to run
              the clustered job on the remote site.</entry>
            </row>

            <row>
              <entry>gridstart</entry>

              <entry>Determines the executable for launching a job. Possible
              values are <emphasis role="bold"><emphasis>Kickstart |
              NoGridStart</emphasis></emphasis> at the moment.</entry>
            </row>

            <row>
              <entry>gridstart.path</entry>

              <entry>Sets the path to the gridstart . This profile is best set
              in the Site Catalog.</entry>
            </row>

            <row>
              <entry>gridstart.arguments</entry>

              <entry>Sets the arguments with which GridStart is used to launch
              a job on the remote site.</entry>
            </row>

            <row>
              <entry>stagein.clusters</entry>

              <entry>This key determines the maximum number of
              <emphasis>stage-in</emphasis> jobs that are can executed locally
              or remotely per compute site per workflow. This is used to
              configure the <emphasis>Bundle</emphasis> Transfer Refiner,
              which is the Default Refiner used in Pegasus. This profile is
              best set in the Site Catalog or in the Properties file</entry>
            </row>

            <row>
              <entry>stagein.local.clusters</entry>

              <entry>This key provides finer grained control in determining
              the number of stage-in jobs that are executed locally and are
              responsible for staging data to a particular remote site. This
              profile is best set in the Site Catalog or in the Properties
              file</entry>
            </row>

            <row>
              <entry>stagein.remote.clusters</entry>

              <entry>This key provides finer grained control in determining
              the number of stage-in jobs that are executed remotely on the
              remote site and are responsible for staging data to it. This
              profile is best set in the Site Catalog or in the Properties
              file</entry>
            </row>

            <row>
              <entry>stageout.clusters</entry>

              <entry>This key determines the maximum number of
              <emphasis>stage-out</emphasis> jobs that are can executed
              locally or remotely per compute site per workflow. This is used
              to configure the <emphasis>Bundle</emphasis> Transfer Refiner, ,
              which is the Default Refiner used in Pegasus.</entry>
            </row>

            <row>
              <entry>stageout.local.clusters</entry>

              <entry>This key provides finer grained control in determining
              the number of stage-out jobs that are executed locally and are
              responsible for staging data from a particular remote site. This
              profile is best set in the Site Catalog or in the Properties
              file</entry>
            </row>

            <row>
              <entry>stageout.remote.clusters</entry>

              <entry>This key provides finer grained control in determining
              the number of stage-out jobs that are executed remotely on the
              remote site and are responsible for staging data from it. This
              profile is best set in the Site Catalog or in the Properties
              file</entry>
            </row>

            <row>
              <entry>group</entry>

              <entry>Tags a job with an arbitrary group identifier. The group
              site selector makes use of the tag.</entry>
            </row>

            <row>
              <entry>change.dir</entry>

              <entry>If true, tells <emphasis>kickstart</emphasis> to change
              into the remote working directory. Kickstart itself is executed
              in whichever directory the remote scheduling system chose for
              the job.</entry>
            </row>

            <row>
              <entry>create.dir</entry>

              <entry>If true, tells <emphasis>kickstart</emphasis> to create
              the the remote working directory before changing into the remote
              working directory. Kickstart itself is executed in whichever
              directory the remote scheduling system chose for the
              job.</entry>
            </row>

            <row>
              <entry>transfer.proxy</entry>

              <entry>If true, tells Pegasus to explicitly transfer the proxy
              for transfer jobs to the remote site. This is useful, when you
              want to use a full proxy at the remote end, instead of the
              limited proxy that is transferred by CondorG.</entry>
            </row>

            <row>
              <entry>transfer.arguments</entry>

              <entry>Allows the user to specify the arguments with which the
              transfer executable is invoked. However certain options are
              always generated for the transfer executable(base-uri
              se-mount-point).</entry>
            </row>

            <row>
              <entry>style</entry>

              <entry>Sets the condor submit file style. If set to globus,
              submit file generated refers to CondorG job submissions. If set
              to condor, submit file generated refers to direct Condor
              submission to the local Condor pool. It applies for glidein,
              where nodes from remote grid sites are glided into the local
              condor pool. The default style that is applied is
              globus.</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>
  </section>

    <section>
    <title>Sources for Profiles</title>

    <para>Profiles may enter the job-processing stream at various stages.
    Depending on the requirements and scope a profile is to apply, profiles
    can be associated at</para>

    <itemizedlist>
      <listitem>
        <para>as user property settings.</para>
      </listitem>

      <listitem>
        <para>dax level</para>
      </listitem>

      <listitem>
        <para>in the site catalog</para>
      </listitem>

      <listitem>
        <para>in the transformation catalog</para>
      </listitem>
    </itemizedlist>

    <para>Unfortunately, a different syntax applies to each level and context.
    This section shows the different profile sources and syntaxes. However, at
    the foundation of each profile lies the triple of namespace, key and
    value.</para>

    <section>
      <title>User Profiles in Properties</title>

      <para>Users can specify all profiles in the properties files where the
      property name is <emphasis role="bold">[namespace].key</emphasis> and
      <emphasis role="bold">value</emphasis> of the property is the value of
      the profile.</para>

      <para>Namespace can be env|condor|globus|dagman|pegasus</para>

      <para>Any profile specified as a property applies to the whole workflow
      unless overridden at the DAX level , Site Catalog , Transformation
      Catalog Level.</para>

      <para>Some profiles that they can be set in the properties file are
      listed below</para>

      <programlisting>env.JAVA_HOME "/software/bin/java"

condor.periodic_release 5
condor.periodic_remove  my_own_expression
condor.stream_error true
condor.stream_output fa

globus.maxwalltime  1000
globus.maxtime      900
globus.maxcputime   10
globus.project      test_project
globus.queue        main_queue

dagman.post.arguments --test arguments
dagman.retry  4
dagman.post simple_exitcode
dagman.post.path.simple_exitcode  /bin/exitcode/exitcode.sh
dagman.post.scope all
dagman.maxpre  12
dagman.priority 13

dagman.bigjobs.maxjobs 1


pegasus.clusters.size 5

pegasus.stagein.clusters 3</programlisting>
    </section>

    <section>
      <title>Profiles in DAX</title>

      <para>The user can associate profiles with logical transformations in
      DAX. Environment settings required by a job's application, or a maximum
      estimate on the run-time are examples for profiles at this stage.</para>

      <programlisting>&lt;job id="ID000001" namespace="asdf" name="preprocess" version="1.0"
 level="3" dv-namespace="voeckler" dv-name="top" dv-version="1.0"&gt;
  &lt;argument&gt;-a top -T10  -i &lt;filename file="voeckler.f.a"/&gt;
 -o &lt;filename file="voeckler.f.b1"/&gt;
 &lt;filename file="voeckler.f.b2"/&gt;&lt;/argument&gt;
  <emphasis role="bold">&lt;profile namespace="pegasus" key="walltime"&gt;2&lt;/profile&gt;
  &lt;profile namespace="pegasus" key="diskspace"&gt;1&lt;/profile&gt;</emphasis>
  …
&lt;/job&gt;
</programlisting>
    </section>

      <section>
      <title>Profiles in Site Catalog</title>

      <para>If it becomes necessary to limit the scope of a profile to a
      single site, these profiles should go into the site catalog. A profile
      in the site catalog applies to all jobs and all application run at the
      site. Commonly, site catalog profiles set environment settings like the
      LD_LIBRARY_PATH, or globus rsl parameters like queue and project
      names.</para>

      <para>Currently, there is no tool to manipulate the site catalog, e.g.
      by adding profiles. Modifying the site catalog requires that you load it
      into your editor.</para>

      <para>The XML version of the site catalog uses the following
      syntax:</para>

      <programlisting><emphasis role="bold">&lt;profile namespace=</emphasis>"<emphasis>namespace</emphasis>" <emphasis
          role="bold">key=</emphasis>"<emphasis>key</emphasis>"&gt;<emphasis>value</emphasis><emphasis
          role="bold">&lt;/profile&gt;</emphasis></programlisting>

      <para>The XML schema requires that profiles are the first children of a
      pool element. If the element ordering is wrong, the XML parser will
      produce errors and warnings:</para>

      <programlisting>&lt;pool handle="isi_condor" gridlaunch="/home/shared/pegasus/bin/kickstart"&gt;
  <emphasis role="bold">&lt;profile namespace="env"
   key="GLOBUS_LOCATION"&gt;/home/shared/globus/&lt;/profile&gt;
  &lt;profile namespace="env"
   key="LD_LIBRARY_PATH" &gt;/home/shared/globus/lib&lt;/profile&gt;</emphasis>
  &lt;lrc url="rls://sukhna.isi.edu" /&gt;
  …
&lt;/pool&gt;
</programlisting>

      <para>The multi-line textual version of the site catalog uses the
      following syntax:</para>

      <programlisting><emphasis role="bold">profile</emphasis> <emphasis>namespace "key" "value"</emphasis></programlisting>

      <para>The order within the textual pool definition is not important.
      Profiles can appear anywhere:</para>

      <programlisting>pool isi_condor {
  gridlaunch "/home/shared/pegasus/bin/kickstart"
  <emphasis role="bold">profile env "GLOBUS_LOCATION" "/home/shared/globus"
  profile env "LD_LIBRARY_PATH" "/home/shared/globus/lib"</emphasis>
  …
}
</programlisting>
      </section>

      <section>
      <title>Profiles in Transformation Catalog</title>

      <para>Some profiles require a narrower scope than the site catalog
      offers. Some profiles only apply to certain applications on certain
      sites, or change with each application and site. Transformation-specific
      and CPU-specific environment variables, or job clustering profiles are
      good candidates. Such profiles are best specified in the transformation
      catalog.</para>

      <para>Profiles associate with a physical transformation and site in the
      transformation catalog. The Database version of the transformation
      catalog also permits the convenience of connecting a transformation with
      a profile.</para>

      <para>The Pegasus tc-client tool is a convenient helper to associate
      profiles with transformation catalog entries. As benefit, the user does
      not have to worry about formats of profiles in the various
      transformation catalog instances.</para>

      <programlisting>tc-client -a -P -E -p /home/shared/executables/analyze -t INSTALLED -r isi_condor -e env::GLOBUS_LOCATION=”/home/shared/globus”</programlisting>

      <para>The above example adds an environment variable GLOBUS_LOCATION to
      the application /home/shared/executables/analyze on site isi_condor. The
      transformation catalog guide has more details on the usage of the
      tc-client.</para>
    </section>
    </section>

     <section>
     <title>Profiles Conflict Resolution</title>

    <para>Irrespective of where the profiles are specified, eventually the
    profiles are associated with jobs. Multiple sources may specify the same
    profile for the same job. For instance, DAX may specify an environment
    variable X. The site catalog may also specify an environment variable X
    for the chosen site. The transformation catalog may specify an environment
    variable X for the chosen site and application. When the job is
    concretized, these three conflicts need to be resolved.</para>

    <para>Pegasus defines a priority ordering of profiles. The higher priority
    takes precedence (overwrites) a profile of a lower priority.</para>

    <orderedlist>
      <listitem>
        <para>Transformation Catalog Profiles</para>
      </listitem>

      <listitem>
        <para>Site Catalog Profiles</para>
      </listitem>

      <listitem>
        <para>DAX Profiles</para>
      </listitem>

      <listitem>
        <para>Profiles in Properties</para>
      </listitem>
    </orderedlist>
    </section>

     <section>
     <title>Details of Profile Handling</title>

    <para>The previous sections omitted some of the finer details for the sake
    of clarity. To understand some of the constraints that Pegasus imposes, it
    is required to look at the way profiles affect jobs.</para>

    <section>
      <title>Details of env Profiles</title>

      <para>Profiles in the env namespace are translated to a
      semicolon-separated list of key-value pairs. The list becomes the
      argument for the Condor environment command in the job's submit
      file.</para>

      <programlisting>######################################################################
# Pegasus WMS  SUBMIT FILE GENERATOR
# DAG : black-diamond, Index = 0, Count = 1
# SUBMIT FILE NAME : findrange_ID000002.sub
######################################################################
globusrsl = (jobtype=single)
<emphasis role="bold">environment=GLOBUS_LOCATION=/shared/globus;LD_LIBRARY_PATH=/shared/globus/lib;</emphasis>
executable = /shared/software/linux/pegasus/default/bin/kickstart
globusscheduler = columbus.isi.edu/jobmanager-condor
remote_initialdir = /shared/CONDOR/workdir/isi_hourglass
universe = globus
…
queue
######################################################################
# END OF SUBMIT FILE
</programlisting>

      <para>Condor-G, in turn, will translate the
      <emphasis>environment</emphasis> command for any remote job into Globus
      RSL environment settings, and append them to any existing RSL syntax it
      generates. To permit proper mixing, all <emphasis>environment</emphasis>
      setting should solely use the env profiles, and none of the Condor nor
      Globus environment settings.</para>

      <para>If <emphasis>kickstart</emphasis> starts a job, it may make use of
      environment variables in its executable and arguments setting.</para>
    </section>

      <section>
      <title>Details of globus Profiles</title>

      <para>Profiles in the <emphasis>globus</emphasis> Namespaces are
      translated into a list of paranthesis-enclosed equal-separated key-value
      pairs. The list becomes the value for the Condor
      <emphasis>globusrsl</emphasis> setting in the job's submit file:</para>

      <programlisting>######################################################################
# Pegasus WMS SUBMIT FILE GENERATOR
# DAG : black-diamond, Index = 0, Count = 1
# SUBMIT FILE NAME : findrange_ID000002.sub
######################################################################
<emphasis role="bold">globusrsl = (jobtype=single)(queue=fast)(project=nvo)</emphasis>
executable = /shared/software/linux/pegasus/default/bin/kickstart
globusscheduler = columbus.isi.edu/jobmanager-condor
remote_initialdir = /shared/CONDOR/workdir/isi_hourglass
universe = globus
…
queue
######################################################################
# END OF SUBMIT FILE
</programlisting>

      <para>For this reason, Pegasus prohibits the use of the
      <emphasis>globusrsl</emphasis> key in the <emphasis>condor</emphasis>
      profile namespace.</para>
    </section>
    </section>
   </section>


  <section>
    <title>Replica Selection</title>

    <para>Each job in the DAX maybe associated with input LFN’s denoting the
    files that are required for the job to run. To determine the physical
    replica (PFN) for a LFN, Pegasus queries the Replica catalog to get all
    the PFN’s (replicas) associated with a LFN. The Replica Catalog may return
    multiple PFN's for each of the LFN's queried. Hence, Pegasus needs to
    select a single PFN amongst the various PFN's returned for each LFN. This
    process is known as replica selection in Pegasus. Users can specify the
    replica selector to use in the properties file.</para>

    <para>This document describes the various Replica Selection Strategies in
    Pegasus.</para>

    <section>
    <title>Configuration</title>

    <para>The user properties determine what replica selector Pegasus Workflow
    Mapper uses. The property <emphasis
    role="bold">pegasus.selector.replica</emphasis> is used to specify the
    replica selection strategy. Currently supported Replica Selection
    strategies are</para>

    <orderedlist>
      <listitem>
        <para>Default</para>
      </listitem>

      <listitem>
        <para>Restricted</para>
      </listitem>

      <listitem>
        <para>Regex</para>
      </listitem>
    </orderedlist>

    <para>The values are case sensitive. For example the following property
    setting will throw a Factory Exception .</para>

    <programlisting>pegasus.selector.replica  default</programlisting>

    <para>The correct way to specify is</para>

    <programlisting>pegasus.selector.replica  Default</programlisting>
  </section>

    <section>
    <title>Supported Replica Selectors</title>

    <para>The various Replica Selectors supported in Pegasus Workflow Mapper
    are explained below</para>

    <section>
      <title>Default</title>

      <para>This is the default replica selector used in the Pegasus Workflow
      Mapper. If the property pegasus.selector.replica is not defined in
      properties, then Pegasus uses this selector.</para>

      <para>This selector looks at each PFN returned for a LFN and checks to
      see if</para>

      <orderedlist>
        <listitem>
          <para>the PFN is a file URL (starting with file:///)</para>
        </listitem>

        <listitem>
          <para>the PFN has a pool attribute matching to the site handle of
          the site where the compute job that requires the input file is to be
          run.</para>
        </listitem>
      </orderedlist>

      <para>If a PFN matching the conditions above exists then that is
      returned by the selector .</para>

      <para><emphasis role="bold">Else,</emphasis> a random PFN is selected
      amongst all the PFN’s that have a pool attribute matching to the site
      handle of the site where a compute job is to be run.</para>

      <para><emphasis role="bold">Else,</emphasis> a random pfn is selected
      amongst all the PFN’s</para>

      <para>To use this replica selector set the following
      property<programlisting>pegasus.selector.replica                  Default</programlisting></para>
    </section>

      <section>
      <title>Restricted</title>

      <para>This replica selector, allows the user to specify good sites and
      bad sites for staging in data to a particular compute site. A good site
      for a compute site X, is a preferred site from which replicas should be
      staged to site X. If there are more than one good sites having a
      particular replica, then a random site is selected amongst these
      preferred sites.</para>

      <para>A bad site for a compute site X, is a site from which replica’s
      should not be staged. The reason of not accessing replica from a bad
      site can vary from the link being down, to the user not having
      permissions on that site’s data.</para>

      <para>The good | bad sites are specified by the following
      properties</para>

      <programlisting>pegasus.replica.*.prefer.stagein.sites
pegasus.replica.*.ignore.stagein.sites</programlisting>

      <para>where the * in the property name denotes the name of the compute
      site. A * in the property key is taken to mean all sites. The value to
      these properties is a comma separated list of sites.</para>

      <para>For example the following settings</para>

      <programlisting>pegasus.selector.replica.*.prefer.stagein.sites            usc
pegasus.replica.uwm.prefer.stagein.sites                   isi,cit
</programlisting>

      <para>means that prefer all replicas from site usc for staging in to any
      compute site. However, for uwm use a tighter constraint and prefer only
      replicas from site isi or cit. The pool attribute associated with the
      PFN's tells the replica selector to what site a replica/PFN is
      associated with.</para>

      <para>The pegasus.replica.*.prefer.stagein.sites property takes
      precedence over pegasus.replica.*.ignore.stagein.sites property i.e. if
      for a site X, a site Y is specified both in the ignored and the
      preferred set, then site Y is taken to mean as only a preferred site for
      a site X.</para>

      <para>To use this replica selector set the following property</para>

      <programlisting>pegasus.selector.replica                  Restricted</programlisting>
    </section>

      <section>
      <title>Regex</title>

      <para>This replica selector allows the user allows the user to specific
      regex expressions that can be used to rank various PFN’s returned from
      the Replica Catalog for a particular LFN. This replica selector selects
      the highest ranked PFN i.e the replica with the lowest rank
      value.</para>

      <para>The regular expressions are assigned different rank, that
      determine the order in which the expressions are employed. The rank
      values for the regex can expressed in user properties using the
      property.</para>

      <programlisting>pegasus.selector.replica.regex.rank.<emphasis
          role="bold">[value]</emphasis>                  regex-expression</programlisting>

      <para>The <emphasis role="bold">[value]</emphasis> in the above property
      is an integer value that denotes the rank of an expression with a rank
      value of 1 being the highest rank.</para>

      <para>For example, a user can specify the following regex expressions
      that will ask Pegasus to prefer file URL's over gsiftp url's from
      example.isi.edu</para>

      <programlisting>pegasus.selector.replica.regex.rank.1                       file://.*
pegasus.selector.replica.regex.rank.2                       gsiftp://example\.isi\.edu.*</programlisting>

      <para>User can specify as many regex expressions as they want.</para>

      <para>Since Pegasus is in Java , the regex expression support is what
      Java supports. It is pretty close to what is supported by Perl. More
      details can be found at
      http://java.sun.com/j2se/1.5.0/docs/api/java/util/regex/Pattern.html</para>

      <para>Before applying any regular expressions on the PFN’s for a
      particular LFN that has to be staged to a site X, the file URL’s that
      don't match the site X are explicitly filtered out.</para>

      <para>To use this replica selector set the following
      property<programlisting>pegasus.selector.replica                  Regex</programlisting></para>
    </section>

      <section>
      <title>Local</title>

      <para>This replica selector always prefers replicas from the local host
      ( pool attribute set to local ) and that start with a file: URL scheme.
      It is useful, when users want to stagein files to a remote site from the
      submit host using the Condor file transfer mechanism.</para>

      <para>To use this replica selector set the following
      property<programlisting>pegasus.selector.replica                  Default</programlisting></para>
    </section>
  </section>
  </section>

  <section>
    <title>Job Clustering</title>

    <para>A large number of workflows executed through the Pegasus Workflow
    Management System, are composed of several jobs that run for only a few
    seconds or so. The overhead of running any job on the grid is usually 60
    seconds or more. Hence, it makes sense to cluster small independent jobs
    into a larger job. This is done while mapping an abstract workflow to a
    concrete workflow. Site specific or transformation specific criteria are
    taken into consideration while clustering smaller jobs into a larger job
    in the concrete workflow. The user is allowed to control the granularity
    of this clustering on a per transformation per site basis.</para>


    <section>
    <title>Overview</title>

    <para>The abstract workflow is mapped onto the various sites by the Site
    Selector. This semi executable workflow is then passed to the clustering
    module. The clustering of the workflow can be either be</para>

    <itemizedlist>
      <listitem>
        <para>level based (horizontal clustering )</para>
      </listitem>

      <listitem>
        <para>label based (label clustering)</para>
      </listitem>
    </itemizedlist>

    <para>The clustering module clusters the jobs into larger/clustered jobs,
    that can then be executed on the remote sites. The execution can either be
    sequential on a single node or on multiple nodes using MPI. To specify
    which clustering technique to use the user has to pass the <emphasis
    role="bold">--cluster</emphasis> option to <emphasis
    role="bold">pegasus-plan</emphasis> .</para>


    <section>
    <title>Generating Clustered Concrete DAG</title>

    <para>The clustering of a workflow is activated by passing the <emphasis
    role="bold">--cluster|-C</emphasis> option to <emphasis
    role="bold">pegasus-plan</emphasis>. The clustering granularity of a
    particular logical transformation on a particular site is dependant upon
    the clustering techniques being used. The executable that is used for
    running the clustered job on a particular site is determined as explained
    in section 7.<programlisting>#Running pegasus-plan to generate clustered workflows

$ <emphasis>pegasus-plan –-dax example.dax --dir ./dags –p siteX –-output local
               --cluster [ comma separated list of clustering techniques]  –verbose
</emphasis>
Valid clustering techniques are horizontal and label.</programlisting></para>

    <para>The naming convention of submit files of the clustered jobs
    is<emphasis role="bold"> merge_NAME_IDX.sub</emphasis> . The NAME is
    derived from the logical transformation name. The IDX is an integer number
    between 1 and the total number of jobs in a cluster. Each of the submit
    files has a corresponding input file, following the naming convention
    <emphasis role="bold">merge_NAME_IDX.in </emphasis>. The input file
    contains the respective execution targets and the arguments for each of
    the jobs that make up the clustered job.</para>

      <section>
      <title>Horizontal Clustering</title>

      <para>In case of horizontal clustering, each job in the workflow is
      associated with a level. The levels of the workflow are determined by
      doing a modified Breadth First Traversal of the workflow starting from
      the root nodes. The level associated with a node, is the furthest
      distance of it from the root node instead of it being the shortest
      distance as in normal BFS. For each level the jobs are grouped by the
      site on which they have been scheduled by the Site Selector. Only jobs
      of same type (txnamespace, txname, txversion) can be clustered into a
      larger job. To use horizontal clustering the user needs to set the
      <emphasis role="bold">--cluster</emphasis> option of <emphasis
      role="bold">pegasus-plan to horizontal</emphasis> .</para>

        <section>

        <title>Controlling Clustering Granularity</title>

        <para>The number of jobs that have to be clustered into a single large
        job, is determined by the value of two parameters associated with the
        smaller jobs. Both these parameters are specified by the use of a
        PEGASUS namespace profile keys. The keys can be specified at any of
        the placeholders for the profiles (abstract transformation in the DAX,
        site in the site catalog, transformation in the transformation
        catalog). The normal overloading semantics apply i.e. profile in
        transformation catalog overrides the one in the site catalog and that
        in turn overrides the one in the DAX. The two parameters are described
        below.</para>

        <itemizedlist>
          <listitem>
            <para><emphasis role="bold">clusters.size factor</emphasis></para>

            <para>The clusters.size factor denotes how many jobs need to be
            merged into a single clustered job. It is specified via the use of
            a PEGASUS namespace profile key “clusters.size”. for e.g. if at a
            particular level, say 4 jobs referring to logical transformation B
            have been scheduled to a siteX. The clusters.size factor
            associated with job B for siteX is say 3. This will result in 2
            clustered jobs, one composed of 3 jobs and another of 2 jobs. The
            clusters.size factor can be specified in the transformation
            catalog as follows</para>

            <programlisting><emphasis role="bold">#site   transformation   pfn            type               architecture  profiles
</emphasis>
siteX    B     /shared/PEGASUS/bin/jobB INSTALLED       INTEL32::LINUX  PEGASUS::clusters.size=3
siteX    C     /shared/PEGASUS/bin/jobC INSTALLED       INTEL32::LINUX  PEGASUS::clusters.size=2
</programlisting>

            <figure>
              <title/>

              <mediaobject>
                <imageobject>
                  <imagedata fileref="images/advanced-clustering-1.png"/>
                </imageobject>
              </mediaobject>
            </figure>
          </listitem>

          <listitem>
            <para><emphasis role="bold">clusters.num factor</emphasis></para>

            <para>The clusters.num factor denotes how many clustered jobs does
            the user want to see per level per site. It is specified via the
            use of a PEGASUS namespace profile key “clusters.num”. for e.g. if
            at a particular level, say 4 jobs referring to logical
            transformation B have been scheduled to a siteX. The
            “clusters.num” factor associated with job B for siteX is say 3.
            This will result in 3 clustered jobs, one composed of 2 jobs and
            others of a single job each. The clusters.num factor in the
            transformation catalog can be specified as follows</para>

            <programlisting><emphasis role="bold">#site  transformation      pfn           type            architecture    profiles
</emphasis>
siteX    B     /shared/PEGASUS/bin/jobB INSTALLED       INTEL32::LINUX  PEGASUS::clusters.num=3
siteX    C     /shared/PEGASUS/bin/jobC INSTALLED       INTEL32::LINUX  PEGASUS::clusters.num=2
</programlisting>

            <para>In the case, where both the factors are associated with the
            job, the clusters.num value supersedes the clusters.size
            value.</para>

            <programlisting><emphasis role="bold">#site  transformation   pfn             type             architecture   profiles
</emphasis>
siteX    B     /shared/PEGASUS/bin/jobB INSTALLED       INTEL32::LINUX PEGASUS::clusters.size=3,clusters.num=3
</programlisting>

            <para>In the above case the jobs referring to logical
            transformation B scheduled on siteX will be clustered on the basis
            of “clusters.num” value. Hence, if there are 4 jobs referring to
            logical transformation B scheduled to siteX, then 3 clustered jobs
            will be created.</para>

            <figure>
              <title/>

              <mediaobject>
                <imageobject>
                  <imagedata fileref="images/advanced-clustering-2.png"/>
                </imageobject>
              </mediaobject>
            </figure>
          </listitem>
        </itemizedlist>
      </section>
    </section>

      <section>
      <title>Label Clustering</title>

      <para>In label based clustering, the user labels the workflow. All jobs
      having the same label value are clustered into a single clustered job.
      This allows the user to create clusters or use a clustering technique
      that is specific to his workflows. If there is no label associated with
      the job, the job is not clustered and is executed as is<figure>
          <title/>

          <mediaobject>
            <imageobject>
              <imagedata fileref="images/advanced-clustering-3.png"/>
            </imageobject>
          </mediaobject>
        </figure></para>

      <para>Since, the jobs in a cluster in this case are not independent, it
      is important the jobs are executed in the correct order. This is done by
      doing a topological sort on the jobs in each cluster. To use label based
      clustering the user needs to set the <emphasis
      role="bold">--cluster</emphasis> option of <emphasis
      role="bold">pegasus-plan</emphasis> to label.</para>

        <section>
        <title>Labelling the Workflow</title>

        <para>The labels for the jobs in the workflow are specified by
        associated <emphasis role="bold">pegasus</emphasis> profile keys with
        the jobs during the DAX generation process. The user can choose which
        profile key to use for labeling the workflow. By default, it is
        assumed that the user is using the PEGASUS profile key label to
        associate the labels. To use another key, in the <emphasis
        role="bold">pegasus</emphasis> namespace the user needs to set the
        following property</para>

        <itemizedlist>
          <listitem>
            <para>pegasus.clusterer.label.key</para>
          </listitem>
        </itemizedlist>

        <para>For example if the user sets <emphasis
        role="bold">pegasus.clusterer.label.key </emphasis>to <emphasis
        role="bold">user_label</emphasis> then the job description in the DAX
        looks as follows</para>

        <programlisting>&lt;adag &gt;
…
  &lt;job id="ID000004" namespace="app" name="analyze" version="1.0" level="1" &gt;
    &lt;argument&gt;-a bottom -T60  -i &lt;filename file="user.f.c1"/&gt;  -o &lt;filename file="user.f.d"/&gt;&lt;/argument&gt;
    &lt;profile namespace=“pegasus” key=“user_label”&gt;p1&lt;/profile&gt;
    &lt;uses file="user.f.c1" link="input" dontRegister="false" dontTransfer="false"/&gt;
    &lt;uses file="user.f.c2" link="input" dontRegister="false" dontTransfer="false"/&gt;
    &lt;uses file="user.f.d" link="output" dontRegister="false" dontTransfer="false"/&gt;
  &lt;/job&gt;
…
&lt;/adag&gt;</programlisting>

        <itemizedlist>
          <listitem>
            <para>The above states that the <emphasis
            role="bold">pegasus</emphasis> profiles with key as <emphasis
            role="bold">user_label</emphasis> are to be used for designating
            clusters.</para>
          </listitem>

          <listitem>
            <para>Each job with the same value for <emphasis
            role="bold">pegasus</emphasis> profile key <emphasis
            role="bold">user_label </emphasis>appears in the same
            cluster.</para>
          </listitem>
        </itemizedlist>
      </section>
    </section>

      <section>
      <title>Recursive Clustering</title>

      <para>In some cases, a user may want to use a combination of clustering
      techniques. For e.g. a user may want some jobs in the workflow to be
      horizontally clustered and some to be label clustered. This can be
      achieved by specifying a comma separated list of clustering techniques
      to the<emphasis role="bold"> –-cluster</emphasis> option of <emphasis
      role="bold">pegasus-plan</emphasis>. In this case the clustering
      techniques are applied one after the other on the workflow in the order
      specified on the command line.</para>

      <para>For example</para>

      <programlisting>$ <emphasis>pegasus-plan –-dax example.dax --dir ./dags --cluster label,horizontal –s siteX –-output local --verbose</emphasis></programlisting>

      <figure>
        <title/>

        <mediaobject>
          <imageobject>
            <imagedata fileref="images/advanced-clustering-4.png"/>
          </imageobject>
        </mediaobject>
      </figure>
    </section>
  </section>

    <section>
    <title>Execution of the Clustered Job</title>

    <para>The execution of the clustered job on the remote site, involves the
    execution of the smaller constituent jobs either</para>

    <itemizedlist>
      <listitem>
        <para><emphasis role="bold">sequentially on a single node of the
        remote site</emphasis></para>

        <para>The clustered job is executed using <emphasis
        role="bold">seqexec</emphasis>, a wrapper tool written in C that is
        distributed as part of the PEGASUS. It takes in the jobs passed to it,
        and ends up executing them sequentially on a single node. To use
        “<emphasis role="bold">seqexec</emphasis>” for executing any clustered
        job on a siteX, there needs to be an entry in the transformation
        catalog for an executable with the logical name seqexec and namespace
        as pegasus.</para>

        <programlisting><emphasis role="bold">#site  transformation   pfn            type                 architecture    profiles</emphasis>

siteX    pegasus::seqexec     /shared/PEGASUS/bin/seqexec INSTALLED       INTEL32::LINUX NULL</programlisting>

        <para>By default, the entry for seqexec on a site is automatically
        picked up if $PEGASUS_HOME or $VDS_HOME is specified in the site
        catalog for that site.</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">On multiple nodes of the remote site using
        MPI</emphasis></para>

        <para>The clustered job is executed using mpiexec, a wrapper mpi
        program written in C that is distributed as part of the PEGASUS. It is
        only distributed as source not as binary. The wrapper ends up being
        run on every mpi node, with the first one being the master and the
        rest of the ones as workers. The number of instances of mpiexec that
        are invoked is equal to the value of the globus rsl key nodecount. The
        master distributes the smaller constituent jobs to the workers.</para>

        <para>For e.g. If there were 10 jobs in the merged job and nodecount
        was 5, then one node acts as master, and the 10 jobs are distributed
        amongst the 4 slaves on demand. The master hands off a job to the
        slave node as and when it gets free. So initially all the 4 nodes are
        given a single job each, and then as and when they get done are handed
        more jobs till all the 10 jobs have been executed.</para>

        <para>To use “mpiexec” for executing the clustered job on a siteX,
        there needs to be an entry in the transformation catalog for an
        executable with the logical name mpiexec and namespace as
        pegasus.</para>

        <programlisting><emphasis role="bold">#site  transformation   pfn            type                 architecture    profiles</emphasis>

siteX    pegasus::seqexec     /shared/PEGASUS/bin/mpiexec INSTALLED       INTEL32::LINUX NULL</programlisting>

        <para>Another added advantage of using mpiexec, is that regular non
        mpi code can be run via MPI.</para>

        <para>Both the clustered job and the smaller constituent jobs are
        invoked via kickstart, unless the clustered job is being run via mpi
        (mpiexec). Kickstart is unable to launch mpi jobs. If kickstart is not
        installed on a particular site i.e. the gridlaunch attribute for site
        is not specified in the site catalog, the jobs are invoked
        directly.</para>
      </listitem>
    </itemizedlist>

      <section>
      <title>Specification of Method of Execution for Clustered Jobs</title>

      <para>The method execution of the clustered job(whether to launch via
      mpiexec or seqexec) can be specified</para>

      <orderedlist>
        <listitem>
          <para><emphasis role="bold">globally in the properties
          file</emphasis></para>

          <para>The user can set a property in the properties file that
          results in all the clustered jobs of the workflow being executed by
          the same type of executable.</para>

          <programlisting><emphasis role="bold">#PEGASUS PROPERTIES FILE</emphasis>
pegasus.clusterer.job.aggregator seqexec|mpiexec</programlisting>

          <para>In the above example, all the clustered jobs on the remote
          sites are going to be launched via the property value, as long as
          the property value is not overridden in the site catalog.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">associating profile key “collapser” with
          the site in the site catalog</emphasis></para>

          <programlisting>&lt;site handle="siteX" gridlaunch = "/shared/PEGASUS/bin/kickstart"&gt;
    &lt;profile namespace="env" key="GLOBUS_LOCATION" &gt;/home/shared/globus&lt;/profile&gt;
    &lt;profile namespace="env" key="LD_LIBRARY_PATH"&gt;/home/shared/globus/lib&lt;/profile&gt;
    &lt;profile namespace="pegasus" key="collapser" &gt;seqexec&lt;/profile&gt;
    &lt;lrc url="rls://siteX.edu" /&gt;
    &lt;gridftp  url="gsiftp://siteX.edu/" storage="/home/shared/work" major="2" minor="4" patch="0" /&gt;
    &lt;jobmanager universe="transfer" url="siteX.edu/jobmanager-fork" major="2" minor="4" patch="0" /&gt;
    &lt;jobmanager universe="vanilla" url="siteX.edu/jobmanager-condor" major="2" minor="4" patch="0" /&gt;
    &lt;workdirectory &gt;/home/shared/storage&lt;/workdirectory&gt;
  &lt;/site&gt;</programlisting>

          <para>In the above example, all the clustered jobs on a siteX are
          going to be executed via seqexec, as long as the value is not
          overridden in the transformation catalog.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">associating profile key “collapser” with
          the transformation that is being clustered, in the transformation
          catalog</emphasis></para>

          <programlisting><emphasis role="bold">#site  transformation   pfn            type                architecture profiles
</emphasis>
siteX    B     /shared/PEGASUS/bin/jobB INSTALLED       INTEL32::LINUX pegasus::clusters.size=3,collapser=mpiexec</programlisting>

          <para>In the above example, all the clustered jobs that consist of
          transformation B on siteX will be executed via mpiexec.</para>

          <para><emphasis role="bold">Note: The clustering of jobs on a site
          only happens only if </emphasis></para>

          <itemizedlist>
            <listitem>
              <para>there exists an entry in the transformation catalog for
              the clustering executable that has been determined by the above
              3 rules</para>
            </listitem>

            <listitem>
              <para>the number of jobs being clustered on the site are more
              than 1</para>
            </listitem>
          </itemizedlist>
        </listitem>
      </orderedlist>
    </section>
  </section>

      <section>
       <title>Outstanding Issues</title>

    <orderedlist>
      <listitem>
        <para><emphasis role="bold">Label Clustering</emphasis></para>

        <para>More rigorous checks are required to ensure that the labeling
        scheme applied by the user is valid.</para>
      </listitem>
    </orderedlist>
   </section>
   </section>
   </section>
  <section>
    <title>Transfers</title>

    <para>As part of the Workflow Mapping Process, Pegasus does data
    management for the executable workflow . It queries a Replica Catalog to
    discover the locations of the input datasets and adds data movement and
    registration nodes in the workflow to</para>

    <orderedlist>
      <listitem>
        <para>stage-in input data to the compute sites ( where the jobs in the
        workflow are executed )</para>
      </listitem>

      <listitem>
        <para>stage-out output data generated by the workflow to the final
        storage site.</para>
      </listitem>

      <listitem>
        <para>stage-in intermediate data between compute sites if
        required.</para>
      </listitem>

      <listitem>
        <para>data registration nodes to catalog the locations of the output
        data on the final storage site into the replica catalog.</para>
      </listitem>
    </orderedlist>

    <para>The separate data movement jobs that are added to the executable
    workflow are responsible for staging data to a workflow specific directory
    accessible to the staging server on a staging site associated with the
    compute sites. Currently, the staging site for a compute site is the
    compute site itself. In the default case, the staging server is usually on
    the headnode of the compute site and has access to the shared filesystem
    between the worker nodes and the head node. Pegasus adds a directory
    creation job in the executable workflow that creates the workflow specific
    directory on the staging server.</para>

    <figure>
      <title>Default Transfer Case : Input Data To Workflow Specific Directory
      on Shared File System</title>

      <mediaobject>
        <imageobject>
          <imagedata fileref="images/default-transfer-sharedfs.png"/>
        </imageobject>
      </mediaobject>
    </figure>

    <para>In addition to data, Pegasus does transfer user executables to the
    compute sites if the executables are not installed on the remote sites
    before hand. This chapter gives an overview of how transfers of data and
    executables is managed in Pegasus.</para>


    <section>
    <title>Local versus Remote Transfers</title>

    <para>As far as possible, Pegasus will ensure that the transfer jobs added
    to the executable workflow are executed on the submit host. By default,
    Pegasus will schedule a transfer to be executed on the remote compute site
    only if there is no way to execute it on the submit host. For e.g if the
    file server specified for the compute site is a file server, then Pegasus
    will schedule all the stage in data movement jobs on the compute site to
    stage-in the input data for the workflow. Another case would be if a user
    has symlinking turned on. In that case, the transfer jobs that symlink
    against the input data on the compute site, will be executed remotely ( on
    the compute site ).</para>

    <para>Users can specify the property <emphasis
    role="bold">pegasus.transfer.*.remote.sites</emphasis> to change the
    default behaviour of Pegasus and force pegasus to run different types of
    transfer jobs for the sites specified on the remote site. The value of the
    property is a comma separated list of compute sites for which you want the
    transfer jobs to run remotely.</para>

    <para>The table below illustrates all the possible variations of the
    property.</para>

    <table>
      <title>Property Variations for pegasus.transfer.*.remote.sites</title>

      <tgroup cols="2">
        <thead>
          <row>
            <entry>Property Name</entry>

            <entry>Applies to</entry>
          </row>
        </thead>

        <tbody>
          <row>
            <entry>pegasus.transfer.stagein.remote.sites</entry>

            <entry>the stage in transfer jobs</entry>
          </row>

          <row>
            <entry>pegasus.transfer.stageout.remote.sites</entry>

            <entry>the stage out transfer jobs</entry>
          </row>

          <row>
            <entry>pegasus.transfer.inter.remote.sites</entry>

            <entry>the inter site transfer jobs</entry>
          </row>

          <row>
            <entry>pegasus.transfer.*.remote.sites</entry>

            <entry>all types of transfer jobs</entry>
          </row>
        </tbody>
      </tgroup>
    </table>

    <para>The prefix for the transfer job name indicates whether the transfer
    job is to be executed locallly ( on the submit host ) or remotely ( on the
    compute site ). For example stage_in_local_ in a transfer job name
    stage_in_local_isi_viz_0 indicates that the transfer job is a stage in
    transfer job that is executed locally and is used to transfer input data
    to compute site isi_viz. The prefix naming scheme for the transfer jobs is
    <emphasis
    role="bold">[stage_in|stage_out|inter]_[local|remote]_</emphasis> .</para>
  </section>

    <section>
    <title>Symlinking Against Input Data</title>

    <para>If input data for a job already exists on a compute site, then it is
    possible for Pegasus to symlink against that data. In this case, the
    remote stage in transfer jobs that Pegasus adds to the executable workflow
    will symlink instead of doing a copy of the data.</para>

    <para>Pegasus determines whether a file is on the same site as the compute
    site, by inspecting the pool attribute associated with the URL in the
    Replica Catalog. If the pool attribute of an input file location matches
    the compute site where the job is scheduled, then that particular input
    file is a candidate for symlinking.</para>

    <para>For Pegasus to symlink against existing input data on a compute
    site, following must be true</para>

    <orderedlist>
      <listitem>
        <para>Property <emphasis role="bold">pegasus.transfer.links</emphasis>
        is set to <emphasis role="bold">true</emphasis></para>
      </listitem>

      <listitem>
        <para>The input file location in the Replica Catalog has the pool
        attribute matching the compute site.</para>
      </listitem>
    </orderedlist>

    <tip>
      <para>To confirm if a particular input file is symlinked instead of
      being copied, look for the destination URL for that file in
      stage_in_remote*.in file. The destination URL will start with symlink://
      .</para>
    </tip>

    <para>In the symlinking case, Pegasus strips out URL prefix from a URL and
    replaces it with a file URL.</para>

    <para>For example if a user has the following URL catalogued in the
    Replica Catalog for an input file f.input</para>

    <programlisting>f.input   gsiftp://server.isi.edu/shared/storage/input/data/f.input pool="isi"</programlisting>

    <para>and the compute job that requires this file executes on a compute
    site named isi , then if symlinking is turned on the data stage in job
    (stage_in_remote_viz_0 ) will have the following source and destination
    specified for the file</para>

    <programlisting>#viz viz
file:///shared/storage/input/data/f.input  symlink://shared-scratch/workflow-exec-dir/f.input
</programlisting>
  </section>

    <section>
    <title>Addition of Separate Data Movement Nodes to Executable
    Workflow</title>

    <para>Pegasus relies on a Transfer Refiner that comes up with the strategy
    on how many data movement nodes are added to the executable workflow. All
    the compute jobs scheduled to a site share the same workflow specific
    directory. The transfer refiners ensure that only one copy of the input
    data is transferred to the workflow execution directory. This is to
    prevent data clobbering . Data clobbering can occur when compute jobs of a
    workflow share some input files, and have different stage in transfer jobs
    associated with them that are staging the shared files to the same
    destination workflow execution directory.</para>

    <para>The default Transfer Refiner used in Pegasus is the Bundle Refiner
    that allows the user to specify how many local|remote stagein|stageout
    jobs are created per execution site.</para>

    <para>The behavior of the refiner is controlled by specifying certain
    pegasus profiles</para>

    <orderedlist>
      <listitem>
        <para>either with the execution sites in the site catalog</para>
      </listitem>

      <listitem>
        <para>OR globally in the properties file</para>
      </listitem>
    </orderedlist>

    <table>
      <title>Pegasus Profile Keys For the Bundle Transfer Refiner</title>

      <tgroup cols="2">
        <thead>
          <row>
            <entry>Profile Key</entry>

            <entry>Description</entry>
          </row>
        </thead>

        <tbody>
          <row>
            <entry>stagein.clusters</entry>

            <entry>This key determines the maximum number of stage-in jobs
            that are can executed locally or remotely per compute site per
            workflow.</entry>
          </row>

          <row>
            <entry>stagein.local.clusters</entry>

            <entry>This key provides finer grained control in determining the
            number of stage-in jobs that are executed locally and are
            responsible for staging data to a particular remote site.</entry>
          </row>

          <row>
            <entry>stagein.remote.clusters</entry>

            <entry>This key provides finer grained control in determining the
            number of stage-in jobs that are executed remotely on the remote
            site and are responsible for staging data to it.</entry>
          </row>

          <row>
            <entry>stageout.clusters</entry>

            <entry>This key determines the maximum number of stage-out jobs
            that are can executed locally or remotely per compute site per
            workflow.</entry>
          </row>

          <row>
            <entry>stageout.local.clusters</entry>

            <entry>This key provides finer grained control in determining the
            number of stage-out jobs that are executed locally and are
            responsible for staging data from a particular remote
            site.</entry>
          </row>

          <row>
            <entry>stageout.remote.clusters</entry>

            <entry>This key provides finer grained control in determining the
            number of stage-out jobs that are executed remotely on the remote
            site and are responsible for staging data from it.</entry>
          </row>
        </tbody>
      </tgroup>
    </table>

    <figure>
      <title>Default Transfer Case : Input Data To Workflow Specific Directory
      on Shared File System</title>

      <mediaobject>
        <imageobject>
          <imagedata fileref="images/bundle-transfer-refiner.png" lang=""/>
        </imageobject>
      </mediaobject>
    </figure>
  </section>

    <section>
    <title>Executable Used for Transfer Jobs</title>

    <para>Pegasus refers to a python script called <emphasis
    role="bold">pegasus-transfer</emphasis> as the executable in the transfer
    jobs to transfer the data. pegasus-transfer is a python based wrapper
    around various transfer clients . pegasus-transfer looks at source and
    destination url and figures out automatically which underlying client to
    use. pegasus-transfer is distributed with the PEGASUS and can be found at
    $PEGASUS_HOME/bin/pegasus-transfer.</para>

    <para>Currently, pegasus-transfer interfaces with the following transfer
    clients</para>

    <table>
      <title>Transfer Clients interfaced to by pegasus-transfer</title>

      <tgroup cols="2">
        <thead>
          <row>
            <entry>Transfer Client</entry>

            <entry>Used For</entry>
          </row>
        </thead>

        <tbody>
          <row>
            <entry>globus-url-copy</entry>

            <entry>staging files to and from a gridftp server.</entry>
          </row>

          <row>
            <entry>lcg-copy</entry>

            <entry>staging files to and from a SRM server.</entry>
          </row>

          <row>
            <entry>wget</entry>

            <entry>staging files from a HTTP server.</entry>
          </row>

          <row>
            <entry>cp</entry>

            <entry>copying files from a POSIX filesystem .</entry>
          </row>

          <row>
            <entry>ln</entry>

            <entry>symlinking against input files.</entry>
          </row>
        </tbody>
      </tgroup>
    </table>

    <para>For remote sites, Pegasus constructs the default path to
    pegasus-transfer on the basis of PEGASUS_HOME env profile specified in the
    site catalog. To specify a different path to the pegasus-transfer client ,
    users can add an entry into the transformation catalog with fully
    qualified logical name as <emphasis
    role="bold">pegasus::pegasus-transfer</emphasis></para>
  </section>

    <section>
    <title>Staging of Executables</title>

    <para>Users can get Pegasus to stage the user executables ( executables
    that the jobs in the DAX refer to ) as part of the transfer jobs to the
    workflow specific execution directory on the compute site. The URL
    locations of the executables need to be specified in the transformation
    catalog as the PFN and the type of executable needs to be set to <emphasis
    role="bold">STAGEABLE</emphasis> .</para>

    <para>The location of a transformation can be specified either in</para>

    <itemizedlist>
      <listitem>
        <para>DAX in the executables section. More details <link
        linkend="dax_transformation_catalog">here</link> .</para>
      </listitem>

      <listitem>
        <para>Transformation Catalog. More details <link
        linkend="transformation">here</link> .</para>
      </listitem>
    </itemizedlist>

    <para>A particular transformation catalog entry of type STAGEABLE is
    compatible with a compute site only if all the System Information
    attributes associated with the entry match with the System Information
    attributes for the compute site in the Site Catalog. The following
    attributes make up the System Information attributes</para>

    <orderedlist>
      <listitem>
        <para>arch</para>
      </listitem>

      <listitem>
        <para>os</para>
      </listitem>

      <listitem>
        <para>osrelease</para>
      </listitem>

      <listitem>
        <para>osversion</para>
      </listitem>
    </orderedlist>

      <section>
      <title>Transformation Mappers</title>

      <para>Pegasus has a notion of transformation mappers that determines
      what type of executables are picked up when a job is executed on a
      remote compute site. For transfer of executables, Pegasus constructs a
      soft state map that resides on top of the transformation catalog, that
      helps in determining the locations from where an executable can be
      staged to the remote site.</para>

      <para>Users can specify the following property to pick up a specific
      transformation mapper</para>

      <programlisting><emphasis role="bold">pegasus.catalog.transformation.mapper</emphasis> </programlisting>

      <para>Currently, the following transformation mappers are
      supported.</para>

      <table>
        <title>Transformation Mappers Supported in Pegasus</title>

        <tgroup cols="2">
          <thead>
            <row>
              <entry>Transformation Mapper</entry>

              <entry>Description</entry>
            </row>
          </thead>

          <tbody>
            <row>
              <entry>Installed</entry>

              <entry>This mapper only relies on transformation catalog entries
              that are of type INSTALLED to construct the soft state map. This
              results in Pegasus never doing any transfer of executables as
              part of the workflow. It always prefers the installed
              executables at the remote sites</entry>
            </row>

            <row>
              <entry>Staged</entry>

              <entry>This mapper only relies on matching transformation
              catalog entries that are of type STAGEABLE to construct the soft
              state map. This results in the executable workflow referring
              only to the staged executables, irrespective of the fact that
              the executables are already installed at the remote end</entry>
            </row>

            <row>
              <entry>All</entry>

              <entry>This mapper relies on all matching transformation catalog
              entries of type STAGEABLE or INSTALLED for a particular
              transformation as valid sources for the transfer of executables.
              This the most general mode, and results in the constructing the
              map as a result of the cartesian product of the matches.</entry>
            </row>

            <row>
              <entry>Submit</entry>

              <entry>This mapper only on matching transformation catalog
              entries that are of type STAGEABLE and reside at the submit host
              (pool local), are used while constructing the soft state map.
              This is especially helpful, when the user wants to use the
              latest compute code for his computations on the grid and that
              relies on his submit host.</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>
  </section>

     <section>
      <title>Staging of Pegasus Worker Package</title>

    <para>Pegasus can optionally stage the pegasus worker package as part of
    the executable workflow to remote workflow specific execution directory.
    The pegasus worker package contains the pegasus auxillary executables that
    are required on the remote site. If the worker package is not staged as
    part of the executable workflow, then Pegasus relies on the installed
    version of the worker package on the remote site. To determine the
    location of the installed version of the worker package on a remote site,
    Pegasus looks for an environment profile PEGASUS_HOME for the site in the
    Site Catalog.</para>

    <para>Users can set the following property to true to turn on worker
    package staging</para>

    <programlisting><emphasis role="bold">pegasus.transfer.worker.package          true</emphasis> </programlisting>

    <para>By default, when worker package staging is turned on pegasus pulls
    the compatible worker package from the Pegasus Website. To specify a
    different worker package location, users can specify the transformation
    <emphasis role="bold">pegasus::worker</emphasis> in the transformation
    catalog with</para>

    <itemizedlist>
      <listitem>
        <para>type set to STAGEABLE</para>
      </listitem>

      <listitem>
        <para>System Information attributes of the transformation catalog
        entry match the System Information attributes of the compute
        site.</para>
      </listitem>

      <listitem>
        <para>the PFN specified should be a remote URL that can be pulled to
        the compute site.</para>
      </listitem>
    </itemizedlist>
  </section>

      <section>
       <title>Second Level Staging</title>

    <para>By default, Pegasus executes the jobs in the workflow specific
    directory created on the shared filesystem of a compute site. However, if
    a user wants Pegasus can execute the jobs on the worker nodes filesystem.
    When the jobs are executed on the worker node, they pull the input data
    for the job from the workflow specific directory on the staging server (
    usually the shared filesystem on the compute site ) to a directory on the
    worker node filesystem, and after the job has completed stages out the
    output files from the worker node to the workflow specific execution
    directory.</para>

    <para> The separate data stagein and stageout jobs are still added to the
    workflow. They are responsible for getting the input data to the workflow
    specific directory on the staging server ( usually the shared filesystem
    on the compute site ) , and pushing out the output data to final storage
    site from that directory.</para>

    <figure>
      <title>Second Level Staging : Getting Data to and from a directory on
      the worker nodes</title>

      <mediaobject>
        <imageobject>
          <imagedata fileref="./images/sls-transfer-worker.png"/>
        </imageobject>
      </mediaobject>
    </figure>

    <para>This mode is especially useful for running in the cloud environments
    where you don't want to setup a shared filesystem between the worker
    nodes. Running in that mode is explained in detail <link
    linkend="running_on_cloud">here.</link> </para>

    <para>To turn on second level staging for the workflows users should set
    the following properties</para>

    <programlisting><emphasis role="bold">pegasus.execute.*.filesystem.local = true   </emphasis>    # Turn on second-level staging (SLS)
<emphasis role="bold">pegasus.transfer.sls.s3.stage.sls.file = false</emphasis>  # Do not transfer .sls files via transfer jobs
<emphasis role="bold">pegasus.gridstart = SeqExec </emphasis>                    # Use SeqExec to launch the jobs</programlisting>
  </section>
  </section>
  <section>
   <title>Hierarchical Workflow</title>

   <section>
    <title>Introduction</title>

    <para>The Abstract Workflow in addition to containing compute jobs, can
    also contain jobs that refer to other workflows. This is useful for
    running large workflows or ensembles of workflows.</para>

    <para>Users can embed two types of workflow jobs in the DAX</para>

    <orderedlist>
      <listitem>
        <para>daxjob - refers to a sub workflow represented as a DAX. During
        the planning of a workflow, the DAX jobs are mapped to condor dagman
        jobs that have pegasus plan invocation on the dax ( referred to in the
        DAX job ) as the prescript.</para>

        <figure>
          <title>Planning of a DAX Job</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="./images/daxjob-mapping.png"/>
            </imageobject>
          </mediaobject>
        </figure>
      </listitem>

      <listitem>
        <para>dagjob - refers to a sub workflow represented as a DAG. During
        the planning of a workflow, the DAG jobs are mapped to condor dagman
        and refer to the DAG file mentioned in the DAG job.</para>

        <figure>
          <title>Planning of a DAG Job</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="./images/dagjob-mapping.png"/>
            </imageobject>
          </mediaobject>
        </figure>
      </listitem>
    </orderedlist>
  </section>

  <section>
    <title>Specifying a DAX Job in the DAX</title>

    <para>Specifying a DAXJob in a DAX is pretty similar to how normal compute
    jobs are specified. There are minor differences in terms of the xml
    element name ( dax vs job ) and the attributes specified. DAXJob XML
    specification is described in detail in the <link linkend="api">chapter on
    DAX API</link> . An example DAX Job in a DAX is shown below</para>

    <programlisting id="dax_job_example" language="">  &lt;dax id="ID000002" name="black.dax" node-label="bar" &gt;
    &lt;profile namespace="dagman" key="maxjobs"&gt;10&lt;/profile&gt;
    &lt;argument&gt;-Xmx1024 -Xms512 -Dpegasus.dir.storage=storagedir  -Dpegasus.dir.exec=execdir -o local -vvvvv --force -s dax_site &lt;/argument&gt;
  &lt;/dax&gt;</programlisting>

    <section>
      <title>DAX File Locations</title>

      <para>The name attribute in the dax element refers to the LFN ( Logical
      File Name ) of the dax file. The location of the DAX file can be
      catalogued either in the</para>

      <para><orderedlist>
          <listitem>
            <para>Replica Catalog</para>
          </listitem>

          <listitem>
            <para>Replica Catalog Section in the <link
            linkend="dax_replica_catalog">DAX</link> .</para>

            <note>
              <para>Currently, only file url's on the local site ( submit host
              ) can be specified as DAX file locations.</para>
            </note>
          </listitem>
        </orderedlist></para>
    </section>

    <section>
      <title>Arguments for a DAX Job</title>

      <para>Users can specify specific arguments to the DAX Jobs. The
      arguments specified for the DAX Jobs are passed to the pegasus-plan
      invocation in the prescript for the corresponding condor dagman job in
      the executable workflow.</para>

      <para>The following options for pegasus-plan are inherited from the
      pegasus-plan invocation of the parent workflow. If an option is
      specified in the arguments section for the DAX Job then that overrides
      what is inherited.</para>

      <table>
        <title>Options inherited from parent workflow</title>

        <tgroup cols="2">
          <thead>
            <row>
              <entry>Option Name</entry>

              <entry>Description</entry>
            </row>
          </thead>

          <tbody>
            <row>
              <entry>--sites</entry>

              <entry>list of execution sites.</entry>
            </row>
          </tbody>
        </tgroup>
      </table>

      <para>It is highly recommended that users <emphasis role="bold">dont
      specify</emphasis> directory related options in the arguments section
      for the DAX Jobs. Pegasus assigns values to these options for the sub
      workflows automatically.</para>

      <orderedlist>
        <listitem>
          <para>--relative-dir</para>
        </listitem>

        <listitem>
          <para>--dir</para>
        </listitem>

        <listitem>
          <para>--relative-submit-dir</para>
        </listitem>
      </orderedlist>
    </section>

    <section>
      <title>Profiles for DAX Job</title>

      <para>Users can choose to specify dagman profiles with the DAX Job to
      control the behavior of the corresponding condor dagman instance in the
      executable workflow. In the example <link
      linkend="dax_job_example">above</link> maxjobs is set to 10 for the sub
      workflow.</para>
    </section>

    <section>
      <title>Execution of the PRE script and Condor DAGMan instance</title>

      <para>The pegasus plan that is invoked as part of the prescript to the
      condor dagman job is executed on the submit host. The log from the
      output of pegasus plan is redirected to a file ( ending with suffix
      pre.log ) in the submit directory of the workflow that contains the DAX
      Job. The path to pegasus-plan is automatically determined.</para>

      <para>The DAX Job maps to a Condor DAGMan job. The path to condor dagman
      binary is determined according to the following rules -</para>

      <orderedlist>
        <listitem>
          <para>entry in the transformation catalog for condor::dagman for
          site local, else</para>
        </listitem>

        <listitem>
          <para>pick up the value of CONDOR_HOME from the environment if
          specified and set path to condor dagman as
          $CONDOR_HOME/bin/condor_dagman , else</para>
        </listitem>

        <listitem>
          <para>pick up the value of CONDOR_LOCATION from the environment if
          specified and set path to condor dagman as
          $CONDOR_LOCATION/bin/condor_dagman , else</para>
        </listitem>

        <listitem>
          <para>pick up the path to condor dagman from what is defined in the
          user's PATH</para>
        </listitem>
      </orderedlist>

      <tip>
        <para>It is recommended that user dagman.maxpre in their properties
        file to control the maximum number of pegasus plan instances launched
        by each running dagman instance.</para>
      </tip>
    </section>
  </section>

  <section>
    <title>Specifying a DAG Job in the DAX</title>

    <para>Specifying a DAGJob in a DAX is pretty similar to how normal compute
    jobs are specified. There are minor differences in terms of the xml
    element name ( dag vs job ) and the attributes specified. DAGJob XML
    specification is described in detail in <link linkend="api"> Chapter 10 - API Reference </link> . An example DAG Job in a DAX is shown below</para>

    <programlisting id="dag_job_example">  &lt;dag id="ID000003" name="black.dag" node-label="foo" &gt;
    &lt;profile namespace="dagman" key="maxjobs"&gt;10&lt;/profile&gt;
    &lt;profile namespace="dagman" key="DIR"&gt;/dag-dir/test&lt;/profile&gt;
  &lt;/dag&gt;</programlisting>

    <section>
      <title>DAG File Locations</title>

      <para>The name attribute in the dag element refers to the LFN ( Logical
      File Name ) of the dax file. The location of the DAX file can be
      catalogued either in the</para>

      <para><orderedlist>
          <listitem>
            <para>Replica Catalog</para>
          </listitem>

          <listitem>
            <para>Replica Catalog Section in the <link
            linkend="dax_replica_catalog">DAX</link> .</para>

            <note>
              <para>Currently, only file url's on the local site ( submit host
              ) can be specified as DAG file locations.</para>
            </note>
          </listitem>
        </orderedlist></para>
    </section>

    <section>
      <title>Profiles for DAG Job</title>

      <para>Users can choose to specify dagman profiles with the DAX Job to
      control the behavior of the corresponding condor dagman instance in the
      executable workflow. In the example <link
      linkend="dag_job_example">above</link> maxjobs is set to 10 for the sub
      workflow.</para>

      <para>The dagman profile DIR allows users to specify the directory in
      which they want the condor dagman instance to execute. In the example
      <link linkend="dag_job_example">above</link> black.dag is set to be
      executed in directory /dag-dir/test . The /dag-dir/test should be
      created beforehand.</para>
    </section>
  </section>

  <section>
    <title>File Dependencies Across DAX Jobs</title>

    <para>In hierarchal workflows , if a sub workflow generates some output
    files required by another sub workflow then there should be an edge
    connecting the two dax jobs. Pegasus will ensure that the prescript for
    the child sub-workflow, has the path to the cache file generated during
    the planning of the parent sub workflow. The cache file in the submit
    directory for a workflow is a textual replica catalog that lists the
    locations of all the output files created in the remote workflow execution
    directory when the workflow executes.</para>

    <para>This automatic passing of the cache file to a child sub-workflow
    ensures that the datasets from the same workflow run are used. However,
    the passing of the locations in a cache file also ensures that Pegasus
    will prefer them over all other locations in the Replica Catalog. If you
    need the Replica Selection to consider locations in the Replica Catalog
    also, then set the following property.</para>

    <programlisting><emphasis role="bold">pegasus.catalog.replica.cache.asrc  true</emphasis></programlisting>

    <para>The above is useful in the case, where you are staging out the
    output files to a storage site, and you want the child sub workflow to
    stage these files from the storage output site instead of the workflow
    execution directory where the files were originally created.</para>
  </section>

  <section>
    <title>Recursion in Hierarchal Workflows</title>

    <para>It is possible for a user to add a dax jobs to a dax that already
    contain dax jobs in them. Pegasus does not place a limit on how many
    levels of recursion a user can have in their workflows. From Pegasus
    perspective recursion in hierarchal workflows ends when a DAX with only
    compute jobs is encountered . However, the levels of recursion are limited
    by the system resources consumed by the DAGMan processes that are running
    (each level of nesting produces another DAGMan process) .</para>

    <para>The figure below illustrates an example with recursion 2 levels
    deep.</para>

    <figure>
      <title>Recursion in Hierarchal Workflows</title>

      <mediaobject>
        <imageobject>
          <imagedata fileref="./images/recursion_in_hierarchal_workflows.png"/>
        </imageobject>
      </mediaobject>
    </figure>

    <para>The execution time-line of the various jobs in the above figure is
    illustrated below.</para>

    <figure>
      <title>Execution Time-line for Hierarchal Workflows</title>

      <mediaobject>
        <imageobject>
          <imagedata fileref="./images/hierarchal_workflows_execution_timeline.png"/>
        </imageobject>
      </mediaobject>
    </figure>
  </section>

  <section>
    <title>Example</title>

    <para>The <ulink
    url="http://en.wikipedia.org/wiki/Galactic_plane">Galactic Plane</ulink>
    workflow is a Hierarchical workflow of many Montage workflows. The example
    is explained in the example workflows chapter <link
    linkend="workflow_of_workflows">here</link> .</para>
  </section>

  </section>
</chapter>









