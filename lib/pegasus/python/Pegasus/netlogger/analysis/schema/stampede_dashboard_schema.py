"""
Contains the code to create and map objects to the Stampede DB schema
via a SQLAlchemy interface.
"""
__author__ = "Monte Goode"
__author__ = "Karan Vahi"

import time
import warnings

from sqlalchemy import *
from sqlalchemy import orm

from Pegasus.netlogger.analysis.schema._base import SABase

CURRENT_SCHEMA_VERSION = 4.0

# Empty classes that will be populated and mapped
# to tables via the SQLAlch mapper.
class DashboardWorkflow(SABase):
    pass

class DashboardWorkflowstate(SABase):
    pass

def initializeToDashboardDB(db, metadata, kw={}):
    """
    Function to create the Workflow schema that just tracks the root
    level workflows, if it does not exist,
    if it does exist, then connect and set up object mappings.

    @type   db: SQLAlch db/engine object.
    @param  db: Engine object to initialize.
    @type   metadata: SQLAlch metadata object.
    @param  metadata: Associated metadata object to initialize.
    @type   kw: dict
    @param  kw: Keywords to pass to Table() functions
    """
    KeyInt = Integer
    # MySQL likes using BIGINT for PKs but some other
    # DB don't like it so swap as needed.
    if db.name == 'mysql':
        KeyInt = BigInteger
        kw['mysql_charset'] = 'latin1'

    if db.name == 'sqlite':
        warnings.filterwarnings('ignore', '.*does \*not\* support Decimal*.')


    # pg_workflow definition
    # ==> Information comes from braindump.txt file

    # wf_uuid = autogenerated by database                   wfuuid, submitted, directory, database connection
    # dax_label = label
    # timestamp = pegasus_wf_time
    # submit_hostname = (currently missing)
    # submit_dir = run
    #

    pg_workflow = Table('workflow', metadata,
                        Column('wf_id', KeyInt, primary_key=True, nullable=False),
                        Column('wf_uuid', VARCHAR(255), nullable=False),
                        Column('dax_label', VARCHAR(255), nullable=True),
                        Column('dax_version', VARCHAR(255), nullable=True),
                        Column('dax_file', VARCHAR(255), nullable=True),
                        Column('dag_file_name', VARCHAR(255), nullable=True),
                        Column('timestamp', NUMERIC(precision=16,scale=6), nullable=True),
                        Column('submit_hostname', VARCHAR(255), nullable=True),
                        Column('submit_dir', TEXT, nullable=True),
                        Column('planner_arguments', TEXT, nullable=True),
                        Column('user', VARCHAR(255), nullable=True),
                        Column('grid_dn', VARCHAR(255), nullable=True),
                        Column('planner_version', VARCHAR(255), nullable=True),
                        Column('db_url', TEXT, nullable=True),
                        **kw
    )

    Index('wf_id_KEY', pg_workflow.c.wf_id, unique=True)
    Index('wf_uuid_UNIQUE', pg_workflow.c.wf_uuid, unique=True)

    orm.mapper(DashboardWorkflow, pg_workflow )

    pg_workflowstate = Table('workflowstate', metadata,
    # All three columns are marked as primary key to produce the desired
    # effect - ie: it is the combo of the three columns that make a row
    # unique.
                             Column('wf_id', KeyInt, ForeignKey('workflow.wf_id', ondelete='CASCADE'), 
                                    nullable=False, primary_key=True),
                             Column('state', Enum('WORKFLOW_STARTED', 'WORKFLOW_TERMINATED'), 
                                nullable=False, primary_key=True),
                             Column('timestamp', NUMERIC(precision=16,scale=6), nullable=False, primary_key=True,
                                    default=time.time()),
                             Column('restart_count', INT, nullable=False),
                             Column('status', INT, nullable=True),
                             **kw
    )

    Index('UNIQUE_WORKFLOWSTATE',
          pg_workflowstate.c.wf_id,
          pg_workflowstate.c.state,
          pg_workflowstate.c.timestamp, unique=True)

    orm.mapper(DashboardWorkflowstate, pg_workflowstate)

    metadata.create_all(db)

