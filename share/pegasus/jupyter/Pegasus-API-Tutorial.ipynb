{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pegasus Workflow Management System\n",
    "## A Python Jupyter Tutorial\n",
    "\n",
    "This tutorial will take you through the steps of running simple workflows using Pegasus Workflow Management System. Pegasus allows scientists to:\n",
    "\n",
    "1. **Automate** their scientific computational work, as portable workflows. Pegasus enables scientists to construct workflows in abstract terms without worrying about the details of the underlying execution environment or the particulars of the low-level specifications required by the middleware (Condor, Globus, or Amazon EC2). It automatically locates the necessary input data and computational resources necessary for workflow execution. It cleans up storage as the workflow is executed so that data-intensive workflows have enough space to execute on storage-constrained resources.\n",
    "\n",
    "2. **Recover** from failures at runtime. When errors occur, Pegasus tries to recover when possible by retrying tasks, and when all else fails, provides a rescue workflow containing a description of only the work that remains to be done. It also enables users to move computations from one resource to another. Pegasus keeps track of what has been done (provenance) including the locations of data used and produced, and which software was used with which parameters.\n",
    "\n",
    "3. **Debug** failures in their computations using a set of system provided debugging tools and an online workflow monitoring dashboard.\n",
    "\n",
    "This tutorial is intended for new users who want to get a quick overview of Pegasus concepts and usage via _Jupyter_.\n",
    "\n",
    "For more information about Pegasus, please visit the Pegasus website: http://pegasus.isi.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scientific Workflows\n",
    "\n",
    "Scientific workflows allow users to easily express multi-step computational tasks, for example retrieve data from an instrument or a database, reformat the data, and run an analysis. A scientific workflow describes the dependencies between the tasks and in most cases the workflow is described as a directed acyclic graph (DAG), where the nodes are tasks and the edges denote the task dependencies. A defining property for a scientific workflow is that it manages data flow. The tasks in a scientific workflow can be everything from short serial tasks to very large parallel tasks (MPI for example) surrounded by a large number of small, serial tasks used for pre- and post-processing. For this tutorial we will be executing the simple workflow depicted below. \n",
    "\n",
    "<img src=\"https://pegasus.isi.edu/documentation/images/tutorial-split-wf.jpg\" alt=\"Split Workflow\" style=\"width: 250px;\"/>\n",
    "\n",
    "In this diagram, the ovals represent tasks and the rectangles represent input and output files. The workflow takes in as input `pegasus.html`. `split` is called on that file and chunks of the original file are output. For each chunk (four in this example), `wc` is invoked and outputs a single file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Pegasus Python API\n",
    "\n",
    "The first step is to import the Pegasus API package and its contents. This includes everything necessary to start running workflows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Pegasus.api import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to specify a \"work\" directory where workflow files will be created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "WORK_DIR = Path(Path.cwd()) / \"pegasus-tutorial\"\n",
    "Path.mkdir(WORK_DIR) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to create a Pegasus Workflow object. Using the diagram above as a reference, we add files and jobs to the workflow accordingly. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the workflow object with name `split`\n",
    "wf = Workflow(\"split\", infer_dependencies=True)\n",
    "\n",
    "# define input/output files used by the split job\n",
    "input_file = File(\"pegasus.html\")\n",
    "split_output_files = [File(\"part.{}\".format(c)) for c in \"abcd\"]\n",
    "\n",
    "# add the split job\n",
    "wf.add_jobs(\n",
    "    Job(\"split\")\n",
    "    .add_args(\"-l\", \"100\", \"-a\", \"1\", input_file, \"part.\")\n",
    "    .add_inputs(input_file)\n",
    "    .add_outputs(*split_output_files, stage_out=False)\n",
    ")\n",
    "\n",
    "# add the wc jobs\n",
    "for c, file in zip(\"abcd\", split_output_files):\n",
    "    wf.add_jobs(\n",
    "        Job(\"wc\")\n",
    "        .add_args(\"-l\", file)\n",
    "        .add_inputs(file)\n",
    "        .set_stdout(\"count.txt.{}\".format(c), stage_out=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Workflow outputs are tagged with `stage_out=True` as these files will be transferred to a designated output directory.\n",
    "- You can manually add dependencies between jobs, however by setting `infer_dependencies=True` in the Workflow constructor, all dependencies will be inferred based on input and output files.\n",
    "- Most methods can be chained together. A complete specification of the API is available [here](www.UPDATETHIS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Catalogs\n",
    "\n",
    "The workflow description that you specify to Pegasus is portable, and usually does not contain any locations to physical input files, executables or cluster end points where jobs are executed. Pegasus uses three information catalogs, the **site catalog**, **transformation catalog**, and **replica catalog**, during the planning process. These catalogs decouple the portable aspects of the workflow from non-portable aspects (physical file names, executable install locations, etc.). In the following sections, we define these catalogs and briefly describe their usage. \n",
    "\n",
    "<img src=\"https://pegasus.isi.edu/documentation/images/tutorial-pegasus-catalogs.png\" width=\"650px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the Site Catalog\n",
    "\n",
    "The site catalog (SC) describes the sites where the workflow jobs are to be executed. In this tutorial we assume that you have a personal condoor pool, which we will refer to as `condorpool`, running on the same host Jupyter is installed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHARED_SCRATCH_DIR = str(WORK_DIR)\n",
    "SHARED_STORAGE_DIR = str(WORK_DIR / \"outputs\")\n",
    "SC_FILE_PATH = str(WORK_DIR / \"SiteCatalog.yml\")\n",
    "sc = (\n",
    "    SiteCatalog()\n",
    "    .add_sites(\n",
    "        Site(\"local\", arch=Arch.X86_64, os_type=OS.LINUX)\n",
    "            .add_directories(\n",
    "                Directory(Directory.SHAREDSCRATCH, SHARED_SCRATCH_DIR)\n",
    "                    .add_file_servers(FileServer(\"file://\" + SHARED_SCRATCH_DIR, Operation.ALL)),\n",
    "\n",
    "                Directory(Directory.SHAREDSTORAGE, SHARED_STORAGE_DIR)\n",
    "                    .add_file_servers(FileServer(\"file://\" + SHARED_STORAGE_DIR, Operation.ALL))\n",
    "            ),\n",
    "\n",
    "        Site(\"condorpool\", arch=Arch.X86_64, os_type=OS.LINUX)\n",
    "            .add_pegasus(style=\"condor\")\n",
    "            .add_condor(universe=\"vanilla\")\n",
    "    ).write(SC_FILE_PATH)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the Replica Catalog\n",
    "\n",
    "All files in a Pegasus workflow are referred to in the workflow object using their Logical File Name (LFN). These LFNs are mapped to Physical File Names (PFNs) when Pegasus plans the workflow. This level of indirection enables Pegasus to map abstract workflows to different execution sites and plan out the required file transfers automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The replica catalog (RC) for our example workflow contains only one entry for the workflowâ€™s only input file. This entry has an LFN of `pegasus.html` with a PFN of `file:///home/tutorial/jupyter/pegasus.html` and the file is stored on the `local` site, which implies that it will need to be transferred to the `condorpool` site when the workflow runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "RC_FILE_PATH = str(WORK_DIR / \"ReplicaCatalog.yml\")\n",
    "rc = ReplicaCatalog()\\\n",
    "        .add_replica(input_file, \"file:/\" + str(WORK_DIR) + \"/pegasus.hmlt\", \"local\")\\\n",
    "        .write(RC_FILE_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the Transformation Catalog\n",
    "\n",
    "The transformation catalog (TC) describes all of the executables (called \"transformations\") used by the workflow. This description includes the site(s) where they are located, the architecture and operating system they are compiled for, and any other information required to properly transfer them to the execution site and run them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TC should contain information about the two transformations used in the workflow above: `split` and `wc`. The TC indicates that both transformations are installed on the `condorpool` site, and are compiled for `x86_65` `linux`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TC_FILE_PATH = str(WORK_DIR / \"TransformationCatalog.yml\")\n",
    "tc = TransformationCatalog()\\\n",
    "        .add_transformations(\n",
    "            Transformation(\"split\")\n",
    "                .add_site(TransformationSite(\"condorpool\", \"file:///usr/bin/split\", False, arch=Arch.X86_64, os_type=OS.LINUX)),\n",
    "            \n",
    "            Transformation(\"wc\")\n",
    "                .add_site(TransformationSite(\"condorpool\", \"file:///usr/bin/wc\", False, arch=Arch.X86_64, os_type=OS.LINUX))\n",
    "        ).write(TC_FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "Before running our `split` workflow, a few configuration properties must be set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONF_FILE_PATH = str(WORK_DIR / \"pegasus.conf\")\n",
    "conf = Properties()\n",
    "conf[\"pegasus.catalog.site.file\"] = SC_FILE_PATH\n",
    "conf[\"pegasus.catalog.site\"] = \"YAML\"\n",
    "conf[\"pegasus.catalog.replica.file\"] = RC_FILE_PATH\n",
    "conf[\"pegasus.catalog.replica\"] = \"YAML\"\n",
    "conf[\"pegasus.catalog.transformation.file\"] = TC_FILE_PATH\n",
    "conf[\"pegasus.catalog.transformation\"] = \"YAML\"\n",
    "with open(CONF_FILE_PATH, \"w\") as f:\n",
    "    conf.write(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Workflow\n",
    "\n",
    "Up until this point we have defined the workflow, the site, replica, and transformation catalogs,\n",
    "and finally a configuration file. By invoking `plan` on the workflow object, the abstract workflow\n",
    "will be planned by __Pegasus__ and converted into an executable workflow that will be executed\n",
    "on the `condorpool`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wf.plan(\n",
    "    dir=WORK_DIR,\n",
    "    conf=CONF_FILE_PATH, \n",
    "    sites=\"condorpool\",\n",
    "    output_site=\"local\",\n",
    "    submit=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}