.\" This file or a portion of this file is licensed under the terms of
.\" the Globus Toolkit Public License, found in file GTPL, or at
.\" http://www.globus.org/toolkit/download/license.html. This notice
.  \" must
.\" appear in redistributions of this file, with or without
.  \" modification.
.\"
.\" Redistributions of this Software, with or without modification,
.  \" must
.\" reproduce the GTPL in: (1) the Software, or (2) the Documentation
.  \" or
.\" some other similar material which is provided with the Software
.  \" (if
.\" any).
.\"
.\" Copyright 1999-2004 University of Chicago and The University of
.\" Southern California. All rights reserved.
.\"
.\"
.\" $Id$
.\"
.\" Authors: Ewa Deelman, Gaurang Mehta, Karan Vahi
.\"
.TH "gencdag" "1" "1.4.7" "GriPhyN Virtual Data System"
.SH NAME
gencdag \- run the concrete planner which is part of Pegasus.
.SH SYNOPSIS
.B gencdag
[\-Dprop [..]] \-d|\-P <dax file|pdax file> [\-p site[,site[..]]] [\-c f1[,f2[..]]] [\-f]
[\-m style] [\-a] [\-C] [\-D <dir for o/p files>] [\-o <output pool>] [\-r[dir name]] [\-s] [\-v] [\-V] [\-h]  
.SH DESCRIPTION
The 
.B gencdag
command takes in as input the DAX generated by
the Abstract Planner of the VDS. It ends up generating
the concrete dag in the form of 
.B condor
submit files, which 
are then can be submitted to an 
.I execution
pool for execution.
.PP
The concrete planner ensures that all the data which is required
for the execution of the dag on the execution pool, is transferred
to it by adding transfer nodes at appropriate points in the dag.
This is done by looking up an appropriate 
.B Replica Mechanism
to determine the locations of the input files for the various jobs.
At present the default replica mechanism used is 
.B RLS
.PP
The concrete planner also tries to reduce a dag, unless specified otherwise. 
This is done  by deleting the jobs whose
output files have been found in some location in the Replica Catalog
(RLS).  At present no cost metrics are used. However preference
is given to a location corresponding to the execution pool.
.PP
The planner can also add nodes to transfer all the 
.B materialized
files to an 
.B output site.
The location on the output site is determined by looking up the 
.B site catalog
file, the path to which is picked up from the 
.I vds.sc.file 
property value.
.SH ARGUMENTS
Any option will be displayed with its long options synonym(s).
.TP
.B \-Dprop
The -D options allows an experienced user to override certain
properties
which influence the program execution, among them the default location
of
the user's properties file and the VDS home location. One may set
several
CLI properties by giving this option multiple times.
.I The -D option(s) must be the first option on the command line. 
A CLI property take precedence over the properties file property 
of the same key.
.TP
.B \-d|\-P \fIfilename
.PD 0
.TP
.PD 1
.B \-\-dax|\-\-pdax \fIfilename
The xml file containing the 
.B abstract 
dag, which has been generated in phase one , by the
.B Abstract
Planner.|The pdax file containing the partition graph that is
generated by running the partitioner of the abstract workflow (DAX).
.IP
This is a mandatory option, which has to be used.
.TP
.B \-a
.PD 0
.TP
.PD 1
.B \-\-authenticate
This option results in authentication tests being performed against
the remote jobmanagers and the gridftp servers for the site that are
being used to map the workflow. The jobmanagers and gridftp servers
against which the authentication tests fail, are not used to run jobs
or transfer data respectively. In case of a site being associated with
muliple jobmanagers and gridftp servers, only the bad ones are purged
from the mapping process.
.IP
At present for the gridftp servers no gsi authentication is
done. However a check alive test is done by opening a socket to the
server and determining if it a ftp server or not.  By default a
timeout of 120 seconds is used for the socket. The timeout value in
seconds can be overriden by specifying the property 
.I vds.auth.gridftp.timeout.
.TP
.B \-b \fIprefix
.PD 0
.TP
.PD 1
.B \-\-basename \fIprefix
The basename prefix to be used while constructing per workflow files
like the dagman file (.dag file) and other workflow specific files
that are created by Condor. Usually this prefix, is taken from the
name attribute specified in the root element of the dax|pdax files.
.TP
.B \-c \fIlist of cache files
.PD 0
.TP
.PD 1
.B \-\-cache \fIlist of cache files
A comma separated list of paths to replica cache files that override
the results from the replica catalog for a particular lfn.
.IP 
The format of the cache file is as follows:
.nf
\f(CB
LFN_1 [site handle 1]
PFN_1
LFN_2 [site handle 2]
PFN_2
 ...
LFN_N [site handle N]
PFN_N
\fP
.fi
.IP
To treat the cache file as supplemental replica catalogs set the
property 
.I vds.cache.asrc 
to true. This results in the mapping in the cache files to be merged
with the mappings in the replica catalog. Thus, for a particular lfn
both the entries in the cache file and replica catalog are available
for replica selection.
.TP
.B \-C
.PD 0
.TP
.PD 1
.B \-\-cluster \fI comma separated list of clustering styles.
This mode of operation results in clustering of n compute jobs into a
larger jobs to reduce remote scheduling overhead. You can specify a
list of clustering techniques to recursively apply them to the
workflow. For example, this allows you to cluster some jobs in the
workflow using horizontal clustering and then use label based
clustering on the intermediate workflow to do vertical clustering.
.IP
The clustered jobs can be run at the remote site, either sequentially
or by using mpi. This can be specified by setting the property
.I vds.job.aggregator.
The property can be overriden by associating the VDS profile key
.I collapser
either with the transformation in the tranformation catalog or the
execution site in the site catalog. The value specified (to the
property or the profile), is the logical name of the transformation
that is to be used for clustering jobs. Note that clustering will only
happen if the corresponding transformations are catalogued in the
transformation catalog.
.IP
VDS is shipped with a clustering executable
.I seqexec
that can be found in
.I $VDS_HOME/bin
directory. It runs the jobs in the clustered job sequentially on the
same node at the remote site.
.IP
In addition, a mpi wrapper
.I mpiexec
is distributed as source with the VDS. It can be found in
$VDS_HOME/src/tools/cluster
directory. The wrapper is run on every mpi node, with the first one
being the master and the rest of the ones as workers. The number of
instances of mpiexec that are invoked is equal to the value of the
globus rsl key nodecount. The master distributes the smaller
constituent jobs to the workers. For e.g. If there were 10 jobs in the
clustered job and nodecount was 5, then one node acts as master, and
the
10 jobs are distributed amongst the 4 slaves on demand.  The master
hands off a job to the slave node as and when it gets free. So
initially all the 4 nodes are given a single job each, and then as and
when they get done are handed more jobs till all the 10 jobs have been
executed.
.IP
By default, seqexec is used for clustering jobs unless overriden
in the properties or by the vds profile key collapser.
.IP
The following type of clustering styles are currently supported
.TP
.B horizontal
is the style of clustering in which jobs on the same level are
aggregated into larger jobs. A level of the workflow is defined as the
greatest distance of a node, from the root of the workflow. Clustering
occurs only on jobs of the same type i.e they refer to the same
logical transformation in the transformation catalog. 
.IP
The granularity of clustering can be specified by associating either
the VDS profile key 
.I collapse
or the VDS profile key
.I bundle
with the transformation.
The collapse key indicates how many jobs need to be clustered into
the larger clustered job. The bundle key indicates how many clustered
jobs are to be created for a particular level at a particular
execution site. If both keys are specified for a particular
transformation, then the bundle key value is used to determine the
clustering granularity.
.TP
.B label
is the style of clustering in which you can label the jobs in your
workflow. The jobs with the same level are put in the same clustered
job. This allows you to aggregate jobs across levels, or in a manner
that is best suited to your application.
.IP
To label the workflow, you need to associate VDS profiles with the
jobs in the DAX. The profile key to use for labelling the workflow can
be set by the property 
.I vds.clusterer.label.key.
It defaults to label, meaning if you have a VDS profile key label
with jobs, the jobs with the same label will go into the same
clustered job. 
.IP  
The clustering guide (VDSUG_PegasusJobClustering.xml) in $VDS_HOME/doc/userguide
directory explains clustering in more detail.
A PDF version can be found online at				
http://www.isi.edu/~vahi/vds/doc/VDSUG_PegasusJobClustering.pdf 

.TP
.B \-D \fIdir name
.PD 0
.TP
.PD 1
.B \--dir \fIdir name
The directory in which you want the output of the planner usually
condor submit files, to be generated.
.IP
By default it is the directory from which one runs the
.B gencdag
(Concrete Planner) command.
.TP
.B \-f
.PD 0
.TP
.PD 1
.B \-\-force
This bypasses the reduction phase in which the abstract DAG is
reduced, on the basis of the locations of the output files returned by
the replica catalog. This is analogous to a 
.B make style
generation of the concrete workflow.
.TP
.B \-m
.PD 0
.TP
.PD 1
.B \-\-megadag \fIstyle
In case of deferred planning, a megadag(outer level dag) is run to
maintain the dependencies between the various partitions. The
dependencies between the partitions are specfied in an XML file
conforming to the pdax format.  The pdax file is created while
partitioning the dax using 
.B partitiondax client.
This option indicates how the dependencies between the jobs in a
partition are maintained, so that jobs are executed in the right order. 
.TP
.B dag 
is the default style and the only one supported currently. This
results in dagman instance being invoked for each partition. The
dagman instance launched for each job is responsible for maintaining
the dependencies between the jobs in a partition. This works well for
cases, where there are a sufficient number of jobs in a partition, as
customarily created by level based (BFS) partitioning or a label based
partitioning. 
.IP
The submit directory for each partition resides in a
separate submit directory under the base directory specified by the 
.B \-\-dir 
option. 
Depending  on the number of partitions. the number of subdirectory
levels underneath the base directory  is automatically  chosen  to
balance  the  directory filling.  
.\"
.\" The options below no longer work for time being
.\"
.\".TP
.\".B noop 
.\"can only be used for the 
.\".B One2One 
.\"partitioning scheme, where each partition consists of one compute
.\"job. During the megadag generation, each partition is expanded to a
.\"linear sequence of noop jobs. The noop jobs are overwritten when the
.\"concrete planner is invoked as a prescript to the first job in the
.\"sequence. 
.\".TP
.\".B daglite
.\"can only be used for the 
.\".B One2One
.\"partitioning scheme, where each partition consists of one compute
.\"job. In this mode, each partition is mapped to a daglite job. The
.\"daglite job maintains the linear dependencies between the concrete
.\"jobs created for the single node partition. The concrete jobs 
.\"(create dir, stagein, compute, stageout, registration) are created
.\"when the concrete workflow is invoked as a prescript to the daglite
.\"job.  
.TP
.PD 1
.B \-\-monitor
.IP
This results in the invocation of a monitoring daemon (tailstatd),
that parses the condor log files, and maintains the state of the
workflow in a database. It is still an experimental feature. The user
still needs to do condor_submit_dag manually, to actually submit the
workflow after gencdag has been run successfully.
.TP
.B \-o \fIoutput site
.PD 0
.TP
.PD 1
.B \-\-o \fIoutput site
The
.B output
site where all the materialized data is transferred to.
.IP
By default the
.B materialized data
remains in the working directory on the
.B execution
site where it was created. Only those output files are transferred to
an
output site for which the transiency attribute (dT) is set to false in
the DAX.
.TP
.B \-p \fIlist of execution sites
.PD 0
.TP
.PD 1
.B \-\-pools \fIlist of execution sites
A comma separated list of execution sites on which the workflow is to be
executed. Each of the sites should have an entry in the site catalog,
that is being used. To run on the submit host, specify the execution
site as 
.B local
.IP
In case this option is not specified, all the sites in the site
catalog are picked up as candidates for running the workflow.
.TP
.PD 0
.B \-r\fI[dirname]
.TP
.PD 1
.B \-\-randomdir\fI[=dirname]
This options results in addition of create dir jobs to the
workflow, corresponding to each remote execution site where parts of
the workflow have been scheduled. The create dir jobs create a
directory in the workdirectory (specified in the site catalog with
each site) on the remote execution sites. All the jobs of the workflow
are then run in the corresponding directory that has been created.  
.IP
By default, a random timestamp based name is generated for the remote
directory being created. However, the user can specify the optional
argument to this option to specify the basename of the directory that
is to be created.
.IP
The create dir jobs refer to the 
.B dirmanager
executable that is shipped as part of the VDS worker package. The
transformation catalog is searched for the transformation named
.B dirmanager 
for all the remote sites where the workflow has been scheduled. 
.TP
.B \-s
.PD 0
.TP
.PD 1
.B \-\-submit
Whether to submit the generated 
.B Condor
submit files to the underlying CondorG using the 
.B kickstart-condor
script in $VDS_HOME/bin directory.
.IP
By default, the Concrete Planner only generates the Condor submit
files and does not submit them.  In the near future, 
.B $VDS_HOME/bin/vds-submit-dag 
will be used to submit to CondorG.
.TP
.B \-v
.PD 0
.TP
.PD 1
.B \-\-verbose
increases the verbosity of messages about what is going on.
.IP
By default, all FATAL ERROR, ERROR , WARNINGS and INFO messages are
logged.
.TP
.B \-h
.PD 0
.TP
.PD 1
.B \-\-help
Displays all the options to the
.B gencdag
command.
.TP
.B \-V
.PD 0
.TP
.PD 1
.B \-\-version
Displays teh current version number of the Griphyn Virtual Data System
Software.
.SH "RETURN VALUE"
If the concrete planner is successfully able to produce a concretized
workflow, the exitcode will be 0. All runtime errors result in an
exitcode of 1. This is usually in the case when you have misconfigured
your catalogs etc. In the case of an error occuring while loading a
specific module implementation at run time, the exitcode will be
2. This is usually due to factory methods failing while loading a
module.  In case of any other error occuring during the running of the
command, the exitcode will be 1. In most cases, the error message
logged should give a clear indication as to where things went wrong.
.SH "PEGASUS PROPERTIES"
This is not an exhaustive list of properties used. For the complete
description and list of properties refer to 
.B $VDS_HOME/etc/sample.properties.
.TP
.B vds.site.selector
Identifies what type of site selector you want to use. If not
specified the default value of 
.B Random
is used. Other supported modes are 
.B RoundRobin
and 
.B NonJavaCallout
that calls out to a external site selector.
.TP
.B vds.transfer
Names the transfer backend to use. 
.IP
If not present, the default value 
.B multiple
is assumed. 
.TP
.B vds.rc
Specifies the type of replica catalog to be used. 
.IP
At present only one that corresponds to the Replica Location Service (
.B rls
) is supported in this distribution.
.TP
.B vds.rc.url
Contact string to access the replica catalog. In case of RLS it is the
RLI url.
.TP
.B vds.dir.exec
A suffix to the workdir in the site catalog to determine the current
working directory. If relative, the value will be appended to the
working directory from the pool.config file. If absolute it
constitutes the  working directory.
.TP 
.B vds.tc.file 
The location of file to use as transformation catalog.
.IP 
If not specified, then the default location of $VDS_HOME/var/tc.data
is used.
.TP 
.B vds.sc
Specifies the type of site catalog to be used. One can use either a
text based or an xml based site catalog.  At present the default is 
.B xml
.TP
.B vds.sc.file
The location of file to use as a site catalog.
If not specified, then default value of
$VDS_HOME/etc/sites.xml is used in case of the xml based site catalog
and $VDS_HOME/etc/sites.txt in case of the text based site catalog.
.SH FILES
.TP
.B $VDS_HOME/etc/vdl-1.21.xsd
is the suggested location of the latest XML schema to read the database.
.TP
.B $VDS_HOME/etc/dax-1.10.xsd
is the suggested location of the latest DAX schema to produce DAX
output.
.b $VDS_HOME/etc/gvds-poolcfg-1.4.xsd
is the suggested location of the latest pool config schema that is
used to create the xml version of the pool config file.
.TP
.B $VDS_HOME/var/tc.data
is the suggested location for the file corresponding to the 
.I Transformation Catalog
.TP
.B $VDS_HOME/etc/sites.xml | $VDS_HOME/etc/sites.txt
is the suggested location for the file containing the pool information.
.TP
.B gvds.jar
contains all compiled Java bytecode to run the Griphyn Virtual Data System.
.SH "ENVIRONMENT VARIABLES"
.TP
.B $VDS_HOME
is the suggested base directory of your the execution environment.
.TP
.B $JAVA_HOME
should be set and point to a valid location to start the intended Java
virtual machine as
.IR $JAVA_HOME/bin/java .
.TP
.B $CLASSPATH
should be set to contain all necessary files for the execution environment.
Please make sure that your 
.I CLASSPATH
includes pointer to the Xerces 2.0.1 classes to run this program.
.SH "SEE ALSO"
.BR gendax(1)
.BR partitiondax(1)
.BR vds-get-sites(1)
.BR tc-client(1)
.BR rc-client(1)
.SH RESTRICTIONS
Plenty. Read the user guide carefully.
.SH AUTHORS
Karan Vahi    <vahi at isi dot edu>
.br
Ewa Deelman   <deelman at isi dot edu>
.br
Gaurang Mehta <gmehta at isi dot edu>
.PP
GriPhyN Virtual Data System -
.B http://vds.isi.edu
.br
Pegasus                     -
.B http://pegasus.isi.edu
.br
GriPhyN                     -
.B http://www.griphyn.org/ 
