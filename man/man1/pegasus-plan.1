.\"  Copyright 2010-2011 University Of Southern California
.\"
.\" Licensed under the Apache License, Version 2.0 (the "License");
.\" you may not use this file except in compliance with the License.
.\" You may obtain a copy of the License at
.\"
.\"  http://www.apache.org/licenses/LICENSE-2.0
.\"
.\"  Unless required by applicable law or agreed to in writing,
.\"  software distributed under the License is distributed on an "AS IS" BASIS,
.\"  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
.\"  See the License for the specific language governing permissions and
.\" limitations under the License.
.\"
.\"
.\" $Id$
.\"
.\" Authors: Ewa Deelman, Gaurang Mehta, Karan Vahi
.\"
.TH "pegasus-plan" "1" "2.2.0" "PEGASUS Workflow Planner"
.SH NAME
pegasus-plan \- runs Pegasus to generate the executable workflow
.SH SYNOPSIS
.B pegasus-plan
[\-Dprop [..]] \-d|\-P <dax file|pdax file> [\-s site[,site[..]]]
[\-b prefix][\-c f1[,f2[..]]] [\-f] [\-j job-prefix][\-m style] [\-a]
[\-C] [\-D <base directory for o/p files>] [\-o <output site>] [\-r[directory
name]] [\--relative-dir <relative directory to base directory>] [\-s] [\-v] [\-V] [\-h]  
.SH DESCRIPTION
The 
.B pegasus-plan
command takes in as input the DAX and generates an executable workflow
usually in form of 
.B condor
submit files, which 
can be submitted to an 
.I execution
site for execution.
.PP
The Pegasus Workflow Planner ensures that all the data  required
for the execution of the executable workflow on the execution site, is
transferred to it by adding transfer nodes at appropriate points in the dag.
This is done by looking up an appropriate 
.B Replica Catalog
to determine the locations of the input files for the various jobs.
At present the default replica mechanism used is 
.B RLS
.PP
The Pegasus Workflow Planner also tries to reduce the workflow, unless
specified otherwise.  
This is done  by deleting the jobs whose
output files have been found in some location in the Replica Catalog
(RLS).  At present no cost metrics are used. However preference
is given to a location corresponding to the execution site.
.PP
The planner can also add nodes to transfer all the 
.B materialized
files to an 
.B output site.
The location on the output site is determined by looking up the 
.B site catalog
file, the path to which is picked up from the 
.I pegasus.catalog.site.file 
property value.
.SH ARGUMENTS
Any option will be displayed with its long options synonym(s).
.TP
.B \-Dprop
The -D options allows an experienced user to override certain
properties
which influence the program execution, among them the default location
of
the user's properties file and the PEGASUS home location. One may set
several
CLI properties by giving this option multiple times.
.I The -D option(s) must be the first option on the command line. 
A CLI property take precedence over the properties file property 
of the same key.
.TP
.B \-d|\-P \fIfilename
.PD 0
.TP
.PD 1
.B \-\-dax|\-\-pdax \fIfilename
The DAX is the XML input file that describles an
.B abstract 
workflow.
The PDAX file describe a partition graph that is
generated by running 
.B partition-dax 
on the abstract workflow (DAX).
.IP
This is a mandatory option, which has to be used.

.\".TP
.\".B \-a
.\".PD 0
.\".TP
.\".PD 1
.\".B \-\-authenticate
.\"This option results in authentication tests being performed against
.\"the remote jobmanagers and the gridftp servers for the site that are
.\"being used to map the workflow. The jobmanagers and gridftp servers
.\"against which the authentication tests fail, are not used to run jobs
.\"or transfer data respectively. In case of a site being associated with
.\"muliple jobmanagers and gridftp servers, only the bad ones are purged
.\"from the mapping process.
.\".IP
.\"At present for the gridftp servers no gsi authentication is
.\"done. However a check alive test is done by opening a socket to the
.\"server and determining if it a ftp server or not.  By default a
.\"timeout of 120 seconds is used for the socket. The timeout value in
.\"seconds can be overriden by specifying the property 
.\".I pegasus.auth.gridftp.timeout.

.TP
.B \-b \fIprefix
.PD 0
.TP
.PD 1
.B \-\-basename \fIprefix
The basename prefix to be used while constructing per workflow files
like the dagman file (.dag file) and other workflow specific files
that are created by Condor. Usually this prefix, is taken from the
name attribute specified in the root element of the dax|pdax files.
.TP
.B \-c \fIlist of cache files
.PD 0
.TP
.PD 1
.B \-\-cache \fIlist of cache files
A comma separated list of paths to replica cache files that override
the results from the replica catalog for a particular lfn.
.IP 
Each entry in the cache file describes a LFN , the corresponding PFN and
the associated attributes. The pool attribute should be specified for
each entry.
.nf
\f(CB
LFN_1 PFN_1 pool=[site handle 1] 
LFN_2 PFN_2 pool=[site handle 2]
 ...
LFN_N PFN_N [site handle N]
\fP
.fi
.IP
To treat the cache files as supplemental replica catalogs set the
property 
.I pegasus.catalog.replica.cache.asrc 
to true. This results in the mapping in the cache files to be merged
with the mappings in the replica catalog. Thus, for a particular lfn
both the entries in the cache file and replica catalog are available
for replica selection.
.TP
.B \-C
.PD 0
.TP
.PD 1
.B \-\-cluster \fI comma separated list of clustering styles.
This mode of operation results in clustering of n compute jobs into a
larger jobs to reduce remote scheduling overhead. You can specify a
list of clustering techniques to recursively apply them to the
workflow. For example, this allows you to cluster some jobs in the
workflow using horizontal clustering and then use label based
clustering on the intermediate workflow to do vertical clustering.
.IP
The clustered jobs can be run at the remote site, either sequentially
or by using mpi. This can be specified by setting the property
.I pegasus.job.aggregator.
The property can be overriden by associating the PEGASUS profile key
.I collapser
either with the transformation in the tranformation catalog or the
execution site in the site catalog. The value specified (to the
property or the profile), is the logical name of the transformation
that is to be used for clustering jobs. Note that clustering will only
happen if the corresponding transformations are catalogued in the
transformation catalog.
.IP
PEGASUS is shipped with a clustering executable
.I seqexec
that can be found in
.I $PEGASUS_HOME/bin
directory. It runs the jobs in the clustered job sequentially on the
same node at the remote site.
.IP
In addition, a mpi wrapper
.I mpiexec
is distributed as source with the PEGASUS. It can be found in
$PEGASUS_HOME/src/tools/cluster
directory. The wrapper is run on every mpi node, with the first one
being the master and the rest of the ones as workers. The number of
instances of mpiexec that are invoked is equal to the value of the
globus rsl key nodecount. The master distributes the smaller
constituent jobs to the workers. For e.g. If there were 10 jobs in the
clustered job and nodecount was 5, then one node acts as master, and
the
10 jobs are distributed amongst the 4 slaves on demand.  The master
hands off a job to the slave node as and when it gets free. So
initially all the 4 nodes are given a single job each, and then as and
when they get done are handed more jobs till all the 10 jobs have been
executed.
.IP
By default, seqexec is used for clustering jobs unless overriden
in the properties or by the pegasus profile key collapser.
.IP
The following type of clustering styles are currently supported
.TP
.B horizontal
is the style of clustering in which jobs on the same level are
aggregated into larger jobs. A level of the workflow is defined as the
greatest distance of a node, from the root of the workflow. Clustering
occurs only on jobs of the same type i.e they refer to the same
logical transformation in the transformation catalog. 
.IP
The granularity of clustering can be specified by associating either
the PEGASUS profile key 
.I collapse
or the PEGASUS profile key
.I bundle
with the transformation.
The collapse key indicates how many jobs need to be clustered into
the larger clustered job. The bundle key indicates how many clustered
jobs are to be created for a particular level at a particular
execution site. If both keys are specified for a particular
transformation, then the bundle key value is used to determine the
clustering granularity.
.TP
.B label
is the style of clustering in which you can label the jobs in your
workflow. The jobs with the same level are put in the same clustered
job. This allows you to aggregate jobs across levels, or in a manner
that is best suited to your application.
.IP
To label the workflow, you need to associate PEGASUS profiles with the
jobs in the DAX. The profile key to use for labelling the workflow can
be set by the property 
.I pegasus.clusterer.label.key.
It defaults to label, meaning if you have a PEGASUS profile key label
with jobs, the jobs with the same label will go into the same
clustered job. 

.TP
.B \-D \fIdir name
.PD 0
.TP
.PD 1
.B \--dir \fIdir name
The base directory where you want the output of the Pegasus Workflow
Planner usually condor submit files, to be generated. Pegasus creates
a directory structure in this base directory on the basis of username,
VO Group and the label of the workflow in the DAX.
.IP
By default the base directory is the directory from which one runs the
.B pegasus-plan
command.

.TP
.PD 0
.B \--relative-dir \fIdir name
The directory relative to the base directory where the executable
workflow it to be generated. This overrides the default directory
structure that Pegasus creates based on username, VO Group and the DAX
label.

.TP
.B \-f
.PD 0
.TP
.PD 1
.B \-\-force
This bypasses the reduction phase in which the abstract DAG is
reduced, on the basis of the locations of the output files returned by
the replica catalog. This is analogous to a 
.B make style
generation of the executable workflow.

.TP
.B \-g
.PD 0
.TP
.PD 1
.B \-\-group
The VO Group to which the user belongs to.

.TP
.B \-j
.PD 0
.TP
.PD 1
.B \-\-job-prefix
The job prefix to be applied for constructing the filenames for the
job submit files.


.TP
.B \-m
.PD 0
.TP
.PD 1
.B \-\-megadag \fIstyle
In case of deferred planning, a megadag(outer level dag) is run to
maintain the dependencies between the various partitions. The
dependencies between the partitions are specfied in an XML file
conforming to the pdax format.  The pdax file is created while
partitioning the dax using 
.B partitiondax client.
This option indicates how the dependencies between the jobs in a
partition are maintained, so that jobs are executed in the right order. 
.TP
.B dag 
is the default style and the only one supported currently. This
results in dagman instance being invoked for each partition. The
dagman instance launched for each job is responsible for maintaining
the dependencies between the jobs in a partition. This works well for
cases, where there are a sufficient number of jobs in a partition, as
customarily created by level based (BFS) partitioning or a label based
partitioning. 
.IP
The submit directory for each partition resides in a
separate submit directory under the base directory specified by the 
.B \-\-dir 
option. 
Depending  on the number of partitions. the number of subdirectory
levels underneath the base directory  is automatically  chosen  to
balance  the  directory filling.  
.\"
.\" The options below no longer work for time being
.\"
.\".TP
.\".B noop 
.\"can only be used for the 
.\".B One2One 
.\"partitioning scheme, where each partition consists of one compute
.\"job. During the megadag generation, each partition is expanded to a
.\"linear sequence of noop jobs. The noop jobs are overwritten when the
.\"concrete planner is invoked as a prescript to the first job in the
.\"sequence. 
.\".TP
.\".B daglite
.\"can only be used for the 
.\".B One2One
.\"partitioning scheme, where each partition consists of one compute
.\"job. In this mode, each partition is mapped to a daglite job. The
.\"daglite job maintains the linear dependencies between the concrete
.\"jobs created for the single node partition. The concrete jobs 
.\"(create dir, stagein, compute, stageout, registration) are created
.\"when the concrete workflow is invoked as a prescript to the daglite
.\"job.  
.TP
.PD 1
.B \-\-monitor
.IP
This results in the invocation of a monitoring daemon (tailstatd),
that parses the condor log files, and maintains the state of the
workflow in a database. It is still an experimental feature. The user
still needs to do condor_submit_dag manually, to actually submit the
workflow after pegasus-plan has been run successfully.

.TP
.B \-n 
.PD 0
.TP
.PD 1
.B \-\-nocleanup
.IP
This results in the generation of the separate cleanup workflow that
removes the directories created during the execution of the executable
workflow. The cleanup workflow is to be submitted after the executable
workflow has finished. 
If this option is not specified, then Pegasus adds cleanup nodes to
the executable workflow itself that cleanup files on the remote sites
when they are no longer required.

.TP
.B \-o \fIoutput site
.PD 0
.TP
.PD 1
.B \-\-o \fIoutput site
The
.B output
site where all the materialized data is transferred to.
.IP
By default the
.B materialized data
remains in the working directory on the
.B execution
site where it was created. Only those output files are transferred to
an
output site for which the transiency attribute (dT) is set to false in
the DAX.
.TP
.B \-p \fIlist of execution sites
.PD 0
.TP
.PD 1
.B \-\-sites \fIlist of execution sites
A comma separated list of execution sites on which the workflow is to be
executed. Each of the sites should have an entry in the site catalog,
that is being used. To run on the submit host, specify the execution
site as 
.B local
.IP
In case this option is not specified, all the sites in the site
catalog are picked up as candidates for running the workflow.
.TP
.PD 0
.B \-r\fI[dirname]
.TP
.PD 1
.B \-\-randomdir\fI[=dirname]
Pegasus Worfklow Planner adds create directory jobs to the executable
workflow that create a directory in which all jobs for that workflow
execute on a particular site. The directory created is in the working
directory (specified in the site catalog with each site). 
.IP
By default, Pegasus duplicates the relative directory structure on the
submit host on the remote site. The user can specify this option
without arguments to create a random timestamp based name for the
execution directory that are created by the create dir jobs.
The user can can specify the optional argument to this option to
specify the basename of the directory that is to be created.
.IP
The create dir jobs refer to the 
.B dirmanager
executable that is shipped as part of the PEGASUS worker package. The
transformation catalog is searched for the transformation named
.B pegasus::dirmanager 
for all the remote sites where the workflow has been
scheduled. Pegasus can create a default path for the dirmanager
executable, if 
.B PEGASUS_HOME
environment variable is associated with the sites in the site catalog
as an environment profile.
.TP
.B \-s
.PD 0
.TP
.PD 1
.B \-\-submit
submit the generated 
.B executable workflow
using 
.B pegasus-run
script in $PEGASUS_HOME/bin directory.
.IP
By default, the Pegasus Workflow Planner only generates the Condor submit
files and does not submit them. 
.TP
.B \-v
.PD 0
.TP
.PD 1
.B \-\-verbose
increases the verbosity of messages about what is going on.
.IP
By default, all FATAL ERROR, ERROR , WARNINGS and INFO messages are
logged.
.TP
.B \-h
.PD 0
.TP
.PD 1
.B \-\-help
Displays all the options to the
.B pegasus-plan
command.
.TP
.B \-V
.PD 0
.TP
.PD 1
.B \-\-version
Displays the current version number of the  Pegasus Workflow Planner
Software.
.SH "RETURN VALUE"
If the Pegasus Workflow Planner is successfully able to produce a concretized
workflow, the exitcode will be 0. All runtime errors result in an
exitcode of 1. This is usually in the case when you have misconfigured
your catalogs etc. In the case of an error occuring while loading a
specific module implementation at run time, the exitcode will be
2. This is usually due to factory methods failing while loading a
module.  In case of any other error occuring during the running of the
command, the exitcode will be 1. In most cases, the error message
logged should give a clear indication as to where things went wrong.
.SH "PEGASUS PROPERTIES"
This is not an exhaustive list of properties used. For the complete
description and list of properties refer to 
.B $PEGASUS_HOME/etc/sample.properties.
.TP
.B pegasus.selector.site
Identifies what type of site selector you want to use. If not
specified the default value of 
.B Random
is used. Other supported modes are 
.B RoundRobin
and 
.B NonJavaCallout
that calls out to a external site selector.
.TP
.B pegasus.transfer.refiner
Names the transfer refiner to use. 
.TP
.B pegasus.catalog.replica
Specifies the type of replica catalog to be used. 
.IP
If not specified, then the value defaults to 
.B RLS
.
.TP
.B pegasus.catalog.replica.url
Contact string to access the replica catalog. In case of RLS it is the
RLI url.
.TP
.B pegasus.dir.exec
A suffix to the workdir in the site catalog to determine the current
working directory. If relative, the value will be appended to the
working directory from the site.config file. If absolute it
constitutes the  working directory.
.TP 
.B pegasus.catalog.transformation
Specifies the type of transformation catalog to be used. One can use either a
file based or a database based transformation catalog.  At present the
default is  
.B File
.TP 
.B pegasus.catalog.transformation.file 
The location of file to use as transformation catalog.
.IP 
If not specified, then the default location of $PEGASUS_HOME/var/tc.data
is used.
.TP 
.B pegasus.catalog.site
Specifies the type of site catalog to be used. One can use either a
text based or an xml based site catalog.  At present the default is 
.B xml
.TP
.B pegasus.catalog.site.file
The location of file to use as a site catalog.
If not specified, then default value of
$PEGASUS_HOME/etc/sites.xml is used in case of the xml based site catalog
and $PEGASUS_HOME/etc/sites.txt in case of the text based site catalog.
.SH FILES
.TP
.B $PEGASUS_HOME/etc/vdl-1.21.xsd
is the suggested location of the latest XML schema to read the database.
.TP
.B $PEGASUS_HOME/etc/dax-1.10.xsd
is the suggested location of the latest DAX schema to produce DAX
output.
.b $PEGASUS_HOME/etc/pegasus-sitecfg-1.4.xsd
is the suggested location of the latest site config schema that is
used to create the xml version of the site config file.
.TP
.B $PEGASUS_HOME/var/tc.data
is the suggested location for the file corresponding to the 
.I Transformation Catalog
.TP
.B $PEGASUS_HOME/etc/sites.xml | $PEGASUS_HOME/etc/sites.txt
is the suggested location for the file containing the site information.
.TP
.B pegasus.jar
contains all compiled Java bytecode to run the Griphyn PEGASUS Planner.
.SH "ENVIRONMENT VARIABLES"
.TP
.B $PEGASUS_HOME
is the suggested base directory of your the execution environment.
.TP
.B $JAVA_HOME
should be set and point to a valid location to start the intended Java
virtual machine as
.IR $JAVA_HOME/bin/java .
.TP
.B $CLASSPATH
should be set to contain all necessary files for the execution environment.
Please make sure that your 
.I CLASSPATH
includes pointer to the Xerces 2.0.1 classes to run this program.
.SH "SEE ALSO"
.BR partitiondax(1)
.BR pegasus-get-sites(1)
.BR tc-client(1)
.BR rc-client(1)
.SH RESTRICTIONS
Plenty. Read the user guide carefully.
.SH AUTHORS
Karan Vahi    <vahi at isi dot edu>
.br
Ewa Deelman   <deelman at isi dot edu>
.br
Gaurang Mehta <gmehta at isi dot edu>
.PP
Pegasus Workflow Planner -
.B http://pegasus.isi.edu

