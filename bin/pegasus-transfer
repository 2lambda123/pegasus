#!/usr/bin/env python

"""
Pegasus utility for transfer of files during a workflow enactment

Usage: pegasus-transfer [options]

"""

##
#  Copyright 2007-2010 University Of Southern California
#
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing,
#  software distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
##

import os
import re
import sys
import errno
import logging
import optparse
import tempfile
import subprocess
import signal
import string
import stat

# --- regular expressions -------------------------------------------------------------

re_parse_url = re.compile(r'([\w]+)://([\w\.\-:@]*)(/[\S]*)')

# --- classes -------------------------------------------------------------------------

class URLPair:

    pair_id   = 0       # the id of the pair in the input, the nth pair in the input
    src_proto = ""      # 
    src_host  = ""      # 
    src_path  = ""      #
    dst_proto = ""      #
    dst_host  = ""      #
    dst_path  = ""      #

    def __init__(self, pair_id):
        """
        Initializes the Job class, setting job name,
        and state, if provided
        """
        self.pair_id = pair_id

    def set_src(self, url):
        self.src_proto, self.src_host, self.src_path = self.parse_url(url)
    
    def set_dst(self, url):
        self.dst_proto, self.dst_host, self.dst_path = self.parse_url(url)
    
    def parse_url(self, url):
        proto = ""
        host = ""
        path = ""

        # default protocol is file://
        if string.find(url, ":") == -1:
            logger.debug("URL without protocol (" + url + ") - assuming file://")
            url = "file://" + url

        # file url is a special cases as it can contain relative paths and env vars
        if string.find(url, "file:") == 0:
            proto = "file"
            # file urls can either start with file://[\w]*/ or file: (no //)
            path = re.sub("^file:(//[\w\.\-:@]*)?", "", url)
            path = expand_env_vars(path)
            # relative path?
            if string.find(path, "/") != 0:
                path = os.getcwd() + "/" + path
            return proto, host, path
        
        # symlink url is a special cases as it can contain relative paths and env vars
        if string.find(url, "symlink:") == 0:
            proto = "symlink"
            # symlink urls can either start with symlink://[\w]*/ or symlink: (no //)
            path = re.sub("^symlink:(//[\w\.\-:@]*)?", "", url)
            path = expand_env_vars(path)
            # relative path?
            if string.find(path, "/") != 0:
                path = os.getcwd() + "/" + path
            return proto, host, path

        # other than file:// urls
        r = re_parse_url.search(url)
        if r:
            # Parse successful
            proto = r.group(1)
            host = r.group(2)
            path = r.group(3)
            return proto, host, path
        else:
            raise RuntimeError("Unable to parse URL: %s" % (url))

    def src_url(self):
        return "%s://%s%s" % (self.src_proto, self.src_host, self.src_path)
    
    def dst_url(self):
        return "%s://%s%s" % (self.dst_proto, self.dst_host, self.dst_path)

    def __cmp__(self, other):
        """
        compares first on protos, then on hosts, then on paths - useful
        for grouping similar types of transfers
        """
        if cmp(self.src_proto, other.src_proto) != 0:
            return cmp(self.src_proto, other.src_proto)
        if cmp(self.dst_proto, other.dst_proto) != 0:
            return cmp(self.dst_proto, other.dst_proto)
        if cmp(self.src_host, other.src_host) != 0:
            return cmp(self.src_host, other.src_host)
        if cmp(self.dst_host, other.dst_host) != 0:
            return cmp(self.dst_host, other.dst_host)
        if cmp(self.src_path, other.src_path) != 0:
            return cmp(self.src_path, other.src_path)
        if cmp(self.dst_path, other.dst_path) != 0:
            return cmp(self.dst_path, other.dst_path)
        return 0


class Alarm(Exception):
    pass


# --- global variables ----------------------------------------------------------------

prog_base = os.path.split(sys.argv[0])[1]   # Name of this program

logger = logging.getLogger("my_logger")

# delay errors to allow all transfers to be tried before exiting
delays_errors = False
delay_exit_code = 0

# this is the map of what tool to use for a given protocol pair (src, dest)
tool_map = {}
tool_map[('file'  , 'file'    )] = 'cp'
tool_map[('file'  , 'gsiftp'  )] = 'gsiftp'
tool_map[('file'  , 'scp'     )] = 'scp'
tool_map[('file'  , 'srm'     )] = 'srm'
tool_map[('file'  , 'symlink' )] = 'symlink'
tool_map[('gsiftp', 'file'    )] = 'gsiftp'
tool_map[('gsiftp', 'gsiftp'  )] = 'gsiftp'
tool_map[('gsiftp', 'srm'     )] = 'srm'
tool_map[('http'  , 'file'    )] = 'webget'
tool_map[('http'  , 'gsiftp'  )] = 'gsiftp'
tool_map[('https' , 'file'    )] = 'webget'
tool_map[('scp'   , 'file'    )] = 'scp'
tool_map[('srm'   , 'gsiftp'  )] = 'srm'
tool_map[('srm'   , 'srm'     )] = 'srm'

tool_info = {}

all_list = []
cp_list = []
symlink_list = []
scp_list = []
webget_list = []
gsiftp_list = []
srm_list = []

# --- functions -----------------------------------------------------------------------


def setup_logger(level_str):
    
    # log to the console
    console = logging.StreamHandler()
    
    # default log level - make logger/console match
    logger.setLevel(logging.INFO)
    console.setLevel(logging.INFO)

    # level - from the command line
    level_str = level_str.lower()
    if level_str == "debug":
        logger.setLevel(logging.DEBUG)
        console.setLevel(logging.DEBUG)
    if level_str == "warning":
        logger.setLevel(logging.WARNING)
        console.setLevel(logging.WARNING)
    if level_str == "error":
        logger.setLevel(logging.ERROR)
        console.setLevel(logging.ERROR)

    # formatter
    formatter = logging.Formatter("%(asctime)s %(levelname)7s:  %(message)s")
    console.setFormatter(formatter)
    logger.addHandler(console)
    logger.debug("Logger has been configured")


def alarm_handler(signum, frame):
    raise Alarm


def expand_env_vars(s):
    re_env_var = re.compile(r'\${?([a-zA-Z0-9_]+)}?')
    s = re.sub(re_env_var, get_env_var, s)
    return s


def get_env_var(match):
    name = match.group(1)
    value = ""
    logger.debug("Looking up " + name)
    if name in os.environ:
        value = os.environ[name]
    return value


def myexec(cmd_line, timeout_secs, delay_err):
    """
    executes shell commands using with timeouts
    """
    global delay_exit_code
    logger.info("Executing: " + cmd_line)
    sys.stdout.flush()

    # set up signal handler for timeout
    signal.signal(signal.SIGALRM, alarm_handler)
    signal.alarm(timeout_secs)

    p = subprocess.Popen(cmd_line + " 2>&1", shell=True)
    try:
        stdoutdata, stderrdata = p.communicate()
    except Alarm:
        p.terminate()
        raise RuntimeError("Command '%s' timed out after %s seconds" % (cmd_line, timeout_secs))
    rc = p.returncode
    if rc != 0:
        if delay_err:
            logger.warning("Command '%s' failed with error code %s. Continuing anyways due to --delay-errors." % (cmd_line, rc))
            delay_exit_code = 3
        else:
            raise RuntimeError("Command '%s' failed with error code %s" % (cmd_line, rc))


def backticks(cmd_line):
    return subprocess.Popen(cmd_line, shell=True, stdout=subprocess.PIPE).communicate()[0]


def check_tool(executable, version_arg, version_regex):
    # initialize the global tool info for this executable
    tool_info[executable] = {}
    tool_info[executable]['full_path'] = None
    tool_info[executable]['version'] = None
    tool_info[executable]['version_major'] = None
    tool_info[executable]['version_minor'] = None
    tool_info[executable]['version_patch'] = None

    # figure out the full path to the executable
    full_path = backticks("which " + executable + " 2>/dev/null") 
    full_path = full_path.rstrip('\n')
    if full_path == "":
        logger.warn("  %-18s Not found (but maybe not required)" %(executable))
        return
    tool_info[executable]['full_path'] = full_path

    # version
    version = backticks(executable + " " + version_arg + " 2>&1")
    version = version.replace('\n', "")
    re_version = re.compile(version_regex)
    result = re_version.search(version)
    if result:
        version = result.group(1)
    tool_info[executable]['version'] = version

    # if possible, break up version into major, minor, patch
    re_version = re.compile("([0-9]+)\.([0-9]+)(\.([0-9]+)){0,1}")
    result = re_version.search(version)
    if result:
        tool_info[executable]['version_major'] = int(result.group(1))
        tool_info[executable]['version_minor'] = int(result.group(2))
        tool_info[executable]['version_patch'] = result.group(4)
    if tool_info[executable]['version_patch'] == None or tool_info[executable]['version_patch'] == "":
        tool_info[executable]['version_patch'] = None
    else:
         tool_info[executable]['version_patch'] = int(tool_info[executable]['version_patch'])

    logger.info("  %-18s Version: %-7s Path: %s" % (executable, version, full_path))


def check_env_and_tools():
    
    # PATH setup
    path = "/usr/bin:/bin"
    if "PATH" in os.environ:
        path = os.environ['PATH']
    path_entries = path.split(':')
    
    # is /usr/bin in the path?
    if not("/usr/bin" in path_entries):
        path_entries.append("/usr/bin")
        path_entries.append("/bin")
       
    # need LD_LIBRARY_PATH for Globus tools
    ld_library_path = ""
    if "LD_LIBRARY_PATH" in os.environ:
        ld_library_path = os.environ['LD_LIBRARY_PATH']
    ld_library_path_entries = ld_library_path.split(':')
    
    # if PEGASUS_HOME is set, prepend it to the PATH (we want it early to override other cruft)
    if "PEGASUS_HOME" in os.environ:
        try:
            path_entries.remove(os.environ['PEGASUS_HOME'] + "/bin")
        except Exception:
            pass
        path_entries.insert(0, os.environ['PEGASUS_HOME'] + "/bin")
    
    # if GLOBUS_LOCATION is set, prepend it to the PATH and LD_LIBRARY_PATH 
    #(we want it early to override other cruft)
    if "GLOBUS_LOCATION" in os.environ:
        try:
            path_entries.remove(os.environ['GLOBUS_LOCATION'] + "/bin")
        except Exception:
            pass
        path_entries.insert(0, os.environ['GLOBUS_LOCATION'] + "/bin")
        try:
            ld_library_path_entries.remove(os.environ['GLOBUS_LOCATION'] + "/lib")
        except Exception:
            pass
        ld_library_path_entries.insert(0, os.environ['GLOBUS_LOCATION'] + "/lib")

    os.environ['PATH'] = ":".join(path_entries)
    os.environ['LD_LIBRARY_PATH'] = ":".join(ld_library_path_entries)
    os.environ['DYLD_LIBRARY_PATH'] = ":".join(ld_library_path_entries)
    logger.info("PATH=" + os.environ['PATH'])
    logger.info("LD_LIBRARY_PATH=" + os.environ['PATH'])
    
    # tools we might need later
    check_tool("globus-version", "--full", "([0-9]+\.[0-9]+\.[0-9]+)")
    check_tool("globus-url-copy", "-version", "([0-9]+\.[0-9]+)")
    check_tool("uberftp", "-version", "([0-9]+\.[0-9]+)")
    check_tool("wget", "--version", "([0-9]+\.[0-9]+)")
    check_tool("lcg-cp", "--version", "lcg_util-([0-9]+\.[0-9]+\.[0-9]+\-[0-9]+)")
    #check_tool("s3cmd", "--version", "([0-9]+\.[0-9]+\.[0-9]+)")


def prepare_local_dir(path):
    """
    makes sure a local path exists before putting files into it
    """
    if not(os.path.exists(path)):
        os.makedirs(path, 0755)


def cp():
    """
    copies locally using /bin/cp
    """
    for i, url_pair in enumerate(cp_list): 
        prepare_local_dir(os.path.dirname(url_pair.dst_path))
        cmd = "/bin/cp %s %s" % (url_pair.src_path, url_pair.dst_path)
        myexec(cmd, 1*60*60, delay_errors)

def symlink():
    """
    symlinks locally using ln
    """
    global delay_exit_code
    for i, url_pair in enumerate(symlink_list): 
        prepare_local_dir(os.path.dirname(url_pair.dst_path))

        # we do not allow dangling symlinks
        if not os.path.exists(url_pair.src_path):
            if delay_errors:
                logger.warning("Symlink source (%s) does not exist. Continuing anyways due to --delay-errors." % (url_pair.src_path))
                delay_exit_code = 4
            else:
                raise RuntimeError("Symlink source (%s) does not exist" % (url_pair.src_path))
            continue

        if os.path.exists(url_pair.src_path) and os.path.exists(url_pair.dst_path):
            # make sure src and target are not the same file - have to compare at the
            # inode level as paths can differ
            src_inode = os.stat(url_pair.src_path)[stat.ST_INO]
            dst_inode = os.stat(url_pair.dst_path)[stat.ST_INO]
            if src_inode == dst_inode:
                logger.warning("symlink: src (%s) and dst (%s) already exists" % (url_pair.src_path, url_pair.dst_path))
                continue

        cmd = "ln -f -s %s %s" % (url_pair.src_path, url_pair.dst_path)
        myexec(cmd, 60, delay_errors)


def scp():
    """
    copies using scp
    """
    for i, url_pair in enumerate(scp_list): 
        cmd = "/usr/bin/scp"
        if url_pair.dst_proto == "file":
            prepare_local_dir(os.path.dirname(url_pair.dst_path))
            cmd += " " + url_pair.src_host + ":" + url_pair.src_path
            cmd += " " + url_pair.dst_path
        else:
            cmd += " " + url_pair.src_path
            cmd += " " + url_pair.dst_host + ":" + url_pair.dst_path
        myexec(cmd, 1*60*60, delay_errors)


def webget():
    """
    pulls http/https using wget
    """
    if len(webget_list) > 0 and tool_info['wget']['full_path'] == None:
        raise RuntimeError("Unable to do http/https transfers becuase wget could not be found")
    for i, url_pair in enumerate(webget_list): 
        prepare_local_dir(os.path.dirname(url_pair.dst_path))
        cmd = tool_info['wget']['full_path']
        if logger.isEnabledFor(logging.DEBUG):
            cmd += " -v"
        else:
            cmd += " -q"
        cmd += " --no-check-certificate -O " + url_pair.dst_path + " " + url_pair.src_url()
        myexec(cmd, 1*60*60, delay_errors)


def gsiftp_is_similar(a, b):
    """
    compares two url_pairs, and determins if they are similar enough to be
    grouped together in one transfer input file
    """
    if a.src_host != b.src_host:
        return False
    if a.dst_host != b.dst_host:
        return False
    if os.path.dirname(a.src_path) != os.path.dirname(b.src_path):
        return False
    if os.path.dirname(a.dst_path) != os.path.dirname(b.dst_path):
        return False
    return True


def gsiftp():
    """
    gsiftp - globus-url-copy for now, maybe uberftp in the future
    """
    if len(gsiftp_list) > 0 and tool_info['globus-url-copy']['full_path'] == None:
        raise RuntimeError("Unable to do gsiftp transfers becuase globus-url-copy could not be found")
    # reverse the list so we can append/pop i nthe right order
    gsiftp_list.reverse()
    while len(gsiftp_list) > 0:
        # create a tmp file with similar (same src host/path, same dst host/path)
        # url pairs
        try:
            tmp_fd, tmp_name = tempfile.mkstemp(prefix="pegasus-transfer-", suffix=".lst", dir="/tmp")
            tmp_file = os.fdopen(tmp_fd, "w+b")
        except:
            raise RuntimeError("Unable to create tmp file for globus-url-copy transfers")
        num_pairs = 0
        curr = gsiftp_list.pop()
        prev = curr
        third_party = curr.src_proto == "gsiftp" and curr.dst_proto == "gsiftp"
        while gsiftp_is_similar(curr, prev):
            logger.debug("   adding %s %s" % (curr.src_url(), curr.dst_url()))
            tmp_file.write("%s %s\n" % (curr.src_url(), curr.dst_url()))
            num_pairs += 1
            if len(gsiftp_list) == 0:
                break
            else:
                prev = curr
                curr = gsiftp_list.pop()
        if not(gsiftp_is_similar(curr, prev)):
            # the last pair is not part of the set and needs to be added back to the
            # beginning of the list
            gsiftp_list.append(curr)
        tmp_file.close()

        logger.info("Grouped %d similar gsiftp transfers together in temporary file %s"
                    %(num_pairs, tmp_name))

        # build command line for globus-url-copy
        cmd = tool_info['globus-url-copy']['full_path'];

        # make output from guc match our current log level
        if logger.isEnabledFor(logging.DEBUG):
            cmd += " -verbose"

        # always try to create directories
        cmd += " -create-dest"
        
        # restart is broken in 4.0.0 and 4.0.1 - see bug#3919
        if tool_info['globus-version']['version_major'] >= 4 \
           and not(tool_info['globus-version']['version_major'] == 4 and
                   tool_info['globus-version']['version_minor'] == 0 and
                   tool_info['globus-version']['version_patch'] <= 1):
            cmd += " -restart"

        # Only do third party transfers for gsiftp->gsiftp. For other combinations, fall
        # back to settings which will for well over for example NAT
        if third_party:
            cmd += " -parallel 4"
            # -fast only for Globus 4 and above
            if tool_info['globus-version']['version_major'] >= 4:
                cmd += " -fast"
        else:
            cmd += " -no-third-party-transfers -no-data-channel-authentication"

        cmd += " -f " + tmp_name
        try:
            myexec(cmd, 6*60*60, delay_errors)
        except Exception, err:
            # in case of failure, make sure we clean up
            os.unlink(tmp_name)
            raise err
        os.unlink(tmp_name)


def srm():
    """
    srm - use lcg-cp (Is this generic enough? Do we need to handle space tokens?)
    """
    if len(srm_list) > 0 and tool_info['lcg-cp']['full_path'] == None:
        raise RuntimeError("Unable to do srm transfers becuase lcg-cp could not be found")
    for i, url_pair in enumerate(srm_list): 
        if url_pair.dst_proto == "file":
            prepare_local_dir(os.path.dirname(url_pair.dst_path))
        cmd = ""
        # lcg-cp is mostly used on OSG, so look for $OSG_GRID and if found source setup.sh
        if "OSG_GRID" in os.environ:
            cmd = ". $OSG_GRID/setup.sh ;"
        cmd = "%s lcg-cp -D srmv2 -vb -b -n 4 %s %s" % (cmd, url_pair.src_url(), url_pair.dst_url())
        myexec(cmd, 6*60*60, delay_errors)


def myexit(rc):
    """
    system exit without a stack trace - silly python
    """
    try:
        sys.exit(rc)
    except SystemExit:
        sys.exit(rc)



# --- main ----------------------------------------------------------------------------

# dup stderr onto stdout
sys.stderr = sys.stdout

# Configure command line option parser
prog_usage = "usage: %s [options]" % (prog_base)
parser = optparse.OptionParser(usage=prog_usage)
parser.add_option("-l", "--loglevel", action = "store", dest = "log_level",
        help = "Log level. Valid levels are: debug,info,warning,error, Default is info.")
parser.add_option("-f", "--file", action = "store", dest = "file",
        help = "File containing URL pairs to be transferred. If not given, list is read from stdin.")
parser.add_option("-d", "--delay-errors", action = "store_true", dest = "delay_errors",
        default = False,
        help = "Continue with transfers even after one fails.")

# Parse command line options
(options, args) = parser.parse_args()
if options.log_level == None:
    options.log_level = "info"
setup_logger(options.log_level)
delay_errors = options.delay_errors

logger.debug("Checking environment and tools")
try:
    check_env_and_tools()
except Exception, err:
    logger.critical(err)
    myexit(1)

# stdin or file input?
if options.file == None:
    logger.info("Reading URL pairs from stdin")
    input_file = sys.stdin
else:
    logger.info("Reading URL pairs from %s" % (options.file))
    try:
        input_file = open(options.file, 'r')
    except Exception, err:
        logger.critical('Error reading url pair list: %s' % (err))
        myexit(1)

line_nr = 0
pair_nr = 0
url_first = True
try:
    for line in input_file.readlines():
        line_nr += 1
        if line[0] != '#' and len(line) > 4:
            line = line.rstrip('\n')
            if url_first:
                pair_nr += 1
                url_pair = URLPair(pair_nr)
                url_pair.set_src(line)
                url_first = False
            else:
                url_pair.set_dst(line)
                all_list.append(url_pair)
                url_first = True
except Exception, err:
    logger.critical('Error reading url pair list: %s' % (err))
    myexit(1)

# we will now sort the list, then put the url pairs into buckets
# after this we have the url_pairs in lists based on what tool is 
# needed for the transfers, and those lists are already sorted so
# the tools can do further grouping based on those lists
logger.debug("Organizing the tranfers into based on transfer type and source/destination")
all_list.sort()
for i, url_pair in enumerate(all_list): 
    if tool_map.has_key((url_pair.src_proto, url_pair.dst_proto)):
        tool = tool_map[(url_pair.src_proto, url_pair.dst_proto)]
        if tool == "cp":
            cp_list.append(url_pair)
        elif tool == "symlink":
            symlink_list.append(url_pair)
        elif tool == "scp":
            scp_list.append(url_pair)
        elif tool == "webget":
            webget_list.append(url_pair)
        elif tool == "gsiftp":
            gsiftp_list.append(url_pair)
        elif tool == "srm":
            srm_list.append(url_pair)
        else:
            logger.critical("Error: No mapping for the tool '%s'" %(tool))
            myexit(1)
    else:
        logger.critical("Error: This tool does not know how to transfer from %s:// to %s://" % (url_pair.src_proto, url_pair.dst_proto))
        myexit(1)

logger.debug("Executing transfer commands")
try:
    cp()
    symlink()
    scp()
    webget()
    gsiftp()
    srm()
except RuntimeError, err:
    logger.critical(err)
    myexit(1)

if delay_errors and delay_exit_code != 0:
    logger.critical("Some transfers failed! See above.")
    myexit(delay_exit_code)

logger.info("All transfers completed successfully.")

myexit(0)


