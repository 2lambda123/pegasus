#!/usr/bin/env python

"""
Pegasus utility for pasing jobstate.log and reporting succesful and failed jobs

Usage: pegasus-analyzer [options]
"""

# Revision : $Revision$

# Import Python modules (some not currently needed, but included for future expansion)
import os
import re
import sys
import time
import errno
import logging
import commands
import optparse

# --- regular expressions -------------------------------------------------------------

re_parse_property = re.compile(r"([^:= \t]+)\s*[:=]?\s*(.*)")

# --- classes -------------------------------------------------------------------------

class Job:

    name = ""			# Job name
    state = ""			# Job state
    sub_file = ""	    	# Submit file for this job
    out_file = ""		# Output file for this job
    err_file = ""		# Error file for this job
    sub_file_parsed = False 	# Flag to tell if we were able to parse this job's submit file
    site = ""			# Site where the job ran
    is_subdag = False		# Flag to tell if job is a SUBDAG job in the dag file
    subdag_dir = ""		# Subdag directory from a SUBDAG job in the dag file
    dagman_out = ""		# dagman.out file for this job (only for clustered jobs)

    def __init__(self, job_name, job_state=""):
	"""
	Initializes the Job class, setting job name,
	and state, if provided
	"""
	self.name = job_name
	self.state = job_state

    def set_state(self, new_state):
	"""
	This function updates a job state
	"""
	self.state = new_state

# --- constants -----------------------------------------------------------------------

MAXLOGFILE = 1000				# For log file rotation, check files .000 to .999

# --- global variables ----------------------------------------------------------------

prog_base = os.path.split(sys.argv[0])[1]	# Name of this program
input_dir = None				# Directory given in -i command line option
jsdl_path = None				# Path of the jobstate.log file
run_tailstatd = 0				# Run tailstatd before trying to analyze the output
quiet_mode = 0					# Prints out/err filenames instead of dumping their contents
jsdl_filename = "jobstate.log"			# Default name of the log file to use
jobs = {}					# List of jobs found in the jobstate.log file
success = 0					# Number of successful jobs
unsubmitted = 0					# Number of unsubmitted jobs
failed_jobs = []				# List of jobs that failed
unknown_jobs = []				# List of jobs that neither succeeded nor failed

# --- functions -----------------------------------------------------------------------

def has_seen(job_name):
    """
    This function returns true if we are already tracking job_name
    """
    if job_name in jobs:
	return True
    return False

def add_job(job_name, job_state=""):
    """
    This function adds a job to our list
    """
    # Don't add the same job twice
    if job_name in jobs:
	return

    newjob = Job(job_name, job_state)
    jobs[job_name] = newjob

def update_job_state(job_name, job_state=""):
    """
    This function updates the job state of a given job
    """
    # Make sure we have this job
    if not job_name in jobs:
	# Print a warning message
	print "error: could not find job %s" % (job_name)
	return

    jobs[job_name].set_state(job_state)

def analyze():
    """
    This function processes all currently known jobs, generating some statistics
    """
    global success, unsubmitted

    for my_job in jobs:
	if (jobs[my_job].state == "POST_SCRIPT_SUCCESS" or
	    jobs[my_job].state == "JOB_SUCCESS"):
	    success = success + 1
	elif (jobs[my_job].state == "POST_SCRIPT_FAILURE" or
	      jobs[my_job].state == "JOB_FAILURE"):
	    failed_jobs.append(my_job)
	elif (jobs[my_job].state == "UNSUBMITTED"):
	    unsubmitted = unsubmitted + 1
	else:
	    # It seems we don't have a final result for this job
	    unknown_jobs.append(my_job)

def process_failed_job(my_job):
    """
    This function opens a submit file and reads site
    and condor dagman log information
    """
    # First we check if this is a SUBDAG job from the dag file
    if my_job.is_subdag:
	# Nothing to do here
	return

    # Create full path for the submit file
    my_job.sub_file = os.path.join(input_dir, my_job.name + ".sub")
    my_job.out_file = os.path.join(input_dir, my_job.name + ".out")
    my_job.err_file = os.path.join(input_dir, my_job.name + ".err")

    submit_fn = my_job.sub_file
    # Try to access submit file
    if os.access(submit_fn, os.R_OK):
	# Open submit file
	try:
	    SUB = open(submit_fn, "r")
	except:
	    # print "error opening submit file: %s" % (submit_fn)
	    # fail silently for now...
	    return
	
	# submit file found
	my_job.sub_file_parsed = True

	# Check if this job includes sub workflows
	if my_job.name.startswith("pegasus-plan"):
	    has_sub_workflow = True
	else:
	    has_sub_workflow = False

	# Parse submit file
	for line in SUB:
	    # First we need to do some trimming...
	    line = line.strip(" \t") # Remove leading and trailing spaces
	    if line.startswith('#'):
		# Skip comments
		continue
	    line = line.rstrip("\n\r") # Remove new lines, if any
	    line = line.split('#')[0] # Remove inline comments too
	    line = line.strip() # Remove any remaining spaces at both ends
	    if len(line) == 0:
		# Skip empty lines
		continue
	    prop = re_parse_property.search(line)
	    if prop:
		# Parse successful
		k = prop.group(1)
		v = prop.group(2)

		# See if it is one of the properties we are looking for...
		if k == "+pegasus_site":
		    my_job.site = v.strip('"')
		    continue
		if k == "environment" and has_sub_workflow:
		    # Ok, we need to find the CONDOR_DAGMAN_LOG entry now...
		    sub_props = v.split(';')
		    for sub_prop_line in sub_props:
			sub_prop_line = sub_prop_line.strip() # Remove any spaces
			if len(sub_prop_line) == 0:
			    continue
			sub_prop = re_parse_property.search(sub_prop_line)
			if sub_prop:
			    if sub_prop.group(1) == "_CONDOR_DAGMAN_LOG":
				my_job.dagman_out = sub_prop.group(2)

	SUB.close()
    else:
	# Was not able to access submit file
	# fail silently for now...
	# print "cannot access submit file: %s" % (submit_fn)
	pass

def find_dag_file(input_dir):
    """
    This function finds the dag file in a given directory.
    We assume there is just one .dag file in the input
    directory.
    """
    try:
	file_list = os.listdir(input_dir)
    except:
	print "error: cannot read directory: %s" % (input_dir)
	sys.exit(1)

    for file in file_list:
	if file.endswith(".dag"):
	    return os.path.join(input_dir, file)

    print "error: could not find any .dag file in %s" % (input_dir)
    sys.exit(1)

def parse_dag_file(dag_fn):
    """
    This function walks through the dag file, learning about
    all jobs before hand.
    """
    # Open dag file
    try:
	DAG = open(dag_fn, "r")
    except:
	print "error: could not open dag file %s: exiting..." % (dag_fn)
	sys.exit(1)

    # Loop through the dag file
    for line in DAG:
	line = line.strip(" \t")
	if line.startswith("#"):
	    # Skip comments
	    continue
	line = line.rstrip("\n\r") # Remove new lines, if any
	line = line.split('#')[0] # Remove inline comments too
	line = line.strip() # Remove any remaining spaces at both ends
	if len(line) == 0:
	    # Skip empty lines
	    continue
	if line.startswith("JOB"):
	    # This is a job line, let's parse it
	    my_job = line.split()
	    if len(my_job) != 3:
		print "warning: confused parsing dag line: %s" % (line)
		continue
	    if not has_seen(my_job[1]):
		add_job(my_job[1], "UNSUBMITTED")
	    else:
		print "warning: job appears twice in dag file: %s" % (my_job[1])
	if line.startswith("SUBDAG EXTERNAL"):
	    # This is a subdag line, parse it to get job name and directory
	    my_job = line.split()
	    if len(my_job) != 6:
		print "warning: confused parsing dag line: %s" % (line)
		continue
	    if not has_seen(my_job[2]):
		add_job(my_job[2], "UNSUBMITTED")
		jobs[my_job[2]].is_subdag = True
		jobs[my_job[2]].subdag_dir = my_job[5]
	    else:
		print "warning: job appears twice in dag file: %s" % (my_job[2])

def parse_jobstate_log(jobstate_fn):
    """
    This function parses the jobstate.log file, loading all job information
    """
    # Open log file
    try:
	JSDL = open(jobstate_fn, "r")
    except:
	print "could not open file %s: exiting..." % (jobstate_fn)
	sys.exit(1)

    # Loop through the log file
    for line in JSDL:
	sp = line.split()
	# Skip lines that don't have enough items
	if len(sp) < 6:
	    continue
	# Skip tailstatd comments
	if sp[1] == "INTERNAL":
	    continue

	# Ok, we have a valid job
	jobname = sp[1]
	jobstate = sp[2]

	# Add to job list if we have never seen this job before
	if not has_seen(jobname):
	    print "warning: job %s not present in dag file" % (jobname)
	    add_job(jobname, jobstate)
	else:
	    # Update job state
	    update_job_state(jobname, jobstate)

    # Close log file
    JSDL.close()

def find_latest_log(log_file_base):
    """
    This function tries to locate the latest log file
    """
    last_log = None
    curr_log = None

    if os.access(log_file_base, os.F_OK):
	last_log = log_file_base

    # Starts from .000
    sf = 0

    while (sf < MAXLOGFILE):
	curr_log = log_file_base + ".%03d" % (sf)
	if os.access(curr_log, os.F_OK):
	    last_log = curr_log
	    sf = sf + 1
	else:
	    break

    return last_log

def rotate_jobstate_log(input_dir):
    """
    This function rotates the jobstate.log file so that it is not overwritten
    when this program invokes tailstatd again.
    """
    # First we check if we have a jobstate.log file
    source_file = os.path.join(input_dir, jsdl_filename)
    
    if not os.access(source_file, os.F_OK):
	# File doesn't exist, we don't have to rotate
	return

    # Now we need to find the latest log file

    # We start from .000
    sf = 0

    while (sf < MAXLOGFILE):
	dest_file = source_file + ".%03d" % (sf)
	if os.access(dest_file, os.F_OK):
	    # Continue to the next one
	    sf = sf + 1
	else:
	    break

    # Safety check to see if we have reached the maximum number of log files
    if sf >= MAXLOGFILE:
	print "error: %s exists, cannot rotate log file anymore!" % (dest_file)
	sys.exit(1)

    # Now that we have source_file and dest_file, try to rotate the logs
    try:
	os.rename(source_file, dest_file)
    except:
	print "error: cannot rename %s to %s" % (source_file, dest_file)
	sys.exit(1)

    # Done!
    return

def invoke_tailstatd(input_dir):
    """
    This function runs tailstatd on the input directory. It first looks for
    a dag.dagman.out file, and then invokes tailstatd on that file
    """
    dagman_out_file = None

    # Look for dagman.out file
    try:
	file_list = os.listdir(input_dir)
    except:
	print "error: cannot read directory: %s" % (input_dir)
	sys.exit(1)

    for file in file_list:
	if file.endswith(".dag.dagman.out"):
	    dagman_out_file = os.path.join(input_dir, file)
	    break

    if dagman_out_file is None:
	print "error: could not find any .dag.dagman.out file in %s" % (input_dir)
	sys.exit(1)

    # Found dagman.out file, now we can invoke tailstatd
    tailstatd_cmd = "tailstatd -n --nodatabase " + dagman_out_file
    print "running: %s" % (tailstatd_cmd)

    try:
	status, output = commands.getstatusoutput(tailstatd_cmd)
    except:
	print "error: could not invoke tailstatd"
	sys.exit(1)

def dump_file(file):
    """
    This function dumps a file to our stdout
    """
    if file is not None:
	try:
	    OUT = open(file, 'r')
	except:
	    print "*** Cannot access: %s" % (file)
	    print
	else:
	    print os.path.split(file)[1].center(80, '-')
	    print
	    # Dump file contents to terminal
	    line = OUT.readline()
	    while line:
		line = line.strip()
		print line
		line = OUT.readline()

	    OUT.close()
	    print

def print_job_info(job):
    """
    This function prints the information about a particular job
    """
    print
    print job.center(80, '=')
    print
    print " last state: %s" % (jobs[job].state)
    process_failed_job(jobs[job])

    # Handle subdag jobs from the dag file
    if jobs[job].is_subdag == True:
	print " This is not a pegasus job."
	print " For more information, please look at the directory below:"
	print " %s" % (jobs[job].subdag_dir)
	print
	return

    if jobs[job].sub_file_parsed == False:
	print "       site: submit file not available"
    else:
	print "       site: %s" % (jobs[job].site or '-')
    print "submit file: %s" % (jobs[job].sub_file)
    print "output file: %s" % (find_latest_log(jobs[job].out_file))
    print " error file: %s" % (find_latest_log(jobs[job].err_file))
    if len(jobs[job].dagman_out) > 0:
	# This job has a sub workflow
	print " This job contains sub workflows!"
	print " Please run the command below for more information:"
	print " %s -i -t %s" % (prog_base, os.path.split(jobs[job].dagman_out)[0])
	print
    print

    # Now dump file contents to screen if we are not in quiet mode
    if not quiet_mode:
	# Print outfile to screen
	out_file = find_latest_log(jobs[job].out_file)
	dump_file(out_file)

	# Print errfile to screen
	err_file = find_latest_log(jobs[job].err_file)
	dump_file(err_file)

def print_top_summary():
    """
    This function prints the summary for the analyzer report,
    which is the same for the long and short output versions
    """
    print "%s: analysis completed!" % (prog_base)
    print
    print "Summary".center(80, '*')
    print
    print " Total jobs         : % 6d (%3.2f%%)" % (len(jobs), 100 * (1.0 * len(jobs)/(len(jobs) or 1)))
    print " # jobs succeeded   : % 6d (%3.2f%%)" % (success, 100 * (1.0 * success/(len(jobs) or 1)))
    print " # jobs failed      : % 6d (%3.2f%%)" % (len(failed_jobs), 100 * (1.0 * len(failed_jobs)/(len(jobs) or 1)))
    print " # jobs unsubmitted : % 6d (%3.2f%%)" % (unsubmitted, 100 * (1.0 * unsubmitted/(len(jobs) or 1)))
    if len(unknown_jobs):
	print " # jobs unknown     : % 6d (%3.2f%%)" % (len(unknown_jobs), 100 * (1.0 * len(unknown_jobs)/(len(jobs) or 1)))
    print


def print_summary():
    """
    This function prints the analyzer report summary
    """

    # First print the summary section
    print_top_summary()

    # Print information about failed jobs
    if len(failed_jobs):
	print "Failed jobs' details".center(80, '*')
	for job in failed_jobs:
	    print_job_info(job)

    # Print information about unknown jobs
    if len(unknown_jobs):
	print "Unknown jobs' details".center(80, '*')
	for job in unknown_jobs:
	    print_job_info(job)

# --- main ----------------------------------------------------------------------------

# Configure command line option parser
prog_usage = "usage: %s [options]" % (prog_base)
parser = optparse.OptionParser(usage=prog_usage)
parser.add_option("-i", action = "store", type = "string", dest = "input_dir",
		  help = "input directory where the jobstate.log file is located, default is the current directory")
parser.add_option("-t", "--tailstatd", action = "store_const", const = 1, dest = "run_tailstatd",
		  help = "run tailstatd before analyzing the output")
parser.add_option("-q", "--quiet", action = "store_const", const = 1, dest = "quiet_mode",
		  help = "output out/err filenames instead of their contents")

# Parse command line options
(options, args) = parser.parse_args()

print "%s: initializing..." % (prog_base)

# Copy options from the command line parser
if options.run_tailstatd is not None:
    run_tailstatd = options.run_tailstatd
if options.quiet_mode is not None:
    quiet_mode = options.quiet_mode
# Select directory where jobstate.log is located
if options.input_dir is not None:
    input_dir = options.input_dir
else:
    input_dir = os.getcwd()

# Invoke tailstatd if requested
if run_tailstatd:
    rotate_jobstate_log(input_dir)
    invoke_tailstatd(input_dir)

# First we learn about jobs by going through the dag file
dag_path = find_dag_file(input_dir)
parse_dag_file(dag_path)

# Combine directory and filename
jsdl_path = os.path.join(input_dir, jsdl_filename)

# Read logfile
parse_jobstate_log(jsdl_path)

# Process our jobs
analyze()

# Print summary of our analysis
print_summary()
