#!/usr/bin/env python

"""
Logging daemon process to update the jobstate.log file from DAGMan logs.
This program is to be run automatically by the pegasus-run command.

Usage: pegasus-monitord [options] dagoutfile
"""

##
#  Copyright 2007-2010 University Of Southern California
#
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing,
#  software distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
##

# Revision : $Revision: 2012 $

# Import Python modules
import os
import re
import sys
import time
import errno
import atexit
import select
import signal
import socket
import logging
import calendar
import commands
import datetime
#import operator
import optparse
import urlparse

# Initialize logging object
logger = logging.getLogger()
# Set default level to WARNING
logger.setLevel(logging.WARNING)
#logger.setLevel(logging.DEBUG)
# Don't send events further up
logger.propagate = 0
# Cached debugging state
g_isdgb = 0

# Ordered logging levels
_LEVELS = (logging.ERROR, logging.WARN, logging.INFO, logging.DEBUG)

# Save our own basename
prog_base = os.path.split(sys.argv[0])[1]

# Import our modules
import common   # Takes care of setting up PEGASUS_HOME and PYTHONPATH
from Pegasus.tools import utils
from Pegasus.tools import properties
from Pegasus.tools import kickstart_parser

# Optional imports, only generate 'warnings' if they fail
nlapi = None
stampede_loader = None
bson = None
amqp = None
NLSimpleParser = None

try:
    from netlogger import nlapi
except:
    logger.info("cannot import netlogger.nlapi")
try:
    from netlogger.analysis.modules import stampede_loader
    from netlogger.analysis.workflow.util import Expunge
except:
    logger.info("cannot import NetLogger's stampede_loader")
try:
    from netlogger.parsers.base import NLSimpleParser
except:
    logger.info("cannot import NL parser")
try:
    import bson
except:
    logger.info("cannot import BSON library, 'bson'")
try:
    from amqplib import client_0_8 as amqp
except:
    logger.info("cannot import AMQP library")

# Import Python > 2.5 modules
try:
    import uuid
    uuid_present = True
except:
    # Import failed, make a note so we can use something else
    uuid_present = False

# Add SEEK_CUR to os if Python version < 2.5
if sys.version_info < (2, 5):
    os.SEEK_CUR = 1

# Compile our regular expressions
# Used while reading the DAG file
re_parse_dag_submit_files = re.compile(r"JOB\s+(\S+)\s(\S+)(\s+DONE)?", re.IGNORECASE)
re_parse_dag_script = re.compile(r"SCRIPT (?:PRE|POST)\s+(\S+)\s(\S+)\s(.*)", re.IGNORECASE)
re_parse_dag_subdag = re.compile(r"SUBDAG EXTERNAL\s+(\S+)\s(\S+)\s?(?:DIR)?\s?(\S+)?", re.IGNORECASE)

# Used in out2log
re_remove_extensions = re.compile(r"(?:\.(?:rescue|dag))+$")

# Used in untaint
re_clean_content = re.compile(r"([^-a-zA-z0-9_\s.,\[\]^\*\?\/\+])")

# Used in parse_sub_file
re_rsl_string = re.compile(r"^\s*globusrsl\W", re.IGNORECASE)
re_rsl_clean = re.compile(r"([-_])")
re_site_parse_gvds = re.compile(r"^\s*\+(pegasus|wf)_(site|resource)\s*=\s*([\'\"])?(\S+)\3")
re_parse_transformation = re.compile(r"^\s*\+pegasus_wf_xformation\s*=\s*(\S+)")
re_parse_derivation = re.compile(r"^\s*\+pegasus_wf_dax_job_id\s*=\s*(\S+)")
re_parse_executable = re.compile(r"^\s*executable\s*=\s*(\S+)")
re_parse_arguments = re.compile(r'^\s*arguments\s*=\s*"([^"\r\n]*)"')
re_parse_environment = re.compile(r'^\s*environment\s*=\s*(.*)')
re_site_parse_euryale = re.compile(r"^\#!\s+site=(\S+)")
re_parse_property = re.compile(r'([^:= \t]+)\s*[:=]?\s*(.*)')
re_parse_input = re.compile(r"^\s*intput\s*=\s*(\S+)")
re_parse_output = re.compile(r"^\s*output\s*=\s*(\S+)")
re_parse_error = re.compile(r"^\s*error\s*=\s*(\S+)")

# Used in process
re_parse_dag_name = re.compile(r"Parsing (.+) ...$")
re_parse_timestamp = re.compile(r"^\s*(\d{1,2})\/(\d{1,2})(\/(\d{1,2}))?\s+(\d{1,2}):(\d{2}):(\d{2})")
re_parse_iso_stamp = re.compile(r"^\s*(\d{4}).?(\d{2}).?(\d{2}).(\d{2}).?(\d{2}).?(\d{2})([.,]\d+)?([Zz]|[-+](\d{2}).?(\d{2}))")
re_parse_event = re.compile(r"Event:\s+ULOG_(\S+) for Condor (?:Job|Node) (\S+)\s+\((-?[0-9]+\.[0-9]+)(\.[0-9]+)?\)$")
re_parse_script_running = re.compile(r"\d{2}\sRunning (PRE|POST) script of (?:Job|Node) (.+)\.{3}")
re_parse_script_done = re.compile(r"\d{2}\s(PRE|POST) Script of (?:Job|Node) (\S+)")
re_parse_script_successful = re.compile(r"completed successfully\.$")
re_parse_script_failed = re.compile(r"failed with status\s+(-?\d+)\.?$")
re_parse_job_submit = re.compile(r"Submitting Condor Node (.+) job")
re_parse_job_submit_error = re.compile(r"ERROR: submit attempt failed")
re_parse_job_failed = re.compile(r"\d{2}\sNode (\S+) job proc \(([0-9\.]+)\) failed with status\s+(-?\d+)\.$")
re_parse_job_successful = re.compile(r"\d{2}\sNode (\S+) job proc \(([0-9\.]+)\) completed successfully\.$")
re_parse_retry = re.compile(r"Retrying node (\S+) \(retry \#(\d+) of (\d+)\)")
re_parse_dagman_condor_id = re.compile(r"\*\* condor_scheduniv_exec\.([0-9\.]+) \(CONDOR_DAGMAN\) STARTING UP")
re_parse_dagman_finished = re.compile(r"\(condor_DAGMAN\)[\w\s]+EXITING WITH STATUS (\d+)$")
re_parse_dagman_pid = re.compile(r"\*\* PID = (\d+)$")
re_parse_condor_version = re.compile(r"\*\* \$CondorVersion: ((\d+\.\d+)\.\d+)")
re_parse_condor_logfile = re.compile(r"Condor log will be written to ([^,]+)")
re_parse_condor_logfile_insane = re.compile(r"\d{2}\s{3,}(\S+)")
re_parse_multiline_files = re.compile(r"All DAG node user log files:")

# Constants
logbase = "monitord.log"       # Basename of daemon logfile
good_rsl = {"maxcputime": 1, "maxtime":1, "maxwalltime": 1}
speak = "PMD/1.0"
PRESCRIPT_TASK_ID = -1                  # id for prescript tasks
POSTSCRIPT_TASK_ID = -2                 # id for postscript tasks
MONITORD_STATE_FILE = "monitord.info"   # filename for writing monitord state information
MAX_SLEEP_TIME = 10     	        # in seconds

unsubmitted_events = {"UN_READY": 1,
                      "PRE_SCRIPT_STARTED": 1,
                      "PRE_SCRIPT_SUCCESS": 1,
                      "PRE_SCRIPT_FAILURE": 1}

# Global variables
wfs = []                        # list of workflow entries monitord is tracking
tracked_workflows = []          # list of workflows we have started tracking
follow_subworkflows = True      # Flag for tracking sub-workflows
root_wf_id = None               # Workflow id of the root workflow
replay_mode = 0                 # disable checking if DAGMan's pid is gone
keep_state = 0                  # Flag for keeping a Workflow's state across several DAGMan start/stop cycles
db_stats = 'no'                 # collect and print database stats at the end of execution
no_events = False               # Flag for disabling event output altogether
event_dest = None               # URL containing the destination of the events
encoding = None	                # Way to encode the data
monitord_exit_code = 0          # Exit code for pegasus-monitord

wf_event_sink = None            # Where wf events go
STAMPEDE_NS = "stampede."       # Event name-space

# Revision handling
revision = "$Revision: 2012 $" # Let cvs handle this, do not edit manually

# Remaining variables
out = None                      # .dag.dagman.out file from command-line
run = None                      # run directory from command-line dagman.out file
server = None                   # server socket
sockfn = None                   # socket filename
condor_dagman_executable = None	# condor_dagman binary location

# Find condor_dagman
condor_dagman_executable = utils.find_exec("condor_dagman")
if condor_dagman_executable is None:
    # Default value
    condor_dagman_executable = "condor_dagman"

class OutputURL:
    """
    Break output URL into named parts for easier handling.
    """
    def __init__(self, url):

        # Fix for Python 2.5 and earlier 2.6, as their urlparse module
        # does not handle these schemes correctly (netloc empty and
        # everything after the scheme in path)
        if (url.startswith("amqp:")
            or url.startswith("mysql:")
            or url.startswith("x-tcp:")
            or url.startswith("sqlite:")):
            self.scheme, rest_url = url.split(":", 1)
            url = "http:" + rest_url

            http_scheme, self.netloc, self.path, self.params, query, frag = urlparse.urlparse(url)
        else:
            # No need to change anything
            self.scheme, self.netloc, self.path, self.params, query, frag = urlparse.urlparse(url)

        host_port = ''
        user_pass = ''

        if '@' in self.netloc:
            user_pass, host_port = self.netloc.split('@', 1)
        else:
            host_port = self.netloc
        if ':' in host_port:
            self.host, portstr = host_port.split(':', 1)
            self.port = int(portstr)
        else:
            self.host = self.netloc
            self.port = None

class EventSink(object):
    """
    Base class for an Event Sink.
    """
    def __init__(self):
        name = self.__class__.__name__.split('.')[-1]
        self._log = logging.getLogger("pegasus.monitord." + name)
        self._isdbg = self._log.isEnabledFor(logging.DEBUG)
        
    def send(self, event, kw):
        """
        Clients call this function to send an event to the sink.
        """
        pass

    def close(self):
        """
        Clients call this function to close the output to this sink.
        """
        pass

class EmptySink(EventSink):
    """
    Empty class, doesn't do anything, events go nowhere...
    Just a placeholder in case we need to do something different later...
    """
    def __init__(self):
        super(EmptySink, self).__init__()

    def send(self, event, kw):
        pass

    def close(self):
        pass

class DBEventSink(EventSink):
    """
    Write wflow event logs to database via loader
    """
    def __init__(self, dest, db_stats=False, **kw):
        assert stampede_loader is not None, "Database destination selected, "+\
               "but cannot import stampede loader"
        self._db = stampede_loader.Analyzer(dest, perf=db_stats, batch="yes")
        super(DBEventSink, self).__init__()
        
    def send(self, event, kw):
        if self._isdbg:
            self._log.debug("send.start event=%s" % (event))
        d = {'event' : "stampede." + event}
        for k, v in kw.iteritems():
            d[k.replace('__','.')] = v
        self._db.notify(d)
        if self._isdbg:
            self._log.debug("send.end event=%s" % (event))
        
    def close(self):
        self._log.debug("close.start")
        self._db.finish()
        self._log.debug("close.end")
    
class FileEventSink(EventSink):
    """
    Write wflow event logs to a file.
    """
    def __init__(self, path, encoder=None, **kw):
        assert nlapi is not None, "File/socket destination selected, "+\
               "but cannot import NetLogger API"
        super(FileEventSink, self).__init__()
        self._output = open(path, 'a')
        self._encoder = encoder

    def send(self, event, kw):
        if self._isdbg:
            self._log.debug("send.start event=%s" % (event))
        self._output.write(self._encoder(event=event, **kw))
        if self._isdbg:
            self._log.debug("send.end event=%s" % (event))

    def close(self):
        self._log.debug("close.start")
        self._output.close()
        self._log.debug("close.end")

class TCPEventSink(EventSink):
    """
    Write wflow event logs to a host:port.
    """
    def __init__(self, host, port, encoder=None, **kw):
        super(TCPEventSink, self).__init__()
        self._encoder = encoder
        self._sock = socket.socket()
        self._sock.connect((host, port))

    def send(self, event, kw):
        if self._isdbg:
            self._log.debug("send.start event=%s" % (event))
        self._sock.send(self._encoder(event=event, **kw))
        if self._isdbg:
            self._log.debug("send.end event=%s" % (event))

    def close(self):
        self._log.debug("close.start")
        self._sock.close()
        self._log.debug("close.end")

class AMQPEventSink(EventSink):
    """
    Write wflow event logs to an AMQP server.
    """
    EXCH_OPTS = {'type' : 'topic'}
    def __init__(self, host, port, exch=None, encoder=None,
                 userid='guest', password='guest', virtual_host='/',
                 ssl=False, connect_timeout=None, **kw):
        super(AMQPEventSink, self).__init__()
        self._encoder = encoder
        self._conn = amqp.Connection(host="%s:%s" % (host, port),
                                     userid=userid, password=password,
                                     virtual_host=virtual_host, ssl=ssl,
                                     connect_timeout=connect_timeout, **kw)
        self._channel = self._conn.channel()
        self._exch = exch
        self._channel.exchange_declare(exch, **self.EXCH_OPTS)

    def send(self, event, kw):
        full_event = STAMPEDE_NS + event
        if self._isdbg:
            self._log.debug("send.start event=%s" % (full_event))
        data = self._encoder(event=event, **kw)
        self._channel.basic_publish(amqp.Message(body=data),
                                    exchange=self._exch, routing_key=full_event)
        if self._isdbg:
            self._log.debug("send.end event=%s" % (event))

    def close(self):
        self._log.debug("close.start")
        self._conn.close()
        self._log.debug("close.end")

def get_default_sqlite_db(dagman_file):
    """
    This function returns the default connection string for a sqlite
    database.
    """
    return "sqlite:///" + dagman_file[:dagman_file.find(".dag.dagman.out")] + ".stampede.db"

def bson_encode(event, **kw):
    """
    Adapt bson.dumps() to NetLogger's Log.write() signature.
    """
    kw['event'] = STAMPEDE_NS + event
    return bson.dumps(kw)        

def create_wf_event_sink(dest, enc=None, **kw):
    """
    Create & return subclass of EventSink, chosen by value of 'dest'
    and parameterized by values (if any) in 'kw'.
    """
    url = OutputURL(dest)
    
    # Pick an encoder

    def pick_encfn(enc_name):
        ##enc_name = url.query.get('enc', 'bp').lower()
        if enc_name is None or enc_name == 'bp':
            assert nlapi is not None, "NetLogger encoding selected, "+\
                "but cannot import nlapi library"
            # NetLogger name=value encoding
            encfn = nlapi.Log(level=nlapi.Level.ALL, prefix=STAMPEDE_NS)
        elif enc_name == 'bson':
            # BSON
            assert bson is not None, "BSON encoding selected, "+\
                "but cannot import bson library"
            encfn = bson_encode
        else:
            raise ValueError("Unknown encoding '%s'" % (enc_name))
        return encfn

    # Branch on scheme
    if url.scheme == '':
        sink = FileEventSink(dest, encoder=pick_encfn(enc), **kw)
        _type, _name = "file", dest
    elif url.scheme == 'file':
        sink = FileEventSink(url.path, encoder=pick_encfn(enc), **kw)
        _type, _name = "file", url.path
    elif url.scheme == 'x-tcp':
        if url.port is None:
            url.port = 14380
        sink = TCPEventSink(url.host, url.port, encoder=pick_encfn(enc), **kw)
        _type, _name = "network", "%s:%s" % (url.host, url.port)
    elif url.scheme == 'amqp':
        assert amqp is not None, "AMQP destination selected, "+\
            "but cannot import AMQP library"
        if url.port is None:
            url.port = 5672 # RabbitMQ default
        while url.path.startswith('/'):
            url.path = url.path[1:]
        sink = AMQPEventSink(url.host, url.port, exch=url.path,
                             encoder=pick_encfn(enc), **kw)
        _type, _name="AMQP", "%s:%s/%s" % (url.host, url.port, url.path)
    else:
        sink = DBEventSink(dest, **kw)
        _type, _name = "DB", dest
    logger.info("output type=%s name=%s" % (_type, _name))

    return sink

def finish_stampede_loader():
    """
    This function is called by the atexit module when monitord exits.
    It is used to make sure the loader has finished loading all data
    into the database. It will also produce stats for benchmarking.
    """
    if wf_event_sink is not None:
        try:
            if db_stats == 'yes' and logger.getEffectiveLevel() > logging.INFO:
                # Make sure log level is enough to display database
                # benchmarking information
                logger.setLevel(logging.INFO)
            wf_event_sink.close()
        except:
            logger.warning("could not call the finish method "+\
                               "in the nl loader class... exiting anyway")

def out2log(rundir, outfile):
    """
    purpose: infer output symlink for Condor common user log
    paramtr: rundir (IN): the run directory
    paramtr: outfile (IN): the name of the out file we use
    returns: the name of the log file to use
    """

    # Get the basename
    my_base = os.path.basename(outfile)
    # NEW: Account for rescue DAGs
    my_base = my_base[:my_base.find(".dagman.out")]
    my_base = re_remove_extensions.sub('', my_base)
    # Add .log extension
    my_base = my_base + ".log"
    # Create path
    my_log = os.path.join(rundir, my_base)

    return my_log, my_base

class Job:
    """
    Class used to keep information needed to track a particular job
    """

    # Variables that describe a job, as per the Stampede schema
    # Some will be initialized in the init method, others will
    # get their values from the kickstart output file when a job
    # finished

    def __init__(self, wf_uuid, name, job_submit_seq):
        """
        This function initializes the job parameters with the
        information available when a job is detected in the
        "PRE_SCRIPT_STARTED" or the "SUBMIT" state. Other parameters
        will remain None until a job finishes and a kickstart output
        file can be parsed.
        """
        self._wf_uuid = wf_uuid
        self._exec_job_id = name
        self._job_submit_seq = job_submit_seq
        self._sched_id = None
        self._site_name = None
        self._host_id = None
        self._remote_user = None
        self._remote_working_dir = None
        self._cluster_start_time = None
        self._cluster_duration = None
        self._job_state = None
        self._job_state_seq = 0
        self._job_state_timestamp = None
        self._job_output_counter = 0
        self._pre_script_start = None
        self._pre_script_done = None
        self._pre_script_exitcode = None
        self._main_job_start = None
        self._main_job_done = None
        self._main_job_transformation = None
        self._main_job_derivation = None
        self._main_job_executable = None
        self._main_job_arguments = None
        self._main_job_exitcode = None
        self._post_script_start = None
        self._post_script_done = None
        self._post_script_exitcode = None
        self._input_file = None
        self._output_file = None
        self._error_file = None
        self._stdout_text = ""
        self._stderr_text = ""
        self._job_dagman_out = None		# _CONDOR_DAGMAN_LOG from environment line for pegasus-plan and subdax_ jobs

    def set_job_state(self, job_state, timestamp, status):
        """
        This function sets the job state for this job. It also updates
        the times the main job and PRE/POST scripts start and finish.
        """
        self._job_state = job_state
        self._job_state_timestamp = int(timestamp)
        # Increment job state sequence
        self._job_state_seq = self._job_state_seq + 1

        # Record timestamp for certain job states
        if job_state == "PRE_SCRIPT_STARTED":
            self._pre_script_start = int(timestamp)
        elif job_state == "PRE_SCRIPT_SUCCESS" or job_state == "PRE_SCRIPT_FAILURE":
            self._pre_script_done = int(timestamp)
            self._pre_script_exitcode = status
        elif job_state == "POST_SCRIPT_STARTED":
            self._post_script_start = int(timestamp)
        elif job_state == "POST_SCRIPT_TERMINATED":
            self._post_script_done = int(timestamp)
        elif job_state == "EXECUTE":
            self._main_job_start = int(timestamp)
        elif job_state == "JOB_TERMINATED":
            self._main_job_done = int(timestamp)
        elif job_state == "JOB_SUCCESS" or job_state == "JOB_FAILURE":
            self._main_job_exitcode = status
        elif job_state == "POST_SCRIPT_SUCCESS" or job_state == "POST_SCRIPT_FAILURE":
            self._post_script_exitcode = status

    def parse_sub_file(self, stamp, submit_file):
        """
        This function parses a job's submit file and returns job
        planning information. In addition, we try to populate the job
        type from information in the submit file.
        # paramtr: stamp(IN): timestamp associated with the log line
        # paramtr: submit_file(IN): submit file name
        # globals: good_rsl(IN): which RSL keys constitute time requirements
        # returns: (largest job time requirement in minutes, destination site)
        # returns: (None, None) if sub file not found
        """
        parse_environment = False
        my_result = None
        my_site = None

        # Update stat record for submit file
        try:
            my_stats = os.stat(submit_file)
        except:
	    # Could not stat file
            logger.error("stat %s" % (submit_file))
            return my_result, my_site

        # Check submit file timestamp
        if stamp < my_stats[8]: #mtime
            logger.info("%s: submit file was modified after job ran: job timestamp=%d, sub file mtime=%d, diff = %d" % (submit_file, stamp, my_stats[8], my_stats[8]-stamp))

        # Check if we need to parse the environment line
        if self._exec_job_id.startswith("pegasus-plan") or self._exec_job_id.startswith("subdax_"):
            parse_environment = True

        try:
            SUB = open(submit_file, "r")
        except:
            logger.error("unable to parse %s" % (submit_file))
            return my_result, my_site

        # Parse submit file
        for my_line in SUB:
            if re_rsl_string.search(my_line):
                # Found RSL string, do parse now
                for my_match in re.findall(r"\(([^)]+)\)", my_line):
                    # Split into key and value
                    my_k, my_v = my_match.split("=", 1)
                    # Remove _- characters from string
                    my_k = re_rsl_clean.sub('', my_k)
                    if my_k.lower() in good_rsl and my_v > my_result:
                        try:
                            my_result = int(my_v)
                        except:
                            my_result = None
            elif re_site_parse_gvds.search(my_line):
                # GVDS agreement
                my_site = re_site_parse_gvds.search(my_line).group(4)
                self._site_name = my_site
            elif re_site_parse_euryale.search(my_line):
                # Euryale specific comment
                my_site = re_site_parse_euryale.search(my_line).group(1)
                self._site_name = my_site
            elif re_parse_transformation.search(my_line):
                # Found line with job transformation
                my_transformation = re_parse_transformation.search(my_line).group(1)
                # Remove quotes, if any
                my_transformation = my_transformation.strip('"')
                self._main_job_transformation = my_transformation
            elif re_parse_derivation.search(my_line):
                # Found line with job derivation
                my_derivation = re_parse_derivation.search(my_line).group(1)
                # Remove quotes, if any
                my_derivation = my_derivation.strip('"')
                self._main_job_derivation = my_derivation
            elif re_parse_executable.search(my_line):
                # Found line with executable
                my_executable = re_parse_executable.search(my_line).group(1)
                # Remove quotes, if any
                my_executable = my_executable.strip('"')
                self._main_job_executable = my_executable
            elif re_parse_arguments.search(my_line):
                # Found line with arguments
                my_arguments = re_parse_arguments.search(my_line).group(1)
                # Remove quotes, if any
                my_arguments = my_arguments.strip('"')
                self._main_job_arguments = my_arguments
            elif re_parse_input.search(my_line):
                # Found line with input file
                my_input = re_parse_input.search(my_line).group(1)
                # Remove quotes, if any
                my_input = my_input.strip('"')
                self._input_file = my_input
            elif re_parse_output.search(my_line):
                # Found line with output file
                my_output = re_parse_output.search(my_line).group(1)
                # Remove quotes, if any
                my_output = my_output.strip('"')
                self._output_file = my_output
            elif re_parse_error.search(my_line):
                # Found line with error file
                my_error = re_parse_error.search(my_line).group(1)
                # Remove quotes, if any
                my_error = my_error.strip('"')
                self._error_file = my_error
            elif parse_environment and re_parse_environment.search(my_line):
                # Found line with environment
                v = re_parse_environment.search(my_line).group(1)
                sub_props = v.split(';')
                for sub_prop_line in sub_props:
                    sub_prop_line = sub_prop_line.strip() # Remove any spaces
                    if len(sub_prop_line) == 0:
                        continue
                    sub_prop = re_parse_property.search(sub_prop_line)
                    if sub_prop:
                        if sub_prop.group(1) == "_CONDOR_DAGMAN_LOG":
                            self._job_dagman_out = sub_prop.group(2)

        SUB.close()

        # All done!
        return my_result, my_site

    def extract_job_info(self, buffer):
        """
        This function reads the output from the kickstart parser and
        extracts the job information for the Stampede schema. It first
        looks for an invocation record, and then for a clustered
        record.

        Returns None if an error occurs, True if an invocation record
        was found, and False if it wasn't.
        """

        # Check if we have anything
        if len(buffer) == 0:
            return None

        # Let's try to find an invocation record...
        my_invocation_found = False
        for my_record in buffer:
            if not "invocation" in my_record:
                # Not this one... skip to the next
                continue
            # Ok, we have an invocation record, extract the information we
            # need. Note that this may overwrite information obtained from
            # the submit file (e.g. the site_name).

            if "resource" in my_record:
                self._site_name = my_record["resource"]
            if "user" in my_record:
                self._remote_user = my_record["user"]
            if "cwd" in my_record:
                self._remote_working_dir = my_record["cwd"]
            if "hostname" in my_record:
                self._host_id = my_record["hostname"]
            if "stdout" in my_record:
                self._stdout_text = utils.quote(my_record["stdout"])
            if "stderr" in my_record:
                self._stderr_text = utils.quote(my_record["stderr"])
            # No need to look further
            my_invocation_found = True
            break

        if not my_invocation_found:
            logger.debug("cannot find invocation record in output")

        # Look for clustered record...
        my_cluster_found = False
        for my_record in buffer:
            if not "clustered" in my_record:
                # Not this one... skip to the next
                continue
            # Ok found it, fill in cluster parameters
            if "duration" in my_record:
                self._cluster_duration = my_record["duration"]
            if "start" in my_record:
                # Convert timestamp to EPOCH
                my_start = utils.epochdate(my_record["start"])
                if my_start is not None:
                    self._cluster_start_time = my_start
            # No need to look further...
            my_cluster_found = True
            break

        if not my_cluster_found:
            logger.debug("cannot find cluster record in output")

        # Done populating Job class with information from the output file
        return my_invocation_found

class Workflow:
    """
    Class used to keep everything needed to track a particular workflow
    """

    def output_to_db(self, event, kwargs):
        """
        This function sends an NetLogger event to the loader class.
        """
        # Sanity check (should also be done elsewhere, but repeated here)
        if self._sink is None:
            return

        # Send event to corresponding sink
        self._sink.send(event, kwargs)

    def parse_dag_file(self, dag_file):
        """
        This function parses the DAG file and determines submit file locations
        """

        # If we already have jobs in our _job_info dictionary, skip reading the dag file
        if len(self._job_info) > 0:
            logger.debug("skipping parsing the dag file, already have job info loaded...")
            return

        dag_file = os.path.join(self._run_dir, dag_file)

        try:
            DAG = open(dag_file, "r")
        except:
            logger.warning("unable to read %s!" % (dag_file))
        else:
            for dag_line in DAG:
                if (dag_line.lower()).find("job") >= 0:
                    # Found Job line, parse it
                    my_match = re_parse_dag_submit_files.search(dag_line)
                    if my_match:
                        if not my_match.group(3):
                            my_jobid = my_match.group(1)
                            my_sub = os.path.join(self._run_dir, my_match.group(2))
                            # Found submit file for not-DONE job
                            if my_jobid in self._job_info:
                                # Entry already exists for this job, just collect submit file info
                                self._job_info[my_jobid][0] = my_sub
                            else:
                                # No entry for this job, let's create a new one
                                self._job_info[my_jobid] = [my_sub, None, None, None, None, False, None, None]
                elif (dag_line.lower()).find("script post") >= 0:
                    # Found SCRIPT POST line, parse it
                    my_match = re_parse_dag_script.search(dag_line)
                    if my_match:
                        my_jobid = my_match.group(1)
                        my_exec = my_match.group(2)
                        my_args = my_match.group(3)
                        if my_jobid in self._job_info:
                            # Entry already exists for this job, just collect post script info
                            self._job_info[my_jobid][3] = my_exec
                            self._job_info[my_jobid][4] = my_args
                        else:
                            # No entry for this job, let's create a new one
                            self._job_info[my_jobid] = [None, None, None, my_exec, my_args, False, None, None]
                elif (dag_line.lower()).find("script pre") >= 0:
                    # Found SCRIPT PRE line, parse it
                    my_match = re_parse_dag_script.search(dag_line)
                    if my_match:
                        my_jobid = my_match.group(1)
                        my_exec = my_match.group(2)
                        my_args = my_match.group(3)
                        if my_jobid in self._job_info:
                            # Entry already exists for this job, just collect pre script info
                            self._job_info[my_jobid][1] = my_exec
                            self._job_info[my_jobid][2] = my_args
                        else:
                            # No entry for this job, let's create a new one
                            self._job_info[my_jobid] = [None, my_exec, my_args, None, None, False, None, None]
                elif (dag_line.lower()).find("subdag external") >= 0:
                    # Found SUBDAG line, parse it
                    my_match = re_parse_dag_subdag.search(dag_line)
                    if my_match:
                        my_jobid = my_match.group(1)
                        my_dag = my_match.group(2)
                        my_dir = my_match.group(3)
                        if my_dir is None:
                            # SUBDAG EXTERNAL line without DIR, let's get it from the DAG path
                            if my_dag is not None:
                                my_dir = os.path.dirname(my_dag)
                        if my_jobid in self._job_info:
                            # Entry already exists for this job, just set subdag flag, and dag/dir info
                            self._job_info[my_jobid][5] = True
                            self._job_info[my_jobid][6] = my_dag
                            self._job_info[my_jobid][7] = my_dir
                        else:
                            # No entry for this job, let's create a new one
                            self._job_info[my_jobid] = [None, None, None, None, None, True, my_dag, my_dir]

            try:
                DAG.close()
            except:
                pass

        # POST-CONDITION: _job_info contains only submit-files of jobs
        # that are not yet done. Normally, this are all submit
        # files. In rescue DAGS, that is an arbitrary subset of all
        # jobs. In addition, _job_info should contain all PRE and POST
        # script information for job in this workflow, and all subdag
        # jobs, with the their dag files, and directories

    def parse_in_file(self, jobname, tasks):
        """
        This function parses the in file for a given job, reading the
        task information and adding to the dictionary of tasks. It
        returns True if parsing was successful, or None, if an error
        was found.
        """
        in_file = os.path.join(self._run_dir, jobname) + ".in"

        try:
            IN = open(in_file, "r")
        except:
            logger.warning("unable to read %s!" % (in_file))
            return None

        tasks_found = 0
        for line in IN:
            line = line.strip()
            if len(line) == 0:
                continue
            if line.startswith("#"):
                line = line.split("#", 1)[1]
                line = line.strip()
                try:
                    my_task_id, my_transformation, my_derivation = line.split(None, 2)
                    my_task_id = int(my_task_id)
                except:
                    # Doesn't look line a proper comment line with embedded info, skipping...
                    continue
                # Update information in dictionary
                try:
                    my_task_info = tasks[my_task_id]
                except:
                    logger.warning("cannot locate task %d in dictionary... skipping this task" % (my_task_id))
                    continue
                my_task_info["transformation"] = my_transformation
                my_task_info["derivation"] = my_derivation
            else:
                # This is regular line, so we assume it is a task
                split_line = line.split(None, 1)
                if len(split_line) == 0:
                    # Nothing here
                    continue
                my_executable = split_line[0]
                if len(split_line) == 2:
                    my_argv = split_line[1]
                else:
                    my_argv = ""
                # Increment the task_found counter, so that we have the correct task index
                tasks_found = tasks_found + 1
                try:
                    my_task_info = tasks[tasks_found]
                except:
                    logger.warning("cannot locate task %d in dictionary... skipping this task" % (tasks_found))
                    continue
                my_task_info["argument-vector"] = my_argv
                my_task_info["name"] = my_executable

        try:
            IN.close()
        except:
            pass

        return True

    def read_workflow_state(self):
        """
        This function reads the job_submit_seq and the job_counters
        dictionary from a file in the workflow's run directory. This
        is used for restarting the logging information from where we
        stopped last time.
        """
        
        my_fn = os.path.join(self._run_dir, MONITORD_STATE_FILE)

        try:
            INPUT = open(my_fn, "r")
        except:
            logger.info("cannot open state file %s, continuing without state..." % (my_fn))
            return
        
        try:
            for line in INPUT:
                # Split the input line in 2, and make the second part an integer
                my_job, my_count = line.split(" ", 1)
                my_job = my_job.strip()
                my_count = int(my_count.strip())
                if my_job == "monitord_job_sequence":
                    # This is the last job_submit_seq used
                    self._job_submit_seq = my_count
                elif my_job == "monitord_dagman_out_sequence":
                    # This is the line we last read from the dagman.out file
                    self._last_processed_line = my_count
                elif my_job == "monitord_workflow_restart_count":
                    # This is the number of restarts we have seen in the past
                    self._restart_count = my_count
                else:
                    # Another job counter
                    self._job_counters[my_job] = my_count
        except:
            logger.error("error processing state file %s" % (my_fn))
        
        # Close the file
        try:
            INPUT.close()
        except:
            pass

        # All done!
        return

    def write_workflow_state(self):
        """
        This function writes the job_submit_seq and the job_counters
        dictionary to a file in the workflow's run directory. This can
        be used later for restarting the logging information from
        where we stop. This function will overwrite the log file every
        time is it called.
        """
        
        my_fn = os.path.join(self._run_dir, MONITORD_STATE_FILE)

        try:
            OUT = open(my_fn, "w")
        except:
            logger.error("cannot open state file %s" % (my_fn))
            return
        
        try:
            # Write first line with the last job_submit_seq used
            OUT.write("monitord_job_sequence %d\n" % (self._job_submit_seq))
            # Then, write the last line number of the dagman.out file we processed
            if self._line > self._last_processed_line:
                OUT.write("monitord_dagman_out_sequence %s\n" % (self._line))
            else:
                OUT.write("monitord_dagman_out_sequence %s\n" % (self._last_processed_line))
            # Next, write the restart count
            OUT.write("monitord_workflow_restart_count %d\n" % (self._restart_count))
            # Finally, write all job_counters
            for my_job in self._job_counters:
                OUT.write("%s %d\n" % (my_job, self._job_counters[my_job]))
        except:
            logger.error("cannot write state to log file %s" % (my_fn))
        
        # Close the file
        try:
            OUT.close()
        except:
            pass

        # All done!
        return

    def db_send_wf_info(self):
        """
        This function sends to the DB information about the workflow
        """
        # Check if database is configured
        if self._sink is None:
            return

        # Start empty
        kwargs = {}
        # Make sure we include the wf_uuid
        kwargs["xwf__id"] = self._wf_uuid
        # Now include others, if they are defined
        if self._dax_label is not None:
            kwargs["dax__label"] = self._dax_label
        if self._dax_version is not None:
            kwargs["dax__version"] = self._dax_version
        if self._dax_index is not None:
            kwargs["dax__index"] = self._dax_index
        if self._dax_file is not None:
            kwargs["dax__file"] = self._dax_file
        if self._dag_file_name is not None:
            kwargs["dag__file__name"] = self._dag_file_name
        if self._timestamp is not None:
            kwargs["ts"] = self._timestamp
        if self._submit_hostname is not None:
            kwargs["submit__hostname"] = self._submit_hostname
        if self._submit_dir is not None:
            kwargs["submit__dir"] = self._submit_dir
        if self._planner_arguments is not None:
            kwargs["argv"] = self._planner_arguments.strip('" \t\n\r')
        if self._user is not None:
            kwargs["user"] = self._user
        if self._grid_dn is not None:
            if self._grid_dn != "null":
                # Only add it if it is not "null"
                kwargs["grid_dn"] = self._grid_dn
        if self._planner_version is not None:
            kwargs["planner__version"] = self._planner_version
        if self._parent_workflow_id is not None:
            kwargs["parent__xwf__id"] = self._parent_workflow_id
        if self._root_workflow_id is not None:
            kwargs["root__xwf__id"] = self._root_workflow_id

        # Send workflow event to database
        self.output_to_db("wf.plan", kwargs)

    def db_send_subwf_link(self, parent_jobid, parent_jobseq):
        """
        This function sends to the DB the information linking a subwf
        to its parent job
        """
        # Check if database is configured
        if self._sink is None:
            return

        # And if we have all needed parameters
        if self._parent_workflow_id is None or parent_jobid is None or parent_jobseq is None:
            return

        # Start empty
        kwargs = {}
        # Make sure we include the wf_uuid, but note that in this
        # particular event, the xwf.id key refers to the parent
        # workflow, while the subwf.id key refers to this workflow
        kwargs["xwf__id"] = self._parent_workflow_id
        if self._timestamp is not None:
            kwargs["ts"] = self._timestamp

        kwargs["subwf__id"] = self._wf_uuid
        kwargs["job__id"] = parent_jobid
        kwargs["job_inst__id"] = parent_jobseq

        # Send sub-workflow event to database
        self.output_to_db("xwf.map.subwf_job", kwargs)

    def db_send_wf_state(self, state):
        """
        This function sends to the DB information about the current
        workflow state
        """
        # Check if database is configured
        if self._sink is None:
            return
        # Make sure parameters are not None
        if state is None:
            return

        # Start empty
        kwargs = {}
        # Make sure we include the wf_uuid
        kwargs["xwf__id"] = self._wf_uuid
        kwargs["ts"] = self._current_timestamp
        # Always decrement the restart count by 1
        kwargs["restart_count"] = self._restart_count - 1
        if state == "end":
            # Add status field for workflow.end event
            kwargs["status"] = wf._dagman_exit_code
            if wf._dagman_exit_code != 0:
                # Set level to Error if workflow did not finish successfully
                kwargs["level"] = "Error"
        state = "xwf." + state

        # Send workflow state event to database
        self.output_to_db(state, kwargs)

    def change_wf_state(self, state):
        """
        This function changes the workflow state, and sends the state
        change to the DB. This function is called as response to
        DAGMan starting/stopping.
        """
        if state == "start":
            logger.info("DAGMan starting with condor id %s" % (self._dagman_condor_id))
            self._JSDB.write("%d INTERNAL *** DAGMAN_STARTED %s ***\n" % (self._current_timestamp, self._dagman_condor_id))
            self._restart_count = self._restart_count + 1
        elif state == "end":
            self._JSDB.write("%d INTERNAL *** DAGMAN_FINISHED %s ***\n" % (self._current_timestamp, self._dagman_exit_code))
        
        self.db_send_wf_state(state)

    def start_wf(self):
        """
        This function initializes basic parameters in the Workflow class. It should
        be called every time DAGMAN starts so that we can wipe out any old state
        in case of restarts.
        """
        # We only wipe state about jobs that have completed
        logger.debug("DAGMan restarted, cleaning up old job information...")
        # Keep list of jobs whose information we want to delete
        jobs_to_delete = []

        # Compile list of jobs whose information we don't need anymore...
        for (my_jobid, my_job_submit_seq) in self._jobs:
            my_job = self._jobs[my_jobid, my_job_submit_seq]
            my_job_state = my_job._job_state
            if my_job_state == "POST_SCRIPT_SUCCESS":
                # This job is done
                jobs_to_delete.append((my_jobid, my_job_submit_seq))
            elif my_job_state == "JOB_SUCCESS":
                if my_jobid in self._job_info and self._job_info[my_jobid][3] is None:
                    # No postscript for this job
                    jobs_to_delete.append((my_jobid, my_job_submit_seq))
                else:
                    logger.debug("keeping job %s..." % (my_jobid))
            else:
                logger.debug("keeping job %s..." % (my_jobid))

        # Delete jobs...
        for (my_jobid, my_job_submit_seq) in jobs_to_delete:
            if my_jobid in self._walltime:
                del self._walltime[my_jobid]
            if my_jobid in self._job_site:
                del self._job_site[my_jobid]
            if my_jobid in self._jobs_map:
                del self._jobs_map[my_jobid]
            if (my_jobid, my_job_submit_seq) in self._jobs:
                del self._jobs[(my_jobid, my_job_submit_seq)]

        # Done!
        return

    def __init__(self, rundir, outfile, database=None,
                 workflow_config_file=None, jsd=None, root_id=None,
                 parent_id=None, parent_jobid=None, parent_jobseq=None):
        """
        This function initializes the workflow object. It looks for
        the workflow configuration file (or for workflow_config_file,
        if specified). Here we also open the jobstate.log file, and
        parse the dag.
        """
        # Initialize class variables
        self._out_file = outfile
        self._run_dir = rundir
        self._static_bp_file = None
        self._parent_workflow_id = parent_id
        self._root_workflow_id = root_id
        self._sink = database
        self._workflow_start = int(time.time())
        self._wf_uuid = None
        self._dag_file_name = None
        self._dax_label = None
        self._dax_version = None
        self._dax_file = None
        self._dax_index = None
        self._timestamp = None
        self._submit_hostname = None
        self._submit_dir = None                 # submit dir from braindump file (run dir, if submit_dir key is not found)
        self._original_submit_dir = None        # submit dir from braindump file (jsd dir, if submit_dir key is not found)
        self._planner_arguments = None
        self._user = None
        self._grid_dn = None
        self._planner_version = None
        self._last_submitted_job = None
        self._jobs_map = {}
        self._jobs = {}
        self._job_submit_seq = 1
        self._log_file = None                   # monitord.log file
        self._jsd_file = None                   # jobstate.log file
        self._JSDB = None                       # Handle for jobstate.log file
        self._job_counters = {}                 # Job counters for figuring out which output file to parse
        self._job_info = {}                     # jobid --> [sub_file, pre_exec, pre_args, post_exec, post_args, is_subdag, subdag_dag, subdag_dir]
        self._valid_braindb = True              # Flag for creating a new brain db if we don't find one
        self._write_wf_uuid = False             # Flag for appending the generated wf_uuid to the braindump file
        self._line = 0                          # line number from dagman.out file
        self._last_processed_line = 0           # line last processed by the monitoring daemon
        self._restart_count = 0                 # Keep track of how many times the workflow was restarted
        self._skipping_recovery_lines = False   # Flag for skipping the repeat duplicate messages generated by DAGMan
        self._dagman_condor_id = None           # Condor id of the current DAGMan
        self._dagman_pid = 0                    # Condor DAGMan's PID
        self._current_timestamp = 0             # Last timestamp from DAGMan
        self._dagman_exit_code = None           # Keep track of when to finish this workflow
        self._monitord_exit_code = 0            # Keep track of errors inside monitord
        self._condorlog = None                  # Condor common logfile
        self._multiline_file_flag = False       # Track multiline user log files, DAGMan > 6.6
        self._walltime = {}                     # jid --> walltime
        self._job_site = {}                     # last site a job was planned for

        # Determine location of jobstate.log file
        if jsd is None:
            self._jsd_file = os.path.join(rundir, utils.jobbase)
        else:
            # Make sure we have an absolute path
            self._jsd_file = os.path.join(rundir, jsd)

        if not os.path.isfile(self._jsd_file):
            logger.info("creating new file %s" % (self._jsd_file))

        try:
            # Create new file, or append to an existing one
            if not replay_mode:
                self._JSDB = open(self._jsd_file, 'a', 0)
            else:
                # Rotate jobstate.log file, if any
                utils.rotate_log_file(self._jsd_file)
                self._JSDB = open(self._jsd_file, 'w', 0)
        except:
            logger.critical("error creating/appending to %s!" % (self._jsd_file))
            self._monitord_exit_code = 1
            return

        if not replay_mode:
            # Recover state from a previous run
            self.read_workflow_state()

        # Parse the braindump file
        wfparams = utils.slurp_braindb(rundir, workflow_config_file)
        missing_keys = {}

        if len(wfparams) == 0:
            # Set flag for creating a braindb file if nothing was read
            self._valid_braindb = False

        # Go through wfparams, and read what we need
        if "wf_uuid" in wfparams:
            if wfparams["wf_uuid"] is not None:
                self._wf_uuid = wfparams["wf_uuid"]
        # If _wf_uuid is not defined, we create a random uuid for this workflow
        if self._wf_uuid is None:
            if uuid_present == True:
                self._wf_uuid = uuid.uuid4()
            else:
                logger.info("uuid module is not present, using time.time() to generate wf_uuid")
                self._wf_uuid = str(time.time())
            self._write_wf_uuid = True
            missing_keys["wf_uuid"] = self._wf_uuid
        # Now that we have the wf_uuid, set root_wf_uuid if not already set
        if self._root_workflow_id is None:
            self._root_workflow_id = self._wf_uuid
        if "dax_label" in wfparams:
            self._dax_label = wfparams["dax_label"]
        else:
            # Use "label" if "dax_label" not found
            if "label" in wfparams:
                self._dax_label = wfparams["label"]
        if "dax_index" in wfparams:
            self._dax_index = wfparams["dax_index"]
        if "dax_version" in wfparams:
            self._dax_version = wfparams["dax_version"]
        if "dax" in wfparams:
            self._dax_file = wfparams["dax"]
        if "dag" in wfparams:
            self._dag_file_name = wfparams["dag"]
        else:
            logger.error("dag not specified in braindump, skipping this (sub-)workflow...")
            self._monitord_exit_code = 1
            return
        if "timestamp" in wfparams:
            self._timestamp = wfparams["timestamp"]
        else:
            # Use "pegasus_wf_time" if "timestamp" not found
            if "pegasus_wf_time" in wfparams:
                self._timestamp = wfparams["pegasus_wf_time"]
        # Convert timestamp from YYYYMMDDTHHMMSSZZZZZ to Epoch
        if self._timestamp is not None:
            # Convert timestamp to epoch
            wf_timestamp = utils.epochdate(self._timestamp)
            if wf_timestamp is not None:
                self._timestamp = wf_timestamp
            else:
                # Couldn't do it, let's just use the current time
                self._timestamp = int(time.time())
        else:
            # No timestamp information is available, just use current time
            self._timestamp = int(time.time())
        if "submit_dir" in wfparams:
            self._submit_dir = wfparams["submit_dir"]
            self._original_submit_dir = os.path.normpath(wfparams["submit_dir"])
        else:
            # Use "run" if "submit_dir" not found
            if "run" in wfparams:
                self._submit_dir = wfparams["run"]
            # Use "jsd" if "submit_dir" is not found
            if "jsd" in wfparams:
                self._original_submit_dir = os.path.dirname(os.path.normpath(wfparams["jsd"]))
        if "planner_version" in wfparams:
            self._planner_version = wfparams["planner_version"]
        else:
            # Use "pegasus_version" if "planner_version" not found
            if "pegasus_version" in wfparams:
                self._planner_version = wfparams["pegasus_version"]
        if "planner_arguments" in wfparams:
            self._planner_arguments = wfparams["planner_arguments"]
        if "submit_hostname" in wfparams:
            self._submit_hostname = wfparams["submit_hostname"]
        if "user" in wfparams:
            self._user = wfparams["user"]
        if "grid_dn" in wfparams:
            self._grid_dn = wfparams["grid_dn"]

        # Say hello.... add start information to JSDB
        self._JSDB.write("%d INTERNAL *** MONITORD_STARTED ***\n" % (self._workflow_start))

        # All done... last step is to send to the database the workflow plan event,
        # along with all the static information generated by pegasus-plan
        # However, we only do this, if this is the first time we run
        if self._sink is not None and self._last_processed_line == 0:
            # Make sure NetLogger parser is available
            if NLSimpleParser is None:
                logger.critical("NetLogger parser is not loaded, exiting...")
                sys.exit(1)
            # Create NetLogger parser
            my_bp_parser = NLSimpleParser(parse_date=False)
            # Figure out static data filename, and create full path name
            my_bp_file = os.path.splitext(self._dag_file_name)[0] + ".static.bp"
            self._static_bp_file = os.path.join(self._run_dir, my_bp_file)

            # Open static bp file
            try:
                my_static_file = open(self._static_bp_file, 'r')
            except:
                logger.critical("cannot find static bp file %s, exiting..." % (self._static_bp_file))
                sys.exit(1)

            # Send workflow plan info to database
            self.db_send_wf_info()

            # If this workflow is a subworkflow and has a parent_id,
            # parent_jobid and parent_jobseq, we send an event to link
            # this workflow's id to the parent job...
            if (self._parent_workflow_id is not None
                and parent_jobid is not None and parent_jobseq is not None):
                self.db_send_subwf_link(parent_jobid, parent_jobseq)

            # Send event to mark the start of the static content
            self.output_to_db("static.start", {})

            # Process static bp file
            try:
                for my_line in my_static_file:
                    my_keys = {}
                    my_keys = my_bp_parser.parseLine(my_line)
                    if len(my_keys) == 0:
                        continue
                    if not "event" in my_keys:
                        logger.error("bad event in static bp file: %s, continuing..." % (my_line))
                        continue
                    my_event = my_keys["event"]
                    del my_keys["event"]
                    # Cut the stampede part, until Karan fixes pegasus-plan
                    if my_event.startswith("stampede."):
                        my_event = my_event.split(".", 1)[1]
                    # Send event to database
                    self.output_to_db(my_event, my_keys)
            except:
                logger.critical("error processing static bp file %s, exiting..." % (self._static_bp_file))
                sys.exit(1)
            # Close static bp file
            try:
                my_static_file.close()
            except:
                logger.warning("error closing static bp file %s, continuing..." % (self._static_bp_file))

            # Send event to mark the end of the static content
            self.output_to_db("static.end", {})

        # Add any missing keys to the braindump file
        if len(missing_keys) > 0:
            utils.add_to_braindb(rundir, missing_keys, workflow_config_file)

    def end_workflow(self):
        """
        This function writes the last line in the jobstate.log and closes
        the file.
        """
        my_workflow_end = int(time.time())

        self._JSDB.write("%d INTERNAL *** MONITORD_FINISHED %d ***\n" % (my_workflow_end, self._monitord_exit_code))
        self._JSDB.close()

        # Save all state to disk so that we can start again later
        self.write_workflow_state()

        if not replay_mode:
            # Write monitord.done file
            my_touch_name = os.path.join(self._run_dir, "monitord.done")
            try:
                TOUCH = open(my_touch_name, "w")
            except:
                logger.error("opening %s" % (my_touch_name))
            else:
                TOUCH.write("%s %.3f\n" % (utils.isodate(my_workflow_end), (my_workflow_end - self._workflow_start)))
                TOUCH.close()

            # Attempt to copy the condor common logfile to the current directory
            if self._condorlog is not None:
                if (os.path.isfile(self._condorlog) and
                    os.access(self._condorlog, os.R_OK) and
                    self._condorlog.find('/') == 0):

                    # Copy common condor log to local directory
                    my_log = out2log(self._run_dir, self._out_file)[0]
                    my_cmd = "/bin/cp -p %s %s.copy" % (self._condorlog, my_log)
                    my_status, my_output = commands.getstatusoutput(my_cmd)

                    if my_status == 0:
                        # Copy successful
                        try:
                            os.unlink(my_log)
                        except:
                            logger.error("removing %s" % (my_log))
                        else:
                            try:
                                os.rename("%s.copy" % (my_log), my_log)
                            except:
                                logger.error("renaming %s.copy to %s" % (my_log, my_log))
                            else:
                                logger.info("copied common log to %s" % (self._run_dir))
                    else:
                        logger.info("%s: %d:%s" % (my_cmd, my_status, my_output))


    def find_jobid(self, jobid):
        """
        This function finds the job_submit_seq of a given jobid by
        checking the _jobs_map dict. Since add_job will update
        _jobs_map, this function will return the job_submit_seq of the
        latest jobid added to the workflow
        """
        if jobid in self._jobs_map:
            return self._jobs_map[jobid]

        # Not found, return None
        return None

    def find_job_submit_seq(self, jobid):
        """
        If a jobid already exists and is in the PRE_SCRIPT_SUCCESS
        mode, this function returns its job_submit_seq. Otherwise, it
        returns None, meaning a new job needs to be created
        """
        # Look for a jobid
        my_job_submit_seq = self.find_jobid(jobid)

        # No such job, return None
        if my_job_submit_seq is None:
            return None

        # Make sure the job is there
        if not (jobid, my_job_submit_seq) in self._jobs:
            logger.warning("cannot find job: %s, %s" % (jobid, my_job_submit_seq))
            return None

        my_job = self._jobs[jobid, my_job_submit_seq]
        if my_job._job_state == "PRE_SCRIPT_SUCCESS" or my_job._job_state == "DAGMAN_SUBMIT":
            # jobid is in "PRE_SCRIPT_SUCCESS" or "DAGMAN_SUBMIT"  state,
            # just return job_submit_seq
            return my_job_submit_seq

        # jobid is in another state, return None
        return None

    def db_send_job_brief(self, my_job, event, status=None):
        """
        This function sends to the DB basic state events for a
        particular job
        """
        # Check if database is configured
        if self._sink is None:
            return

        # Start empty
        kwargs = {}

        # Make sure we include the wf_uuid, name, and job_submit_seq
        kwargs["xwf__id"] = my_job._wf_uuid
        kwargs["job__id"] = my_job._exec_job_id
        kwargs["job_inst__id"] = my_job._job_submit_seq
        kwargs["ts"] = my_job._job_state_timestamp
        kwargs["js__id"] = my_job._job_state_seq
        if my_job._sched_id is not None:
            kwargs["sched__id"] = my_job._sched_id
        if status is not None:
            kwargs["status"] = str(status)
            if status != 0:
                kwargs["level"] = "Error"

        # Send job state event to database
        self.output_to_db("job_inst." + event, kwargs)

    def db_send_job_start(self, my_job):
        """
        This function sends to the DB the main.start event
        """
        # Check if database is configured
        if self._sink is None:
            return

        # Start empty
        kwargs = {}

        # Make sure we include the wf_uuid, name, and job_submit_seq
        kwargs["xwf__id"] = my_job._wf_uuid
        kwargs["job__id"] = my_job._exec_job_id
        kwargs["job_inst__id"] = my_job._job_submit_seq
        kwargs["ts"] = my_job._job_state_timestamp
        kwargs["js__id"] = my_job._job_state_seq

        if my_job._input_file is not None:
            kwargs["stdin.file"] = my_job._input_file
        if my_job._output_file is not None:
            kwargs["stdout.file"] = my_job._output_file
        if my_job._error_file is not None:
            kwargs["stderr.file"] = my_job._error_file
        if my_job._sched_id is not None:
            kwargs["sched__id"] = my_job._sched_id

        # Send job state event to database
        self.output_to_db("job_inst.main.start", kwargs)

    def db_send_job_end(self, my_job, status=None):
        """
        This function sends to the DB the main.end event
        """
        # Check if database is configured
        if self._sink is None:
            return

        # Start empty
        kwargs = {}

        # Make sure we include the wf_uuid, name, and job_submit_seq
        kwargs["xwf__id"] = my_job._wf_uuid
        kwargs["job__id"] = my_job._exec_job_id
        kwargs["job_inst__id"] = my_job._job_submit_seq
        kwargs["ts"] = my_job._job_state_timestamp
        kwargs["js__id"] = my_job._job_state_seq

        if my_job._site_name is not None:
            kwargs["site"] = my_job._site_name
        else:
            kwargs["site"] = ""
        if my_job._remote_user is not None:
            kwargs["user"] = my_job._remote_user
        else:
            kwargs["user"] = ""
        if my_job._remote_working_dir is not None:
            kwargs["work_dir"] = my_job._remote_working_dir
        else:
            kwargs["work_dir"] = ""
        if my_job._cluster_start_time is not None:
            kwargs["cluster__start"] = my_job._cluster_start_time
        if my_job._cluster_duration is not None:
            kwargs["cluster__dur"] = my_job._cluster_duration
        if my_job._main_job_start is not None and my_job._main_job_done is not None:
            # If we have both timestamps, let's try to compute the local duration
            try:
                my_duration = int(my_job._main_job_done) - int(my_job._main_job_start)
                kwargs["local_duration"] = my_duration
            except:
                # Nothing to do, this is not mandatory
                pass
        if my_job._input_file is not None:
            kwargs["stdin__file"] = my_job._input_file
        else:
            # This is not mandatory, according to the schema
            pass
        if my_job._output_file is not None:
            kwargs["stdout__file"] = my_job._output_file + ".%03d" % (my_job._job_output_counter)
        else:
            kwargs["stdout__file"] = ""
        if my_job._error_file is not None:
            kwargs["stderr__file"] = my_job._error_file + ".%03d" % (my_job._job_output_counter)
        else:
            kwargs["stderr__file"] = ""
        kwargs["stdout__text"] = my_job._stdout_text
        kwargs["stderr__text"] = my_job._stderr_text

        if my_job._sched_id is not None:
            kwargs["sched__id"] = my_job._sched_id
        if status is not None:
            kwargs["status"] = str(status)
            if status != 0:
                kwargs["level"] = "Error"
        else:
            kwargs["status"] = str(-1)
            kwargs["level"] = "Error"

        # Send job state event to database
        self.output_to_db("job_inst.main.end", kwargs)

    def db_send_task_start(self, my_job, task_type, task_id=None, invocation_record={}):
        """
        This function sends to the database task start
        events. task_type is either "PRE_SCRIPT", "MAIN_JOB", or
        "POST_SCRIPT"
        """
        # Check if database is configured
        if self._sink is None:
            return

        # Start empty
        kwargs = {}

        # Sanity check, verify task type
        if task_type != "PRE_SCRIPT" and task_type != "POST_SCRIPT" and task_type != "MAIN_JOB":
            logger.warning("unknown task type: %s" % (task_type))
            return

        # Make sure we include the wf_uuid, name, and job_submit_seq
        kwargs["xwf__id"] = my_job._wf_uuid
        kwargs["job__id"] = my_job._exec_job_id
        kwargs["job_inst__id"] = my_job._job_submit_seq

        if task_type == "PRE_SCRIPT":
            # This is a PRE SCRIPT invocation
            # Add PRE_SCRIPT task id to this event
            kwargs["inv__id"] = PRESCRIPT_TASK_ID
            kwargs["ts"] = my_job._pre_script_start
        elif task_type == "POST_SCRIPT":
            # This is a POST SCRIPT invocation
            kwargs["inv__id"] = POSTSCRIPT_TASK_ID
            kwargs["ts"] = my_job._post_script_start
        elif task_type == "MAIN_JOB":
            # This is a MAIN JOB invocation
            if task_id is not None:
                kwargs["inv__id"] = task_id
            else:
                logger.warning("warning: task id is not specified... skipping task...")
                return
            if "start" in invocation_record:
                # Need to convert it to epoch data
                my_start = utils.epochdate(invocation_record["start"])
            else:
                # Not in the invocation record, let's use our own time keeping
                my_start = my_job._main_job_start
                if my_start is None:
                    # This must be a zero duration job (without an ULOG_EXECUTE), just use the end time
                    my_start = my_job._main_job_done
            if my_start is not None:
                kwargs["ts"] = my_start

        # Send job event to database
        self.output_to_db("inv.start", kwargs)

    def db_send_task_end(self, my_job, task_type, task_id=None, invocation_record={}):
        """
        This function sends to the database task end events with all
        the information. task_type is either "PRE_SCRIPT", "MAIN_JOB",
        or "POST_SCRIPT"
        """
        # Check if database is configured
        if self._sink is None:
            return

        # Start empty
        kwargs = {}

        # Sanity check, verify task type
        if task_type != "PRE_SCRIPT" and task_type != "POST_SCRIPT" and task_type != "MAIN_JOB":
            logger.warning("unknown task type: %s" % (task_type))
            return

        # Make sure we include the wf_uuid, name, and job_submit_seq
        kwargs["xwf__id"] = my_job._wf_uuid
        kwargs["job__id"] = my_job._exec_job_id
        kwargs["job_inst__id"] = my_job._job_submit_seq

        if task_type == "PRE_SCRIPT":
            # This is a PRE SCRIPT invocation
            kwargs["inv__id"] = PRESCRIPT_TASK_ID
            kwargs["transformation"] = "dagman::pre"
            # For prescript tasks, nothing to put in the task_id field
            if my_job._pre_script_start is not None:
                kwargs["start_time"] = my_job._pre_script_start
            else:
                kwargs["start_time"] = my_job._pre_script_done
            try:
                kwargs["dur"] = my_job._pre_script_done - my_job._pre_script_start
            except:
                # Duration cannot be determined, possibly a missing PRE_SCRIPT_START event
                kwargs["dur"] = 0
            kwargs["exitcode"] = str(my_job._pre_script_exitcode)
            if my_job._exec_job_id in self._job_info:
                if self._job_info[my_job._exec_job_id][1] is not None:
                    kwargs["executable"] = self._job_info[my_job._exec_job_id][1]
                else:
                    kwargs["executable"] = ""
                if self._job_info[my_job._exec_job_id][2] is not None:
                    kwargs["argv"] = self._job_info[my_job._exec_job_id][2]
            else:
                kwargs["executable"] = ""
            kwargs["ts"] = my_job._pre_script_done
        elif task_type == "POST_SCRIPT":
            # This is a POST SCRIPT invocation
            kwargs["inv__id"] = POSTSCRIPT_TASK_ID
            kwargs["transformation"] = "dagman::post"
            # For postscript tasks, nothing to put in the task_id field
            if my_job._post_script_start is not None:
                kwargs["start_time"] = my_job._post_script_start
            else:
                kwargs["start_time"] = my_job._post_script_done
            try:
                kwargs["dur"] = my_job._post_script_done - my_job._post_script_start
            except:
                # Duration cannot be determined, possibly a missing POST_SCRIPT_START event
                kwargs["dur"] = 0
            kwargs["exitcode"] = str(my_job._post_script_exitcode)
            if my_job._exec_job_id in self._job_info:
                if self._job_info[my_job._exec_job_id][3] is not None:
                    kwargs["executable"] = self._job_info[my_job._exec_job_id][3]
                else:
                    kwargs["executable"] = ""
                if self._job_info[my_job._exec_job_id][4] is not None:
                    kwargs["argv"] = self._job_info[my_job._exec_job_id][4]
            else:
                kwargs["executable"] = ""
            kwargs["ts"] = my_job._post_script_done
        elif task_type == "MAIN_JOB":
            # This is a MAIN JOB invocation
            if task_id is not None:
                kwargs["inv__id"] = task_id
            else:
                logger.warning("warning: task id is not specified... skipping task...")
                return
            if "transformation" in invocation_record:
                kwargs["transformation"] = invocation_record["transformation"]
            else:
                if my_job._main_job_transformation is not None:
                    kwargs["transformation"] = my_job._main_job_transformation
                else:
                    if (my_job._exec_job_id in self._job_info and
                        self._job_info[my_job._exec_job_id][5] == True):
                        kwargs["transformation"] = "condor::dagman"
            if "derivation" in invocation_record:
                if invocation_record["derivation"] != "null":
                    # Make sure it is not "null"
                    kwargs["task__id"] = invocation_record["derivation"]
            else:
                # Nothing to do if we cannot get the derivation from the kickstart recort
                pass
            if "start" in invocation_record:
                # Need to convert it to epoch data
                my_start = utils.epochdate(invocation_record["start"])
            else:
                # Not in the invocation record, let's use our own time keeping
                my_start = my_job._main_job_start
                if my_start is None:
                    # This must be a zero duration job (without an ULOG_EXECUTE), just use the end time
                    my_start = my_job._main_job_done
            if my_start is not None:
                kwargs["start_time"] = my_start
            if "duration" in invocation_record:
                kwargs["dur"] = invocation_record["duration"]
            else:
                # Duration not in the invocation record
                if my_job._main_job_start is not None and my_job._main_job_done is not None:
                    try:
                        my_duration = int(my_job._main_job_done) - int(my_job._main_job_start)
                    except:
                        my_duration = None
                    if my_duration is not None:
                        kwargs["dur"] = my_duration
                elif my_job._main_job_done is not None:
                    # This must be a zero duration job (without an ULOG_EXECUTE)
                    # In this case, duration should be set to ZERO
                    kwargs["dur"] = 0
            if my_start is not None and "duration" in invocation_record:
                # Calculate timestamp for when this task finished
                try:
                    kwargs["ts"] = int(my_start + int(invocation_record["duration"]))
                except:
                    # Something went wrong, just use the time the main job finished
                    kwargs["ts"] = my_job._main_job_done
            else:
                kwargs["ts"] = my_job._main_job_done
            if "exitcode" in invocation_record:
                kwargs["exitcode"] = invocation_record["exitcode"]
            else:
                if my_job._main_job_exitcode is not None:
                    kwargs["exitcode"] = str(my_job._main_job_exitcode)
            if "name" in invocation_record:
                kwargs["executable"] = invocation_record["name"]
            else:
                if my_job._main_job_executable is not None:
                    kwargs["executable"] = my_job._main_job_executable
                else:
                    if (my_job._exec_job_id in self._job_info and
                        self._job_info[my_job._exec_job_id][5] == True):
                        kwargs["executable"] = condor_dagman_executable
                    else:
                        kwargs["executable"] = ""
            if "argument-vector" in invocation_record:
                kwargs["argv"] = invocation_record["argument-vector"]
            else:
                if my_job._main_job_arguments is not None:
                    kwargs["argv"] = my_job._main_job_arguments

        if "exitcode" in kwargs:
            if kwargs["exitcode"] != "0":
                kwargs["level"] = "Error"
        else:
            kwargs["level"] = "Error"

        # Send job event to database
        self.output_to_db("inv.end", kwargs)

    def db_send_host_info(self, my_job, record):
        """
        This function sends host information collected from the
        kickstart record to the database.
        """
        # Check if database is configured
        if self._sink is None:
            return

        # Start empty
        kwargs = {}

        # Make sure we include the wf_uuid, name, and job_submit_seq
        kwargs["xwf__id"] = my_job._wf_uuid
        kwargs["job__id"] = my_job._exec_job_id
        kwargs["job_inst__id"] = my_job._job_submit_seq

        # Add information about the host
        if "hostname" in record:
            kwargs["hostname"] = record["hostname"]
        else:
            # Don't know what the hostname is
            kwargs["hostname"] = "unknown"
        if "hostaddr" in record:
            kwargs["ip"] = record["hostaddr"]
        else:
            # Don't know what the ip address is
            kwargs["ip_address"] = "unknown"
        if "resource" in record:
            kwargs["site"] = record["resource"]
        else:
            # Don't know what the site name is
            kwargs["site"] = "unknown"
        if "total" in record:
            kwargs["total_memory"] = record["total"]
        else:
            # This is not mandatory
            pass
        if "system" in record and "release" in record and "machine" in record:
            kwargs["uname"] = record["system"] + "-" + record["release"] + "-" + record["machine"]
        else:
            # This is not mandatory
            pass

        # Add timestamp
        kwargs["ts"] = self._current_timestamp

        # Send host event to database
        self.output_to_db("job_inst.host.info", kwargs)

    def parse_job_output(self, my_job, job_state):
        """
        This function tries to parse the kickstart output file of a
        given job and collect information for the stampede schema.
        """
        my_output = []
        skip_kickstart_parsing = False

        # Check if this is a subdag job
        if (my_job._exec_job_id in self._job_info and
            self._job_info[my_job._exec_job_id][5] == True):
            # Disable kickstart_parsing...
            skip_kickstart_parsing = True

        # If job is a subdag job, skip looking for its kickstart output
        if not skip_kickstart_parsing:
            # Compose kickstart output file name (base is the filename before rotation)
            my_job_output_fn_base = os.path.join(self._run_dir, my_job._exec_job_id) + ".out"
            my_job_output_fn = my_job_output_fn_base + ".%03d" % (my_job._job_output_counter)

            # First assume we will find rotated file
            my_parser = kickstart_parser.Parser(my_job_output_fn)
            my_output = my_parser.parse_stampede()

            # Check if we were able to find it
            if my_parser._open_error == True:
                # File wasn't there, look for the file before the rotation
                my_parser.__init__(my_job_output_fn_base)
                my_output = my_parser.parse_stampede()

                if my_parser._open_error == True:
                    # Couldn't find it again, one last try, as it might have just been moved
                    my_parser.__init__(my_job_output_fn)
                    my_output = my_parser.parse_stampede()

            # Check if successful
            if my_parser._open_error == True:
                logger.info("unable to find output file for job %s" % (my_job._exec_job_id))

        # Initialize task id counter
        my_task_id = 1

        if len(my_output) > 0:
            # Parsing the output file resulted in some info... let's parse it

            # Add job information to the Job class.
            my_invocation_found = my_job.extract_job_info(my_output)

            if my_invocation_found:
                # Loop through all records
                for record in my_output:
                    # Skip non-invocation records
                    if not "invocation" in record:
                        continue
	
                    # Send task information to the database
                    self.db_send_task_start(my_job, "MAIN_JOB", my_task_id, record)
                    self.db_send_task_end(my_job, "MAIN_JOB", my_task_id, record)

                    # Increment task id counter
                    my_task_id = my_task_id + 1

                    # Send host information to the database
                    self.db_send_host_info(my_job, record)
            else:
                # No invocation found, but possibly task records are present...
                my_tasks = {}
                for record in my_output:
                    if "task" in record:
                        # Ok, this is a task record
                        if not "id" in record:
                            logger.warning("id missing from task record... skipping to next one")
                            continue
                        try:
                            my_id = int(record["id"])
                        except:
                            logger.warning("task id looks invalid, cannot convert it to int: %s skipping to next" % (record["id"]))
                            continue
                        # Add to our list
                        my_tasks[my_id] = record

                if len(my_tasks) > 0:
                    # Now, bring information from the .in file
                    my_status = self.parse_in_file(my_job._exec_job_id, my_tasks)
                    if my_status is True:
                        # Parsing the in file completed, now generate tasks by task order
                        for i in sorted(my_tasks):
                            record = my_tasks[i]
                            # Take care of renaming the exitcode field
                            if "status" in record:
                                record["exitcode"] = record["status"]
                            # Validate record
                            if (not "transformation" in record or not "derivation" in record or
                                not "start" in record or not "duration" in record or
                                not "name" in record or not "argument-vector" in record):
                                logger.info("task %d has incomplete information, skipping it..." % (i))
                                continue
                            # Ok, it all validates, send task information to the database
                            self.db_send_task_start(my_job, "MAIN_JOB", my_task_id, record)
                            self.db_send_task_end(my_job, "MAIN_JOB", my_task_id, record)

                            # Increment task id counter
                            my_task_id = my_task_id + 1
                else:
                    # No tasks found...
                    logger.info("no tasks found for job %s..." % (my_job._exec_job_id))
        else:
            # If we don't have any records, we only generate 1 task
            self.db_send_task_start(my_job, "MAIN_JOB", my_task_id)
            self.db_send_task_end(my_job, "MAIN_JOB", my_task_id)

    def add_job(self, jobid, job_state, sched_id=None):
        """
        This function adds a new job to our list of jobs. It first checks if
        the job is already in our list in the PRE_SCRIPT_SUCCESS state, if so,
        we just update its sched id. Otherwise we create a new Job container.
        In any case, we always set the job state to job_state.
        """

        my_job_submit_seq = self.find_job_submit_seq(jobid)

        if my_job_submit_seq is not None:
            # Job already exists
            if not (jobid, my_job_submit_seq) in self._jobs:
                logger.warning("cannot find job: %s, %s" % (jobid, my_job_submit_seq))
                return

            my_job = self._jobs[jobid, my_job_submit_seq]

            # Set sched_id
            if sched_id is not None:
                my_job._sched_id = sched_id

            # Update job state
            my_job._job_state = job_state
            my_job._job_state_timestamp = int(self._current_timestamp)
        else:
            # This is a new job, we have to do everything from scratch
            my_job_submit_seq = self._job_submit_seq

            # Make sure job is not already there
            if (jobid, my_job_submit_seq) in self._jobs:
                logger.warning("trying to add job twice: %s, %s" % (jobid, my_job_submit_seq))
                return

            # Create new job container
            my_job = Job(self._wf_uuid, jobid, my_job_submit_seq)
            # Set job state
            my_job._job_state = job_state
            my_job._job_state_timestamp = int(self._current_timestamp)
            # Set sched_id
            my_job._sched_id = sched_id
            # Add job to our list of jobs
            self._jobs[jobid, my_job_submit_seq] = my_job

            # Add/Update job in our job map
            self._jobs_map[jobid] = my_job_submit_seq

            # Update job_submit_seq
            self._job_submit_seq = self._job_submit_seq + 1

        # Update job counter if this job is in the SUBMIT state
        if job_state == "SUBMIT":
            if jobid in self._job_counters:
                # Counter already exists for this job, just increate it by 1
                self._job_counters[jobid] = self._job_counters[jobid] + 1
            else:
                # No counter for this job yet
                self._job_counters[jobid] = 0
            # Now, we set the job output counter for this particular job
            my_job._job_output_counter = self._job_counters[jobid]

        return my_job_submit_seq

    def job_update_info(self, jobid, job_submit_seq, sched_id=None):
        """
        This function adds info to an exising job.
        """

        # Make sure job is already there
        if not (jobid, job_submit_seq) in self._jobs:
            logger.warning("cannot find job: %s, %s" % (jobid, job_submit_seq))
            return

        my_job = self._jobs[jobid, job_submit_seq]
        # Set sched_id
        my_job._sched_id = sched_id

        # Everything done
        return

    def update_job_state(self, jobid, job_submit_seq, job_state, status, walltime):
        """
        This function updates a	job's state, and also writes
        a line in our output file.
        """
        # Find job
        if job_submit_seq is None:
            # Need to get job_submit_seq from our hash table
            if jobid in self._jobs_map:
                job_submit_seq = self._jobs_map[jobid]
        if not (jobid, job_submit_seq) in self._jobs:
            logger.warning("cannot find job: %s, %s" % (jobid, job_submit_seq))
            return
        # Got it
        my_job = self._jobs[jobid, job_submit_seq]
        # Update job state
        my_job.set_job_state(job_state, self._current_timestamp, status)

        # Make status a string so we can print properly
        if status is not None:
            status = str(status)

        # Create content -- use one space only
        my_line = "%d %s %s %s %s %s %d" % (self._current_timestamp, jobid, job_state,
                                            status or my_job._sched_id or '-',
                                            my_job._site_name or '-',
                                            walltime or '-',
                                            job_submit_seq or '-')
        logger.info("new state %s" % (my_line))

        # Prepare for atomic append
        self._JSDB.write("%s\n" % (my_line))

        if self._sink is None:
            # Not using a database, nothing else to do!
            return

        # Check if we need to send any tasks to the database
        if job_state == "PRE_SCRIPT_SUCCESS" or job_state == "PRE_SCRIPT_FAILURE":
            # PRE script finished
            self.db_send_task_start(my_job, "PRE_SCRIPT")
            self.db_send_task_end(my_job, "PRE_SCRIPT")
        elif job_state == "POST_SCRIPT_SUCCESS" or job_state == "POST_SCRIPT_FAILURE":
            # POST script finished
            self.db_send_task_start(my_job, "POST_SCRIPT")
            self.db_send_task_end(my_job, "POST_SCRIPT")
        elif job_state == "JOB_SUCCESS" or job_state == "JOB_FAILURE":
            # Main job has ended
            self.parse_job_output(my_job, job_state)

        # Now, figure out what state event we need to send to the database
        if job_state == "PRE_SCRIPT_STARTED":
            self.db_send_job_brief(my_job, "pre.start")
        elif job_state == "PRE_SCRIPT_TERMINATED":
            self.db_send_job_brief(my_job, "pre.term")
        elif job_state == "PRE_SCRIPT_SUCCESS":
            self.db_send_job_brief(my_job, "pre.end", 0)
        elif job_state == "PRE_SCRIPT_FAILED":
            self.db_send_job_brief(my_job, "pre.end", -1)
        elif job_state == "SUBMIT":
            self.db_send_job_brief(my_job, "submit.start")
            self.db_send_job_brief(my_job, "submit.end", 0)
        elif job_state == "GRID_SUBMIT":
            self.db_send_job_brief(my_job, "grid.submit.start")
            self.db_send_job_brief(my_job, "grid.submit.end", 0)
        elif job_state == "GLOBUS_SUBMIT":
            self.db_send_job_brief(my_job, "globus.submit.start")
            self.db_send_job_brief(my_job, "globus.submit.end", 0)
        elif job_state == "SUBMIT_FAILED":
            self.db_send_job_brief(my_job, "submit.start")
            self.db_send_job_brief(my_job, "submit.end", -1)
        elif job_state == "GLOBUS_SUBMIT_FAILED":
            self.db_send_job_brief(my_job, "globus.submit.start")
            self.db_send_job_brief(my_job, "globus.submit.end", -1)
        elif job_state == "GRID_SUBMIT_FAILED":
            self.db_send_job_brief(my_job, "grid.submit.start")
            self.db_send_job_brief(my_job, "grid.submit.end", -1)
        elif job_state == "EXECUTE":
            self.db_send_job_start(my_job)
        elif job_state == "REMOTE_ERROR":
            self.db_send_job_brief(my_job, "remote_error")
        elif job_state == "IMAGE_SIZE":
            self.db_send_job_brief(my_job, "image.info")
        elif job_state == "JOB_TERMINATED":
            self.db_send_job_brief(my_job, "main.term", 0)
        elif job_state == "JOB_SUCCESS":
            self.db_send_job_end(my_job, 0)
        elif job_state == "JOB_FAILURE":
            self.db_send_job_end(my_job, -1)
        elif job_state == "JOB_HELD":
            self.db_send_job_brief(my_job, "held.start")
        elif job_state == "JOB_EVICTED":
            self.db_send_job_brief(my_job, "main.term", -1)
        elif job_state == "JOB_RELEASED":
            self.db_send_job_brief(my_job, "held.end", 0)
        elif job_state == "POST_SCRIPT_STARTED":
            self.db_send_job_brief(my_job, "post.start")
        elif job_state == "POST_SCRIPT_TERMINATED":
            self.db_send_job_brief(my_job, "post.term")
        elif job_state == "POST_SCRIPT_SUCCESS":
            self.db_send_job_brief(my_job, "post.end", 0)
        elif job_state == "POST_SCRIPT_FAILED":
            self.db_send_job_brief(my_job, "post.end", -1)

    def parse_job_sub_file(self, jobid, job_submit_seq):
        """
        This function calls a function in the Job class to parse
        a job's submit file and extract planning information
        """

        # Find job
        if not (jobid, job_submit_seq) in self._jobs:
            logger.warning("cannot find job: %s, %s" % (jobid, job_submit_seq))
            return None, None

        # Check if we have an entry for this job
        if not jobid in self._job_info:
            return None, None

        # Make sure if we have a file for this entry
        # (should always be there, except for SUBDAG jobs)
        if self._job_info[jobid][0] is None:
            return None, None

        # Got everything
        my_job = self._jobs[jobid, job_submit_seq]

        # Parse sub file
        my_diff, my_site = my_job.parse_sub_file(self._current_timestamp, self._job_info[jobid][0])

        # Change input, output, and error files to be relative to the submit directory
        try:
            if my_job._input_file.find(self._original_submit_dir) >= 0:
                # Path to file includes original submit_dir, let's try to remove it
                my_job._input_file = os.path.normpath(my_job._input_file.replace((self._original_submit_dir + os.sep), '', 1))
        except:
            # Something went wrong, let's just keep what we had...
            pass
        try:
            if my_job._output_file.find(self._original_submit_dir) >= 0:
                # Path to file includes original submit_dir, let's try to remove it
                my_job._output_file = os.path.normpath(my_job._output_file.replace((self._original_submit_dir + os.sep), '', 1))
        except:
            # Something went wrong, let's just keep what we had...
            pass
        try:
            if my_job._error_file.find(self._original_submit_dir) >= 0:
                # Path to file includes original submit_dir, let's try to remove it
                my_job._error_file = os.path.normpath(my_job._error_file.replace((self._original_submit_dir + os.sep), '', 1))
        except:
            # Something went wrong, let's just use what we had...
            pass

        # All done
        return my_diff, my_site

    def has_subworkflow(self, jobid):
        """
        This function returns a new dagman.out file to follow if the
        job is either a SUBDAG job, a pegasus-plan, or a subdax_
        job. Otherwise, it returns None.
        """
        # This shouldn't be the case...
        if not jobid in self._job_info:
            return None

        # First we take care of SUBDAG jobs
        if self._job_info[jobid][5] == True:
            # We cannot go into SUBDAG workflows as they are not
            # planned by Pegasus and do not contain the information
            # needed by the 3.1 Stampede schema.
            return None
#            # This is a SUBDAG job, first check if dag is there
#            if self._job_info[jobid][6] is None:
#                return None
#            # Looks ok, return new dagman.out
#            my_dagman_out = self._job_info[jobid][6] + ".dagman.out"
        else:
            # Now check if this is a pegasus-plan or a subdax_ job

            # First, look for a jobid
            my_job_submit_seq = self.find_jobid(jobid)

            # No such job, return None
            if my_job_submit_seq is None:
                return None

            # Make sure the job is there
            if not (jobid, my_job_submit_seq) in self._jobs:
                logger.warning("cannot find job: %s, %s" % (jobid, my_job_submit_seq))
                return None

            my_job = self._jobs[jobid, my_job_submit_seq]
            my_dagman_out = my_job._job_dagman_out
            if my_dagman_out is None:
                return None

        # Got it!
        my_dagman_out = os.path.normpath(my_dagman_out)
        if my_dagman_out.find(self._original_submit_dir) >= 0:
            # Path to new dagman.out file includes original submit_dir, let's try to change it
            new_dagman_out = os.path.normpath(my_dagman_out.replace((self._original_submit_dir + os.sep), '', 1))
            # Join with current run directory
            new_dagman_out = os.path.join(self._run_dir, new_dagman_out)
            return new_dagman_out

#        try:
#            my_dagman_out = os.path.relpath(my_dagman_out, self._original_submit_dir)
#        except:
#            pass

        return my_dagman_out

# Workflow Entry Class
class WorkflowEntry:
    """
    Class used to store one workflow entry
    """
    run_dir = None			# Run directory for the workflow
    dagman_out = None			# Location of the dagman.out file
    n_retries = 0			# Number of retries for looking for the dagman.out file
    wf = None				# Pointer to the Workflow class for this Workflow
    DMOF = None				# File pointer once we open the dagman.out file
    ml_buffer = ''			# Buffer for reading the dagman.out file
    ml_retries = 0			# Keep track of how many times we have looked for new content
    ml_current = 0			# Keep track of where we are in the dagman.out file
    delete_workflow = False		# Flag for dropping this workflow
    sleep_time = None			# Time to sleep for this workflow

jsd = None				# location of jobstate.log file
nodaemon = 0				# foreground mode
logfile = None				# location of monitord.log file
millisleep = None			# emulated run mode delay
adjustment = 0				# time zone adjustment (@#~! Condor)
fuse = 60				# maximum wait for each plotting subscript

# Parse command line options
prog_usage = "usage: %s [options] workflow.dag.dagman.out" % (prog_base)
prog_desc = """Mandatory arguments: outfile is the log file produced by Condor DAGMan, usually ending in the suffix ".dag.dagman.out"."""

parser = optparse.OptionParser(usage=prog_usage, description=prog_desc)

parser.add_option("-a", "--adjust", action = "store", type = "int", dest = "adjustment",
		  help = "adjust for time zone differences by i seconds, default 0")
parser.add_option("-N", "--foreground", action = "store_const", const = 2, dest = "nodaemon",
		  help = "(Condor) don't daemonize %s; go through motions as if" % (prog_base))
parser.add_option("-n", "--no-daemon", action = "store_const", const = 1, dest = "nodaemon",
		  help = "(debug) don't daemonize %s; keep it in the foreground" % (prog_base))
parser.add_option("--show", action = "store_const", const = 1, dest = "doplot",
		  help = "create diagrams of workflow upon normal exit")
parser.add_option("--fuse", action = "store", type = "int", dest = "fuse",
		  help = "maximum wait for each plotting subscript, default %d s" % (fuse))
parser.add_option("-j", "--job", action = "store", type = "string", dest = "jsd",
		  help = "alternative job state file to write, default is %s in the workflow's directory"
		  % (utils.jobbase))
parser.add_option("-l", "--log", action = "store", type = "string", dest = "logfile",
		  help = "alternative %s log file, default is %s in the workflow's directory"
		  % (prog_base, logbase))
parser.add_option("--conf", action = "store", type = "string", dest = "config_properties",
		  help = "specifies the properties' file to use. This option overrides all other property files.")
parser.add_option("--no-recursive", action = "store_const", const = 1, dest = "disable_subworkflows",
		  help = "disable pegasus-monitord the automatic follow any sub-workflows that are found")
parser.add_option("--no-database", "--nodatabase", "--no-events", action = "store_const", const = 0, dest = "no_events",
		  help = "turn off event generation completely, and overrides the URL in the -d option")
parser.add_option("-S", "--sim", action = "store", type = "int", dest = "millisleep",
		  help = "Developer: simulate delays between reads by sleeping ms milliseconds")
parser.add_option("-r", "--replay", action = "store_const", const = 1, dest = "replay_mode",
		  help = "disables checking for DAGMan's pid while running %s" % (prog_base))
parser.add_option("--db-stats", action = "store_const", const = "yes", dest = "db_stats",
                  help = "collect and print database stats at the end")
parser.add_option("--keep-state", action = "store_const", const = 1, dest = "keep_state",
                  help = "keep state across several DAGMan start/stop cycles (development option)")
parser.add_option("-v", "--verbose", action="count", default=0, dest="vb", help="Increase verbosity, repeatable")
grp = optparse.OptionGroup(parser, "Output options")
grp.add_option("-d", "--dest", action="store", dest="event_dest", metavar="PATH or URL",
               help="Output destination URL [<scheme>]<params>, where "
               "scheme = [empty] | x-tcp:// | DB-dialect+driver://. "
               "For empty scheme, params are a file path with '-' meaning standard output. "
               "For x-tcp scheme, params are TCP host[:port=14380]. "
               "For DB, use SQLAlchemy engine URL. "
               "(default=sqlite:///<dagman-output-file>.stampede.db)",   default=None)
grp.add_option("-e", "--encoding", action='store', dest='enc', default="bp", metavar='FORMAT',
                help="How to encode log events: bson | bp (default=%default)")
parser.add_option_group(grp)

# Re-insert our base name to avoid optparse confusion when printing error messages
# (options, args) = parser.parse_args(sys.argv[0:]) # Does not work 100%
sys.argv.insert(0, prog_base)
(options, args) = parser.parse_args()

# Remaining argument is .dag.dagman.out file
if len(args) != 1:
    parser.print_help()
    sys.exit(1)

out = args[0]

if not out.endswith(".dagman.out"):
    parser.print_help()
    sys.exit(1)

# Turn into absolute filename
out = os.path.abspath(out)

# Infer run directory
run = os.path.dirname(out)

# Resolve command-line options conflicts
if options.event_dest is not None and options.no_events is not None:
    logger.warning("the --no-events and --dest options conflict, please use only one of them")
    sys.exit(1)

# Get the location of the properties file from braindump
top_level_wf_params = utils.slurp_braindb(run)
# Add None if properties tag is missing
if not "properties" in top_level_wf_params:
    top_level_wf_params["properties"] = None

# Parse, and process properties
props = properties.Properties()
props.new(config_file=options.config_properties, rundir_propfile=top_level_wf_params["properties"])

doplot = utils.make_boolean(props.property("pegasus.monitord.show"))
fuse = int(props.property("pegasus.monitord.fuse") or 300)
if fuse < 60:
    fuse = 60

# Copy command line options into our variables

if options.vb == 0:
    lvl = logging.WARN
elif options.vb == 1:
    lvl = logging.INFO
else:
    lvl = logging.DEBUG

# Set logging level
logger.setLevel(lvl)
# Cache whether debugging
g_isdbg = logger.isEnabledFor(logging.DEBUG)

if options.adjustment is not None:
    adjustment = options.adjustment
if options.nodaemon is not None:
    nodaemon = options.nodaemon
if options.doplot is not None:
    doplot = options.doplot
if options.fuse is not None:
    fuse = options.fuse
if options.jsd is not None:
    jsd = options.jsd
if options.logfile is not None:
    logfile = options.logfile
if options.millisleep is not None:
    millisleep = options.millisleep
if options.replay_mode is not None:
    replay_mode = options.replay_mode
    # Replay mode always runs in foreground
    nodaemon = 1
if options.disable_subworkflows is not None:
    follow_subworkflows = False
if options.db_stats is not None:
    db_stats = options.db_stats
if options.keep_state is not None:
    keep_state = options.keep_state
if options.event_dest is None:
    if options.no_events is not None:
        # Turn off event generation
        no_events = True
    else:
        if props.property("pegasus.monitord.events") is not None:
            # Set event generation according to properties (default is True)
            no_events = not utils.make_boolean(props.property("pegasus.monitord.events"))
        else:
            # Default is to generate events
            no_events = False

    if props.property("pegasus.monitord.output") is None:
        # No command-line or property specified, use default
        event_dest = get_default_sqlite_db(out)
    else:
        # Ok, get it from the properties file
        event_dest = props.property("pegasus.monitord.output")
else:
    # Use command-line option
    event_dest = options.event_dest

if options.enc is not None:
    # Get encoding from command-line options
    encoding = options.enc
else:
    if props.property("pegasus.monitord.encoding") is not None:
        # Get encoding from property
        encoding = props.property("pegasus.monitord.encoding")

# Sanity check
if fuse < 60:
    fuse = 60

# Use default monitord logfile if user hasn't specified another file
if not logfile:
    logfile = os.path.join(run, logbase)
logfile = os.path.abspath(logfile)

# Check if the user-provided jsd file is an absolute path, if so, we
# disable recursive mode
if jsd is not None:
    if os.path.isabs(jsd):
        # Yes, this is an absolute path
        follow_subworkflows = False
        logger.warning("jsd file is an absolute filename, disabling sub-workflow tracking")

#
# --- functions ---------------------------------------------------------------------------
#

def sendmsg(client_connection, msg):
    """
    purpose: send all data to socket connection, try several time if necessary
    paramtr: client_connection(IN): socket connection to send data
    paramtr: msg(IN): message to send
    returns: None on error, 1 on success
    """
    my_total_bytes_sent = 0

    while my_total_bytes_sent < len(msg):
        try:
            my_bytes_sent = client_connection.send(msg[my_total_bytes_sent:])
        except:
            logger.error("writing to socket!")
            return None

        my_total_bytes_sent = my_total_bytes_sent + my_bytes_sent

    return 1

def systell(fh):
    """
    purpose: make things symmetric, have a systell for sysseek
    paramtr: fh (IO): filehandle
    returns: current file position
    """
    os.lseek(fh, 0, os.SEEK_CUR)

def add(wf, jobid, event, sched_id=None, status=None):
    """
    This function processes events related to jobs' state changes. It
    creates a new job, when needed, and by calling the workflow's
    update_job_state method, it causes output to be generated (both to
    jobstate.log and to the backend configured to receive events). wf
    is the workflow object for this operation, jobid is the id for the
    job (job_name), event is the actual state associated with this
    event (SUBMIT, EXECUTE, etc). sched_id is the scheduler's id for
    this particular job instance, and status is the exitcode for the
    job. This function returns the job_submit_seq for the
    corresponding jobid.
    """

    my_site = None
    my_time = None
    my_job_submit_seq = None

    # Remove existing site info during replanning
    if event in unsubmitted_events:
        if jobid in wf._job_site:
            del wf._job_site[jobid]
        if jobid in wf._walltime:
            del wf._walltime[jobid]

    # Variables originally from submit file information
    if jobid in wf._job_site:
        my_site = wf._job_site[jobid]
    if jobid in wf._walltime:
        my_time = wf._walltime[jobid]

    # A PRE_SCRIPT_START event always means a new job
    if event == "PRE_SCRIPT_STARTED":
        # This is a new job, we need to add it to the workflow
        my_job_submit_seq = wf.add_job(jobid, event)

    # A DAGMAN_SUBMIT event requires a new job (unless this was
    # already done by a PRE_SCRIPT_STARTED event, but we let the
    # add_job function figure this out).
    if event == "DAGMAN_SUBMIT":
        wf._last_submitted_job = jobid
        my_job_submit_seq = wf.add_job(jobid, event)

        # Nothing else to do... we should stop here...
        return my_job_submit_seq

    # A SUBMIT event brings sched id and job type information (it can also be
    # a new job for us when there is no PRE_SCRIPT)
    if event == "SUBMIT":
        # Add job to our workflow (if not alredy there), will update sched_id in both cases
        my_job_submit_seq = wf.add_job(jobid, event, sched_id=sched_id)

        # Obtain planning information from the submit file when entering Condor,
        # Figure out how long the job _intends_ to run maximum
        my_time, my_site = wf.parse_job_sub_file(jobid, my_job_submit_seq)

        if my_site == "!!SITE!!":
            my_site = None

        # If not None, convert into seconds
        if my_time is not None:
            my_time = my_time * 60
            logger.info("job %s requests %d s walltime" % (jobid, my_time))
            wf._walltime[jobid] = my_time
        else:
            logger.info("job %s does not request a walltime" % (jobid))

        # Remember the run-site
        if my_site is not None:
            logger.info("job %s is planned for site %s" % (jobid, my_site))
            wf._job_site[jobid] = my_site
        else:
            logger.info("job %s does not have a site information!" % (jobid))

    # Get job_submit_seq if we don't already have it
    if my_job_submit_seq is None:
        my_job_submit_seq = wf.find_jobid(jobid)

    if my_job_submit_seq is None:
        logger.warning("cannot find job_submit_seq for job: %s" % (jobid))
        # Nothing else to do...
        return None

    # Make sure job has the updated state
    wf.update_job_state(jobid, my_job_submit_seq, event, status, my_time)

    return my_job_submit_seq

def process_dagman_out(wf, log_line):
    """
    This function processes a log line from the dagman.out file and
    calls either the add function to generate a jobstate.log output
    line, or calls the corresponding workflow class method in order to
    track the various events that happen during the life of a
    workflow. It returns a tuple containing the new DAGMan output
    file, with the parent jobid and sequence number if we need to
    follow a sub-workflow.
    """

    # Keep track of line count
    wf._line = wf._line + 1

    # Make sure we have not already seen this line
    # This is used in the case of rescue dags, for skipping
    # what we have already seen in the dagman.out file
    if wf._line <= wf._last_processed_line:
        return

    # Strip end spaces, tabs, and <cr> and/or <lf>
    log_line = log_line.rstrip()

    # Check log_line for timestamp at the beginning
    timestamp_found = False
    my_expr = re_parse_timestamp.search(log_line)
    
    if my_expr is not None:
        # Found time stamp, let's assume valid log line
        curr_time = time.localtime()
        adj_time = list(curr_time)
        adj_time[1] = int(my_expr.group(1)) # Month
        adj_time[2] = int(my_expr.group(2)) # Day
        adj_time[3] = int(my_expr.group(5)) # Hours
        adj_time[4] = int(my_expr.group(6)) # Minutes
        adj_time[5] = int(my_expr.group(7)) # Seconds
        adj_time[8] = -1 # DST, let Python figure it out

        if my_expr.group(3) is not None:
            # New timestamp format
            adj_time[0] = int(my_expr.group(4)) + 2000 # Year

        wf._current_timestamp = time.mktime(adj_time) + adjustment
        timestamp_found = True
    else:
        # FIXME: Use method from utils.py, do not re-invent the wheel!
        # FIXME: Slated for 3.1
        my_expr = re_parse_iso_stamp.search(log_line)
        if my_expr is not None:
            # /^\s*(\d{4}).?(\d{2}).?(\d{2}).(\d{2}).?(\d{2}).?(\d{2})([.,]\d+)?([Zz]|[-+](\d{2}).?(\d{2}))/
            dt = "%04d-%02d-%02d %02d:%02d:%02d" % (int(my_expr.group(1)),
                                                    int(my_expr.group(2)),
                                                    int(my_expr.group(3)),
                                                    int(my_expr.group(4)),
                                                    int(my_expr.group(5)),
                                                    int(my_expr.group(6)))
            my_time = datetime.datetime(*(time.strptime(dt, "%Y-%m-%d %H:%M:%S")[0:6]))

            tz = my_expr.group(8)
            if tz.upper() != 'Z':
                # no zulu time, has zone offset
                my_offset = datetime.timedelta(hours=int(my_expr.group(9)),
                                               minutes=int(my_expr.group(10)))

                # adjust for time zone offset
                if tz[0] == '-':
                    my_time = my_time + my_offset
                else:
                    my_time = my_time - my_offset

            # Turn my_time into Epoch format
            wf._current_timestamp = int(calendar.timegm(my_time.timetuple())) + adjustment
            timestamp_found = True

    if timestamp_found:
        split_log_line = log_line.split(None, 3)
        if len(split_log_line) >= 3:
            logger.debug("debug: ## %d: %s" % (wf._line, split_log_line[2][:64]))

        # If in recovery mode, check if we reached the end of it
        if wf._skipping_recovery_lines:
            if log_line.find("...done with RECOVERY mode") >= 0:
                wf._skipping_recovery_lines = False
            return

        # Search for more content
        if re_parse_event.search(log_line) is not None:
            # Found ULOG Event
            my_expr = re_parse_event.search(log_line)
            # groups = jobid, event, sched_id
            my_event = my_expr.group(1)
            my_jobid = my_expr.group(2)
            my_sched_id = my_expr.group(3)
            my_job_submit_seq = add(wf, my_jobid, my_event, sched_id=my_sched_id)
            if my_event == "SUBMIT" and follow_subworkflows == True:
                # For SUBMIT ULOG events, check if this is a sub-workflow
                my_new_dagman_out = wf.has_subworkflow(my_jobid)
                # Ok, return result to main loop
                return (my_new_dagman_out, my_jobid, my_job_submit_seq)
        elif re_parse_job_submit.search(log_line) is not None:
            # Found a DAGMan job submit event
            my_expr = re_parse_job_submit.search(log_line)
            # groups = jobid
            add(wf, my_expr.group(1), "DAGMAN_SUBMIT")
        elif re_parse_job_submit_error.search(log_line) is not None:
            # Found a DAGMan job submit error event
            if wf._last_submitted_job is not None:
                add(wf, wf._last_submitted_job, "SUBMIT_FAILED")
            else:
                logger.warning("found submit error in dagman.out, but last job is not set")
        elif re_parse_script_running.search(log_line) is not None:
            # Pre scripts are not regular Condor event
            # Starting of scripts is not a regular Condor event
            my_expr = re_parse_script_running.search(log_line)
            # groups = script, jobid
            my_script = my_expr.group(1).upper()
            my_jobid = my_expr.group(2)
            add(wf, my_jobid, "%s_SCRIPT_STARTED" % (my_script))
        elif re_parse_script_done.search(log_line) is not None:
            my_expr = re_parse_script_done.search(log_line)
            # groups = script, jobid
            my_script = my_expr.group(1).upper()
            my_jobid = my_expr.group(2)
            if re_parse_script_successful.search(log_line) is not None:
                # Remember success with artificial jobstate
                add(wf, my_jobid, "%s_SCRIPT_SUCCESS" % (my_script), status=0)
            elif re_parse_script_failed.search(log_line) is not None:
                # Remember failure with artificial jobstate
                my_expr = re_parse_script_failed.search(log_line)
                # groups = exit code (error status)
                try:
                    my_exit_code = int(my_expr.group(1))
                except:
                    # Unable to convert exit code to integer -- should not happen
                    logger.warning("unable to convert exit code to integer!")
                    my_exit_code = 1
                add(wf, my_jobid, "%s_SCRIPT_FAILURE" % (my_script), status=my_exit_code)
            else:
                # Ignore
                logger.warning("unknown pscript state: %s" % (log_line[-14:]))
        elif re_parse_job_failed.search(log_line) is not None:
            # Job has failed
            my_expr = re_parse_job_failed.search(log_line)
            # groups = jobid, schedid, jobstatus
            my_jobid = my_expr.group(1)
            my_sched_id = my_expr.group(2)
            try:
                my_jobstatus = int(my_expr.group(3))
            except:
                # Unable to convert exit code to integet -- should not happen
                logger.warning("unable to convert exit code to integer!")
                my_jobstatus = 1
            # remember failure with artificial jobstate
            add(wf, my_jobid, "JOB_FAILURE", sched_id=my_sched_id, status=my_jobstatus)
        elif re_parse_job_successful.search(log_line) is not None:
            # Job succeeded
            my_expr = re_parse_job_successful.search(log_line)
            my_jobid = my_expr.group(1)
            my_sched_id = my_expr.group(2)
            # remember success with artificial jobstate
            add(wf, my_jobid, "JOB_SUCCESS", sched_id=my_sched_id, status=0)
        elif re_parse_dagman_finished.search(log_line) is not None:
            # DAG finished -- done parsing
            my_expr = re_parse_dagman_finished.search(log_line)
            # groups = exit code
            try:
                wf._dagman_exit_code = int(my_expr.group(1))
            except:
                # Cannot convert exit code to integer!
                logger.warning("cannot convert DAGMan's exit code to integer!")
                wf._dagman_exit_code = 0
                wf._monitord_exit_code = 1
            logger.info("DAGMan finished with exit code %s" % (wf._dagman_exit_code))
            # Send info to database
            wf.change_wf_state("end")
        elif re_parse_dagman_condor_id.search(log_line) is not None:
            # DAGMan starting, capture its condor id
            my_expr = re_parse_dagman_condor_id.search(log_line)
            wf._dagman_condor_id = my_expr.group(1)
            if not keep_state:
                # Initialize workflow parameters
                wf.start_wf()
        elif re_parse_dagman_pid.search(log_line) is not None and not replay_mode:
            # DAGMan's pid, but only set pid if not running in replay mode
            # (otherwise pid may belong to another process)
            my_expr = re_parse_dagman_pid.search(log_line)
            # groups = DAGMan's pid
            try:
                wf._dagman_pid = int(my_expr.group(1))
            except:
                logger.critical("cannot set pid: %s" % (my_expr.group(1)))
                sys.exit(42)
            logger.info("DAGMan runs at pid %d" % (wf._dagman_pid))
        elif re_parse_dag_name.search(log_line) is not None:
            # Found the dag filename, read dag, and generate start event for the database
            my_expr = re_parse_dag_name.search(log_line)
            my_dag = my_expr.group(1)
            # Parse dag file
            logger.info("using dag %s" % (my_dag))
            wf.parse_dag_file(my_dag)
            # Send the delayed workflow start event to database
            wf.change_wf_state("start")
        elif re_parse_condor_version.search(log_line) is not None:
            # Version of this logfile format
            my_expr = re_parse_condor_version.search(log_line)
            # groups = condor version, condor major
            my_condor_version = my_expr.group(1)
            my_condor_major = my_expr.group(2)
            logger.info("Using DAGMan version %s" % (my_condor_version))
        elif (re_parse_condor_logfile.search(log_line) is not None or
              wf._multiline_file_flag == True and re_parse_condor_logfile_insane.search(log_line) is not None):
            # Condor common log file location, DAGMan 6.6
            if re_parse_condor_logfile.search(log_line) is not None:
                my_expr = re_parse_condor_logfile.search(log_line)
            else:
                my_expr = re_parse_condor_logfile_insane.search(log_line)
            wf._condorlog = my_expr.group(1)
            logger.info("Condor writes its logfile to %s" % (wf._condorlog))

            # Make a symlink for NFS-secured files
            my_log, my_base = out2log(wf._run_dir, wf._out_file)
            if os.path.islink(my_log):
                logger.info("symlink %s already exists" % (my_log))
            elif os.access(my_log, os.R_OK):
                logger.info("%s is a regular file, not touching" % (my_base))
            else:
                logger.info("trying to create local symlink to common log")
                if os.access(wf._condorlog, os.R_OK) or not os.access(wf._condorlog, os.F_OK):
                    if os.access(my_log, os.R_OK):
                        try:
                            os.rename(my_log, "%s.bak" % (my_log))
                        except:
                            logger.warning("error renaming %s to %s.bak" % (my_log, my_log))
                    try:
                        os.symlink(wf._condorlog, my_log)
                    except:
                        logger.info("unable to symlink %s" % (wf._condorlog))
                    else:
                        logger.info("symlink %s -> %s" % (wf._condorlog, my_log))
                else:
                    logger.info("%s exists but is not readable!" % (wf._condorlog))
            # We only expect one of such files
            wf._multiline_file_flag = False
        elif re_parse_multiline_files.search(log_line) is not None:
            # Multiline user log files, DAGMan > 6.6
            wf._multiline_file_flag = True
        elif log_line.find("Running in RECOVERY mode...") >= 0:
            # Entering recovery mode, skip lines until we reach the end
            wf._skipping_recovery_lines = True
            return
    else:
        # Could not parse timestamp
        logger.info( "time stamp format not recognized" )

def server_socket(low, hi, bind_addr="127.0.0.1"):
    """
    purpose: create a local TCP server socket to listen to sitesel requests
    paramtr: low (IN): minimum port from bind range
    paramtr: hi (IN): maximum port from bind range
    paramtr: bind_addr (IN): optional hostaddr_in to bind to , defaults to LOOPBACK
    returns: open socket, or None on error
    """

    # Create socket
    try:
        my_socket = socket.socket(socket.AF_INET,
                                  socket.SOCK_STREAM,
                                  socket.getprotobyname("tcp"))
    except:
        logger.critical("could not create socket!")
        sys.exit(42)

    # Set options
    try:
        my_socket.setsockopt(socket.SOL_SOCKET,
                             socket.SO_REUSEADDR,
                             1)
    except:
        logger.critical("setsockopt SO_REUSEADDR!")
        sys.exit(42)

    # Bind to a free port
    my_port = low
    for my_port in range(low, hi):
        try:
            my_res = my_socket.bind((bind_addr, my_port))
        except:
            # Go to next port
            continue
        else:
            break

    if my_port >= hi:
        logger.critical("no free port to bind to!")
        sys.exit(42)

    # Make server socket non-blocking to not have a race condition
    # when doing select() before accept() on a server socket
    try:
        my_socket.setblocking(0)
    except:
        logger.critical("setblocking!")
        sys.exit(42)

    # Start listener
    try:
        my_socket.listen(socket.SOMAXCONN)
    except:
        logger.critical("listen!")
        sys.exit(42)

    # Return socket
    return my_socket

def untaint(text):
    """
    purpose: do not trust anything we get from the internet
    paramtr: text(IN): text to untaint
    returns: cleaned text, without any "special" characters
    """

    if text is None:
        return None

    my_text = re_clean_content.sub('', str(text))

    return my_text

def list_workflows(client_conn, param=""):
    """
    purpose: lists the workflows currently being tracked
    globals: wfs(IN): array of workflows
    """

    for workflow_entry in wfs:
        if workflow_entry.wf is not None:
            my_wf = workflow_entry.wf
            my_line = "%d - %s\r\n" % (my_wf._workflow_start, my_wf._wf_uuid)
        sendmsg(client_conn, my_line)

jumptable = {'list-wf': list_workflows}

def service_request(server):
    """
    purpose: accept an incoming connection and service its request
    paramtr: server(IN): server socket with a pending connection request
    returns: number of status lines, or None in case of error
    """

    # First, we accept the connection
    try:
        my_conn, my_addr = server.accept()
    except:
        logger.error("accept!")
        return None

    my_count = 0
    logger.info("processing request from %s:%d" % (my_addr[0], my_addr[1]))

    # TODO: Can only handle 1 line up to 1024 bytes long, should fix this later
    # Read line fron socket
    while True:
        try:
            my_buffer = my_conn.recv(1024)
        except socket.error, e:
            if e[0] == 35:
                continue
            else:
                logger.error("recv: %d:%s" % (e[0], e[1]))
                try:
                    # Close socket
                    my_conn.close()
                except:
                    pass
                return None
        else:
            # Received line, leave loop
            break

    if my_buffer == '':
        # Nothing else to read
        try:
            my_conn.close()
        except:
            pass
        return my_count

    # Removed leading/trailing spaces/tabs, trailing \r\n
    my_buffer = my_buffer.strip()
    # Do not trust anything we get from the internet
    my_buffer = untaint(my_buffer)
    
    # Create list of tokens
    my_args = my_buffer.split()
    
    if len(my_args) < 3:
        # Clearly not enough information
        sendmsg(my_conn, "%s 204 No Content\r\n" % (speak))
        try:
            my_conn.close()
        except:
            pass
        return my_count

    # Read information we need
    my_proto = my_args.pop(0).upper()
    my_method = my_args.pop(0)
    my_what = my_args.pop(0)

    if my_proto != speak:
        # Illegal or unknown protocol
        sendmsg(my_conn, "%s 400 Bad request\r\n" % (speak))
    elif my_method.upper() != "GET":
        # Unsupported method
        sendmsg(my_conn, "%s 405 Method not allowed\r\n" % (speak))
    elif not my_what in jumptable:
        # Request item is not supported
        sendmsg(my_conn, "%s 501 Not implemented\r\n" % (speak))
    else:
        # OK, process the command
        sendmsg(my_conn, "%s 200 OK\r\n" % (speak))
        my_count = jumptable[my_what](my_conn, " ".join(my_args).lower())

    try:
        my_conn.close()
    except:
        pass

    return my_count
	
def check_request(server, timeout=0):
    """
    purpose: check for a pending service request, and service it
    paramtr: server(IN): server socket
    paramtr: timeout(IN, OPT): timeout in seconds, defaults to 0
    returns: return value of select on server socket
    """

    my_input_list = [server]
    my_input_ready, my_output_ready, my_except_ready = select.select(my_input_list, [], [], timeout)

    if len(my_input_ready) == 1:
        service_request(server)

    return len(my_input_ready)

def sleepy(retries, server=None):
    """
    purpose: sleep or work on client requests
    paramtr: retries (IN): number of retries we are in
    paramtr: server (IN): server listening socket, may be None
    returns: Nothing
    """
    my_sleeptime = sleeptime(retries)

    if server is not None:
        # elaborate iterative (non-concurrent) internet daemon
        my_start = int(time.time())
        my_diff = 0
        while my_diff < my_sleeptime:
            my_diff = int(time.time()) - my_start
            if my_diff < my_sleeptime:
                check_request(server, my_sleeptime - my_diff)
            else:
                check_request(server, 0)
    else:
        # no server, just sleep regularly
        if my_sleeptime is not None:
            time.sleep(my_sleeptime)
	
def sleeptime(retries):
    """
    purpose: compute suggested sleep time as a function of retries
    paramtr: $retries (IN): number of retries so far
    returns: recommended sleep time in seconds
    """
    if retries < 5:
        my_y = 1
    elif retries < 50:
        my_y = 5
    elif retries < 500:
        my_y = 30
    else:
        my_y = 60

    return my_y

def purge_wf_uuid_from_database(rundir, output_db):
    """
    This function purges a workflow id from the output database.
    """
    if output_db.lower().find('sqlite:///') != 0:
        # Not a SQLite database, nothing to do
        return

    # Ok, we have a SQLite database, let's get the filename and check if it exists
    filename = output_db[10:]

    # Check if SQLite database exists
    if not os.path.isfile(filename):
        # No, nothing to do
        return

    # Ok, file is there, let's continue

    # Parse the braindump file
    wfparams = utils.slurp_braindb(rundir)
    
    if "wf_uuid" in wfparams:
        if wfparams["wf_uuid"] is not None:
            # Get wf_uuid
            wf_uuid = wfparams["wf_uuid"]
            e = Expunge(output_db, wf_uuid)
            e.expunge()

            # Done, make this connection go away
            e = None

def prog_sighup_handler(signum, frame):
    """
    This function catches SIGHUP.
    """
    logger.info("ignoring signal %d" % (signum))

def prog_sigint_handler(signum, frame):
    """
    This function catches SIGINT.
    """
    logger.info("graceful exit on signal %d" % (signum))
    # Go through all workflows we are tracking
    for my_wf in wfs:
        if my_wf.wf is not None:
            # Update monitord exit code
            if my_wf.wf._monitord_exit_code == 0:
                my_wf.wf._monitord_exit_code = 1
            # Close open files
            my_wf.wf.end_workflow()
    # All done!
    sys.exit(1)

def prog_sigusr1_handler(signum, frame):
    """
    This function increases the log level to the next one.
    """
    global g_isdbg

    cur_level = logger.getEffectiveLevel()

    try:
        idx = _LEVELS.index(cur_level)
        if idx < len(_LEVELS):
            logger.setLevel(_LEVELS[idx + 1])
    except ValueError:
        logger.setLevel(logging.INFO)
        logger.error("Unknown current level = %s, setting to INFO" % (cur_level))

    g_isdbg = logger.isEnabledFor(logging.DEBUG)


def prog_sigusr2_handler(signum, frame):
    """
    This function decreases the log level to the previous one.
    """
    global g_isdbg

    cur_level = logger.getEffectiveLevel()

    try:
        idx = _LEVELS.index(cur_level)
        if idx > 0:
            logger.setLevel(_LEVELS[idx - 1])
    except ValueError:
        logger.setLevel(logging.WARN)
        logger.error("Unknown current level = %s, setting to WARN" % (cur_level))

    g_isdbg = logger.isEnabledFor(logging.DEBUG)

#
# --- at exit handlers -------------------------------------------------------------------
#

def socket_exit_handler():
    """
    This function closes the socket server, and removes the sockfn file.
    """
    if server is not None:
        server.close()
        try:
            os.unlink(sockfn)
        except:
            # Just be silent
            pass

#
# --- main ------------------------------------------------------------------------------
#

# Turn into daemon process
if nodaemon == 0:
    utils.daemonize()
    # Open logfile as stdout
    try:
        sys.stdout = open(logfile, "w", 0)
    except:
        logger.critical("could not open %s!" % (logfile))
        sys.exit(1)
elif nodaemon == 2:
    utils.keep_foreground()
    # Open logfile as stdout
    try:
        sys.stdout = open(logfile, "w", 0)
    except:
        logger.critical("could not open %s!" % (logfile))
        sys.exit(1)
else:
    # Hack to make stdout unbuffered
    sys.stdout = os.fdopen(sys.stdout.fileno(), "w", 0)

# Close stdin
sys.stdin.close()
# dup stderr onto stdout
sys.stderr = sys.stdout

# Create wf_event_sink object
if no_events:
    # Create an empty sink when no event generation is needed
    #    wf_event_sink = EmptySink()
    wf_event_sink = None # Avoid parsing kickstart output if not
                         # generating bp file or database events
else:
    try:
        wf_event_sink = create_wf_event_sink(event_dest, db_stats=db_stats, enc=encoding)
        atexit.register(finish_stampede_loader)
    except ValueError, err:
        logger.critical("Cannot create output: %s. Abort." % (err))
        sys.exit(1)

# If in replay mode and it is a DB, attempt to purge wf_uuid_first
if wf_event_sink is not None:
    if replay_mode and isinstance(wf_event_sink, DBEventSink):
        purge_wf_uuid_from_database(run, event_dest)

# Say hello
logger.info("starting [%s], using pid %d" % (revision, os.getpid()))
if millisleep is not None:
    logger.info("using simulation delay of %d ms" % (millisleep))

# Ignore dying shells
signal.signal(signal.SIGHUP, prog_sighup_handler)

# Die nicely when asked to (Ctrl+C, system shutdown)
signal.signal(signal.SIGINT, prog_sigint_handler)

# Permit dynamic changes of debug level
signal.signal(signal.SIGUSR1, prog_sigusr1_handler)
signal.signal(signal.SIGUSR2, prog_sigusr2_handler)

# No need to create server socket in replay mode
if not replay_mode:
    # Create server socket for communication with site selector
    sockfn = os.path.join(os.path.dirname(out), "monitord.sock")
    server = server_socket(49152, 65536)
    # Take care of closing socket when we exit
    atexit.register(socket_exit_handler)

    # Save our address so that site selectors know where to connect
    if server is not None:
        my_host, my_port = server.getsockname()
        try:
            OUT = open(sockfn, "w")
            OUT.write("%s %d\n" % (my_host, my_port))
            OUT.close()
        except:
            logger.warning("unable to write %s!" % (sockfn))

# For future reference
plus = ''
if "LD_LIBRARY_PATH" in os.environ:
    for my_path in os.environ["LD_LIBRARY_PATH"].split(':'):
        logger.info("env: LD_LIBRARY_PATH%s=%s" % (plus, my_path))
        plus = '+'

if "GLOBUS_TCP_PORT_RANGE" in os.environ:
    logger.info("env: GLOBUS_TCP_PORT_RANGE=%s" % (os.environ["GLOBUS_TCP_PORT_RANGE"]))
else:
    logger.info("env: GLOBUS_TCP_PORT_RANGE=")
if "GLOBUS_TCP_SOURCE_RANGE" in os.environ:
    logger.info("env: GLOBUS_TCP_SOURCE_RANGE=%s" % (os.environ["GLOBUS_TCP_SOURCE_RANGE"]))
else:
    logger.info("env: GLOBUS_TCP_SOURCE_RANGE=")
if "GLOBUS_LOCATION" in os.environ:
    logger.info("env: GLOBUS_LOCATION=%s" % (os.environ["GLOBUS_LOCATION"]))
else:
    logger.info("env: GLOBUS_LOCATION=")

# Ok! Let's start now...

# Instantiate workflow class
wf = Workflow(run, out, database=wf_event_sink, jsd=jsd)
# If everything went well, create a workflow entry for this workflow
if wf._monitord_exit_code == 0:
    workflow_entry = WorkflowEntry()
    workflow_entry.run_dir = run
    workflow_entry.dagman_out = out
    workflow_entry.wf = wf

    # And add it to our list of workflows
    wfs.append(workflow_entry)
    if replay_mode:
        tracked_workflows.append(out)

    # Also set the root workflow id
    root_wf_id = wf._wf_uuid

#
# --- main loop begin --------------------------------------------------------------------
#

# Loop while we have workflows to follow...
while (len(wfs) > 0):
    # Go through each of our workflows
    for workflow_entry in wfs:

        # Check if we are waiting for the dagman.out file to appear...
        if workflow_entry.DMOF is None:

            # Yes... check if it has shown up...

            # First, we test if the file is already there, in case we are running in replay mode
            if replay_mode:
                try:
                    f_stat = os.stat(workflow_entry.dagman_out)
                except:
                    logger.critical("error: workflow not started, %s does not exist, dropping this workflow..." % (workflow_entry.dagman_out))
                    workflow_entry.delete_workflow = True
                    # Go to the next workflow_entry in the for loop
                    continue

            try:
                f_stat = os.stat(workflow_entry.dagman_out)
            except OSError, e:
                if errno.errorcode[e.errno] == 'ENOENT':
                    # File doesn't exist yet, keep looking
                    workflow_entry.n_retries = workflow_entry.n_retries + 1
                    if workflow_entry.n_retries > 100:
                        # We tried too long, just exit
                        logger.critical("%s never made an appearance" % (workflow_entry.dagman_out))
                        workflow_entry.delete_workflow = True
                        # Go to the next workflow_entry in the for loop
                        continue
                    # Continue waiting
                    logger.info("waiting for dagman.out file, retry %d" % (workflow_entry.n_retries))
                    workflow_entry.sleep_time = time.time() + sleeptime(workflow_entry.n_retries)
                else:
                    # Another error
                    logger.critical("stat %s" % (workflow_entry.dagman.out))
                    workflow_entry.delete_workflow = True
                    # Go to the next workflow_entry in the for loop
                    continue
            except:
                # Another exception
                logger.critical("stat %s" % (workflow_entry.dagman.out))
                workflow_entry.delete_workflow = True
                # Go to the next workflow_entry in the for loop
                continue
            else:
                # Found it, open dagman.out file
                try:
                    workflow_entry.DMOF = open(workflow_entry.dagman_out, "r")
                except:
                    logger.critical("opening %s" % (workflow_entry.dagman_out))
                    workflow_entry.delete_workflow = True
                    # Go to the next workflow_entry in the for loop
                    continue

        if workflow_entry.DMOF is not None:
            # Say Hello
            logger.debug("wake up and smell the silicon")

            try:
                f_stat = os.stat(workflow_entry.dagman_out)
            except:
                # stat error
                logger.critical("stat %s" % (workflow_entry.dagman_out))
                workflow_entry.delete_workflow = True
                # Go to the next workflow_entry in the for loop
                continue

            # f_stat[6] is the file size
            if f_stat[6] == workflow_entry.ml_current:
                # Death by natural causes
                if workflow_entry.wf._dagman_exit_code is not None and not replay_mode:
                    logger.info("workflow %s ended" % (workflow_entry.dagman_out))
                    workflow_entry.delete_workflow = True
                    # Go to the next workflow_entry in the for loop
                    continue

                # Check if DAGMan is alive -- if we know where it lives
                if workflow_entry.ml_retries > 10 and workflow_entry.wf._dagman_pid > 0:
                    # Just send signal 0 to check if the pid is ours
                    try:
                        os.kill(int(workflow_entry.wf._dagman_pid), 0)
                    except:
                        logger.critical("DAGMan is gone! Sudden death syndrome detected!")
                        workflow_entry.wf._monitord_exit_code = 42
                        workflow_entry.delete_workflow = True
                        # Go to the next workflow_entry in the for loop
                        continue

                # No change, wait a while
                workflow_entry.ml_retries = workflow_entry.ml_retries + 1
                if workflow_entry.ml_retries > 17280:
                    # Too long without change
                    logger.critical("too long without action, stopping workflow %s" % (workflow_entry.dagman_out))
                    workflow_entry.delete_workflow = True
                    # Go to the next workflow_entry in the for loop
                    continue

                # In replay mode, we can be a little more aggresive
                if replay_mode and workflow_entry.ml_retries > 5:
                    # We are in replay mode, so we should have everything here
                    logger.info("no more action, stopping workflow %s" % (workflow_entry.dagman_out))
                    workflow_entry.delete_workflow = True
                    # Go to the next workflow_entry in the for loop
                    continue

            elif f_stat[6] < workflow_entry.ml_current:
                # Truncated file, booh!
                logger.critical("%s file truncated, time to exit" % (workflow_entry.dagman_out))
                workflow_entry.delete_workflow = True
                # Go to the next workflow_entry in the for loop
                continue

            elif f_stat[6] > workflow_entry.ml_current:
                # We have something to read!
                try:
                    ml_rbuffer = workflow_entry.DMOF.read(32768)
                except:
                    # Error while reading
                    logger.critical("while reading %s" % (workflow_entry.dagman_out))
                    workflow_entry.wf._monitord_exit_code = 42
                    workflow_entry.delete_workflow = True
                    # Go to the next workflow_entry in the for loop
                    continue
                if len(ml_rbuffer) == 0:
                    # Detected EOF
                    logger.critical("detected EOF, resetting position to %d" % (workflow_entry.ml_current))
                    workflow_entry.DMOF.seek(workflow_entry.ml_current)
                else:
                    # Something in the read buffer, merge it with our buffer
                    workflow_entry.ml_buffer = workflow_entry.ml_buffer + ml_rbuffer
                    # Look for end of line
                    ml_pos = workflow_entry.ml_buffer.find('\n')
                    while (ml_pos >= 0):
                        # Take out 1 line, and adjust buffer
                        process_output = process_dagman_out(workflow_entry.wf, workflow_entry.ml_buffer[0:ml_pos])
                        workflow_entry.ml_buffer = workflow_entry.ml_buffer[ml_pos+1:]
                        ml_pos = workflow_entry.ml_buffer.find('\n')

                        # Do we need to start following another workflow?
                        if type(process_output) is tuple and len(process_output) == 3 and process_output[0] is not None:
                            # Unpack the output tuple
                            new_dagman_out = process_output[0]
                            parent_jobid = process_output[1]
                            parent_jobseq = process_output[2]
                            # Only if we are not already tracking it...
                            tracking_already = False
                            new_dagman_out = os.path.abspath(new_dagman_out)
                            # Add the current run directory in case this is a relative path
                            new_dagman_out = os.path.join(workflow_entry.run_dir, new_dagman_out)
                            if replay_mode:
                                # Check if we started tracking this subworkflow in the past
                                if new_dagman_out in tracked_workflows:
                                    # Yes, no need to do it again...
                                    logger.info("already tracking workflow: %s, not adding" % (new_dagman_out))
                                    tracking_already = True
                            else:
                                # Not in replay mode, let's check if we are currently tracking this subworkflow
                                for my_wf in wfs:
                                    if my_wf.dagman_out == new_dagman_out and not my_wf.delete_workflow:
                                        # Found it, exit loop
                                        tracking_already = True
                                        logger.info("already tracking workflow: %s, not adding" % (new_dagman_out))
                                        break
                            if not tracking_already:
                                logger.info("found new workflow to track: %s" % (new_dagman_out))
                                # Not tracking this workflow, let's try to add it to our list
                                new_run_dir = os.path.dirname(new_dagman_out)
                                parent_wf_id = workflow_entry.wf._wf_uuid
                                new_wf = Workflow(new_run_dir, new_dagman_out, database=wf_event_sink,
                                                  parent_id=parent_wf_id, parent_jobid=parent_jobid,
                                                  parent_jobseq=parent_jobseq, root_id=root_wf_id, jsd=jsd)

                                if new_wf._monitord_exit_code == 0:
                                    new_workflow_entry = WorkflowEntry()
                                    new_workflow_entry.run_dir = new_run_dir
                                    new_workflow_entry.dagman_out = new_dagman_out
                                    new_workflow_entry.wf = new_wf

                                    # And add it to our list of workflows
                                    wfs.append(new_workflow_entry)
                                    # Don't forget to add it to our list, so we don't do it again in replay mode
                                    if replay_mode:
                                        tracked_workflows.append(new_dagman_out)

                        if millisleep is not None:
                            if server is not None:
                                check_request(server, millisleep / 1000.0)
                            else:
                                time.sleep(millisleep / 1000.0)

                    ml_pos = workflow_entry.DMOF.tell()
                    logger.info("processed chunk of %d byte" % (ml_pos - workflow_entry.ml_current -len(workflow_entry.ml_buffer)))
                    workflow_entry.ml_current = ml_pos
                    workflow_entry.ml_retries = 0

            workflow_entry.sleep_time = time.time() + sleeptime(workflow_entry.ml_retries)
    
    # Go through the workflows again, and finish any marked ones
    wf_index = 0
    while wf_index < len(wfs):
        workflow_entry = wfs[wf_index]
        if workflow_entry.delete_workflow == True:
            logger.info("finishing workflow: %s" % (workflow_entry.dagman_out))
            # Close dagman.out file, if any
            if workflow_entry.DMOF is not None:
                workflow_entry.DMOF.close()
            # Close jobstate.log, if any
            if workflow_entry.wf is not None:
                workflow_entry.wf.end_workflow()
            # Delete this workflow from our list
            deleted_entry = wfs.pop(wf_index)
            # Don't move index to next one
        else:
            # Mode index to next workflow
            wf_index = wf_index + 1

    # Skip sleeping, if we have no more workflow to track...
    if len(wfs) == 0:
        continue

    # Periodically check for service requests
    if server is not None:
        check_request(server)

    # All done... let's figure out how long to sleep...
    time_to_sleep = time.time() + MAX_SLEEP_TIME
    for workflow_entry in wfs:
        # Figure out if we have anything more urgent to do
        if workflow_entry.sleep_time < time_to_sleep:
            time_to_sleep = workflow_entry.sleep_time

    # Sleep
    if not replay_mode:
        time_to_sleep = time_to_sleep - time.time()
        if time_to_sleep < 0:
            time_to_sleep = 0
        time.sleep(time_to_sleep)

#
# --- main loop end -----------------------------------------------------------------------
#

if not replay_mode:
    # Finish trailing connection requests
    while (check_request(server)):
        pass
    server.close()
    server = None
    try:
        os.unlink(sockfn)
    except:
        # Just be silent
        pass

# Try to run the hurricane graphics
if doplot:
    pass
else:
    logger.info("skipping plots")

# done
logger.info("finishing, exit with 0")
sys.exit(0)
