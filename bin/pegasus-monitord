#!/usr/bin/env python

"""
Small daemon process to update the job state file from DAGMan logs.
This program is to be run automatically by the pegasus-run command.

Usage: pegasus-monitord [options] dagoutfile
"""

##
#  Copyright 2007-2010 University Of Southern California
#
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing,
#  software distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
##

# Revision : $Revision: 2012 $

# Import Python modules
import os
import re
import sys
import time
import errno
import atexit
import select
import signal
import socket
import logging
import calendar
import commands
import datetime
import operator
import optparse

# Initialize logging object
logger = logging.getLogger()
# Set default level to WARNING
logger.setLevel(logging.WARNING)
#logger.setLevel(logging.DEBUG)
# Format our log messages the way we want
cl = logging.StreamHandler()

# Don't add funcName to the formatter for Python versions < 2.5
if sys.version_info < (2, 5):
    formatter = logging.Formatter("%(asctime)s:%(filename)s:%(lineno)d: %(levelname)s: %(message)s")
else:
    formatter = logging.Formatter("%(asctime)s:%(filename)s:%(funcName)s:%(lineno)d: %(levelname)s: %(message)s")
cl.setFormatter(formatter)
logger.addHandler(cl)

LOG_LEVELS = {'debug': logging.DEBUG,
              'info': logging.INFO,
              'warning': logging.WARNING,
              'error': logging.ERROR,
              'critical': logging.CRITICAL}

INCREASE_LOG_LEVELS = {logging.DEBUG: logging.DEBUG,
                       logging.INFO: logging.DEBUG,
                       logging.WARNING: logging.INFO,
                       logging.ERROR: logging.WARNING,
                       logging.CRITICAL: logging.ERROR}

DECREASE_LOG_LEVELS = {logging.DEBUG: logging.INFO,
                       logging.INFO: logging.WARNING,
                       logging.WARNING: logging.ERROR,
                       logging.ERROR: logging.CRITICAL,
                       logging.CRITICAL: logging.CRITICAL}

# Save our own basename
prog_base = os.path.split(sys.argv[0])[1]

# Figure out where our lib/Python directory is located, and put that in our module search path
if "PEGASUS_HOME" in os.environ:
    lib_path = os.path.join(os.environ["PEGASUS_HOME"], "lib/python")
    # Check if this is a directory
    if os.path.isdir(lib_path):
        # Insert this directory in our search path
        os.sys.path.insert(0, lib_path)
    else:
        # Something is wrong!
        logger.critical("PEGASUS_HOME appears to be set incorrectly... exiting!")
        sys.exit(1)
else:
    # PEGASUS_HOME is not set, let's try to figure out where it is
    pegasus_home = os.path.normpath(os.path.join(os.path.dirname(sys.argv[0]), ".."))
    os.environ["PEGASUS_HOME"] = pegasus_home
    lib_path = os.path.join(pegasus_home, "lib/python")

    # Check if this is a directory
    if os.path.isdir(lib_path):
        # Insert this directory in our search path
        os.sys.path.insert(0, lib_path)
    else:
        # Cannot figure out what to do!
        logger.critical("cannot find Pegasus's Python library directory... exiting!")
        sys.exit(1)

# Import our modules
from Pegasus.tools import utils
from Pegasus.tools import properties
from Pegasus.tools import kickstart_parser

# Import Python > 2.5 modules
try:
    import uuid
    uuid_present = True
except:
    # Import failed, make a note so we can use something else
    uuid_present = False

# Add SEEK_CUR to os if Python version < 2.5
if sys.version_info < (2, 5):
    os.SEEK_CUR = 1

# Compile our regular expressions

# Used in the initialization code of the Workflow class, while reading the DAG file
re_parse_dag_submit_files = re.compile(r"JOB\s+(\S+)\s(\S+)(\s+DONE)?", re.IGNORECASE)
re_parse_dag_script = re.compile(r"SCRIPT (?:PRE|POST)\s+(\S+)\s(\S+)\s(.*)", re.IGNORECASE)
re_parse_dag_subdag = re.compile(r"SUBDAG EXTERNAL\s+(\S+)\s(\S+)\sDIR\s(\S+)", re.IGNORECASE)
re_parse_dag_edge = re.compile(r"PARENT\s+(\S+)\s+CHILD\s+(\S+)")

# Used in out2log
re_remove_extensions = re.compile(r"(?:\.(?:rescue|dag))+$")

# Used in untaint
re_clean_content = re.compile(r"([^-a-zA-z0-9_\s.,\[\]^\*\?\/\+])")

# Used in parse_sub_file
re_rsl_string = re.compile(r"^\s*globusrsl\W", re.IGNORECASE)
re_rsl_clean = re.compile(r"([-_])")
re_site_parse_gvds = re.compile(r"^\s*\+(pegasus|wf)_(site|resource)\s*=\s*([\'\"])?(\S+)\3")
re_parse_jobtype = re.compile(r"^\s*\+pegasus_job_class\s*=\s*(\S+)")
re_parse_transformation = re.compile(r"^\s*\+pegasus_wf_xformation\s*=\s*(\S+)")
re_parse_executable = re.compile(r"^\s*executable\s*=\s*(\S+)")
re_parse_arguments = re.compile(r'^\s*arguments\s*=\s*"([^"\r\n]*)"')
re_site_parse_euryale = re.compile(r"^\#!\s+site=(\S+)")

# Used in process
re_parse_dag_name = re.compile(r"Parsing (.+) ...$")
re_parse_timestamp = re.compile(r"^\s*(\d{1,2})\/(\d{1,2})(\/(\d{1,2}))?\s+(\d{1,2}):(\d{2}):(\d{2})")
re_parse_event = re.compile(r"Event:\s+ULOG_(\S+) for Condor (?:Job|Node) (\S+)\s+\((-?[0-9]+\.[0-9]+)(\.[0-9]+)?\)$")
re_parse_script_running = re.compile(r"\d{2}\sRunning (PRE|POST) script of (?:Job|Node) (.+)\.{3}")
re_parse_script_done = re.compile(r"\d{2}\s(PRE|POST) Script of (?:Job|Node) (\S+)")
re_parse_script_successful = re.compile(r"completed successfully\.$")
re_parse_script_failed = re.compile(r"failed with status\s+(-?\d+)\.?$")
re_parse_job_submit = re.compile(r"Submitting Condor Node (.+) job")
re_parse_job_submit_error = re.compile(r"ERROR: submit attempt failed")
re_parse_job_failed = re.compile(r"\d{2}\sNode (\S+) job proc \(([0-9\.]+)\) failed with status\s+(-?\d+)\.$")
re_parse_job_successful = re.compile(r"\d{2}\sNode (\S+) job proc \(([0-9\.]+)\) completed successfully\.$")
re_parse_retry = re.compile(r"Retrying node (\S+) \(retry \#(\d+) of (\d+)\)")
re_parse_dagman_condor_id = re.compile(r"\*\* condor_scheduniv_exec\.([0-9\.]+) \(CONDOR_DAGMAN\) STARTING UP")
re_parse_dagman_finished = re.compile(r"\(condor_DAGMAN\)[\w\s]+EXITING WITH STATUS (\d+)$")
re_parse_dagman_pid = re.compile(r"\*\* PID = (\d+)$")
re_parse_condor_version = re.compile(r"\*\* \$CondorVersion: ((\d+\.\d+)\.\d+)")
re_parse_condor_logfile = re.compile(r"Condor log will be written to ([^,]+)")
re_parse_condor_logfile_insane = re.compile(r"\d{2}\s{3,}(\S+)")
re_parse_multiline_files = re.compile(r"All DAG node user log files:")

# Constants
debug_level = logging.WARNING	# For now
logbase = "monitord.log" 	# Basename of daemon logfile
brainkeys = {}
brainkeys["required"] = ["basedir", "vogroup", "label", "rundir"]
brainkeys["optional"] = ["dax", "dag", "jsd", "run", "pegasushome"]
good_rsl = {"maxcputime": 1, "maxtime":1, "maxwalltime": 1}
speak = "TSSP/1.0"
PRESCRIPT_TASK_ID = -1			# id for prescript tasks
POSTSCRIPT_TASK_ID = -2			# id for postscript tasks
MONITORD_STATE_FILE = "monitord.info" 	# filename for writing monitord state information

# Events that constitute a pending job. Note that event SUBMIT is
# excluded on purpose since due to throttling inside DAGMan and
# Condor-G, a locally SUBMITted job may only become remotely
# GLOBUS_SUBMITted as throttles permits
pending_job_events = {"GLOBUS_SUBMIT": 1,
		      "GRID_SUBMIT": 1}

# Events that constitute a running job
running_job_events = {"EXECUTE": 1,
		      "GLOBUS_RESOURCE_DOWN": 1,
		      "GLOBUS_RESOURCE_UP": 1,
		      "JOB_SUSPENDED": 1,
		      "JOB_UNSUSPENDED": 1}

unsubmitted_events = {"UN_READY": 1,
		      "PRE_SCRIPT_STARTED": 1,
		      "PRE_SCRIPT_SUCCESS": 1,
		      "PRE_SCRIPT_FAILURE": 1}

# Global variables
wf = None			# instance of workflow class
replay_mode = 0			# disable checking if DAGMan's pid is gone
use_db = 1			# flag for using the database
single_db = 0			# flag for using a single db at the top level workflow
output_db = None		# connection string for the database
db_stats = 'no'			# collect and print database stats at the end of execution
use_nllog = 0			# flag for using Netlogger's log api
use_nllogfile = 0		# flag for using a file to log Netlogger events
nllog_string = None		# Filename or host:port when using Netlogger's log api
pending = {} 			# remember when GLOBUS_SUBMIT was entered
				# jid --> [stamp, condor_id, wtime, site]
running = {}			# ditto for EXECUTE for hidden starvation
				# jid --> [stamp, condor_id, wtime, site]
job_site = {}			# last site a job was planned for
jobstate = {}			# jid --> [stamp, event, condor_id, wtime, site]
siteinfo = {}			# site --> { [RPSF] --> [ #, mtime] }
walltime = {}			# jid --> walltime
waiting = {}			# site --> stamp/60 --> [ #P->R, sum(ptime)]

# Revision handling
revision = "$Revision: 2012 $" # Let cvs handle this, do not edit manually

# Remaining variables
out = None			# .dag.dagman.out file from command-line
run = None			# run directory from command-line dagman.out file
server = None			# server socket
sockfn = None			# socket filename
condor_dagman_executable = None	# condor_dagman binary location
workdb = None			# Instance of the database

jobtypes = {0: "unassigned",
	    1: "compute",
	    2: "stage-in",
	    3: "stage-out",
	    4: "replica registration",
	    5: "inter pool",
	    6: "create dir",
	    7: "staged compute",
	    8: "cleanup",
	    9: "symlink stage-in job"}

# Find condor_dagman
condor_dagman_executable = utils.find_exec("condor_dagman")
if condor_dagman_executable is None:
    # Default value
    condor_dagman_executable = "condor_dagman"

def finish_stampede_loader():
    """
    This function is called by the atexit module when monitord exits.
    It is used to make sure the loader has finished loading all data
    into the database. It will also produce stats for benchmarking.
    """
    if workdb is not None:
        try:
            if db_stats == 'yes' and logger.getEffectiveLevel() > logging.INFO:
                # Make sure log level is enough to display database benchmarking information
                logger.setLevel(logging.INFO)
            workdb.finish()
        except:
            logger.warning("could not call the finish method in the nl loader class... exiting anyway")

def translate_job_type(my_jobtype_id):
    """
    This function translates an integer jobtype into its more
    user-friendly string representation that can be used in databases.
    """
    try:
	my_jt = int(my_jobtype_id)
    except:
	# Failed to convert string into integer
	return None
    if my_jt in jobtypes:
	# Found it, return the appropriate string
	return jobtypes[my_jt]
    # my_jt is not in the jobtypes conversion map, return None
    return None

def out2log(rundir, outfile):
    # purpose: infer output symlink for Condor common user log
    # paramtr: rundir (IN): the run directory
    # paramtr: outfile (IN): the name of the out file we use
    # returns: the name of the log file to use

    # Get the basename
    my_base = os.path.basename(outfile)
    # NEW: Account for rescue DAGs
    my_base = my_base[:my_base.find(".dagman.out")]
    my_base = re_remove_extensions.sub('', my_base)
    # Add .log extension
    my_base = my_base + ".log"
    # Create path
    my_log = os.path.join(rundir, my_base)

    return my_log, my_base

class Job:
    """
    Class used to keep information needed to track a particular job
    """

    # Variables that describe a job, as per the Stampede schema
    # Some will be initialized in the init method, others will
    # get their values from the kickstart output file when a job
    # finished
    _wf_uuid = None
    _job_submit_seq = None
    _condor_id = None
    _name = None
    _jobtype = None
    _clustered = None
    _site_name = None
    _host_id = None
    _remote_user = None
    _remote_working_dir = None
    _cluster_start_time = None
    _cluster_duration = None
    _job_state = None
    _job_state_seq = 0
    _job_state_timestamp = None
    _walltime = None
    _job_site = None
    _job_info = [-1, -1, -1, -1]
    _job_output_counter = None
    _pre_script_start = None
    _pre_script_done = None
    _pre_script_exitcode = None
    _main_job_start = None
    _main_job_done = None
    _main_job_transformation = None
    _main_job_executable = None
    _main_job_arguments = None
    _main_job_exitcode = None
    _post_script_start = None
    _post_script_done = None
    _post_script_exitcode = None

    def __init__(self, wf_uuid, name, job_submit_seq):
	"""
	This function initializes the job parameters with the information
	available when a job is detected in the "PRE_SCRIPT_STARTED" or the
	"SUBMIT" state. Other parameters will remain None until a job
	finishes and a kickstart output file can be parsed.
	"""
	self._wf_uuid = wf_uuid
	self._name = name
	self._job_submit_seq = job_submit_seq

    def set_job_state(self, job_state, timestamp, status):
	"""
	This function sets the job state for this job. It also updates the
        times the main job and PRE/POST scripts start and finish.
	"""
	self._job_state = job_state
	self._job_state_timestamp = int(timestamp)
	# Increment job state sequence
	self._job_state_seq = self._job_state_seq + 1

	# Record timestamp for certain job states
	if job_state == "PRE_SCRIPT_STARTED":
	    self._pre_script_start = int(timestamp)
	elif job_state == "PRE_SCRIPT_SUCCESS" or job_state == "PRE_SCRIPT_FAILURE":
	    self._pre_script_done = int(timestamp)
	    self._pre_script_exitcode = status
	elif job_state == "POST_SCRIPT_STARTED":
	    self._post_script_start = int(timestamp)
	elif job_state == "POST_SCRIPT_TERMINATED":
	    self._post_script_done = int(timestamp)
	elif job_state == "EXECUTE":
	    self._main_job_start = int(timestamp)
	elif job_state == "JOB_TERMINATED":
	    self._main_job_done = int(timestamp)
	elif job_state == "JOB_SUCCESS" or job_state == "JOB_FAILURE":
	    self._main_job_exitcode = status
	elif job_state == "POST_SCRIPT_SUCCESS" or job_state == "POST_SCRIPT_FAILURE":
	    self._post_script_exitcode = status

    def parse_sub_file(self, stamp, my_fn):
	"""
	This function parses a job's submit file and returns job
	planning information. In addition, we try to populate the job
	type from information in the submit file.
	# paramtr: stamp(IN): timestamp associated with the log line
	# paramtr: my_fn(IN): submit file name
	# class m: _job_info(MODIFIED): [dev#, ino#, size, mtime]
	# globals: good_rsl(IN): which RSL keys constitute time requirements
	# returns: (largest job time requirement in minutes, destination site)
	# returns: (None, None) if sub file not found
	"""
	my_result = None
	my_site = None

	# Update stat record for submit file
	try:
	    my_stats = os.stat(my_fn)
	except:
	    # Could not stat file
	    logger.error("stat %s" % (my_fn))
	    return my_result, my_site

	self._job_info[0] = my_stats[2] # dev #
	self._job_info[1] = my_stats[1] # inode #
	self._job_info[2] = my_stats[6] # Size
	self._job_info[3] = my_stats[8] # mtime

	if stamp < self._job_info[3]:
            logger.warning("stamp=%d, mtime=%d, diff = %d" % (stamp, self._job_info[3], self._job_info[3]-stamp))
	    logger.info("skipping %s (reparsing events)" % (my_fn))
	    return my_result, my_site

	try:
	    SUB = open(my_fn, "r")
	except:
	    logger.error("unable to parse %s" % (my_fn))
	    return my_result, my_site

	for my_line in SUB:
	    if re_rsl_string.search(my_line):
		# Found RSL string, do parse now
		for my_match in re.findall(r"\(([^)]+)\)", my_line):
		    # Split into key and value
		    my_k, my_v = my_match.split("=", 1)
		    # Remove _- characters from string
		    my_k = re_rsl_clean.sub('', my_k)
		    if my_k.lower() in good_rsl and my_v > my_result:
			try:
			    my_result = int(my_v)
			except:
			    my_result = None
	    elif re_site_parse_gvds.search(my_line):
		# GVDS agreement
		my_site = re_site_parse_gvds.search(my_line).group(4)
                self._site_name = my_site
	    elif re_site_parse_euryale.search(my_line):
		# Euryale specific comment
		my_site = re_site_parse_euryale.search(my_line).group(1)
                self._site_name = my_site
	    elif re_parse_jobtype.search(my_line):
		# Found line with jobtype information
		my_jobtype_id = re_parse_jobtype.search(my_line).group(1)
		# Convert jobtype_id into a string jobtype
		my_jobtype = translate_job_type(my_jobtype_id)
		if my_jobtype is not None:
		    self._jobtype = my_jobtype
	    elif re_parse_transformation.search(my_line):
		# Found line with job transformation
		my_transformation = re_parse_transformation.search(my_line).group(1)
		# Remove quotes, if any
		my_transformation = my_transformation.strip('"')
		self._main_job_transformation = my_transformation
            elif re_parse_executable.search(my_line):
                # Found line with executable
                my_executable = re_parse_executable.search(my_line).group(1)
                # Remove quotes, if any
                my_executable = my_executable.strip('"')
                self._main_job_executable = my_executable
            elif re_parse_arguments.search(my_line):
                # Found line with arguments
                my_arguments = re_parse_arguments.search(my_line).group(1)
                # Remove quotes, if any
                my_arguments = my_arguments.strip('"')
                self._main_job_arguments = my_arguments

	SUB.close()

	# All done!
	return my_result, my_site

    def extract_job_info(self, buffer):
	"""
	This function reads the output from the kickstart output parser and
	extracts the job information for the Stampede schema.

        Returns True if successful, or None if an error occurs.
	"""

	# Check if we have anything
	if len(buffer) == 0:
	    return None

	# Check if first record is an invocation record (it should be!)
	if not "invocation" in buffer[0]:
	    logger.debug("cannot find invocation record!")
	    return None

	# Ok, we have an invocation record, extract the information we
        # need. Note that this may overwrite information obtained from
        # the submit file (e.g. the site_name).
	my_record = buffer[0]
	if "resource" in my_record:
	    self._site_name = my_record["resource"]
	if "user" in my_record:
	    self._remote_user = my_record["user"]
	if "cwd" in my_record:
	    self._remote_working_dir = my_record["cwd"]
	if "hostname" in my_record:
	    self._host_id = my_record["hostname"]

	# Set clustered flag
	if len(buffer) > 1:
	    self._clustered = True
	else:
	    self._clustered = False

	# Fill in cluster parameters
	if "clustered" in buffer[len(buffer) - 1]:
	    my_record = buffer[len(buffer) - 1]
	    if "duration" in my_record:
		self._cluster_duration = my_record["duration"]
	    if "start" in my_record:
		# Convert timestamp to EPOCH
		my_start = utils.epochdate(my_record["start"], short=False)
		if my_start is not None:
		    self._cluster_start_time = my_start

	# Done populating Job class with information from the kickstart output file
	return True
	
class Workflow:
    """
    Class used to keep everything needed to track a particular workflow
    """

    # Variables that describe a workflow, as per the Stampede schema
    # These are initialized in the init method
    _db = None
    _wf_uuid = None
    _dax_label = None
    _timestamp = None
    _submit_hostname = None
    _submit_dir = None
    _planner_arguments = None
    _user = None
    _grid_dn = None
    _planner_version = None
    _parent_workflow_id = None
    _last_submitted_job = None
    _jobs_map = {}
    _jobs = {}
    _job_submit_seq = 1
    _out_file = None			# .dag.dagman.out file
    _run_dir = None			# run directory for this workflow
    _log_file = None			# monitord.log file
    _jsd_file = None			# jobstate.log file
    _JSDB = None			# Handle for jobstate.log file
    _job_counters = {}			# Job counters for figuring out which output file to parse
    _job_info = {}			# jobid --> [sub_file, pre_exec, pre_args, post_exec, post_args, is_subdag, subdag_dag, subdag_dir]
    _valid_braindb = True		# Flag for creating a new brain db if we don't find one
    _line = 0 				# line number from dagman.out file
    _last_processed_line = 0		# line last processed by the monitoring daemon
    _restart_count = 0			# Keep track of how many times the workflow was restarted
    _skipping_recovery_lines = False	# Flag for skipping the repeat duplicate messages generated by DAGMan
    _dagman_condor_id = None		# Condor id of the current DAGMan
    _dagman_pid = 0			# Condor DAGMan's PID
    _workflow_start = 0			# Time we started tracking this workflow (initialized in __init__)
    _current_timestamp = 0		# Last timestamp from DAGMan
    _terminate = None			# Keep track of when to finish this workflow
    _condorlog = None			# Condor common logfile
    _multiline_file_flag = False	# Track multiline user log files, DAGMan > 6.6

    def output_to_db(self, event, kwargs):
	"""
	This function sends an NetLogger event to the loader class.
	"""
        # Sanity check (should also be done elsewhere, but repeated here)
        if self._db is None:
            return

        if use_nllog == 1:
            # We are using NetLogger's logging api to output our logs
            self._db.write(event=event, **kwargs)

        if use_db == 1:
            # Before we send anything to the loader, we need to make a few adjustments

            # Add event key
            kwargs["event"] = "stampede." + event

            # Translate __ keys to .
            if "wf__id" in kwargs:
                kwargs["wf.id"] = str(kwargs.pop("wf__id"))
            if "parent__wf__id" in kwargs:
                kwargs["parent.wf.id"] = kwargs.pop("parent__wf__id")
            if "job__id" in kwargs:
                kwargs["job.id"] = kwargs.pop("job__id")
            if "condor__id" in kwargs:
                kwargs["condor.id"] = kwargs.pop("condor__id")
            if "js__id" in kwargs:
                kwargs["js.id"] = kwargs.pop("js__id")
            if "task__id" in kwargs:
                kwargs["task.id"] = kwargs.pop("task__id")

            # Send it to the loader
            self._db.notify(kwargs)

    def db_send_edge(self, parent, child):
	"""
	This function sends to the DB information about a parent-child relationship.
	"""
	# Check if database is configured
	if self._db is None:
	    return
	# Make sure parameters are not None
	if parent is None or child is None:
	    return

	# Start empty
	kwargs = {}
	# Make sure we include the wf_uuid
	kwargs["wf__id"] = self._wf_uuid
	kwargs["ts"] = self._current_timestamp
        kwargs["parent"] = parent
        kwargs["child"] = child
	
	# Send workflow state event to database
	self.output_to_db("edge", kwargs)

    def parse_dag_file(self, dag_file):
	"""
	This function parses the DAG file and determines submit file locations
	"""

        # If we already have jobs in our _job_info dictionary, skip reading the dag file
        if len(self._job_info) > 0:
            logger.debug("skipping parsing the dag file, already have job info loaded...")
            return

        dag_file = os.path.join(self._run_dir, dag_file)

	try:
	    DAG = open(dag_file, "r")
	except:
	    logger.warning("unable to read %s!" % (dag_file))
	else:
	    for dag_line in DAG:
		if (dag_line.lower()).find("job") >= 0:
		    # Found Job line, parse it
		    my_match = re_parse_dag_submit_files.search(dag_line)
		    if my_match:
			if not my_match.group(3):
			    my_jobid = my_match.group(1)
			    my_sub = os.path.join(self._run_dir, my_match.group(2))
			    # Found submit file for not-DONE job
			    if my_jobid in self._job_info:
				# Entry already exists for this job, just collect submit file info
				self._job_info[my_jobid][0] = my_sub
			    else:
				# No entry for this job, let's create a new one
				self._job_info[my_jobid] = [my_sub, None, None, None, None, False, None, None]
                elif (dag_line.lower()).find("script post") >= 0:
		    # Found SCRIPT POST line, parse it
		    my_match = re_parse_dag_script.search(dag_line)
		    if my_match:
			my_jobid = my_match.group(1)
			my_exec = my_match.group(2)
			my_args = my_match.group(3)
			if my_jobid in self._job_info:
			    # Entry already exists for this job, just collect post script info
			    self._job_info[my_jobid][3] = my_exec
			    self._job_info[my_jobid][4] = my_args
			else:
			    # No entry for this job, let's create a new one
			    self._job_info[my_jobid] = [None, None, None, my_exec, my_args, False, None, None]
		elif (dag_line.lower()).find("script pre") >= 0:
		    # Found SCRIPT PRE line, parse it
		    my_match = re_parse_dag_script.search(dag_line)
		    if my_match:
			my_jobid = my_match.group(1)
			my_exec = my_match.group(2)
			my_args = my_match.group(3)
			if my_jobid in self._job_info:
			    # Entry already exists for this job, just collect pre script info
			    self._job_info[my_jobid][1] = my_exec
			    self._job_info[my_jobid][2] = my_args
			else:
			    # No entry for this job, let's create a new one
			    self._job_info[my_jobid] = [None, my_exec, my_args, None, None, False, None, None]
                elif (dag_line.lower()).find("subdag external") >= 0:
                    # Found SUBDAG line, parse it
                    my_match = re_parse_dag_subdag.search(dag_line)
                    if my_match:
                        my_jobid = my_match.group(1)
                        my_dag = my_match.group(2)
                        my_dir = my_match.group(3)
                        if my_jobid in self._job_info:
                            # Entry already exists for this job, just set subdag flag, and dag/dir info
			    self._job_info[my_jobid][5] = True
			    self._job_info[my_jobid][6] = my_dag
                            self._job_info[my_jobid][7] = my_dir
                        else:
                            # No entry for this job, let's create a new one
                            self._job_info[my_jobid] = [None, None, None, None, None, True, my_dag, my_dir]
                elif re_parse_dag_edge.search(dag_line) is not None:
                    # Found edge
                    if self._restart_count > 0:
                        # Skip these for restarts
                        continue
                    my_match = re_parse_dag_edge.search(dag_line)
                    if my_match:
                        my_parent = my_match.group(1)
                        my_child = my_match.group(2)
                        self.db_send_edge(my_parent, my_child)
			
	    DAG.close()

	# POST-CONDITION: _job_info contains only submit-files of jobs
	# that are not yet done. Normally, this are all submit
	# files. In rescue DAGS, that is an arbitrary subset of all
	# jobs. In addition, _job_info should contain all PRE and POST
	# script information for job in this workflow, and all subdag
        # jobs, with the their dag files, and directories

    def read_workflow_state(self):
        """
        This function reads the job_submit_seq and the job_counters
        dictionary from a file in the workflow's run directory. This
        is used for restarting the logging information from where we
        stopped last time.
        """
        
        my_fn = os.path.join(self._run_dir, MONITORD_STATE_FILE)

        try:
            INPUT = open(my_fn, "r")
        except:
            logger.info("cannot open state file %s" % (my_fn))
            return
        
        try:
            for line in INPUT:
                # Split the input line in 2, and make the second part an integer
                my_job, my_count = line.split(" ", 1)
                my_job = my_job.strip()
                my_count = int(my_count.strip())
                if my_job == "monitord_job_sequence":
                    # This is the last job_submit_seq used
                    self._job_submit_seq = my_count
                elif my_job == "monitord_dagman_out_sequence":
                    # This is the line we last read from the dagman.out file
                    self._last_processed_line = my_count
                elif my_job == "monitord_workflow_restart_count":
                    # This is the number of restarts we have seen in the past
                    self._restart_count = my_count
                else:
                    # Another job counter
                    self._job_counters[my_job] = my_count
        except:
            logger.error("error processing state file %s" % (my_fn))
        
        # Close the file
        try:
            INPUT.close()
        except:
            pass

        # All done!
        return

    def write_workflow_state(self):
        """
        This function writes the job_submit_seq and the job_counters
        dictionary to a file in the workflow's run directory. This can
        be used later for restarting the logging information from
        where we stop. This function will overwrite the log file every
        time is it called.
        """
        
        my_fn = os.path.join(self._run_dir, MONITORD_STATE_FILE)

        try:
            OUT = open(my_fn, "w")
        except:
            logger.error("cannot open state file %s" % (my_fn))
            return
        
        try:
            # Write first line with the last job_submit_seq used
            OUT.write("monitord_job_sequence %d\n" % (self._job_submit_seq))
            # Then, write the last line number of the dagman.out file we processed
            if self._line > self._last_processed_line:
                OUT.write("monitord_dagman_out_sequence %s\n" % (self._line))
            else:
                OUT.write("monitord_dagman_out_sequence %s\n" % (self._last_processed_line))
            # Next, write the restart count
            OUT.write("monitord_workflow_restart_count %d\n" % (self._restart_count))
            # Finally, write all job_counters
            for my_job in self._job_counters:
                OUT.write("%s %d\n" % (my_job, self._job_counters[my_job]))
        except:
            logger.error("cannot write state to log file %s" % (my_fn))
        
        # Close the file
        try:
            OUT.close()
        except:
            pass

        # All done!
        return

    def db_send_wf_info(self):
	"""
	This function sends to the DB information about the workflow
	"""
	# Start empty
	kwargs = {}
	# Make sure we include the wf_uuid
	kwargs["wf__id"] = self._wf_uuid
	# Now include others, if they are defined
	if self._dax_label is not None:
	    kwargs["dax_label"] = self._dax_label
	if self._timestamp is not None:
	    kwargs["ts"] = self._timestamp
	if self._submit_hostname is not None:
	    kwargs["submit_hostname"] = self._submit_hostname
	if self._submit_dir is not None:
	    kwargs["submit_dir"] = self._submit_dir
	if self._planner_arguments is not None:
	    kwargs["planner_arguments"] = self._planner_arguments
	if self._user is not None:
	    kwargs["user"] = self._user
	if self._grid_dn is not None:
	    kwargs["grid_dn"] = self._grid_dn
	if self._planner_version is not None:
	    kwargs["planner_version"] = self._planner_version
	if self._parent_workflow_id is not None:
	    kwargs["parent__wf__id"] = self._parent_workflow_id
	else:
	    kwargs["parent__wf__id"] = "None"

	# Send workflow event to database
	self.output_to_db("workflow.plan", kwargs)

    def db_send_wf_state(self, state):
	"""
	This function sends to the DB information about the current workflow state
	"""
	# Check if database is configured
	if self._db is None:
	    return
	# Make sure parameters are not None
	if state is None:
	    return

	# Start empty
	kwargs = {}
	# Make sure we include the wf_uuid
	kwargs["wf__id"] = self._wf_uuid
	kwargs["ts"] = self._current_timestamp
        # Always decrement the restart count by 1
        kwargs["restart_count"] = self._restart_count - 1
	state = "workflow." + state

	# Send workflow state event to database
	self.output_to_db(state, kwargs)

    def change_wf_state(self, state):
	"""
	This function changes the workflow state, and sends the state change to
        the DB. This function is called as response to DAGMan starting/stopping.
        """
        if state == "start":
            logger.info("DAGMan starting with condor id %s" % (self._dagman_condor_id))
            self._JSDB.write("%d INTERNAL *** DAGMAN_STARTED %s ***\n" % (self._current_timestamp, self._dagman_condor_id))
            self._restart_count = self._restart_count + 1
        elif state == "end":
            self._JSDB.write("%d INTERNAL *** DAGMAN_FINISHED %s ***\n" % (self._current_timestamp, self._terminate))
        
        self.db_send_wf_state(state)

    def __init__(self, rundir, outfile, extra_params=None, database=None,
                 workflow_config_file=None, jsd=None, parent_id=None):
	"""
	This function initializes the workflow object. It looks
        for the workflow configuration file (or for workflow_config_file,
        if specified). Keys in wfparams override values in the configuration
        file. Here we also open the jobstate.log file, and parse the dag.
	"""
	# Initialize basic class variables
        self._out_file = outfile
	self._run_dir = rundir
        self._parent_workflow_id = parent_id
	self._db = database
        self._workflow_start = int(time.time())

        # Determine location of jobstate.log file
        if jsd is None:
            self._jsd_file = os.path.join(rundir, utils.jobbase)
        else:
            # Make sure we have an absolute path
            self._jsd_file = os.path.join(rundir, jsd)

        if not os.path.isfile(self._jsd_file):
            logger.info("creating new file %s" % (self._jsd_file))

        try:
            # Create new file, or append to an existing one
            if not replay_mode:
                self._JSDB = open(self._jsd_file, 'a', 0)
            else:
                # Rotate jobstate.log file, if any
                utils.rotate_log_file(self._jsd_file)
                self._JSDB = open(self._jsd_file, 'w', 0)
        except:
            logger.critical("error creating/appending to %s!" % (self._jsd_file))
            sys.exit(1)

        # Say hello.... add start information to JSDB
        self._JSDB.write("%d INTERNAL *** MONITORD_STARTED ***\n" % (self._workflow_start))

        if not replay_mode:
            # Recover state from a previous run
            self.read_workflow_state()

        # Parse the braindump file
        wfparams = utils.slurp_braindb(rundir, workflow_config_file)

        if len(wfparams) == 0:
            # Set flag for creating a braindb file if nothing was read
            self._valid_braindb = False

        # Update wfparams with keys from extra_params, if any
        if extra_params is not None:
            wfparams.update(extra_params)

        # Go through wfparams, and read what we need
        if "wf_uuid" in wfparams:
            if wfparams["wf_uuid"] is not None:
                self._wf_uuid = wfparams["wf_uuid"]
        # If _wf_uuid is not defined, we create a random uuid for this workflow
        if self._wf_uuid is None:
            if uuid_present == True:
                self._wf_uuid = uuid.uuid4()
            else:
                logger.info("uuid module is not present, using time.time() to generate wf_uuid")
                self._wf_uuid = str(time.time())
        if "dax_label" in wfparams:
            self._dax_label = wfparams["dax_label"]
        else:
            # Use "label" if "dax_label" not found
            if "label" in wfparams:
                self._dax_label = wfparams["label"]
        if "timestamp" in wfparams:
            self._timestamp = wfparams["timestamp"]
        else:
            # Use "pegasus_wf_time" if "timestamp" not found
            if "pegasus_wf_time" in wfparams:
                self._timestamp = wfparams["pegasus_wf_time"]
        # Convert timestamp from YYYYMMDDTHHMMSSZZZZZ to Epoch
        if self._timestamp is not None:
            try:
                # Split date/time and timezone information
                dt = self._timestamp[:-5]
                tz = self._timestamp[-5:]
                # Convert date/time to datetime format
                # my_time = datetime.datetime.strptime(dt, "%Y%m%dT%H%M%S")
                # Python < 2.5 doesn't have the datetime.datetime.strptime function
                my_time = datetime.datetime(*(time.strptime(dt, "%Y%m%dT%H%M%S")[0:6]))
                # Split timezone in hours and minutes
                my_hour = int(tz[:-2])
                my_min = int(tz[-2:])
                # Calculate offset
                my_offset = datetime.timedelta(hours=my_hour, minutes=my_min)
                # Subtract offset
                my_time = my_time - my_offset
                # Turn my_time into Epoch format
                self._timestamp = int(calendar.timegm(my_time.timetuple()))
            except:
                logger.warning("error converting timestamp %s to Epoch format" % self._timestamp)
        else:
            # No timestamp information is available, just use current time
            self._timestamp = int(time.time())
        if "submit_dir" in wfparams:
            self._submit_dir = wfparams["submit_dir"]
        else:
            # Use "run" if "submit_dir" not found
            if "run" in wfparams:
                self._submit_dir = wfparams["run"]
        if "planner_version" in wfparams:
            self._planner_version = wfparams["planner_version"]
        else:
            # Use "pegasus_version" if "planner_version" not found
            if "pegasus_version" in wfparams:
                self._planner_version = wfparams["pegasus_version"]
        if "submit_hostname" in wfparams:
            self._submit_hostname = wfparams["submit_hostname"]
        if "user" in wfparams:
            self._user = wfparams["user"]
        if "grid_dn" in wfparams:
            self._grid_dn = wfparams["grid_dn"]

        # All done... last step is to send to the database the workflow plan event
        # However, we only do this, if this is the first time we run
        if self._db is not None and self._last_processed_line == 0:
            # Add workflow info to database
            self.db_send_wf_info()

    def end_workflow(self):
        """
        This function writes the last line in the jobstate.log and closes
        the file.
        """
        my_workflow_end = int(time.time())

        self._JSDB.write("%d INTERNAL *** MONITORD_FINISHED %d ***\n" % (my_workflow_end, self._terminate))
        self._JSDB.close()

        if not replay_mode:
            # Write monitord.done file
            my_touch_name = os.path.join(self._run_dir, "monitord.done")
            try:
                TOUCH = open(my_touch_name, "w")
            except:
                logger.error("opening %s" % (my_touch_name))
            else:
                TOUCH.write("%s %.3f\n" % (utils.isodate(my_workflow_end), (my_workflow_end - self._workflow_start)))
                TOUCH.close()

            # Attempt to copy the condor common logfile to the current directory
            if self._condorlog is not None:
                if (os.path.isfile(self._condorlog) and
                    os.access(self._condorlog, os.R_OK) and
                    self._condorlog.find('/') == 0):

                    # Copy common condor log to local directory
                    my_log = out2log(self._run_dir, self._out_file)[0]
                    my_cmd = "/bin/cp -p %s %s.copy" % (self._condorlog, my_log)
                    my_status, my_output = commands.getstatusoutput(my_cmd)

                    if my_status == 0:
                        # Copy successful
                        try:
                            os.unlink(my_log)
                        except:
                            logger.error("removing %s" % (my_log))
                        else:
                            try:
                                os.rename("%s.copy" % (my_log), my_log)
                            except:
                                logger.error("renaming %s.copy to %s" % (my_log, my_log))
                            else:
                                logger.info("copied common log to %s" % (self._run_dir))
                    else:
                        logger.info("%s: %d:%s" % (my_cmd, my_status, my_output))


    def find_jobid(self, jobid):
	"""
	This function finds the job_submit_seq of a given jobid by checking
	the _jobs_map dict. Since add_job will update _jobs_map, this function
	will return the job_submit_seq of the latest jobid added to the workflow
	"""
	if jobid in self._jobs_map:
	    return self._jobs_map[jobid]

	# Not found, return None
	return None

    def find_job_submit_seq(self, jobid):
	"""
	If a jobid already exists and is in the PRE_SCRIPT_SUCCESS mode, this
	function returns its job_submit_seq. Otherwise, it returns None, meaning
	a new job needs to be created
	"""
	# Look for a jobid
	my_job_submit_seq = self.find_jobid(jobid)

	# No such job, return None
	if my_job_submit_seq is None:
	    return None

	# Make sure the job is there
	if not (jobid, my_job_submit_seq) in self._jobs:
	    logger.warning("cannot find job: %s, %s" % (jobid, my_job_submit_seq))
	    return None

	my_job = self._jobs[jobid, my_job_submit_seq]
	if my_job._job_state == "PRE_SCRIPT_SUCCESS" or my_job._job_state == "DAGMAN_SUBMIT":
	    # jobid is in "PRE_SCRIPT_SUCCESS" or "DAGMAN_SUBMIT"  state,
            # just return job_submit_seq
	    return my_job_submit_seq

	# jobid is in another state, return None
	return None

    def db_send_job_info(self, my_job, job_state):
	"""
	This function sends to the DB information about a particular job
	"""
	# Start empty
	kwargs = {}

	# Make sure we include the wf_uuid, name, and job_submit_seq
	kwargs["wf__id"] = my_job._wf_uuid
	kwargs["name"] = my_job._name
	kwargs["job__id"] = my_job._job_submit_seq
	kwargs["ts"] = self._current_timestamp
	if my_job._condor_id is not None:
	    kwargs["condor__id"] = my_job._condor_id
	if my_job._jobtype is not None:
	    kwargs["jobtype"] = my_job._jobtype
	if my_job._clustered is not None:
	    kwargs["clustered"] = my_job._clustered
	if my_job._site_name is not None:
	    kwargs["site_name"] = my_job._site_name
	if my_job._remote_user is not None:
	    kwargs["remote_user"] = my_job._remote_user
	if my_job._remote_working_dir is not None:
	    kwargs["remote_working_dir"] = my_job._remote_working_dir
	if my_job._cluster_start_time is not None:
	    kwargs["cluster_start_time"] = my_job._cluster_start_time
	if my_job._cluster_duration is not None:
	    kwargs["cluster_duration"] = my_job._cluster_duration
	if job_state == "JOB_SUCCESS" or job_state == "JOB_FAILURE":
	    event_type = "job.mainjob.end"
	elif job_state == "PRE_SCRIPT_STARTED":
	    event_type = "job.prescript.start"
	elif job_state == "SUBMIT":
	    event_type = "job.mainjob.start"
	else:
	    logger.warning("unknown job state: %s" % (job_state))
	    return

	# Send job event to database
	self.output_to_db(event_type, kwargs)

    def db_send_job_note(self, my_job, event_type):
	"""
	This function sends to the DB a notification about a particular job
	"""
	# Start empty
	kwargs = {}

	# Make sure we include the wf_uuid, name, and job_submit_seq
	kwargs["wf__id"] = my_job._wf_uuid
	kwargs["name"] = my_job._name
	kwargs["job__id"] = my_job._job_submit_seq
	kwargs["ts"] = self._current_timestamp

	# Send job event to database
	self.output_to_db(event_type, kwargs)

    def db_send_job_state(self, my_job):
	"""
	This function sends to the DB job state information for a particular job
	"""
	# Start empty
	kwargs = {}

	# Make sure we include the wf_uuid, name, and job_submit_seq
	kwargs["wf__id"] = my_job._wf_uuid
	kwargs["name"] = my_job._name
	kwargs["job__id"] = my_job._job_submit_seq
	kwargs["state"] = my_job._job_state
	kwargs["ts"] = my_job._job_state_timestamp
	kwargs["js__id"] = my_job._job_state_seq

	# Send job state event to database
	self.output_to_db("job.state", kwargs)

    def db_send_task_info(self, my_job, task_type, task_id, invocation_record={}):
	"""
	This function sends to the database task
	information. task_type is either "PRE SCRIPT", "MAIN JOB", or
	"POST SCRIPT"
	"""
	# Start empty
	kwargs = {}

	# Sanity check, verify task type
	if task_type != "PRE SCRIPT" and task_type != "POST SCRIPT" and task_type != "MAIN JOB":
	    logger.warning("unknown task type: %s" % (task_type))
	    return

	# Make sure we include the wf_uuid, name, and job_submit_seq
	kwargs["wf__id"] = my_job._wf_uuid
	kwargs["name"] = my_job._name
	kwargs["job__id"] = my_job._job_submit_seq

	# Add task id to this event
	kwargs["task__id"] = task_id

	if task_type == "PRE SCRIPT":
	    # This is a PRE SCRIPT task
	    event_type = "task.prescript"
	    kwargs["transformation"] = "dagman::pre"
	    kwargs["start_time"] = my_job._pre_script_start
	    kwargs["duration"] = my_job._pre_script_done - my_job._pre_script_start
	    kwargs["exitcode"] = my_job._pre_script_exitcode
	    if my_job._name in self._job_info:
		kwargs["executable"] = self._job_info[my_job._name][1]
		kwargs["arguments"] = self._job_info[my_job._name][2]
	    kwargs["ts"] = my_job._pre_script_done
	elif task_type == "POST SCRIPT":
	    # This is a POST SCRIPT task
	    event_type = "task.postscript"
	    kwargs["transformation"] = "dagman::post"
	    kwargs["start_time"] = my_job._post_script_start
	    kwargs["duration"] = my_job._post_script_done - my_job._post_script_start
	    kwargs["exitcode"] = my_job._post_script_exitcode
	    if my_job._name in self._job_info:
		kwargs["executable"] = self._job_info[my_job._name][3]
		kwargs["arguments"] = self._job_info[my_job._name][4]
	    kwargs["ts"] = my_job._post_script_done
	elif task_type == "MAIN JOB":
	    # This is a MAIN JOB task
	    event_type = "task.mainjob"
	    if "transformation" in invocation_record:
		kwargs["transformation"] = invocation_record["transformation"]
            else:
                if my_job._main_job_transformation is not None:
                    kwargs["transformation"] = my_job._main_job_transformation
                else:
                    if (my_job._name in self._job_info and
                        self._job_info[my_job._name][5] == True):
                        kwargs["transformation"] = "condor::dagman"
	    if "start" in invocation_record:
                # Need to convert it to epoch data
		my_start = utils.epochdate(invocation_record["start"])
            else:
                # Not in the invocation record, let's use our own time keeping
                my_start = my_job._main_job_start
            if my_start is not None:
                kwargs["start_time"] = my_start
	    if "duration" in invocation_record:
		kwargs["duration"] = invocation_record["duration"]
            else:
                # Duration not in the invocation record
                if my_job._main_job_start is not None and my_job._main_job_done is not None:
                    try:
                        my_duration = int(my_job._main_job_done) - int(my_job._main_job_start)
                    except:
                        my_duration = None
                    if my_duration is not None:
                        kwargs["duration"] = my_duration
	    if my_start is not None and "duration" in invocation_record:
		# Calculate timestamp for when this task finished
		try:
		    kwargs["ts"] = int(my_start + int(invocation_record["duration"]))
		except:
		    # Something went wrong, just use the time the main job finished
		    kwargs["ts"] = my_job._main_job_done
	    else:
		kwargs["ts"] = my_job._main_job_done
	    if "exitcode" in invocation_record:
		kwargs["exitcode"] = invocation_record["exitcode"]
            else:
                if my_job._main_job_exitcode is not None:
                    kwargs["exitcode"] = my_job._main_job_exitcode
	    if "name" in invocation_record:
		kwargs["executable"] = invocation_record["name"]
            else:
                if my_job._main_job_executable is not None:
                    kwargs["executable"] = my_job._main_job_executable
                else:
                    if (my_job._name in self._job_info and
                        self._job_info[my_job._name][5] == True):
                        kwargs["executable"] = condor_dagman_executable
	    if "argument-vector" in invocation_record:
		kwargs["arguments"] = invocation_record["argument-vector"]
            else:
                if my_job._main_job_arguments is not None:
                    kwargs["arguments"] = my_job._main_job_arguments

	# Send job event to database
	self.output_to_db(event_type, kwargs)

    def db_send_host_info(self, my_job, record):
	"""
	This function sends host information collected from the kickstart record to the database.
	"""
	# Start empty
	kwargs = {}

	# Make sure we include the wf_uuid, name, and job_submit_seq
	kwargs["wf__id"] = my_job._wf_uuid
	kwargs["name"] = my_job._name
	kwargs["job__id"] = my_job._job_submit_seq

	# Add information about the host
	if "hostname" in record:
	    kwargs["hostname"] = record["hostname"]
	else:
	    # Don't know what the hostname is
	    kwargs["hostname"] = "unknown"
	if "hostaddr" in record:
	    kwargs["ip_address"] = record["hostaddr"]
	else:
	    # Don't know what the ip address is
	    kwargs["ip_address"] = "unknown"
	if "resource" in record:
	    kwargs["site_name"] = record["resource"]
	else:
	    # Don't know what the site name is
	    kwargs["site_name"] = "unknown"
	if "total" in record:
	    kwargs["total_ram"] = record["total"]
	if "system" in record and "release" in record and "machine" in record:
	    kwargs["uname"] = record["system"] + "-" + record["release"] + "-" + record["machine"]

	# Add timestamp
	kwargs["ts"] = self._current_timestamp

	# Send host event to database
	self.output_to_db("host", kwargs)

    def parse_job_output(self, my_job, job_state):
	"""
	This function tries to parse the kickstart output file of a given job and
	collect information for the stampede schema.
	"""
        my_output = []
        skip_kickstart_parsing = False

        # Check if this is a subdag job
        if (my_job._name in self._job_info and
            self._job_info[my_job._name][5] == True):
            skip_kickstart_parsing = True

        # If job is a subdag job, skip looking for its kickstart output
        if not skip_kickstart_parsing:
            # Compose kickstart output file name (base is the filename before rotation)
            my_job_output_fn_base = os.path.join(self._run_dir, my_job._name) + ".out"
            my_job_output_fn = my_job_output_fn_base + ".%03d" % (my_job._job_output_counter)

            # First assume we will find rotated file
            my_parser = kickstart_parser.Parser(my_job_output_fn)
            my_output = my_parser.parse_stampede()

            # Check if we were able to find it
            if my_parser._open_error == True:
                # File wasn't there, look for the file before the rotation
                my_parser.__init__(my_job_output_fn_base)
                my_output = my_parser.parse_stampede()

                if my_parser._open_error == True:
                    # Couldn't find it again, one last try, as it might have just been moved
                    my_parser.__init__(my_job_output_fn)
                    my_output = my_parser.parse_stampede()

            # Check if successful
            if my_parser._open_error == True:
                logger.info("unable to find output file for job %s" % (my_job._name))

	# Initialize task id counter
	my_task_id = 1

        if len(my_output) > 0:
            # Add job information (from the kickstart output file) to
            # the Job class.
            my_job.extract_job_info(my_output)

            # Send updated info to the database
            self.db_send_job_info(my_job, job_state)

            # Loop through all records
            for record in my_output:
                # Skip non-invocation records
                if not "invocation" in record:
                    continue
	
                # Send task information to the database
                self.db_send_task_info(my_job, "MAIN JOB", my_task_id, record)

                # Increment task id counter
                my_task_id = my_task_id + 1

                # Send host information to the database
                self.db_send_host_info(my_job, record)
        else:
            # No kickstart output is available, still send updated job
            # information to the database.
            self.db_send_job_info(my_job, job_state)

            # If we don't have kickstart, we only generate 1 task
            self.db_send_task_info(my_job, "MAIN JOB", my_task_id)

    def add_job(self, jobid, job_state, condor_id=None):
	"""
	This function adds a new job to our list of jobs. It first checks if
	the job is already in our list in the PRE_SCRIPT_SUCCESS state, if so,
	we just update its condor id. Otherwise we create a new Job container.
	In any case, we always set the job state to job_state.
	"""

	my_job_submit_seq = self.find_job_submit_seq(jobid)

	if my_job_submit_seq is not None:
	    # Job already exists
	    if not (jobid, my_job_submit_seq) in self._jobs:
		logger.warning("cannot find job: %s, %s" % (jobid, my_job_submit_seq))
		return

	    my_job = self._jobs[jobid, my_job_submit_seq]

	    # Set condor_id
	    if condor_id is not None:
		my_job._condor_id = condor_id

	    # Update job state
	    my_job._job_state = job_state
	    my_job._job_state_timestamp = int(self._current_timestamp)
	else:
	    # This is a new job, we have to do everything from scratch
	    my_job_submit_seq = self._job_submit_seq

	    # Make sure job is not already there
	    if (jobid, my_job_submit_seq) in self._jobs:
		logger.warning("trying to add job twice: %s, %s" % (jobid, my_job_submit_seq))
		return

	    # Create new job container
	    my_job = Job(self._wf_uuid, jobid, my_job_submit_seq)
	    # Set job state
	    my_job._job_state = job_state
	    my_job._job_state_timestamp = int(self._current_timestamp)
	    # Set condor_id
	    my_job._condor_id = condor_id
	    # Set job type as "compute" for now, will change when submit file is parsed
	    my_job._jobtype = "compute"
	    # Add job to our list of jobs
	    self._jobs[jobid, my_job_submit_seq] = my_job

	    # Add/Update job in our job map
	    self._jobs_map[jobid] = my_job_submit_seq

	    # Update job_submit_seq
	    self._job_submit_seq = self._job_submit_seq + 1

	# Update job counter if this job is in the SUBMIT state
	if job_state == "SUBMIT":
	    if jobid in self._job_counters:
		# Counter already exists for this job, just increate it by 1
		self._job_counters[jobid] = self._job_counters[jobid] + 1
	    else:
		# No counter for this job yet
		self._job_counters[jobid] = 0
	    # Now, we set the job output counter for this particular job
	    my_job._job_output_counter = self._job_counters[jobid]

        # Don't send a database event for the DAGMAN_SUBMIT state
        if job_state == "DAGMAN_SUBMIT":
            return my_job_submit_seq

	# All done!
	if self._db is not None:
	    # Send job event to database
	    self.db_send_job_info(my_job, job_state)

	return my_job_submit_seq

    def job_update_info(self, jobid, job_submit_seq, condor_id=None):
	"""
	This function adds info to an exising job.
	"""

	# Make sure job is already there
	if not (jobid, job_submit_seq) in self._jobs:
	    logger.warning("cannot find job: %s, %s" % (jobid, job_submit_seq))
	    return

	my_job = self._jobs[jobid, job_submit_seq]
	# Set condor_id
	my_job._condor_id = condor_id

	# Everything done
	return

    def update_job_state(self, jobid, job_submit_seq, job_state, status, walltime):
	"""
	This function updates a	job's state, and also writes
	a line in our output file.
	"""
	# Find job
	if job_submit_seq is None:
	    # Need to get job_submit_seq from our hash table
	    if jobid in self._jobs_map:
		job_submit_seq = self._jobs_map[jobid]
	if not (jobid, job_submit_seq) in self._jobs:
	    logger.warning("cannot find job: %s, %s" % (jobid, job_submit_seq))
	    return
	# Got it
	my_job = self._jobs[jobid, job_submit_seq]
	# Update job state
	my_job.set_job_state(job_state, self._current_timestamp, status)

        # Make status a string so we can print properly
        if status is not None:
            status = str(status)

        # Create content -- use one space only
        my_line = "%d %s %s %s %s %s %d" % (self._current_timestamp, jobid, job_state,
                                            status or my_job._condor_id or '-',
                                            my_job._site_name or '-',
                                            walltime or '-',
                                            job_submit_seq or '-')
        logger.info("new state %s" % (my_line))

        # Prepare for atomic append
        self._JSDB.write("%s\n" % (my_line))

	if self._db is None:
	    # Not using a database, nothing else to do!
	    return

	# Send jobstate event to database
	self.db_send_job_state(my_job)

	# Check if we need to send any job notifications to the
	# database that are not already done in add_job (mainjob
	# start, pre_script start) or in parse_job_output (mainjob
	# start and finish)
	if job_state == "POST_SCRIPT_STARTED":
	    # POST script started
	    self.db_send_job_note(my_job, "job.postscript.start")
	if job_state == "POST_SCRIPT_FAILURE" or job_state == "POST_SCRIPT_SUCCESS":
	    # POST script finished
	    self.db_send_job_note(my_job, "job.postscript.end")
	elif job_state == "PRE_SCRIPT_FAILURE" or job_state == "PRE_SCRIPT_SUCCESS":
	    # PRE script finished
	    self.db_send_job_note(my_job, "job.prescript.end")

	# Check if we need to send any tasks to the database
	if job_state == "POST_SCRIPT_FAILURE" or job_state == "POST_SCRIPT_SUCCESS":
	    # POST script finished
	    self.db_send_task_info(my_job, "POST SCRIPT", POSTSCRIPT_TASK_ID)
	elif job_state == "PRE_SCRIPT_FAILURE" or job_state == "PRE_SCRIPT_SUCCESS":
	    # PRE script finished
	    self.db_send_task_info(my_job, "PRE SCRIPT", PRESCRIPT_TASK_ID)
	elif job_state == "JOB_SUCCESS" or job_state == "JOB_FAILURE":
	    # Main job has ended
	    self.parse_job_output(my_job, job_state)

    def parse_job_sub_file(self, jobid, job_submit_seq):
	"""
	This function calls a function in the Job class to parse
	a job's submit file and extract planning information
	"""

	# Find job
	if not (jobid, job_submit_seq) in self._jobs:
	    logger.warning("cannot find job: %s, %s" % (jobid, job_submit_seq))
	    return None, None

	# Check if we have an entry for this job
	if not jobid in self._job_info:
	    return None, None

	# Make sure if we have a file for this entry (should always be there)
	if self._job_info[jobid][0] is None:
	    return None, None

	# Got everything
	my_job = self._jobs[jobid, job_submit_seq]

	# Parse sub file
	my_diff, my_site = my_job.parse_sub_file(self._current_timestamp, self._job_info[jobid][0])

	# All done
	return my_diff, my_site

def make_boolean(value):
    # purpose: convert an input string into something boolean
    # paramtr: $x (IN): a property value
    # returns: 0 (false) or 1 (true)
    my_val = str(value)
    if (my_val.lower() == 'true' or
	my_val.lower() == 'on' or
	my_val.lower() == 'yes' or
	my_val.isdigit() and int(value) > 0):
	return 1

    return 0

# Parse properties
props = properties.Properties()
props.new(properties.PARSE_ALL)
doplot = make_boolean(props.property("pegasus.tailstatd.show"))

fuse = int(props.property("pegasus.tailstatd.fuse") or 300)
if fuse < 60:
    fuse = 60

jsd = None				# location of jobstate.log file
nodaemon = 0				# foreground mode
logfile = None				# location of monitord.log file
millisleep = None			# emulated run mode delay
config = {}				# braindump database (textual file)
adjustment = 0				# time zone adjustment (@#~! Condor)

# Parse command line options
prog_usage = "usage: %s [options] workflow.dag.dagman.out" % (prog_base)
prog_desc = """Mandatory arguments: outfile is the log file produced by Condor DAGMan, usually ending in the suffix ".dag.dagman.out"."""

parser = optparse.OptionParser(usage=prog_usage, description=prog_desc)

parser.add_option("-d", "--debug", action = "store", type = "string", dest = "debug_level",
		  help = "set log level, possible values are INFO, DEBUG, WARNING, ERROR, CRITICAL. Default level is %s and dynamic adjustments via signals USR1 (incr) and USR2 (decr)"
		  % logging.getLevelName(debug_level))
parser.add_option("-a", "--adjust", action = "store", type = "int", dest = "adjustment",
		  help = "adjust for time zone differences by i seconds, default 0")
parser.add_option("-N", "--foreground", action = "store_const", const = 2, dest = "nodaemon",
		  help = "(Condor) don't daemonize %s; go through motions as if" % (prog_base))
parser.add_option("-n", "--no-daemon", action = "store_const", const = 1, dest = "nodaemon",
		  help = "(debug) don't daemonize %s; keep it in the foreground" % (prog_base))
parser.add_option("--show", action = "store_const", const = 1, dest = "doplot",
		  help = "create diagrams of workflow upon normal exit")
parser.add_option("--fuse", action = "store", type = "int", dest = "fuse",
		  help = "maximum wait for each plotting subscript, default %d s" % (fuse))
parser.add_option("-j", "--job", action = "store", type = "string", dest = "jsd",
		  help = "alternative job state file to write, default is %s in the workflow's directory"
		  % (utils.jobbase))
parser.add_option("-l", "--log", action = "store", type = "string", dest = "logfile",
		  help = "alternative %s log file, default is %s in the workflow's directory"
		  % (prog_base, logbase))
parser.add_option("-C", "--config", action = "append", type = "string", dest = "config_opts",
		  help = "k=v defines configurations instead of reading from braindump.txt. Required keys include %s. Suggested keys include %s"
		  % (brainkeys["required"], brainkeys["optional"]))
#parser.add_option("-D", "--database", action = "store_const", const = 1, dest = "use_db",
#		  help = "turn on database entries for work DB (this is the default mode and overrides the broker option below)")
parser.add_option("--nodatabase", action = "store_const", const = 0, dest = "use_db",
		  help = "turn off logging information to the database (this option must be given when using the NL api's option's below)")
parser.add_option("-S", "--sim", action = "store", type = "int", dest = "millisleep",
		  help = "Developer: simulate delays between reads by sleeping ms milliseconds")
parser.add_option("-r", "--replay", action = "store_const", const = 1, dest = "replay_mode",
		  help = "disables checking for DAGMan's pid while running %s" % (prog_base))
parser.add_option("-o", "--output-db", action = "store", type = "string", dest = "output_db",
		  help = "name of the database file to use, default is workflow basename in the submit directory (this should contain the complete connection string to the database, for example to use SQLite, it should be sqlite:///<path_to_file>)")
parser.add_option("-s", "--single-db", action = "store_const", const = 1, dest = "single_db",
		  help = "use a single db at the top level workflow")
parser.add_option("--db-stats", action = "store_const", const = "yes", dest = "db_stats",
                  help = "collect and print database stats at the end")
parser.add_option("-f", "--output-file", action = "store", type = "string", dest = "file_string",
		  help = "Turn on using NetLogger's log api and specifies the file to where log data (cannot be used together with the database)")
parser.add_option("-H", "--host", action = "store", type = "string", dest = "host_string",
                  help = "Turn on using NetLogger's log api and specifies the host:port to where send data (cannot be used together with the database, or the log to file option)")

# Re-insert our base name to avoid optparse confusion when printing error messages
# (options, args) = parser.parse_args(sys.argv[0:]) # Does not work 100%
sys.argv.insert(0, prog_base)
(options, args) = parser.parse_args()

# Copy command line options into our variables
if options.debug_level is not None:
    try:
        # Convert the user-specified debug level (string) into a logging level value
        debug_level = LOG_LEVELS.get(options.debug_level.lower(), debug_level)
        logger.setLevel(debug_level)
    except:
        logger.warning("could not set logging level to %s" % options.debug_level)
if options.adjustment is not None:
    adjustment = options.adjustment
if options.nodaemon is not None:
    nodaemon = options.nodaemon
if options.doplot is not None:
    doplot = options.doplot
if options.fuse is not None:
    fuse = options.fuse
if options.jsd is not None:
    jsd = options.jsd
if options.logfile is not None:
    logfile = options.logfile
if options.use_db is not None:
    use_db = options.use_db
if options.millisleep is not None:
    millisleep = options.millisleep
if options.replay_mode is not None:
    replay_mode = options.replay_mode
    # Replay mode always runs in foreground
    nodaemon = 1
if options.output_db is not None:
    output_db = options.output_db
if options.single_db is not None:
    single_db = options.single_db
if options.db_stats is not None:
    db_stats = options.db_stats
if options.file_string is not None or options.host_string is not None:
    if options.file_string is not None and options.host_string is not None:
        logger.warning("cannot log to both file and host/port, NL api logging disabled")
    else:
        if options.file_string is not None:
            use_nllogfile = 1
            nllog_string = options.file_string
        else:
            nllog_string = options.host_string
        # Only turn on the NL log mode if the database logging is turned off
        if use_db == 0:
            use_nllog = 1
        else:
            logger.warning("NL api logging disabled, in order to use the logging api, please also specify --nodatabase.")

# Walk through any config properties
if options.config_opts is not None:
    for prop in options.config_opts:
	prop = prop.strip()
	try:
	    k, v = prop.split("=", 1)
	except:
	    parser.print_help()
	    sys.exit(1)
	config[k] = v

# Sanity check
if fuse < 60:
    fuse = 60

# Remaining argument is .dag.dagman.out file
if len(args) != 1:
    parser.print_help()
    sys.exit(1)

out = args[0]

if not out.endswith(".dagman.out"):
    parser.print_help()
    sys.exit(1)

# Turn into absolute filename
out = os.path.abspath(out)

# Infer run directory
run = os.path.dirname(out)

# Use default monitord logfile if user hasn't specified another file
if not logfile:
    logfile = os.path.join(run, logbase)
logfile = os.path.abspath(logfile)

# To use NetLogger's command-line parser
# from netlogger.nllog import get_root_logger, OptionParser
# Import NetLogger DB loader
if use_db == 1:
    try:
	from netlogger.analysis.modules import stampede_loader
    except:
	logger.warning("cannot import NetLogger's stampede_loader library, disabling database output!")
	use_db = 0
# Import NetLogger nlapi for using the logging api
if use_nllog == 1:
    try:
        from netlogger import nlapi
    except:
        logger.warning("cannot import NetLogger's nlapi library, disabling the logging api output!")
        use_nllog = 0

#
# --- functions ---------------------------------------------------------------------------
#

def sendmsg(client_connection, msg):
    # purpose: send all data to socket connection, try several time if necessary
    # paramtr: client_connection(IN): socket connection to send data
    # paramtr: msg(IN): message to send
    # returns: None on error, 1 on success
    my_total_bytes_sent = 0

    while my_total_bytes_sent < len(msg):
	try:
	    my_bytes_sent = client_connection.send(msg[my_total_bytes_sent:])
	except:
	    logger.error("writing to socket!")
	    return None

	my_total_bytes_sent = my_total_bytes_sent + my_bytes_sent

    return 1

def systell(fh):
    # purpose: make things symmetric, have a systell for sysseek
    # paramtr: fh (IO): filehandle
    # returns: current file position
    os.lseek(fh, 0, os.SEEK_CUR)

def aggregate(site, stamp, pending):
    # purpose: aggregates pending information into raster intervals
    # paramtr: site(IN): run site
    # paramtr: stamp(IN): current timestamp
    # paramtr: pending(IN): pending record

    my_slot = int(stamp / 60)
    my_diff = abs(stamp - pending[0])

    # FIXME: Insert clean-up code here to remove any but last four slots

    # Check if keys are in dictionary
    if not site in waiting:
	waiting[site] = {}
    if not my_slot in waiting[site]:
	waiting[site][my_slot] = [0, 0]

    # Aggregate information
    waiting[site][my_slot][0] = waiting[site][my_slot][0] + 1
    waiting[site][my_slot][1] = waiting[site][my_slot][1] + my_diff

    my_n = waiting[site][my_slot][0]
    logger.debug("%s:%s %s / %d = %.3f" % (site, my_slot, my_diff, my_n, my_diff / my_n))

def add(wf, jobid, event, condor_id=None, status=None):
    # purpose: append atomically a line to the jobstate file
    # paramtr: wf(IN): workflow object for this operation
    # paramtr: jobid(IN): what job
    # paramtr: event(IN): new status of job
    # paramtr: condor_id(IN, OPT): condor id
    # paramtr: status(IN, OPT): exitcode
    # returns: Nothing

    my_site = None
    my_time = None
    my_job_submit_seq = None

    # Remove existing site info during replanning
    if event in unsubmitted_events:
	if jobid in job_site:
	    del job_site[jobid]
	if jobid in walltime:
	    del walltime[jobid]

    # Variables originally from submit file information
    if jobid in job_site:
	my_site = job_site[jobid]
    if jobid in walltime:
	my_time = walltime[jobid]

    # A PRE_SCRIPT_START event always means a new job
    if event == "PRE_SCRIPT_STARTED":
	# This is a new job, we need to add it to the workflow
	my_job_submit_seq = wf.add_job(jobid, event)

    # A SUBMIT event brings condor id and job type information (it can also be
    # a new job for us when there is no PRE_SCRIPT)
    if event == "SUBMIT":
	# Add job to our workflow (if not alredy there), will update condor_id in both cases
	my_job_submit_seq = wf.add_job(jobid, event, condor_id=condor_id)

	# Obtain planning information from the submit file when entering Condor,
	# Figure out how long the job _intends_ to run maximum
	my_time, my_site = wf.parse_job_sub_file(jobid, my_job_submit_seq)

	if my_site == "!!SITE!!":
	    my_site = None

	# If not None, convert into seconds
	if my_time is not None:
	    my_time = my_time * 60
            logger.info("job %s requests %d s walltime" % (jobid, my_time))
	    walltime[jobid] = my_time
	else:
            logger.info("job %s does not request a walltime" % (jobid))

	# Remember the run-site
	if my_site is not None:
            logger.info("job %s is planned for site %s" % (jobid, my_site))
	    job_site[jobid] = my_site
	else:
            logger.info("job %s does not have a site information!" % (jobid))

    # Get job_submit_seq if we don't already have it
    if my_job_submit_seq is None:
	my_job_submit_seq = wf.find_jobid(jobid)

    if my_job_submit_seq is None:
	logger.warning("cannot find job_submit_seq for job: %s" % (jobid))

    # A SUBMIT_FAILED event requires us to send a job mainjob start event
    if event == "SUBMIT_FAILED":
        if my_job_submit_seq is not None:
            if (jobid, my_job_submit_seq) in wf._jobs:
                my_job = wf._jobs[jobid, my_job_submit_seq]
                wf.db_send_job_info(my_job, "SUBMIT")
            else:
                logger.warning("cannot find job: %s, %s" % (jobid, my_job_submit_seq))

    # Make sure job has the updated state
    wf.update_job_state(jobid, my_job_submit_seq, event, status, my_time)
    jobstate[jobid] = [wf._current_timestamp, event, condor_id, my_time, my_site]

    # Remember when we changed into a pending state
    if event in pending_job_events:
	if not jobid in pending:
	    # Remember when -- and which Condor ID
	    pending[jobid] = [wf._current_timestamp, condor_id, my_time, my_site]
	else:
            logger.info("%s remains a pending event for %s" % (event, jobid))
    else:
	# Remember time spent in pending, if previous state was pending
	if jobid in jobstate:
	    if jobstate[jobid][1] in pending_job_events and jobid in pending:
		aggregate(my_site, wf_current_timestamp, pending[jobid])

	# Remove when transitioning into any other state
	if jobid in pending:
	    del pending[jobid]

    # Remember when we changed into a running state
    if event in running_job_events:
	if not jobid in running:
	    # Remember when -- and which Condor ID
	    running[jobid] = [wf._current_timestamp, condor_id, my_time, my_site]
	else:
            logger.info("%s remains a running event for %s" % (event, jobid))
    else:
	# Remove when transitioning into any other state
	if jobid in running:
	    del running[jobid]

    # NEW: maintain site statistics
    if my_site is not None:
	if event in pending_job_events:
	    my_state = 'P'
	elif event in running_job_events:
	    my_state = 'R'
	elif event == "POST_SCRIPT_SUCCESS":
	    my_state = 'S'
	elif event == "POST_SCRIPT_FAILURE":
	    my_state = 'F'
	else:
	    my_state = 'O'
    else:
	my_state = None

    if my_site in siteinfo and jobid in siteinfo[my_site]:
	my_old = siteinfo[my_site][jobid]
	if my_old != my_state:
	    # Job changed state
	    if my_old != 'S' and my_old != 'F':
		# Gauge
		if my_old in siteinfo[my_site]:
		    siteinfo[my_site][my_old] = siteinfo[my_site][my_old] - 1
		else:
		    # Should not have to do this!
		    siteinfo[my_site][my_old] = 0
	    if my_state in siteinfo[my_site]:
		siteinfo[my_site][my_state] = siteinfo[my_site][my_state] + 1
	    else:
		siteinfo[my_site][my_state] = 1
    else:
	if my_site is not None:
	    if not my_site in siteinfo:
		# First add first-level dictionary
		siteinfo[my_site] = {}
	    # Add new state
	    siteinfo[my_site][my_state] = 1

    if my_site is not None:
	if my_state == 'S':
	    siteinfo[my_site]["mtime_succ"] = wf._current_timestamp
	if my_state == 'F':
	    siteinfo[my_site]["mtime_fail"] = wf._current_timestamp
	siteinfo[my_site][jobid] = my_state
	siteinfo[my_site]["mtime"] = wf._current_timestamp

    return

def process(wf, log_line):
    # purpose: process a log line and look for the information we are interested in
    # paramtr: log_line(IN): the line

    # Keep track of line count
    wf._line = wf._line + 1

    # Make sure we have not already seen this line
    # This is used in the case of rescue dags, for skipping what we have already seen in the dagman.out file
    if wf._line <= wf._last_processed_line:
        return

    # Strip end spaces, tabs, and <cr> and/or <lf>
    log_line = log_line.rstrip()

    # Check log_line for timestamp at the beginning
    my_expr = re_parse_timestamp.search(log_line)

    if my_expr is not None:
        split_log_line = log_line.split(None, 3)
        if len(split_log_line) >= 3:
            logger.debug("debug: ## %d: %s" % (wf._line, split_log_line[2][:64]))
	
	# Found time stamp, let's assume valid log line
	curr_time = time.localtime()
	adj_time = list(curr_time)
        adj_time[1] = int(my_expr.group(1)) # Month
        adj_time[2] = int(my_expr.group(2)) # Day
        adj_time[3] = int(my_expr.group(5)) # Hours
        adj_time[4] = int(my_expr.group(6)) # Minutes
        adj_time[5] = int(my_expr.group(7)) # Seconds

        if my_expr.group(3) is not None:
            # New timestamp format
            adj_time[0] = int(my_expr.group(4)) + 2000 # Year

	wf._current_timestamp = time.mktime(adj_time) + adjustment

	# Search for more content
	if re_parse_event.search(log_line) is not None:
	    # Found Event
	    my_expr = re_parse_event.search(log_line)
	    # groups = jobid, event, condor_id
	    add(wf, my_expr.group(2), my_expr.group(1), condor_id=my_expr.group(3))
        elif re_parse_job_submit.search(log_line) is not None:
            # Found job submit event
            my_expr = re_parse_job_submit.search(log_line)
            wf._last_submitted_job = my_expr.group(1)
            wf.add_job(my_expr.group(1), "DAGMAN_SUBMIT")
        elif re_parse_job_submit_error.search(log_line) is not None:
            # Found job submit error event
            if wf._last_submitted_job is not None:
                add(wf, wf._last_submitted_job, "SUBMIT_FAILED")
            else:
                logger.warning("found submit error in dagman.out, but last job is not set")
	elif re_parse_script_running.search(log_line) is not None:
	    # Pre scripts are not regular Condor event
	    # Starting of scripts is not a regular Condor event
	    my_expr = re_parse_script_running.search(log_line)
	    # groups = script, jobid
	    add(wf, my_expr.group(2), "%s_SCRIPT_STARTED" % (my_expr.group(1).upper()))
	elif re_parse_script_done.search(log_line) is not None:
	    my_expr = re_parse_script_done.search(log_line)
	    # groups = script, jobid
	    my_script = my_expr.group(1).upper()
	    my_jobid = my_expr.group(2)
	    if re_parse_script_successful.search(log_line) is not None:
		# Remember success with artificial jobstate
		add(wf, my_jobid, "%s_SCRIPT_SUCCESS" % (my_script), status=0)
	    elif re_parse_script_failed.search(log_line) is not None:
		# Remember failure with artificial jobstate
		my_expr = re_parse_script_failed.search(log_line)
		# groups = exit code (error status)
		try:
		    my_exit_code = int(my_expr.group(1))
		except:
		    # Unable to convert exit code to integer -- should not happen
		    logger.warning("unable to convert exit code to integer!")
		    my_exit_code = 1
		add(wf, my_jobid, "%s_SCRIPT_FAILURE" % (my_script), status=my_exit_code)
	    else:
		# Ignore
		logger.warning("unknown pscript state: %s" % (log_line[-14:]))
	elif re_parse_job_failed.search(log_line) is not None:
	    # Job has failed
	    my_expr = re_parse_job_failed.search(log_line)
	    # groups = jobid, condorid, jobstatus
	    my_jobid = my_expr.group(1)
	    my_condorid = my_expr.group(2)
	    try:
		my_jobstatus = int(my_expr.group(3))
	    except:
		# Unable to convert exit code to integet -- should not happen
		logger.warning("unable to convert exit code to integer!")
		my_jobstatus = 1
	    # remember failure with artificial jobstate
	    add(wf, my_jobid, "JOB_FAILURE", condor_id=my_condorid, status=my_jobstatus)
	elif re_parse_job_successful.search(log_line) is not None:
	    # Job succeeded
	    my_expr = re_parse_job_successful.search(log_line)
	    my_jobid = my_expr.group(1)
	    my_condorid = my_expr.group(2)
	    # remember success with artificial jobstate
	    add(wf, my_jobid, "JOB_SUCCESS", condor_id=my_condorid, status=0)
	elif re_parse_dagman_finished.search(log_line) is not None:
	    # DAG finished -- done parsing
	    my_expr = re_parse_dagman_finished.search(log_line)
	    # groups = exit code
	    try:
		wf._terminate = int(my_expr.group(1))
	    except:
		# Cannot convert exit code to integer!
		logger.warning("cannot convert DAGMan's exit code to integer!")
		wf._terminate = 0
	    logger.info("DAGMan finished with exit code %s" % (wf._terminate))
	    # Send info to database
	    wf.change_wf_state("end")
            # Save all state to disk so that we can start again later
            wf.write_workflow_state()
        elif re_parse_dagman_condor_id.search(log_line) is not None:
            # DAGMan starting, capture its condor id
            my_expr = re_parse_dagman_condor_id.search(log_line)
            wf._dagman_condor_id = my_expr.group(1)
	elif re_parse_dagman_pid.search(log_line) is not None and not replay_mode:
	    # DAGMan's pid, but only set pid if not running in replay mode
            # (otherwise pid may belong to another process)
            my_expr = re_parse_dagman_pid.search(log_line)
            # groups = DAGMan's pid
            try:
                wf._dagman_pid = int(my_expr.group(1))
            except:
                logger.critical("cannot set pid: %s" % (my_expr.group(1)))
                sys.exit(42)
	    logger.info("DAGMan runs at pid %d" % (wf._dagman_pid))
        elif re_parse_dag_name.search(log_line) is not None:
            # Found the dag filename, read dag, and generate start event for the database
            my_expr = re_parse_dag_name.search(log_line)
            my_dag = my_expr.group(1)
            # Parse dag file
            logger.info("using dag %s" % (my_dag))
            wf.parse_dag_file(my_dag)
	    # Send the delayed workflow start event to database
	    wf.change_wf_state("start")
	elif re_parse_condor_version.search(log_line) is not None:
	    # Version of this logfile format
	    my_expr = re_parse_condor_version.search(log_line)
	    # groups = condor version, condor major
	    my_condor_version = my_expr.group(1)
	    my_condor_major = my_expr.group(2)
	    logger.info("Using DAGMan version %s" % (my_condor_version))
	elif (re_parse_condor_logfile.search(log_line) is not None or
	      wf._multiline_file_flag == True and re_parse_condor_logfile_insane.search(log_line) is not None):
	    # Condor common log file location, DAGMan 6.6
	    if re_parse_condor_logfile.search(log_line) is not None:
		my_expr = re_parse_condor_logfile.search(log_line)
	    else:
		my_expr = re_parse_condor_logfile_insane.search(log_line)
	    wf._condorlog = my_expr.group(1)
	    logger.info("Condor writes its logfile to %s" % (wf._condorlog))

	    # Make a symlink for NFS-secured files
	    my_log, my_base = out2log(wf._run_dir, wf._out_file)
	    if os.path.islink(my_log):
		logger.info("symlink %s already exists" % (my_log))
	    elif os.access(my_log, os.R_OK):
		logger.info("%s is a regular file, not touching" % (my_base))
	    else:
		logger.info("trying to create local symlink to common log")
		if os.access(wf._condorlog, os.R_OK) or not os.access(wf._condorlog, os.F_OK):
		    if os.access(my_log, os.R_OK):
			try:
			    os.rename(my_log, "%s.bak" % (my_log))
			except:
			    logger.warning("error renaming %s to %s.bak" % (my_log, my_log))
		    try:
			os.symlink(wf._condorlog, my_log)
		    except:
			logger.info("unable to symlink %s" % (wf._condorlog))
		    else:
			logger.info("symlink %s -> %s" % (wf._condorlog, my_log))
		else:
		    logger.info("%s exists but is not readable!" % (wf._condorlog))
	    # We only expect one of such files
	    wf._multiline_file_flag = False
	elif re_parse_multiline_files.search(log_line) is not None:
	    # Multiline user log files, DAGMan > 6.6
	    wf._multiline_file_flag = True

def server_socket(low, hi, bind_addr="127.0.0.1"):
    # purpose: create a local TCP server socket to listen to sitesel requests
    # paramtr: low (IN): minimum port from bind range
    # paramtr: hi (IN): maximum port from bind range
    # paramtr: bind_addr (IN): optional hostaddr_in to bind to , defaults to LOOPBACK
    # returns: open socket, or None on error

    # Create socket
    try:
	my_socket = socket.socket(socket.AF_INET,
				  socket.SOCK_STREAM,
				  socket.getprotobyname("tcp"))
    except:
        logger.critical("could not create socket!")
        sys.exit(42)

    # Set options
    try:
	my_socket.setsockopt(socket.SOL_SOCKET,
			     socket.SO_REUSEADDR,
			     1)
    except:
        logger.critical("setsockopt SO_REUSEADDR!")
        sys.exit(42)

    # Bind to a free port
    my_port = low
    for my_port in range(low, hi):
	try:
	    my_res = my_socket.bind((bind_addr, my_port))
	except:
	    # Go to next port
	    continue
	else:
	    break

    if my_port >= hi:
        logger.critical("no free port to bind to!")
        sys.exit(42)

    # Make server socket non-blocking to not have a race condition
    # when doing select() before accept() on a server socket
    try:
	my_socket.setblocking(0)
    except:
        logger.critical("setblocking!")
        sys.exit(42)

    # Start listener
    try:
	my_socket.listen(socket.SOMAXCONN)
    except:
        logger.critical("listen!")
        sys.exit(42)

    # Return socket
    return my_socket

def show_job(client_conn, jid, ref_array=None):
    # purpose: print job information onto the given socket descriptor
    # paramtr: client_conn(IN): socket connection to client
    # paramtr: jid(IN): job id from jobstate
    # paramtr: refarray(IN, OPT): array from jobstate
    # globals: jobstate(IN): jobstate dictionary
    # returns: None on error, 1 on success

    # Get refarray from jobstate if not provided
    if ref_array is None:
	if jid in jobstate:
	    ref_array = jobstate[jid]
	else:
	    return None

    my_line = "%s %u %s %s %s %s\r\n" % (jid, 
					 ref_array[0],
					 ref_array[1],
					 ref_array[2] or '-',
					 ref_array[3] or '-',
					 ref_array[4] or '-')

    return sendmsg(client_conn, my_line)

def service_request_job(client_conn, job=None):
    # purpose: service a request for a job status
    # paramtr: client_conn(IN): socket connection to client
    # paramtr: job(IN, OPT): Either an asterix "*" for all (default), or a specific job
    # returns: number of entries written

    my_count = 0

    if (job is None) or job == '*':
	# All jobs, this is an optimization over regexp match below
	for my_key in jobstate:
	    if show_job(client_conn, my_key, jobstate[my_key]) is None:
		break
	    my_count = my_count + 1
    elif job in jobstate:
	# Looking for a specific job
	if show_job(client_conn, job, jobstate[job]) is not None:
	    my_count = my_count + 1
    else:
	# Treat job as a regular expression
	for my_key in jobstate:
	    if re.search(job, my_key):
		# Match!
		if show_job(client_conn, my_key, jobstate[my_key]) is None:
		    break
		my_count = my_count + 1

    return my_count

def show_site(client_conn, site, site_values=None):
    # purpose: print site information onto the given socket descriptor
    # paramtr: client_conn(IN): socket connection to client
    # paramtr: site(IN): site id from siteinfo
    # paramtr: site_values(IN, OPT): array from siteinfo
    # globals: siteinfo(IN): site dictionary
    # globals: waiting(IN): history of P->R transitions
    # returns: None on error, 1 on success

    # Get site_values from jobstate if not provided
    if site_values is None:
	if site in siteinfo:
	    site_values = siteinfo[site]
	else:
	    return None

    if "mtime_fail" in site_values:
	my_mtime_fail = utils.isodate(site_values["mtime_fail"], False, True)
    else:
	my_mtime_fail = "null"

    if "mtime_succ" in site_values:
	my_mtime_succ = utils.isodate(site_values["mtime_succ"], False, True)
    else:
	my_mtime_succ = "null"

    if "P" in site_values:
	my_state_p = int(site_values["P"])
    else:
	my_state_p = 0
    if "R" in site_values:
	my_state_r = int(site_values["R"])
    else:
	my_state_r = 0
    if "O" in site_values:
	my_state_o = int(site_values["O"])
    else:
	my_state_o = 0
    if "S" in site_values:
	my_state_s = int(site_values["S"])
    else:
	my_state_s = 0
    if "F" in site_values:
	my_state_f = int(site_values["F"])
    else:
	my_state_f = 0

    my_line = "%-16s %-21s %4u %4u %4u %6u %6u %-20s %s\r\n" % (site,
								utils.isodate(site_values["mtime"], False, True),
								my_state_p,
								my_state_r,
								my_state_o,
								my_state_s,
								my_state_f,
								my_mtime_succ,
								my_mtime_fail)

    # Check if we output to a file or to a socket
    if type(client_conn).__name__ == "file":
	# Write information to a file
	client_conn.write(my_line)
    else:
	# Send information to client (type(client_conn).__name__ should be _socketobject)
	if sendmsg(client_conn, my_line) is None:
	    return None

    # No need to proceed if nothing in waiting
    if not site in waiting:
	return 1

    # Sort dictionary and create a list sorted by timestamp (reverse)
    my_sorted_vals = sorted(waiting[site].items(), reverse=True)

    # Sort dictionary by value (not key) and create a list, this is
    # not what we want to do here, as it will create a list sorted by
    # jobs
    # my_sorted_vals = sorted(waiting[site].items(), key=operator.itemgetter(1))

    for i in range(0, len(my_sorted_vals)):
	my_val = my_sorted_vals[i]
	if my_val[1][0] == 0:
	    my_number = my_val[1][1]
	else:
	    my_number = my_val[1][1] / my_val[1][0]
	my_line = "\t%-20s %lu %lu %.3f\r\n" % (utils.isodate(my_val[0] * 60, False, True),
						my_val[1][0],
						my_val[1][1],
						my_number)
	if type(client_conn).__name__ == "file":
	    # Write to file
	    client_conn.write(my_line)
	else:
	    # type(client_conn).__name__ should be _socketobject
	    if sendmsg(client_conn, my_line) is None:
		return None

    # All done!
    return 1

def service_request_site(client_conn, site=None):
    # purpose: service a request for a site status
    # paramtr: client_conn(IN): socket connection to client, or file object to output to a file
    # paramtr: site(IN, OPT): Either an asterix "*" for all (default), or a specific site
    # returns: number of entries written

    my_count = 0

    if (site is None) or site == '*':
	# All sites, this is an optimization over regexp match below
	for my_key in siteinfo:
	    if show_site(client_conn, my_key, siteinfo[my_key]) is None:
		break
	    my_count = my_count + 1
    elif site in siteinfo:
	# Looking for a specific site
	if show_site(client_conn, site, siteinfo[site]) is not None:
	    my_count = my_count + 1
    else:
	# Treat site as a regular expression
	for my_key in siteinfo:
	    if re.search(site, my_key):
		# Match!
		if show_site(client_conn, my_key, siteinfo[my_key]) is None:
		    break
		my_count = my_count + 1

    return my_count

def untaint(text):
    # purpose: do not trust anything we get from the internet
    # paramtr: text(IN): text to untaint
    # returns: cleaned text, without any "special" characters

    if text is None:
	return None

    my_text = re_clean_content.sub('', str(text))

    return my_text

jumptable = {'site': service_request_site,
	     'job': service_request_job}

def service_request(server):
    # purpose: accept an incoming connection and service its request
    # paramtr: server(IN): server socket with a pending connection request
    # returns: number of status lines, or None in case of error

    # First, we accept the connection
    try:
	my_conn, my_addr = server.accept()
    except:
	logger.error("accept!")
	return None

    my_count = 0
    logger.info("processing request from %s:%d" % (my_addr[0], my_addr[1]))

    # TODO: Can only handle 1 line up to 1024 bytes long, should fix this later
    # Read line fron socket
    while True:
	try:
	    my_buffer = my_conn.recv(1024)
	except socket.error, e:
	    if e[0] == 35:
		continue
	    else:
		logger.error("recv: %d:%s" % (e[0], e[1]))
		try:
		    # Close socket
		    my_conn.close()
		except:
		    pass
		return None
	else:
	    # Received line, leave loop
	    break
	    
    if my_buffer == '':
	# Nothing else to read
	try:
	    my_conn.close()
	except:
	    pass
	return my_count

    # Removed leading/trailing spaces/tabs, trailing \r\n
    my_buffer = my_buffer.strip()
    # Do not trust anything we get from the internet
    my_buffer = untaint(my_buffer)
    
    # Create list of tokens
    my_args = my_buffer.split()
    
    if len(my_args) < 3:
	# Clearly not enough information
	sendmsg(my_conn, "%s 204 No Content\r\n" % (speak))
	try:
	    my_conn.close()
	except:
	    pass
	return my_count

    # Read information we need
    my_method = my_args.pop(0)
    my_proto = my_args.pop()
    my_what = my_args.pop().lower()
    
    if my_proto != speak:
	# Illegal or unknown protocol
	sendmsg(my_conn, "%s 400 Bad request\r\n" % (speak))
    elif my_method.upper() != "GET":
	# Unsupported method
	sendmsg(my_conn, "%s 405 Method not allowed\r\n" % (speak))
    elif not my_what in jumptable:
	# Request item is not supported
	sendmsg(my_conn, "%s 501 Not implemented\r\n" % (speak))
    else:
	# OK
	sendmsg(my_conn, "%s 200 OK\r\n" % (speak))
	if len(my_args) > 0:
	    my_count = jumptable[my_what](my_conn, str(my_args.pop(0)))
	else:
	    my_count = jumptable[my_what](my_conn)

    try:
	my_conn.close()
    except:
	pass

    return my_count
	
def check_request(server, timeout=0):
    # purpose: check for a pending service request, and service it
    # paramtr: server(IN): server socket
    # paramtr: timeout(IN, OPT): timeout in seconds, defaults to 0
    # returns: return value of select on server socket

    my_input_list = [server]
    my_input_ready, my_output_ready, my_except_ready = select.select(my_input_list, [], [], timeout)

    if len(my_input_ready) == 1:
	service_request(server)

    return len(my_input_ready)

def sleepy(retries, server=None):
    # purpose: sleep or work on client requests
    # paramtr: retries (IN): number of retries we are in
    # paramtr: server (IN): server listening socket, may be None
    # returns: Nothing
    my_sleeptime = sleeptime(retries)

    if server is not None:
	# elaborate iterative (non-concurrent) internet daemon
	my_start = int(time.time())
	my_diff = 0
	while my_diff < my_sleeptime:
	    my_diff = int(time.time()) - my_start
	    if my_diff < my_sleeptime:
		check_request(server, my_sleeptime - my_diff)
	    else:
		check_request(server, 0)
    else:
	# no server, just sleep regularly
	if my_sleeptime is not None:
	    time.sleep(my_sleeptime)
	
def sleeptime(retries):
    # purpose: compute suggested sleep time as a function of retries
    # paramtr: $retries (IN): number of retries so far
    # returns: recommended sleep time
    if retries < 5:
	my_y = 1
    elif retries < 50:
	my_y = 5
    elif retries < 500:
	my_y = 30
    else:
	my_y = 60

    return my_y

def prog_sighup_handler(signum, frame):
    pass

def prog_sigint_handler(signum, frame):
    logger.info("graceful exit on signal %d" % (signum))
    # Close output files
    if wf is not None:
        wf.end_workflow(1)
    sys.exit(1)

def prog_sigusr1_handler(signum, frame):
    """
    This function increases the log level to the next one.
    """
    debug_level = INCREASE_LOG_LEVELS.get(debug_level, debug_level)
    logger.setLogLevel(debug_level)

def prog_sigusr2_handler(signum, frame):
    """
    This function decreases the log level to the previous one.
    """
    debug_level = DECREASE_LOG_LEVELS.get(debug_level, debug_level)
    logger.setLogLevel(debug_level)

#
# --- at exit handlers -------------------------------------------------------------------
#

def socket_exit_handler():
    if server is not None:
	server.close()
	try:
	    os.unlink(sockfn)
	except:
	    # Just be silent
	    pass

#
# --- main ------------------------------------------------------------------------------
#

# Turn into daemon process
if nodaemon == 0:
    utils.daemonize()
    # Open logfile as stdout
    try:
	sys.stdout = open(logfile, "w", 0)
    except:
	logger.critical("could not open %s!" % (logfile))
	sys.exit(1)
elif nodaemon == 2:
    utils.keep_foreground()
    # Open logfile as stdout
    try:
	sys.stdout = open(logfile, "w", 0)
    except:
	logger.critical("could not open %s!" % (logfile))
	sys.exit(1)
else:
    # Hack to make stdout unbuffered
    sys.stdout = os.fdopen(sys.stdout.fileno(), "w", 0)

# Close stdin
sys.stdin.close()
# dup stderr onto stdout
sys.stderr = sys.stdout

# Initialize database
if use_db == 1:
    # Figure out the database connection string
    if output_db is None:
	output_db = "sqlite:///" + out[:out.find(".dag.dagman.out")] + ".stampede.db"
    workdb = stampede_loader.Analyzer(output_db, perf=db_stats)
    atexit.register(finish_stampede_loader)

# Initialize the logging API
if use_nllog == 1:
#    rotate_log_file(os.path.join(run, output_db))
    # using nlapi to create log file or sending data to a host:port (and not use the loader)
    if use_nllogfile == 1:
        nllog_string = os.path.join(run, nllog_string)
    workdb = nlapi.Log(level=nlapi.Level.ALL, prefix="stampede.", logfile=nllog_string)

# Say hello
logger.info("starting [%s], using pid %d" % (revision, os.getpid()))
if millisleep is not None:
    logger.info("using simulation delay of %d ms" % (millisleep))

# Instantiate workflow class
wf = Workflow(run, out, config, database=workdb)

# Ignore dying shells
signal.signal(signal.SIGHUP, prog_sighup_handler)

# Die nicely when asked to (Ctrl+C, system shutdown)
signal.signal(signal.SIGINT, prog_sigint_handler)

# Permit dynamic changes of debug level
signal.signal(signal.SIGUSR1, prog_sigusr1_handler)
signal.signal(signal.SIGUSR2, prog_sigusr2_handler)

# No need to create server socket in replay mode
if not replay_mode:
    # Create server socket for communication with site selector
    sockfn = os.path.join(os.path.dirname(out), "monitord.sock")
    server = server_socket(49152, 65536)
    # Take care of closing socket when we exit
    atexit.register(socket_exit_handler)

    # Save our address so that site selectors know where to connect
    if server is not None:
	my_host, my_port = server.getsockname()
	try:
	    OUT = open(sockfn, "w")
	    OUT.write("%s %d\n" % (my_host, my_port))
	    OUT.close()
	except:
	    logger.warning("unable to write %s!" % (sockfn))

# For future reference
plus = ''
if "LD_LIBRARY_PATH" in os.environ:
    for my_path in os.environ["LD_LIBRARY_PATH"].split(':'):
	logger.info("env: LD_LIBRARY_PATH%s=%s" % (plus, my_path))
	plus = '+'

if "GLOBUS_TCP_PORT_RANGE" in os.environ:
    logger.info("env: GLOBUS_TCP_PORT_RANGE=%s" % (os.environ["GLOBUS_TCP_PORT_RANGE"]))
else:
    logger.info("env: GLOBUS_TCP_PORT_RANGE=")
if "GLOBUS_TCP_SOURCE_RANGE" in os.environ:
    logger.info("env: GLOBUS_TCP_SOURCE_RANGE=%s" % (os.environ["GLOBUS_TCP_SOURCE_RANGE"]))
else:
    logger.info("env: GLOBUS_TCP_SOURCE_RANGE=")
if "GLOBUS_LOCATION" in os.environ:
    logger.info("env: GLOBUS_LOCATION=%s" % (os.environ["GLOBUS_LOCATION"]))
else:
    logger.info("env: GLOBUS_LOCATION=")

# Now we wait for the .out file to appear
f_stat = None
n_retries = 0

# Test if dagman.out file is there in case we are running in replay_mode
if replay_mode:
    try:
	f_stat = os.stat(out)
    except:
	logger.critical("error: workflow not started, %s does not exist, exiting..." % (out))
        sys.exit(1)

# Start looking for the file
while True:
    try:
	f_stat = os.stat(out)
    except OSError, e:
	if errno.errorcode[e.errno] == 'ENOENT':
	    # File doesn't exist yet, keep looking
	    n_retries = n_retries + 1
	    if n_retries > 100:
		# We tried too long, just exit
		logger.critical("%s never made an appearance" % (out))
                sys.exit(42)
	    # Continue waiting
	    logger.info("waiting for out file, retry %d" % ( n_retries))
	    sleepy(n_retries, server)
	else:
	    # Another error
	    logger.critical("stat %s" % (out))
            sys.exit(42)
    except:
	logger.critical("stat %s" % (out))
        sys.exit(42)
    else:
	# Found file!
	break

# post condition: stat is a valid stat record from the dagman.out file

# open dagman.out file
try:
    DMOF = open(out, "r")
except:
    logger.critical("opening %s" % (out))
    sys.exit(42)

#
# --- main loop --------------------------------------------------------------------------
#

ml_buffer = ''
ml_rbuffer = ''
ml_retries = 0
ml_current = 0
ml_pos = 0

while True:
    # Say Hello
    logger.debug("wake up and smell the silicon")

    # Periodically check for service requests
    if server is not None:
	check_request(server)

    try:
	f_stat = os.stat(out)
    except:
	# stat error
	logger.critical("stat %s" % (out))
        sys.exit(42)

    # f_stat[6] is the file size
    if f_stat[6] == ml_current:
	# Death by natural causes
	if wf._terminate is not None:
	    break

	# Check if DAGMan is alive -- if we know where it lives
	if ml_retries > 10 and wf._dagman_pid > 0:
	    # Just send signal 0 to check if the pid is ours
	    try:
		os.kill(int(wf._dagman_pid), 0)
	    except:
		logger.critical("DAGMan is gone! Sudden death syndrome detected!")
		wf._terminate = 42
		break
	
	# No change, wait a while
	ml_retries = ml_retries + 1
	if ml_retries > 17280:
	    # Too long without change
	    logger.critical("too long without action, self-destructing")
	    break

	sleepy(ml_retries, server)
    elif f_stat[6] < ml_current:
	# Truncated file, booh!
	logger.critical("file truncated, time to exit")
	break
    elif f_stat[6] > ml_current:
	# We have something to read!
	try:
	    ml_rbuffer = DMOF.read(32768)
	except:
	    # Error while reading
	    logger.critical("while reading")
            sys.exit(42)
	if len(ml_rbuffer) == 0:
	    # Detected EOF
	    logger.critical("detected EOF, resetting position to %d" % (ml_current))
	    DMOF.seek(ml_current)
	else:
	    # Something in the read buffer, merge it with our buffer
	    ml_buffer = ml_buffer + ml_rbuffer
	    # Look for end of line
	    ml_pos = ml_buffer.find('\n')
	    while (ml_pos > 0):
		# Take out 1 line, and adjust buffer
		process(wf, ml_buffer[0:ml_pos])
		ml_buffer = ml_buffer[ml_pos+1:]
		ml_pos = ml_buffer.find('\n')

		if millisleep is not None:
		    if server is not None:
			check_request(server, millisleep / 1000.0)
		    else:
			time.sleep(millisleep / 1000.0)

	    ml_pos = DMOF.tell()
	    logger.info("processed chunk of %d byte" % (ml_pos - ml_current -len(ml_buffer)))
	    ml_current = ml_pos
	    ml_retries = 0

DMOF.close()

if not replay_mode:
    # Finish trailing connection requests
    while (check_request(server)):
	pass
    server.close()
    server = None
    try:
	os.unlink(sockfn)
    except:
	# Just be silent
	pass

    # Dump siteinfo when we are done
    try:
	SI = open(os.path.join(run, "sitedump.txt"), 'w')
    except:
	logger.warning("could not create file %s" % (os.path.join(run, "sitedump.txt")))
    else:
	service_request_site(SI)
	SI.close()
    
# Finish, and close output file
wf.end_workflow()

# Try to run the hurricane graphics
if doplot:
    pass
else:
    logger.info("skipping plots")

# done
logger.info("finishing, exit with %d" % (wf._terminate))
sys.exit(wf._terminate)

