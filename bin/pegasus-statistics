#!/usr/bin/env python

import os
import re
import sys
import logging
import optparse
import math
import tempfile

# Initialize logging object
logger = logging.getLogger()

import common
from Pegasus.tools import utils
from netlogger.analysis.workflow.stampede_statistics import StampedeStatistics
from datetime import timedelta

#regular expressions
re_parse_property = re.compile(r'([^:= \t]+)\s*[:=]?\s*(.*)')

#Global variables----
prog_base = os.path.split(sys.argv[0])[1]	# Name of this program

workflow_statistics_file_name ="workflow.txt";
job_statistics_file_name ="jobs.txt";
logical_transformation_statistics_file_name ="breakdown.txt";
output_dir=''
condor = 0				# pure condor run - no GRID_SUBMIT events

job_stats_col_name =['#Job','Site','Kickstart','Post' ,'DAGMan','CondorQTime','Resource','Runtime','Seqexec','Seqexec-Delay']
job_stats_col_size =[35,15,15,15,15,15,15,15,15,15]

worklow_status_col_name =["#" ,"Original","Succeeded","Failed","Unsubmitted","Unknown"]
worklow_status_col_size =[20,20,20,20,20,20]

worklow_statistics_col_name = ["#" ,"Original","Succeeded","Failed"]
worklow_statistics_col_size =[20,20,20,20]


class JobStatistics:
	def __init__(self):
		self.name ='-'
		self.site ='-'
		self.kickstart ='-'
		self.post ='-'
		self.dagman ='-'
		self.condor_delay ='-'
		self.resource ='-'
		self.runtime ='-'
		self.condorQlen ='-'
		self.seqexec='-'
		self.seqexec_delay ='-'
		
	def getFormattedJobStatistics(self):
		formatted_job_stats = [self.name]
		if self.site is None:
			formatted_job_stats.append('-')
		else:
			formatted_job_stats.append(self.site)
		if self.kickstart is None:
			formatted_job_stats.append('-')
		else:
			formatted_job_stats.append(self.kickstart)
		if self.post is None:
			formatted_job_stats.append('-')
		else:
			formatted_job_stats.append(self.post)
		
		if self.dagman is None:
			formatted_job_stats.append('-')
		else:
			formatted_job_stats.append(self.dagman)
		
		if self.condor_delay is None:
			formatted_job_stats.append('-')
		else:
			formatted_job_stats.append(self.condor_delay)
				
		if self.resource is None:
			formatted_job_stats.append('-')
		else:
			formatted_job_stats.append(self.resource)
		
		if self.runtime is None:
			formatted_job_stats.append('-')
		else:
			formatted_job_stats.append(self.runtime)
		
		if self.seqexec is None:
			formatted_job_stats.append('-')
		else:
			formatted_job_stats.append(self.seqexec)
			
		if self.seqexec_delay is None:
			formatted_job_stats.append('-')
		else:
			formatted_job_stats.append(self.seqexec_delay)
		return formatted_job_stats


def setup_logger(level_str):
	level_str = level_str.lower()
	if level_str == "debug":
		logger.setLevel(logging.DEBUG)
	if level_str == "warning":
		logger.setLevel(logging.WARNING)
	if level_str == "error":
		logger.setLevel(logging.ERROR)
	if level_str == "info":
		logger.setLevel(logging.INFO)
	return




def formatted_wf_summary_legends():
	formatted_wf_statistics_legend ="#legends \n"
	
	formatted_wf_statistics_legend +="""
Workflow status - Workflow status table contains the information about the planned jobs and task.
It shows the status of the last retry of job and task 
"""
	formatted_wf_statistics_legend +="""
Workflow statistics - Workflow statistics table contains the information about the  jobs and task 
actually executed during workflow run.
"""
	formatted_wf_statistics_legend +="""
Workflow wall time - The walltime from the start of the workflow execution to the end as
reported by the DAGMAN.In case of rescue dag the value is the cumulative
of all retries.
"""
	formatted_wf_statistics_legend += """
Workflow cumulative job wall time - The sum of the walltime of all jobs as reported by kickstart. In case of
job retries the value is the cumulative of all retries. For workflows
having sub workflow jobs (i.e SUBDAG and SUBDAX jobs), the walltime
value includes jobs from the sub workflows as well.
"""
	formatted_wf_statistics_legend += """
Cumulative job walltime as seen from submit side - The sum of the walltime of all jobs as reported by DAGMan. This is
similar to the regular cumulative job walltime, but includes job
management overhead and delays. In case of job retries the value is the
cumulative of all retries. For workflows having sub workflow jobs (i.e
SUBDAG and SUBDAX jobs), the walltime value includes jobs from the sub
workflows as well.
"""
	return formatted_wf_statistics_legend



def formatted_wf_status_legends():
	formatted_wf_statistics_legend ="#legends \n"
	
	formatted_wf_statistics_legend +="""
Workflow status - Workflow status table contains the information about the planned jobs and task.
It shows the status of the last retry of job and task 
"""
	formatted_wf_statistics_legend +="""
Workflow statistics - Workflow statistics table contains the information about the  jobs and task 
actually executed during workflow run..
"""
	formatted_wf_statistics_legend +="""
Workflow wall time - The walltime from the start of the workflow execution to the end as
reported by the DAGMAN.In case of rescue dag the value is the cumulative
of all retries.
"""
	formatted_wf_statistics_legend += """
Workflow cumulative job wall time - The sum of the walltime of all jobs as reported by kickstart. In case of
job retries the value is the cumulative of all retries. 
"""
	formatted_wf_statistics_legend += """
Cumulative job walltime as seen from submit side - The sum of the walltime of all jobs as reported by DAGMan. This is
similar to the regular cumulative job walltime, but includes job
management overhead and delays. In case of job retries the value is the
cumulative of all retries.
"""
	return formatted_wf_statistics_legend

def formatted_job_stats_legends():
	formatted_job_stats_legend="#legends \n"
	formatted_job_stats_legend +="#Job - the name of the job \n"
	formatted_job_stats_legend +="#Site - the site where the job ran \n"
	formatted_job_stats_legend +="#Kickstart - the actual duration of the job in seconds on the remote compute node \n"
	formatted_job_stats_legend +="#Post - the postscript time as reported by DAGMan \n"
	formatted_job_stats_legend +="#DAGMan - the time between the last parent job  of a job completes and the job gets submitted.For root jobs it is the time between the dagman start time and the time job gets submitted.\n"
	formatted_job_stats_legend +="#CondorQTime - the time between submission by DAGMan and the remote Grid submission. It is an estimate of the time spent in the condor q on the submit node \n"
	formatted_job_stats_legend +="#Resource - the time between the remote Grid submission and start of remote execution . It is an estimate of the time job spent in the remote queue \n"
	formatted_job_stats_legend +="#Runtime - the time spent on the resource as seen by Condor DAGMan . Is always >=kickstart \n"
	#formatted_job_stats_legend +="#CondorQLen - the number of outstanding jobs in the queue when this job was released \n"
	formatted_job_stats_legend +="#Seqexec -  the time taken for the completion of a clustered job\n"
	formatted_job_stats_legend +="#Seqexec-Delay - the time difference between the time for the completion of a clustered job and sum of all the individual tasks kickstart time\n"
	return formatted_job_stats_legend
	
def formatted_transformation_stats_legends():
	formatted_transformation_stats_legend="#legends \n"
	formatted_transformation_stats_legend +="#Transformation - name of the transformation.\n"
	formatted_transformation_stats_legend +="#Count - the number of times the transformation was executed. \n"
	formatted_transformation_stats_legend +="#Mean(sec.) - the mean of the transformation runtime. \n"
	formatted_transformation_stats_legend +="#Min(sec.) - the minimum transformation runtime value. \n"
	formatted_transformation_stats_legend +="#Max(sec.) - the maximum transformation runtime value. \n"
	formatted_transformation_stats_legend +="#Total(sec.) - the cumulative of transformation runtime. \n"
	#formatted_transformation_stats_legend +=("#Variance(sec.) - the variance of the transformation runtime. \n"
	return formatted_transformation_stats_legend
	

def write_to_file(file_path, mode, content):
	try:
		fh = open(file_path, mode)
		fh.write(content)
	except IOError:
		logger.error("Unable to write to file " + file_path)
		sys.exit(1)
	else:
		fh.close()

def format_seconds(duration):
	#TODO add millisecond precision
	sec = int(duration)
	formatted_duration = ''
	days = sec / 86400
	sec -= 86400*days
	hrs = sec / 3600
	sec -= 3600*hrs
	mins = sec / 60
	sec -= 60*mins
	if days >= 1:
		if days == 1:
			formatted_duration  += str(days) + ' day, '
		else:
			formatted_duration  += str(days) + ' days, '
	if hrs >=1:
		if hrs == 1:
			formatted_duration  += str(hrs) + ' hr. ' 
		else:
			formatted_duration  += str(hrs) + ' hrs. '
	if mins >=1:
		if mins == 1:
			formatted_duration  += str(mins) + ' min. '
		else:
			formatted_duration  += str(mins) + ' mins. '
	if sec >=1:
		if sec ==1:
			formatted_duration  += str(sec) + " sec."
		else:
			formatted_duration  += str(sec) + " secs."
	return formatted_duration
	
	
#----------print workflow details--------
"""
	Prints the workflow statistics information
	Param: the workflow reference
	"""
def print_workflow_details(output_db_url , wf_uuid):
	expanded_workflow_stats = StampedeStatistics(output_db_url)
 	expanded_workflow_stats.initialize(wf_uuid)
 	expanded_workflow_stats.set_job_filter('all')
 	
 	# print workflow statistics
	wf_uuid_list = [wf_uuid]
	desc_wf_uuid_list = expanded_workflow_stats.get_descendant_workflow_ids()
	for wf_det in desc_wf_uuid_list:
		wf_uuid_list.append(wf_det.wf_uuid)
	
	wf_stats_file = os.path.join(output_dir,  workflow_statistics_file_name)
	jobs_stats_file = os.path.join(output_dir,  job_statistics_file_name)
	transformation_stats_file = os.path.join(output_dir, logical_transformation_statistics_file_name)
	
	write_to_file(wf_stats_file, "w" , formatted_wf_status_legends())
	write_to_file(jobs_stats_file, "w" , formatted_job_stats_legends())
	write_to_file(transformation_stats_file, "w" , formatted_transformation_stats_legends())
	for sub_wf_uuid in wf_uuid_list:
		individual_workflow_stats = StampedeStatistics(output_db_url , False)
 		individual_workflow_stats.initialize(sub_wf_uuid)
 		individual_workflow_stats.set_job_filter('all')
 		content = print_individual_wf_job_stats(individual_workflow_stats , sub_wf_uuid)
		write_to_file(jobs_stats_file, "a" , content)
		content = print_individual_workflow_stats(individual_workflow_stats , sub_wf_uuid)
		write_to_file(wf_stats_file, "a" , content)
		individual_workflow_stats.set_job_filter('all')
		content = print_individual_wf_transformation_stats(individual_workflow_stats , sub_wf_uuid)
		write_to_file(transformation_stats_file, "a" , content)
	print "SUMMARY".center(100, '*')
	print formatted_wf_summary_legends()
	print_workflow_summary(expanded_workflow_stats)
	
	print "Workflow execution statistics     : "
	print wf_stats_file +"\n"
	print "Job statistics                    : "
	print jobs_stats_file +"\n"
	print "Logical transformation statistics : "
	print transformation_stats_file +"\n"
	print "".center(100, '*')
	return


def print_workflow_summary(workflow_stats):
	# status
	total_jobs = workflow_stats.get_total_jobs_status()
	total_succeeded_jobs = workflow_stats.get_total_succeeded_jobs_status()
	total_failed_jobs = workflow_stats.get_total_failed_jobs_status()
	total_unsubmitted_jobs = workflow_stats.get_total_unknown_jobs_status()
	total_unknown_jobs = total_jobs - (total_succeeded_jobs + total_failed_jobs + total_unsubmitted_jobs)
	total_tasks = workflow_stats.get_total_tasks_status()
	total_succeeded_tasks = workflow_stats.get_total_succeeded_tasks_status()
	total_failed_tasks = workflow_stats.get_total_failed_tasks_status()
	
	print "\nWorkflow Status "
	print "".center(20, '-')
	print print_row(worklow_status_col_name , worklow_status_col_size)
	content =["Jobs" , str(total_jobs), str(total_succeeded_jobs), str(total_failed_jobs), str(total_unsubmitted_jobs), str(total_unknown_jobs)]		
	print print_row(content, worklow_status_col_size)
	content = ["Tasks " , str(total_tasks).ljust(20) , str(total_succeeded_tasks), str(total_failed_tasks), str('NA'), str('NA')]
	print print_row(content, worklow_status_col_size)
	# statistics
	total_jobs = workflow_stats.get_total_jobs_statistics()
	total_succeeded_jobs = workflow_stats.get_total_succeeded_jobs_statistics()
	total_failed_jobs = workflow_stats.get_total_failed_jobs_statistics()
	total_tasks = workflow_stats.get_total_tasks_statistics()
	total_succeeded_tasks = workflow_stats.get_total_succeeded_tasks_statistics()
	total_failed_tasks = workflow_stats.get_total_failed_tasks_statistics()
	
	print "\nWorkflow Statistics "
	print "".center(20, '-')
	print print_row(worklow_statistics_col_name , worklow_statistics_col_size)
	content = ["Jobs  " , str(total_jobs), str(total_succeeded_jobs), str(total_failed_jobs)]
	print  print_row(content, worklow_statistics_col_size)
	content = ["Tasks " , str(total_tasks), str(total_succeeded_tasks), str(total_failed_tasks)]
	print  print_row(content, worklow_statistics_col_size)
	workflow_wall_time = workflow_stats.get_workflow_wall_time()
	print "\nWorkflow wall time                               : " + format_seconds(workflow_wall_time[0].duration) +"\n"
	print "Workflow cumulative job wall time                : " + format_seconds(workflow_stats.get_workflow_cum_job_wall_time()) +"\n"
	print "Cumulative job walltime as seen from submit side : " + format_seconds(workflow_stats.get_submit_side_job_wall_time()) +"\n"
	


def print_row(content , format):
	row_str =""
	for index in range(len(content)):
		row_str += (content[index].ljust(format[index]))
	return row_str
	


def print_individual_workflow_stats(workflow_stats , wf_uuid):
	
	# workflow status
	workflow_status_table_header_str = print_row(worklow_status_col_name, worklow_status_col_size)
	
	# non sub wf
	workflow_stats.set_job_filter('nonsub')
	total_jobs = workflow_stats.get_total_jobs_status()
	total_succeeded_jobs = workflow_stats.get_total_succeeded_jobs_status()
	total_failed_jobs = workflow_stats.get_total_failed_jobs_status()
	total_unsubmitted_jobs = workflow_stats.get_total_unknown_jobs_status()
	total_unknown_jobs = total_jobs - (total_succeeded_jobs + total_failed_jobs + total_unsubmitted_jobs)
	content = ["Jobs",str(total_jobs),str(total_succeeded_jobs), str(total_failed_jobs) , str(total_unsubmitted_jobs) ,str(total_unknown_jobs)]
	jobs_status_str = print_row(content,worklow_status_col_size)
	
	# dax jobs
	workflow_stats.set_job_filter('dax')
	total_jobs = workflow_stats.get_total_jobs_status()
	total_succeeded_jobs = workflow_stats.get_total_succeeded_jobs_status()
	total_failed_jobs = workflow_stats.get_total_failed_jobs_status()
	total_unsubmitted_jobs = workflow_stats.get_total_unknown_jobs_status()
	total_unknown_jobs = total_jobs - (total_succeeded_jobs + total_failed_jobs + total_unsubmitted_jobs)
	content =["SUB DAX",str(total_jobs),str(total_succeeded_jobs), str(total_failed_jobs) , str(total_unsubmitted_jobs) ,str(total_unknown_jobs)]
	subdax_status_str = print_row(content,worklow_status_col_size) 
	
	# dag jobs
	workflow_stats.set_job_filter('dag')
	total_jobs = workflow_stats.get_total_jobs_status()
	total_succeeded_jobs = workflow_stats.get_total_succeeded_jobs_status()
	total_failed_jobs = workflow_stats.get_total_failed_jobs_status()
	total_unsubmitted_jobs = workflow_stats.get_total_unknown_jobs_status()
	total_unknown_jobs = total_jobs - (total_succeeded_jobs + total_failed_jobs + total_unsubmitted_jobs)
	content =["SUB DAG",str(total_jobs),str(total_succeeded_jobs), str(total_failed_jobs) , str(total_unsubmitted_jobs) ,str(total_unknown_jobs)]
	subdag_status_str = print_row(content,worklow_status_col_size) 
	
	
	#tasks
	workflow_stats.set_job_filter('all')
	total_tasks = workflow_stats.get_total_tasks_status()
	total_succeeded_tasks = workflow_stats.get_total_succeeded_tasks_status()
	total_failed_tasks = workflow_stats.get_total_failed_tasks_status()
	content =["Tasks", str(total_tasks) , str(total_succeeded_tasks) , str(total_failed_tasks), str('NA'), str('NA')]
	tasks_status_str =  print_row(content,worklow_status_col_size)
	
	wf_stats_str =  "\n\n#" +wf_uuid +"\n" 
	wf_stats_str +=  "\nWorkflow Status " + "\n"
	wf_stats_str += "".center(20, '-') +"\n"
	wf_stats_str += workflow_status_table_header_str +"\n"
	wf_stats_str += jobs_status_str +"\n"
	wf_stats_str += subdax_status_str +"\n"
	wf_stats_str += subdag_status_str +"\n"
	wf_stats_str += tasks_status_str +"\n"
	
	
	# workflow statistics
	workflow_statistics_table_header_str = print_row(worklow_statistics_col_name,worklow_statistics_col_size )
	
	# non sub
	workflow_stats.set_job_filter('nonsub')
	total_jobs = workflow_stats.get_total_jobs_statistics()
	total_succeeded_jobs = workflow_stats.get_total_succeeded_jobs_statistics()
	total_failed_jobs = workflow_stats.get_total_failed_jobs_statistics()
	content =["Jobs" ,  str(total_jobs) , str(total_succeeded_jobs) , str(total_failed_jobs)]
	job_statistics_str    = print_row(content,worklow_statistics_col_size)
	
	# dax jobs
	workflow_stats.set_job_filter('dax')
	total_jobs = workflow_stats.get_total_jobs_statistics()
	total_succeeded_jobs = workflow_stats.get_total_succeeded_jobs_statistics()
	total_failed_jobs = workflow_stats.get_total_failed_jobs_statistics()
	content =["SUB DAX" ,  str(total_jobs) , str(total_succeeded_jobs) , str(total_failed_jobs)]
	subdax_statistics_str = print_row(content,worklow_statistics_col_size)
	
	# dag jobs
	workflow_stats.set_job_filter('dag')
	total_jobs = workflow_stats.get_total_jobs_statistics()
	total_succeeded_jobs = workflow_stats.get_total_succeeded_jobs_statistics()
	total_failed_jobs = workflow_stats.get_total_failed_jobs_statistics()
	content =["SUB DAG" ,  str(total_jobs) , str(total_succeeded_jobs) , str(total_failed_jobs)]
	subdag_statistics_str = print_row(content,worklow_statistics_col_size) 
	
	#tasks
	workflow_stats.set_job_filter('all')
	total_tasks = workflow_stats.get_total_tasks_statistics()
	total_succeeded_tasks = workflow_stats.get_total_succeeded_tasks_statistics()
	total_failed_tasks = workflow_stats.get_total_failed_tasks_statistics()
	content = ["Tasks" , str(total_tasks), str(total_succeeded_tasks), str(total_failed_tasks)]
	task_statistics_str   = print_row(content,worklow_statistics_col_size)
	
	wf_stats_str += "\nWorkflow Statistics " +"\n"
	wf_stats_str += "".center(20, '-') +"\n"				
	wf_stats_str +=  workflow_statistics_table_header_str +"\n"
	wf_stats_str +=  job_statistics_str +"\n"
	wf_stats_str +=  subdax_statistics_str +"\n"
	wf_stats_str +=  subdag_statistics_str +"\n"
	wf_stats_str +=  task_statistics_str +"\n"
	
	
	workflow_wall_time = workflow_stats.get_workflow_wall_time()
	wf_stats_str += "\nWorkflow wall time                                : " + format_seconds(workflow_wall_time[0].duration) +"\n"
	wf_stats_str += "Workflow cumulative job wall time :               : " + format_seconds(workflow_stats.get_workflow_cum_job_wall_time()) +"\n"
	wf_stats_str += "Cumulative job walltime as seen from submit side: : " + format_seconds(workflow_stats.get_submit_side_job_wall_time()) +"\n"
	return wf_stats_str
	
	
def print_individual_wf_job_stats(workflow_stats , wf_uuid):
	job_stats_dict={}
	job_status_str = "\n\n# " +  wf_uuid +"\n"
	for index in range(len(job_stats_col_name)):
			job_status_str += (job_stats_col_name[index].ljust(job_stats_col_size[index]))
	
	#Job
	for job in workflow_stats.get_job_name():
		job_stats = JobStatistics()
		job_stats.name = job.job_name
		job_stats_dict[job.job_id] = job_stats
	#Site
	for job in workflow_stats.get_job_site():
		job_stats = job_stats_dict[job.job_id]
		job_stats.site = job.sites
		
	# kickstart
	for job in workflow_stats.get_job_kickstart():
		job_stats = job_stats_dict[job.job_id]
		job_stats.kickstart = job.kickstart
	#Post
	for job in workflow_stats.get_post_time():
		job_stats = job_stats_dict[job.job_id]
		job_stats.post = job.postTime
	
	# runtime
	for job in workflow_stats.get_job_runtime():
		job_stats = job_stats_dict[job.job_id]
		job_stats.runtime = job.runtime
	
	#dagman
	for job in workflow_stats.get_dagman_delay():
		job_stats = job_stats_dict[job.job_id]
		job_stats.dagman = job.dagmanDelay
	
	# condorQTime
	for job in workflow_stats.get_condor_q_time():
		job_stats = job_stats_dict[job.job_id]
		job_stats.condor_delay = job.condorQTime
	
	#resource Delay
	for job in workflow_stats.get_resource_delay():
		job_stats = job_stats_dict[job.job_id]
		job_stats.resource = job.resourceTime
	
	#seqexec
	for job in workflow_stats.get_job_seqexec():
		job_stats = job_stats_dict[job.job_id]
		job_stats.seqexec = job.seqexec
	
	#seqexec-delay
	for job in workflow_stats.get_job_seqexec_delay():
		job_stats = job_stats_dict[job.job_id]
		job_stats.seqexec_delay = job.seqexec_delay
		
	# printing
	content_list = []
	# find the pretty print length
	#Add sort
	keys = job_stats_dict.keys()
	keys.sort()	
	for key in keys:
		job_stat = job_stats_dict[key]
		job_det =job_stat.getFormattedJobStatistics()
		index = 0
		for content in job_det:
			job_status_str += str(content).ljust(job_stats_col_size[index])
			index = index + 1
	return job_status_str

def print_individual_wf_transformation_stats(workflow_stats , wf_uuid):
	transformation_status_str = "\n\n# " +  wf_uuid +"\n"
	transformation_status_str += "Transformation".ljust(80) +"Count".ljust(20) +"Min".ljust(20)+"Max".ljust(20) +"Avg".ljust(20)+"Total".ljust(20)
	for transformation in workflow_stats.get_transformation_statistics():
		transformation_status_str += "\n"
		transformation_status_str += (transformation.transformation).ljust(80)
		transformation_status_str += str(transformation.count).ljust(20)
		transformation_status_str += str(transformation.min).ljust(20)
		transformation_status_str += str(transformation.max).ljust(20)
		transformation_status_str += str(transformation.avg).ljust(20)
		transformation_status_str += str(transformation.sum).ljust(20)
	return transformation_status_str
	
	
	
	
# ---------main----------------------------------------------------------------------------

# Configure command line option parser
prog_usage = prog_base +" [options] SUBMIT DIRECTORY" 
parser = optparse.OptionParser(usage=prog_usage)
parser.add_option("-o", "--output", action = "store", dest = "output_dir",
		help = "writes the output to given directory.")
parser.add_option("-c", "--condor", action = "store_const", const = 1, dest = "condor",
		help = "pure condor run - no GRID_SUBMIT events")
parser.add_option("-l", "--loglevel", action = "store", dest = "log_level",
		help = "Log level. Valid levels are: debug,info,warning,error, Default is info.")
# Parse command line options
(options, args) = parser.parse_args()

print prog_base +" : initializing..."
if len(args) < 1:
	parser.error("Please specify Submit Directory")
	sys.exit(1)

if len(args) > 1:
	parser.error("Invalid argument")
	sys.exit(1) 

submit_dir = os.path.abspath(args[0])

# Copy options from the command line parser
if options.condor is not None:
	condor = options.condor
if options.output_dir is not None:
	output_dir = options.output_dir
	if not os.path.isdir(output_dir):
		logger.warning("Output directory doesn't exists. Creating directory... ")
		try:
			os.mkdir(output_dir)
		except:
			logger.error("Unable to create output directory."+output_dir)
			sys.exit(1) 	
else :
	output_dir = tempfile.mkdtemp()
if options.log_level == None:
	options.log_level = "info"
setup_logger(options.log_level)	
#Getting values from braindump file
config = utils.slurp_braindb(submit_dir)
if not config:
	logger.warning("could not process braindump.txt ")
	sys.exit(1)
wf_uuid = ''
if (config.has_key('wf_uuid')):
	wf_uuid = config['wf_uuid']
else:
	logger.error("workflow id cannot be found in the braindump.txt ")
	sys.exit(1)

dag_file_name =''
if (config.has_key('dag')):
	dag_file_name = config['dag']
else:
	logger.error("Dag file name cannot be found in the braindump.txt ")
	sys.exit(1)	

# Create the sqllite db url
output_db_file =submit_dir +"/"+ dag_file_name[:dag_file_name.find(".dag")] + ".stampede.db"
output_db_url = "sqlite:///" + output_db_file
if os.path.isfile(output_db_file):
	print_workflow_details(output_db_url,wf_uuid )
	
else:
	logger.error("Unable to find database file in "+submit_dir)
	sys.exit(1)
sys.exit(0)

