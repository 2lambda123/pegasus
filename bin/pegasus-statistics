#!/usr/bin/env python

import os
import re
import sys
import logging
import optparse
import math
import tempfile

# Initialize logging object
logger = logging.getLogger()

import common
from Pegasus.tools import utils
from Pegasus.plots_stats import utils as stats_utils
from netlogger.analysis.workflow.stampede_statistics import StampedeStatistics
from datetime import timedelta

#regular expressions
re_parse_property = re.compile(r'([^:= \t]+)\s*[:=]?\s*(.*)')

#Global variables----
prog_base = os.path.split(sys.argv[0])[1]	# Name of this program

workflow_statistics_file_name ="workflow.txt";
job_statistics_file_name ="jobs.txt";
logical_transformation_statistics_file_name ="breakdown.txt";
calc_wf_stats = False
calc_wf_summary = False
calc_jb_stats = False
calc_tf_stats = False



transformation_stats_col_name =["Transformation","Count","Succeeded" , "Failed", "Min","Max","Mean","Total"]
transformation_stats_col_size =[80,12,12,12,20,20,20,12]

job_stats_col_name =['#Job','Try','Site','Kickstart','Post' ,'CondorQTime','Resource','Runtime','Seqexec','Seqexec-Delay']
job_stats_col_size =[60,4,15,12,12,12,12,12,12,12]

worklow_summary_col_name =["Type" ,"Succeeded","Failed","Unsubmitted" ,"Total" , " " ,"Retries" , "Total Run (Retries Included)"]
worklow_summary_col_size =[20,20,20,20,20,5,20,20]

worklow_status_col_name =["#","Type" ,"Succeeded","Failed","Unsubmitted" ,"Total" , " " ,"Retries" , "Total Run (Retries Included)" ,"Workflow Retries"]
worklow_status_col_size =[40,15,12,12,12,12,5,12,30,18]



class JobStatistics:
	def __init__(self):
		self.name = None
		self.site = None
		self.kickstart =None
		self.post = None
		self.condor_delay = None
		self.resource = None
		self.runtime = None
		self.condorQlen =None
		self.seqexec= None
		self.seqexec_delay = None
		self.retry_count = 0
	
	def getFormattedJobStatistics(self):
		"""
		Returns the formatted job statistics information  
		@return:    formatted job statistics information
		"""
		formatted_job_stats = [self.name]
		formatted_job_stats.append(str(self.retry_count))
		if self.site is None:
			formatted_job_stats.append('-')
		else:
			formatted_job_stats.append(self.site)
		formatted_job_stats.append(round_to_str(self.kickstart))
		formatted_job_stats.append(round_to_str(self.post))
		formatted_job_stats.append(round_to_str(self.condor_delay))
		formatted_job_stats.append(round_to_str(self.resource))
		formatted_job_stats.append(round_to_str(self.runtime))
		formatted_job_stats.append(round_to_str(self.seqexec))
		formatted_job_stats.append(round_to_str(self.seqexec_delay))
		return formatted_job_stats



def setup_logger(level_str):
	"""
	Sets the logging level  
	@param level_str:  logging level
	"""
	level_str = level_str.lower()
	if level_str == "debug":
		logger.setLevel(logging.DEBUG)
	if level_str == "warning":
		logger.setLevel(logging.WARNING)
	if level_str == "error":
		logger.setLevel(logging.ERROR)
	if level_str == "info":
		logger.setLevel(logging.INFO)
	return



def formatted_wf_summary_legends():
	"""
	Returns the workflow summary legend  
	@return :  workflow summary legend
	"""
	formatted_wf_statistics_legend ="#legends \n"
	
	formatted_wf_statistics_legend +="""
Workflow summary - Summary of the workflow execution. It shows total
		tasks/jobs/sub workflows run, how many succeeded/failed etc.
		In case of hierarchical workflow the calculation shows the 
		statistics across all the sub workflow.
"""
	formatted_wf_statistics_legend +="""
Workflow wall time - The walltime from the start of the workflow execution
		to the end as reported by the DAGMAN.In case of rescue dag the value
		is the cumulative of all retries.
"""
	formatted_wf_statistics_legend += """
Workflow cumulative job wall time - The sum of the walltime of all jobs as
		reported by kickstart. In case of job retries the value is the
		cumulative of all retries. For workflows having sub workflow jobs
		(i.e SUBDAG and SUBDAX jobs), the walltime value includes jobs from
		the sub workflows as well.
"""
	formatted_wf_statistics_legend += """
Cumulative job walltime as seen from submit side - The sum of the walltime of
		all jobs as reported by DAGMan. This is similar to the regular
		cumulative job walltime, but includes job management overhead and
		delays. In case of job retries the value is the cumulative of all
		retries. For workflows having sub workflow jobs (i.e SUBDAG and
		SUBDAX jobs), the walltime value includes jobs from the sub workflows
		as well.
"""
	return formatted_wf_statistics_legend



def formatted_wf_status_legends():
	"""
	Returns the workflow table legend
	@return :  workflow table legend
	"""
	formatted_wf_statistics_legend ="#legends \n"
	
	formatted_wf_statistics_legend +="""
Workflow summary - Summary of the workflow execution. It shows total
		tasks/jobs/sub workflows run, how many succeeded/failed etc.
		In case of hierarchical workflow the calculation shows the 
		statistics of each individual sub workflow.
"""
	return formatted_wf_statistics_legend


def formatted_job_stats_legends():
	"""
	Returns the job table legend 
	@return :  job table legend
	"""
	formatted_job_stats_legend="#legends \n"
	formatted_job_stats_legend +="#Job - the name of the job \n"
	formatted_job_stats_legend +="#Try - the number representing the job instance run count.  \n"
	formatted_job_stats_legend +="#Site - the site where the job ran \n"
	formatted_job_stats_legend +="#Kickstart - the actual duration of the job instance in seconds on the remote compute node \n"
	formatted_job_stats_legend +="#Post - the postscript time as reported by DAGMan \n"
	formatted_job_stats_legend +="""#CondorQTime - the time between submission by DAGMan and the remote Grid submission. 
	It is an estimate of the time spent in the condor q on the submit node \n"""
	formatted_job_stats_legend +="""#Resource - the time between the remote Grid submission and start of remote execution. 
	It is an estimate of the time job spent in the remote queue \n"""
	formatted_job_stats_legend +="""#Runtime - the time spent on the resource as seen by Condor DAGMan . 
	Is always >=kickstart \n"""
	formatted_job_stats_legend +="#Seqexec -  the time taken for the completion of a clustered job\n"
	formatted_job_stats_legend +="""#Seqexec-Delay - the time difference between the time for the completion of a clustered
		job and sum of all the individual tasks kickstart time\n"""
	return formatted_job_stats_legend

def formatted_transformation_stats_legends():
	"""
	Returns the transformation table legend
	@return :  transformation table legend
	"""	
	formatted_transformation_stats_legend="#legends \n"
	formatted_transformation_stats_legend +="#Transformation - name of the transformation.\n"
	formatted_transformation_stats_legend +="#Count - the number of times the invocations corresponding to the transformation was executed. \n"
	formatted_transformation_stats_legend +="#Succeeded - the count of the succeeded invocations corresponding to the transformation. \n"
	formatted_transformation_stats_legend +="#Failed - the count of the failed invocations corresponding to the transformation. \n"
	formatted_transformation_stats_legend +="#Mean(sec.) - the mean of the invocation runtime corresponding to the transformation. \n"
	formatted_transformation_stats_legend +="#Min(sec.) - the minimum invocation runtime value corresponding to the transformation. \n"
	formatted_transformation_stats_legend +="#Max(sec.) - the maximum invocation runtime value corresponding to the transformation. \n"
	formatted_transformation_stats_legend +="#Total(sec.) - the cumulative of invocation runtime corresponding to the transformation. \n"
	return formatted_transformation_stats_legend
	

def write_to_file(file_path, mode, content):
	"""
	Utility method for writing content to a given file
	@param file_path :  file path
	@param mode :   file writing mode 'a' append , 'w' write
	@param content :  content to write to file 
	"""
	try:
		fh = open(file_path, mode)
		fh.write(content)
	except IOError:
		logger.error("Unable to write to file " + file_path)
		sys.exit(1)
	else:
		fh.close()

def format_seconds(duration):
	"""
	Utility for converting time to a readable format
	@param duration :  time in seconds and miliseconds
	@return time in format day,hour, min,sec
	"""
	return stats_utils.format_seconds(duration)
	
	


def print_workflow_details(output_db_url , wf_uuid , output_dir):
	"""
	Prints the workflow statistics information of all workflows
	@param output_db_url :  time in seconds and miliseconds
	@param wf_uuid  : uuid of the top level workflow
	"""
	
	try:
		expanded_workflow_stats = StampedeStatistics(output_db_url)
		expanded_workflow_stats.initialize(wf_uuid)
 	except:
 		logger.error("Failed to load the database." + output_db_url )
		sys.exit(1)
 	
 	# print workflow statistics
	wf_uuid_list = [wf_uuid]
	desc_wf_uuid_list = expanded_workflow_stats.get_descendant_workflow_ids()
	for wf_det in desc_wf_uuid_list:
		wf_uuid_list.append(wf_det.wf_uuid)
	
	if calc_wf_stats:
		wf_stats_file = os.path.join(output_dir,  workflow_statistics_file_name)
		write_to_file(wf_stats_file, "w" , formatted_wf_status_legends())
		workflow_status_table_header_str = print_row(worklow_status_col_name, worklow_status_col_size)
		write_to_file(wf_stats_file, "a" , workflow_status_table_header_str)
	if calc_jb_stats:
		jobs_stats_file = os.path.join(output_dir,  job_statistics_file_name)
		write_to_file(jobs_stats_file, "w" , formatted_job_stats_legends())
	if calc_tf_stats:
		transformation_stats_file = os.path.join(output_dir, logical_transformation_statistics_file_name)
		write_to_file(transformation_stats_file, "w" , formatted_transformation_stats_legends())
	
	for sub_wf_uuid in wf_uuid_list:
		individual_workflow_stats = StampedeStatistics(output_db_url , False)
 		individual_workflow_stats.initialize(sub_wf_uuid)
 		
 		if calc_jb_stats:
 			individual_workflow_stats.set_job_filter('all')
 			content = print_individual_wf_job_stats(individual_workflow_stats , sub_wf_uuid)
 			write_to_file(jobs_stats_file, "a" , content)
		if calc_tf_stats:
			individual_workflow_stats.set_job_filter('all')
			content = print_wf_transformation_stats(individual_workflow_stats , sub_wf_uuid)
			write_to_file(transformation_stats_file, "a" , content)
		if calc_wf_stats:
			individual_workflow_stats.set_job_filter('all')
			content = print_individual_workflow_stats(individual_workflow_stats , sub_wf_uuid)
			write_to_file(wf_stats_file, "a" , content)
	print "SUMMARY".center(100, '*')
	if calc_wf_summary:	
		print formatted_wf_summary_legends()
		print_workflow_summary(expanded_workflow_stats)
	if calc_wf_stats:
		content = print_individual_workflow_stats(expanded_workflow_stats , "Total")
		write_to_file(wf_stats_file, "a" , content)
		print "Workflow execution statistics     : "
		print wf_stats_file +"\n"
	if calc_jb_stats:
		print "Job statistics                    : "
		print jobs_stats_file +"\n"
	if calc_tf_stats:
		expanded_workflow_stats.set_job_filter('all')
		content = print_wf_transformation_stats(expanded_workflow_stats , "All")
		write_to_file(transformation_stats_file, "a" , content)
		print "Logical transformation statistics : "
		print transformation_stats_file +"\n"
	print "".center(100, '*')
	return


def convert_to_str(value):
	"""
	Utility for returning a str representation of the given value.
	Return '-' if value is None
	@parem value : the given value that need to be converted to string
	"""
	if value is None:
		return '-'
	return str(value)
	



def print_workflow_summary(workflow_stats ):
	"""
	Prints the workflow statistics summary of an top level workflow
	@param workflow_stats :  workflow statistics object reference
	"""
	# status
	workflow_stats.set_job_filter('nonsub')
	# Tasks
	total_tasks = workflow_stats.get_total_tasks_status()
	total_succeeded_tasks = workflow_stats.get_total_succeeded_tasks_status()
	total_failed_tasks = workflow_stats.get_total_failed_tasks_status()
	total_unsubmitted_tasks = total_tasks -(total_succeeded_tasks + total_failed_tasks)
	total_task_retries =  workflow_stats.get_total_tasks_retries()
	total_invocations = total_succeeded_tasks + total_failed_tasks + total_task_retries
	# Jobs
	total_jobs = workflow_stats.get_total_jobs_status()
	total_succeeded_jobs = workflow_stats.get_total_succeeded_jobs_status()
	total_failed_jobs = workflow_stats.get_total_failed_jobs_status()
	total_unsubmitted_jobs = total_jobs - (total_succeeded_jobs + total_failed_jobs )
	total_job_retries = workflow_stats.get_total_jobs_retries()
	total_job_instance_retries =  total_succeeded_jobs + total_failed_jobs + total_job_retries
	# Sub workflows
	workflow_stats.set_job_filter('subwf')
	total_sub_wfs = workflow_stats.get_total_jobs_status()
	total_succeeded_sub_wfs = workflow_stats.get_total_succeeded_jobs_status()
	total_failed_sub_wfs = workflow_stats.get_total_failed_jobs_status()
	total_unsubmitted_sub_wfs = total_sub_wfs - (total_succeeded_sub_wfs + total_failed_sub_wfs)
	total_sub_wfs_retries = workflow_stats.get_total_jobs_retries()
	total_sub_wfs_tries =  total_succeeded_sub_wfs + total_failed_sub_wfs + total_sub_wfs_retries
	
	print "".center(sum(worklow_summary_col_size), '-')
	print print_row(worklow_summary_col_name , worklow_summary_col_size)
	
	content = ["Tasks " , convert_to_str(total_succeeded_tasks), convert_to_str(total_failed_tasks), convert_to_str(total_unsubmitted_tasks) ,convert_to_str(total_tasks),"||",convert_to_str(total_task_retries), convert_to_str(total_invocations) ]
	print print_row(content, worklow_summary_col_size)
	
	content =[ "Jobs" , convert_to_str(total_succeeded_jobs), convert_to_str(total_failed_jobs), convert_to_str(total_unsubmitted_jobs) , convert_to_str(total_jobs) ,"||",str(total_job_retries), convert_to_str(total_job_instance_retries) ]		
	print print_row(content, worklow_summary_col_size)
	
	content =[ "Sub Workflows" , convert_to_str(total_succeeded_sub_wfs), convert_to_str(total_failed_sub_wfs), convert_to_str(total_unsubmitted_sub_wfs) , convert_to_str(total_sub_wfs) ,"||",str(total_sub_wfs_retries), convert_to_str(total_sub_wfs_tries) ]		
	print print_row(content, worklow_summary_col_size)
	
	print "".center(sum(worklow_summary_col_size), '-')
	
	workflow_states_list = workflow_stats.get_workflow_states()
	workflow_wall_time = stats_utils.get_workflow_wall_time(workflow_states_list)
		
	print "\nWorkflow wall time                               : " + format_seconds(workflow_wall_time) +"\n"
	print "Workflow cumulative job wall time                : " + format_seconds(workflow_stats.get_workflow_cum_job_wall_time()) +"\n"
	print "Cumulative job walltime as seen from submit side : " + format_seconds(workflow_stats.get_submit_side_job_wall_time()) +"\n"
	

def print_row(content , format):
	"""
	Utility method for generating formatted row based on the format given
	@param content :  list of column values
	@param format  :  column_size of each columns
	"""
	row_str =""
	for index in range(len(content)):
		row_str += (content[index].ljust(format[index]))
	return row_str
	

def print_individual_workflow_stats(workflow_stats , title):
	"""
	Prints the workflow statistics of workflow
	@param workflow_stats :  workflow statistics object reference
	@param wf_uuid  : uuid of the workflow
	"""
	content_str =""
	# individual workflow status
	
	# workflow status
	workflow_stats.set_job_filter('all')
	total_wf_retries = workflow_stats.get_workflow_retries() 
	content = [title,"","","" , "", "" ,"||","", "" ,convert_to_str(total_wf_retries) ]
	wf_status_str = print_row(content, worklow_status_col_size)
			
	#tasks
	workflow_stats.set_job_filter('nonsub')
	total_tasks = workflow_stats.get_total_tasks_status()
	total_succeeded_tasks = workflow_stats.get_total_succeeded_tasks_status()
	total_failed_tasks = workflow_stats.get_total_failed_tasks_status()
	total_unsubmitted_tasks = total_tasks -(total_succeeded_tasks + total_failed_tasks )
	total_task_retries =  workflow_stats.get_total_tasks_retries()
	total_task_invocations = total_succeeded_tasks + total_failed_tasks + total_task_retries
	content =["","Tasks",  convert_to_str(total_succeeded_tasks) , convert_to_str(total_failed_tasks), convert_to_str(total_unsubmitted_tasks) , convert_to_str(total_tasks) ,"||",convert_to_str(total_task_retries), convert_to_str(total_task_invocations) ,""]
	tasks_status_str =  print_row(content,worklow_status_col_size)
	
	# job status
	workflow_stats.set_job_filter('nonsub')
	total_jobs = workflow_stats.get_total_jobs_status()
	total_succeeded_jobs = workflow_stats.get_total_succeeded_jobs_status()
	total_failed_jobs = workflow_stats.get_total_failed_jobs_status()
	total_unsubmitted_jobs = total_jobs - (total_succeeded_jobs + total_failed_jobs )
	total_job_retries = workflow_stats.get_total_jobs_retries()
	total_job_invocations =  total_succeeded_jobs + total_failed_jobs + total_job_retries
	content = ["","Jobs",convert_to_str(total_succeeded_jobs), convert_to_str(total_failed_jobs) , convert_to_str(total_unsubmitted_jobs), convert_to_str(total_jobs) ,"||",convert_to_str(total_job_retries), convert_to_str(total_job_invocations) ,"" ]
	jobs_status_str = print_row(content,worklow_status_col_size)
	
	# sub workflow
	workflow_stats.set_job_filter('subwf')
	total_sub_wfs = workflow_stats.get_total_jobs_status()
	total_succeeded_sub_wfs = workflow_stats.get_total_succeeded_jobs_status()
	total_failed_sub_wfs = workflow_stats.get_total_failed_jobs_status()
	total_unsubmitted_sub_wfs = total_sub_wfs - (total_succeeded_sub_wfs + total_failed_sub_wfs )
	total_sub_wfs_retries = workflow_stats.get_total_jobs_retries()
	total_sub_wfs_invocations =  total_succeeded_sub_wfs + total_failed_sub_wfs + total_sub_wfs_retries
	content = ["","Sub Workflows",convert_to_str(total_succeeded_sub_wfs), convert_to_str(total_failed_sub_wfs) , convert_to_str(total_unsubmitted_sub_wfs), convert_to_str(total_sub_wfs) ,"||",convert_to_str(total_sub_wfs_retries), convert_to_str(total_sub_wfs_invocations) ,"" ]
	sub_wf_status_str = print_row(content,worklow_status_col_size)
	
	
	content_str += "".center(sum(worklow_status_col_size), '-') +"\n"
	content_str += wf_status_str +"\n"
	content_str += tasks_status_str +"\n"
	content_str += jobs_status_str +"\n"
	content_str += sub_wf_status_str +"\n"
	
	return content_str
	

def print_individual_wf_job_stats(workflow_stats , wf_uuid):
	"""
	Prints the job statistics of workflow
	@param workflow_stats :  workflow statistics object reference
	@param wf_uuid  : uuid of the workflow
	"""	
	job_stats_dict={}
	job_stats_list=[]
	job_retry_count_dict={}
	job_status_str = "\n\n# " +  wf_uuid +"\n"
	for index in range(len(job_stats_col_name)):
			job_status_str += (job_stats_col_name[index].ljust(job_stats_col_size[index]))
	job_status_str +="\n"
			
	wf_job_stats_list =  workflow_stats.get_job_statistics()
	
	for job in wf_job_stats_list:
		job_stats = JobStatistics()
		job_stats.name = job.job_name
		job_stats.site = job.site
		job_stats.kickstart = job.kickstart
		job_stats.post = job.post_time
		job_stats.runtime = job.runtime
		job_stats.condor_delay = job.condor_q_time
		job_stats.resource = job.resource_delay
		job_stats.seqexec = job.seqexec
		if job_stats.seqexec is not None and job_stats.kickstart is not None:
			job_stats.seqexec_delay = (float(job_stats.seqexec) - float(job_stats.kickstart))
		if job_retry_count_dict.has_key(job.job_name):
			job_retry_count_dict[job.job_name] +=1
		else:
			job_retry_count_dict[job.job_name] = 1
		job_stats.retry_count = job_retry_count_dict[job.job_name]
		job_stats_list.append(job_stats)
	
	# printing
	content_list = []
	# find the pretty print length
	for job_stat in job_stats_list:
		job_det =job_stat.getFormattedJobStatistics()
		index = 0
		for content in job_det:
			job_status_str += str(content).ljust(job_stats_col_size[index])
			index = index + 1
	return job_status_str


"""
Utility method for rounding the float value to rounded string
@param value :  value to round 
@param to    :  how many decimal points to round to
"""
def round_to_str(value , to=3):
	rounded_value = '-'
	if value is None:
		return rounded_value
	rounded_value = str(round(float(value) , to))
	return rounded_value	
	
	

"""
Prints the transformation statistics of workflow
@param workflow_stats :  workflow statistics object reference
@param title  : title of the transformation statistics
"""	
def print_wf_transformation_stats(workflow_stats , title):
	transformation_status_str = "\n\n# " +  title +"\n"
	transformation_status_str += print_row(transformation_stats_col_name , transformation_stats_col_size)
	for transformation in workflow_stats.get_transformation_statistics():
		transformation_status_str += "\n"
		content = [transformation.transformation ,str(transformation.count),str(transformation.success) , str(transformation.failure), round_to_str(transformation.min),round_to_str(transformation.max),round_to_str(transformation.avg),round_to_str(transformation.sum)]
		transformation_status_str += print_row(content , transformation_stats_col_size)
	return transformation_status_str
	
"""
Sets the statistics level 
@param stats_level
"""
def set_statistics_level(stats_level):
	global calc_wf_stats
	global calc_wf_summary
	global calc_jb_stats
	global calc_tf_stats
	if stats_level =='all':
		calc_wf_stats = True
		calc_wf_summary = True
		calc_jb_stats = True
		calc_tf_stats = True
	elif stats_level =='summary':
		calc_wf_summary = True
	elif stats_level =='wf_stats':
		calc_wf_stats = True
	elif stats_level == 'jb_stats':
		calc_jb_stats = True
	else:
		calc_tf_stats = True
	
	
# ---------main----------------------------------------------------------------------------

def main():
	# Configure command line option parser
	prog_usage = prog_base +" [options] SUBMIT DIRECTORY" 
	parser = optparse.OptionParser(usage=prog_usage)
	parser.add_option("-o", "--output", action = "store", dest = "output_dir",
			help = "writes the output to given directory.")
	parser.add_option("-c","--conf", action = "store", type = "string", dest = "config_properties",
			  help = "specifies the properties file to use. This option overrides all other property files.")
	parser.add_option("-s", "--statisticslevel", action = "store", dest = "statistics_level",
			choices=['all', 'summary', 'wf_stats','jb_stats','tf_stats' ],
			help = "Valid levels are: all,summary,wf_stats,jb_stats,tf_stats Default is summary.")
	parser.add_option("-l", "--loglevel", action = "store", dest = "log_level",
			choices=['debug', 'info', 'warning','error' ],
			help = "Log level. Valid levels are: debug,info,warning,error, Default is info.")
	# Parse command line options
	(options, args) = parser.parse_args()
	
	print prog_base +" : initializing..."
	if len(args) < 1:
		parser.error("Please specify Submit Directory")
		sys.exit(1)
	
	if len(args) > 1:
		parser.error("Invalid argument")
		sys.exit(1) 
	
	submit_dir = os.path.abspath(args[0])
	
	# Copy options from the command line parser
	if options.statistics_level is not None:
		 statistics_level = options.statistics_level
	else:
		statistics_level =  'summary'
	set_statistics_level(statistics_level)
	global output_dir
	if options.output_dir is not None:
		output_dir = options.output_dir
		if not os.path.isdir(output_dir):
			logger.warning("Output directory doesn't exists. Creating directory... ")
			try:
				os.mkdir(output_dir)
			except:
				logger.error("Unable to create output directory."+output_dir)
				sys.exit(1) 	
	else :
		output_dir = tempfile.mkdtemp()
	if options.log_level == None:
		options.log_level = "info"
	setup_logger(options.log_level)
	output_db_url , wf_uuid = stats_utils.get_db_url_wf_uuid(submit_dir , options.config_properties)
	if output_db_url is not None:
		print_workflow_details(output_db_url,wf_uuid ,output_dir)
	sys.exit(0)

if __name__ == '__main__':
	main()

