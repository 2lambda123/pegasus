#!/usr/bin/env python

import os
import re
import sys
import logging
import optparse
import math
import tempfile
import traceback

# Initialize logging object
logger = logging.getLogger()

import common
from Pegasus.tools import utils
from Pegasus.plots_stats import utils as stats_utils
from netlogger.analysis.workflow.stampede_statistics import StampedeStatistics
from datetime import datetime

#regular expressions
re_parse_property = re.compile(r'([^:= \t]+)\s*[:=]?\s*(.*)')

#Global variables----
prog_base = os.path.split(sys.argv[0])[1]	# Name of this program


workflow_summary_file_name ="summary.txt";
workflow_statistics_file_name ="workflow.txt";
job_statistics_file_name ="jobs.txt";
logical_transformation_statistics_file_name ="breakdown.txt";
time_statistics_file_name ="time.txt"
calc_wf_stats = False
calc_wf_summary = False
calc_jb_stats = False
calc_tf_stats = False
calc_ti_stats = False
time_filter = None
NEW_LINE_STR ="\n"
DEFAULT_OUTPUT_DIR = "statistics"

transformation_stats_col_name =["Transformation","Count","Succeeded" , "Failed", "Min","Max","Mean","Total"]
transformation_stats_col_size =[60,12,12,12,20,20,20,12]

job_stats_col_name =['#Job','Try','Site','Kickstart','Post' ,'CondorQTime','Resource','Runtime','Seqexec','Seqexec-Delay']
job_stats_col_size =[60,4,15,12,12,12,12,12,12,12]

worklow_summary_col_name =["Type" ,"Succeeded","Failed","Incomplete" ,"Total" , " " ,"Retries" , "Total Run (Retries Included)"]
worklow_summary_col_size =[20,20,20,20,20,5,20,20]

worklow_status_col_name =["#","Type" ,"Succeeded","Failed","Incomplete" ,"Total" , " " ,"Retries" , "Total Run (Retries Included)" ,"Workflow Retries"]
worklow_status_col_size =[40,15,12,12,12,12,5,12,30,18]

time_stats_col_name =["Date","Count","Runtime"]
time_stats_col_size =[30,20,20]
time_host_stats_col_name =["Date","Host", "Count","Runtime (Sec.)"]
time_host_stats_col_size =[30,80,20,20]

class JobStatistics:
	def __init__(self):
		self.name = None
		self.site = None
		self.kickstart =None
		self.post = None
		self.condor_delay = None
		self.resource = None
		self.runtime = None
		self.condorQlen =None
		self.seqexec= None
		self.seqexec_delay = None
		self.retry_count = 0
	
	def getFormattedJobStatistics(self):
		"""
		Returns the formatted job statistics information  
		@return:    formatted job statistics information
		"""
		formatted_job_stats = [self.name]
		formatted_job_stats.append(str(self.retry_count))
		if self.site is None:
			formatted_job_stats.append('-')
		else:
			formatted_job_stats.append(self.site)
		formatted_job_stats.append(round_to_str(self.kickstart))
		formatted_job_stats.append(round_to_str(self.post))
		formatted_job_stats.append(round_to_str(self.condor_delay))
		formatted_job_stats.append(round_to_str(self.resource))
		formatted_job_stats.append(round_to_str(self.runtime))
		formatted_job_stats.append(round_to_str(self.seqexec))
		formatted_job_stats.append(round_to_str(self.seqexec_delay))
		return formatted_job_stats



def setup_logger(level_str):
	"""
	Sets the logging level  
	@param level_str:  logging level
	"""
	level_str = level_str.lower()
	if level_str == "debug":
		logger.setLevel(logging.DEBUG)
	if level_str == "warning":
		logger.setLevel(logging.WARNING)
	if level_str == "error":
		logger.setLevel(logging.ERROR)
	if level_str == "info":
		logger.setLevel(logging.INFO)
	return



def formatted_wf_summary_legends():
	"""
	Returns the workflow summary legend  
	@return :  workflow summary legend
	"""
	formatted_wf_statistics_legend ="#legends \n"
	
	formatted_wf_statistics_legend +="""
Workflow summary - Summary of the workflow execution. It shows total
		tasks/jobs/sub workflows run, how many succeeded/failed etc.
		In case of hierarchical workflow the calculation shows the 
		statistics across all the sub workflows.It shows the following 
		statistics about tasks, jobs and sub workflows.
		* Succeeded - total count of succeeded tasks/jobs/sub workflows.
		* Failed - total count of failed tasks/jobs/sub workflows.
		* Incomplete - total count of tasks/jobs/sub workflows that are 
		not in succeeded or failed state. This includes all the jobs 
		that are not submitted, submitted but not completed etc. This  
		is calculated as  difference between 'total' count and sum of 
		'succeeded' and 'failed' count.
		* Total - total count of tasks/jobs/sub workflows.
		* Retries - total retry count of tasks/jobs/sub workflows.
		* Total Run - total count of tasks/jobs/sub workflows executed 
		during workflow run. This is the cumulative of retries, 
		succeeded and failed count. 
"""
	formatted_wf_statistics_legend +="""
Workflow wall time - The walltime from the start of the workflow execution
		to the end as reported by the DAGMAN.In case of rescue dag the value
		is the cumulative of all retries.
"""
	formatted_wf_statistics_legend += """
Workflow cumulative job wall time - The sum of the walltime of all jobs as
		reported by kickstart. In case of job retries the value is the
		cumulative of all retries. For workflows having sub workflow jobs
		(i.e SUBDAG and SUBDAX jobs), the walltime value includes jobs from
		the sub workflows as well.
"""
	formatted_wf_statistics_legend += """
Cumulative job walltime as seen from submit side - The sum of the walltime of
		all jobs as reported by DAGMan. This is similar to the regular
		cumulative job walltime, but includes job management overhead and
		delays. In case of job retries the value is the cumulative of all
		retries. For workflows having sub workflow jobs (i.e SUBDAG and
		SUBDAX jobs), the walltime value includes jobs from the sub workflows
		as well.
"""
	return formatted_wf_statistics_legend



def formatted_wf_status_legends():
	"""
	Returns the workflow table legend
	@return :  workflow table legend
	"""
	formatted_wf_statistics_legend ="#legends \n"
	
	formatted_wf_statistics_legend +="""
Workflow summary - Summary of the workflow execution. It shows total
		tasks/jobs/sub workflows run, how many succeeded/failed etc.
		In case of hierarchical workflow the calculation shows the 
		statistics of each individual sub workflow.The file also 
		contains a 'Total' table at the bottom which is the cummulative 
		of all the individual statistics details.t shows the following 
		statistics about tasks, jobs and sub workflows.
		*Workflow Retries - number of times a workflow was retried.
		* Succeeded - total count of succeeded tasks/jobs/sub workflows.
		* Failed - total count of failed tasks/jobs/sub workflows.
		* Incomplete - total count of tasks/jobs/sub workflows that are 
		not in succeeded or failed state. This includes all the jobs 
		that are not submitted, submitted but not completed etc. This  
		is calculated as  difference between 'total' count and sum of 
		'succeeded' and 'failed' count.
		* Total - total count of tasks/jobs/sub workflows.
		* Retries - total retry count of tasks/jobs/sub workflows.
		* Total Run - total count of tasks/jobs/sub workflows executed 
		during workflow run. This is the cumulative of retries, 
		succeeded and failed count.
"""
	return formatted_wf_statistics_legend
	
def formatted_job_stats_legends():
	"""
	Returns the job table legend 
	@return :  job table legend
	"""
	formatted_job_stats_legend="#legends \n"
	formatted_job_stats_legend +="#Job - the name of the job \n"
	formatted_job_stats_legend +="#Try - the number representing the job instance run count.  \n"
	formatted_job_stats_legend +="#Site - the site where the job ran \n"
	formatted_job_stats_legend +="#Kickstart - the actual duration of the job instance in seconds on the remote compute node \n"
	formatted_job_stats_legend +="#Post - the postscript time as reported by DAGMan \n"
	formatted_job_stats_legend +="""#CondorQTime - the time between submission by DAGMan and the remote Grid submission. 
	It is an estimate of the time spent in the condor q on the submit node \n"""
	formatted_job_stats_legend +="""#Resource - the time between the remote Grid submission and start of remote execution. 
	It is an estimate of the time job spent in the remote queue \n"""
	formatted_job_stats_legend +="""#Runtime - the time spent on the resource as seen by Condor DAGMan . 
	Is always >=kickstart \n"""
	formatted_job_stats_legend +="#Seqexec -  the time taken for the completion of a clustered job\n"
	formatted_job_stats_legend +="""#Seqexec-Delay - the time difference between the time for the completion of a clustered
		job and sum of all the individual tasks kickstart time\n"""
	return formatted_job_stats_legend

def formatted_transformation_stats_legends():
	"""
	Returns the transformation table legend
	@return :  transformation table legend
	"""	
	formatted_transformation_stats_legend="#legends \n"
	formatted_transformation_stats_legend +="#Transformation - name of the transformation.\n"
	formatted_transformation_stats_legend +="#Count - the number of times the invocations corresponding to the transformation was executed. \n"
	formatted_transformation_stats_legend +="#Succeeded - the count of the succeeded invocations corresponding to the transformation. \n"
	formatted_transformation_stats_legend +="#Failed - the count of the failed invocations corresponding to the transformation. \n"
	formatted_transformation_stats_legend +="#Min(sec.) - the minimum invocation runtime value corresponding to the transformation. \n"
	formatted_transformation_stats_legend +="#Max(sec.) - the maximum invocation runtime value corresponding to the transformation. \n"
	formatted_transformation_stats_legend +="#Mean(sec.) - the mean of the invocation runtime corresponding to the transformation. \n"
	formatted_transformation_stats_legend +="#Total(sec.) - the cumulative of invocation runtime corresponding to the transformation. \n"
	return formatted_transformation_stats_legend


def formatted_time_stats_legends():
	"""
	Returns the time table legend
	@return :  time table legend
	"""	
	filter = str(time_filter)
	formatted_time_stats_legend="#legends" + NEW_LINE_STR
	formatted_time_stats_legend +="#Job instance statistics per " + filter + " : " +" the number of job instances run, total runtime sorted by " + filter+ NEW_LINE_STR
	formatted_time_stats_legend +="#Invocation statistics per " + filter + " : " +" the number of invocations , total runtime sorted by " + filter+ NEW_LINE_STR
	formatted_time_stats_legend +="#Job instance statistics by host per " + filter + " : " +" the number of job instance run, total runtime on each host sorted by " + filter+ NEW_LINE_STR
	formatted_time_stats_legend +="#Invocation by host per " + filter + " : " +" the number of invocations, total runtime on each host sorted by " + filter + NEW_LINE_STR
	
	return formatted_time_stats_legend	

def write_to_file(file_path, mode, content):
	"""
	Utility method for writing content to a given file
	@param file_path :  file path
	@param mode :   file writing mode 'a' append , 'w' write
	@param content :  content to write to file 
	"""
	try:
		fh = open(file_path, mode)
		fh.write(content)
	except IOError:
		logger.error("Unable to write to file " + file_path)
		sys.exit(1)
	else:
		fh.close()

def format_seconds(duration):
	"""
	Utility for converting time to a readable format
	@param duration :  time in seconds and miliseconds
	@return time in format day,hour, min,sec
	"""
	return stats_utils.format_seconds(duration)
	
	


def print_workflow_details(output_db_url , wf_uuid , output_dir):
	"""
	Prints the workflow statistics information of all workflows
	@param output_db_url :  time in seconds and miliseconds
	@param wf_uuid  : uuid of the top level workflow
	"""
	
	try:
		expanded_workflow_stats = StampedeStatistics(output_db_url)
		expanded_workflow_stats.initialize(wf_uuid)
 	except:
 		logger.error("Failed to load the database." + output_db_url )
 		logger.warning(traceback.format_exc())
		sys.exit(1)
 	
 	# print workflow statistics
	wf_uuid_list = [wf_uuid]
	desc_wf_uuid_list = expanded_workflow_stats.get_descendant_workflow_ids()
	for wf_det in desc_wf_uuid_list:
		wf_uuid_list.append(wf_det.wf_uuid)
	
	if calc_wf_stats:
		wf_stats_file = os.path.join(output_dir,  workflow_statistics_file_name)
		write_to_file(wf_stats_file, "w" , formatted_wf_status_legends())
		workflow_status_table_header_str = print_row(worklow_status_col_name, worklow_status_col_size)
		workflow_status_table_header_str +=NEW_LINE_STR
		write_to_file(wf_stats_file, "a" , workflow_status_table_header_str)
	if calc_jb_stats:
		jobs_stats_file = os.path.join(output_dir,  job_statistics_file_name)
		write_to_file(jobs_stats_file, "w" , formatted_job_stats_legends())
	if calc_tf_stats:
		transformation_stats_file = os.path.join(output_dir, logical_transformation_statistics_file_name)
		write_to_file(transformation_stats_file, "w" , formatted_transformation_stats_legends())
	if calc_ti_stats:
		time_stats_file = os.path.join(output_dir, time_statistics_file_name)
		write_to_file(time_stats_file, "w" , formatted_time_stats_legends())
		content = print_statistics_by_time_and_host(expanded_workflow_stats)
		write_to_file(time_stats_file, "a" , content)
		
	if calc_jb_stats or calc_tf_stats or calc_wf_stats:
		for sub_wf_uuid in wf_uuid_list:
			try:
				individual_workflow_stats = StampedeStatistics(output_db_url , False)
				individual_workflow_stats.initialize(sub_wf_uuid)
			except:
 				logger.error("Failed to load the database." + output_db_url )
 				logger.warning(traceback.format_exc())
				sys.exit(1)
			wf_det = individual_workflow_stats.get_workflow_details()[0]
			title =  str(sub_wf_uuid) + " (" + str(wf_det.dax_label) +")"
			logger.info("Generating statistics information about the workflow " + title +" ... ")
			if calc_jb_stats:
				logger.debug("Generating job instance statistics information about the workflow " + title +" ... ")
				individual_workflow_stats.set_job_filter('all')
				content = print_individual_wf_job_stats(individual_workflow_stats , title)
				write_to_file(jobs_stats_file, "a" , content)
			if calc_tf_stats:
				logger.debug("Generating invocation statistics information about the workflow " + title +" ... ")
				individual_workflow_stats.set_job_filter('all')
				content = print_wf_transformation_stats(individual_workflow_stats , title)
				write_to_file(transformation_stats_file, "a" , content)
			if calc_wf_stats:
				logger.debug("Generating workflow statistics information about the workflow " + title  +" ... ")
				individual_workflow_stats.set_job_filter('all')
				content = print_individual_workflow_stats(individual_workflow_stats , title)
				write_to_file(wf_stats_file, "a" , content)
			individual_workflow_stats.close()
	stats_output =  NEW_LINE_STR+ "SUMMARY".center(100, '*')
	stats_output +=  NEW_LINE_STR
	if calc_wf_summary:
		summary_output = formatted_wf_summary_legends()
		summary_output +=  NEW_LINE_STR
		logger.info("Generating workflow summary ... " )
		summary_output += print_workflow_summary(expanded_workflow_stats)
		wf_summary_file = os.path.join(output_dir,  workflow_summary_file_name)
		write_to_file(wf_summary_file, "w" , summary_output)
		stats_output += summary_output
		stats_output +=  NEW_LINE_STR
		stats_output += "Summary                           : "
		stats_output += wf_summary_file +"\n"
	if calc_wf_stats:
		stats_output +=  NEW_LINE_STR
		content = print_individual_workflow_stats(expanded_workflow_stats , "Total")
		write_to_file(wf_stats_file, "a" , content)
		stats_output += "Workflow execution statistics     : "
		stats_output += wf_stats_file +"\n"
	if calc_jb_stats:
		stats_output +=  NEW_LINE_STR
		stats_output += "Job instance statistics           : "
		stats_output += jobs_stats_file +"\n"
	if calc_tf_stats:
		stats_output +=  NEW_LINE_STR
		expanded_workflow_stats.set_job_filter('all')
		content = print_wf_transformation_stats(expanded_workflow_stats , "All")
		write_to_file(transformation_stats_file, "a" , content)
		stats_output += "Transformation statistics         : "
		stats_output += transformation_stats_file +"\n"
	if calc_ti_stats:
		stats_output +=  NEW_LINE_STR
		stats_output += "Time statistics                   : "
		stats_output += time_stats_file +"\n"
	expanded_workflow_stats.close()
	stats_output += NEW_LINE_STR
	stats_output += "".center(100, '*')
	print stats_output
	return


def convert_to_str(value):
	"""
	Utility for returning a str representation of the given value.
	Return '-' if value is None
	@parem value : the given value that need to be converted to string
	"""
	if value is None:
		return '-'
	return str(value)
	



def print_workflow_summary(workflow_stats ):
	"""
	Prints the workflow statistics summary of an top level workflow
	@param workflow_stats :  workflow statistics object reference
	"""
	# status
	workflow_stats.set_job_filter('nonsub')
	# Tasks
	total_tasks = workflow_stats.get_total_tasks_status()
	total_succeeded_tasks = workflow_stats.get_total_succeeded_tasks_status()
	total_failed_tasks = workflow_stats.get_total_failed_tasks_status()
	total_unsubmitted_tasks = total_tasks -(total_succeeded_tasks + total_failed_tasks)
	total_task_retries =  workflow_stats.get_total_tasks_retries()
	total_invocations = total_succeeded_tasks + total_failed_tasks + total_task_retries
	# Jobs
	total_jobs = workflow_stats.get_total_jobs_status()
	total_succeeded_jobs = workflow_stats.get_total_succeeded_jobs_status()
	total_failed_jobs = workflow_stats.get_total_failed_jobs_status()
	total_unsubmitted_jobs = total_jobs - (total_succeeded_jobs + total_failed_jobs )
	total_job_retries = workflow_stats.get_total_jobs_retries()
	total_job_instance_retries =  total_succeeded_jobs + total_failed_jobs + total_job_retries
	# Sub workflows
	workflow_stats.set_job_filter('subwf')
	total_sub_wfs = workflow_stats.get_total_jobs_status()
	total_succeeded_sub_wfs = workflow_stats.get_total_succeeded_jobs_status()
	total_failed_sub_wfs = workflow_stats.get_total_failed_jobs_status()
	total_unsubmitted_sub_wfs = total_sub_wfs - (total_succeeded_sub_wfs + total_failed_sub_wfs)
	total_sub_wfs_retries = workflow_stats.get_total_jobs_retries()
	total_sub_wfs_tries =  total_succeeded_sub_wfs + total_failed_sub_wfs + total_sub_wfs_retries
	
	summary_str = "".center(sum(worklow_summary_col_size), '-')
	summary_str += NEW_LINE_STR
	summary_str+= print_row(worklow_summary_col_name , worklow_summary_col_size)
	
	content = ["Tasks " , convert_to_str(total_succeeded_tasks), convert_to_str(total_failed_tasks), convert_to_str(total_unsubmitted_tasks) ,convert_to_str(total_tasks),"||",convert_to_str(total_task_retries), convert_to_str(total_invocations) ]
	summary_str += NEW_LINE_STR
	summary_str+= print_row(content, worklow_summary_col_size)
	
	content =[ "Jobs" , convert_to_str(total_succeeded_jobs), convert_to_str(total_failed_jobs), convert_to_str(total_unsubmitted_jobs) , convert_to_str(total_jobs) ,"||",str(total_job_retries), convert_to_str(total_job_instance_retries) ]
	summary_str += NEW_LINE_STR		
	summary_str+= print_row(content, worklow_summary_col_size)
	
	content =[ "Sub Workflows" , convert_to_str(total_succeeded_sub_wfs), convert_to_str(total_failed_sub_wfs), convert_to_str(total_unsubmitted_sub_wfs) , convert_to_str(total_sub_wfs) ,"||",str(total_sub_wfs_retries), convert_to_str(total_sub_wfs_tries) ]
	summary_str += NEW_LINE_STR		
	summary_str+= print_row(content, worklow_summary_col_size)
	summary_str += NEW_LINE_STR
	summary_str+= "".center(sum(worklow_summary_col_size), '-')
	summary_str += NEW_LINE_STR
	
	workflow_states_list = workflow_stats.get_workflow_states()
	workflow_wall_time = stats_utils.get_workflow_wall_time(workflow_states_list)
	summary_str += NEW_LINE_STR
	if workflow_wall_time is None:
		summary_str += "Workflow wall time                               : -\n"
	else:
		summary_str += "Workflow wall time                               : %-20s (total %d seconds)\n" % \
				(format_seconds(workflow_wall_time), (workflow_wall_time))
	summary_str += NEW_LINE_STR
	workflow_cum_job_wall_time = workflow_stats.get_workflow_cum_job_wall_time()
	if workflow_cum_job_wall_time is None:
		summary_str += "Workflow cumulative job wall time                : -\n"
	else:
		summary_str += "Workflow cumulative job wall time                : %-20s (total %d seconds)\n" % \
			(format_seconds(workflow_cum_job_wall_time),workflow_cum_job_wall_time)
	summary_str += NEW_LINE_STR	
	submit_side_job_wall_time =  workflow_stats.get_submit_side_job_wall_time()
	if submit_side_job_wall_time is None:
		summary_str += "Cumulative job walltime as seen from submit side : -\n"
	else:
		summary_str += "Cumulative job walltime as seen from submit side : %-20s (total %d seconds)\n" % \
			(format_seconds(submit_side_job_wall_time), submit_side_job_wall_time)
	return summary_str

def print_row(content , format):
	"""
	Utility method for generating formatted row based on the format given
	@param content :  list of column values
	@param format  :  column_size of each columns
	"""
	row_str =""
	for index in range(len(content)):
		row_str += (content[index].ljust(format[index]))
	return row_str
	

def print_individual_workflow_stats(workflow_stats , title):
	"""
	Prints the workflow statistics of workflow
	@param workflow_stats :  workflow statistics object reference
	@param title  : title of the workflow table
	"""
	content_str =""
	# individual workflow status
	
	# workflow status
	workflow_stats.set_job_filter('all')
	total_wf_retries = workflow_stats.get_workflow_retries() 
	content = [title,convert_to_str(total_wf_retries) ]
	retry_col_size =  worklow_status_col_size[len(worklow_status_col_size) -1]
	wf_status_str = print_row(content, [sum(worklow_status_col_size) -retry_col_size , retry_col_size])
			
	#tasks
	workflow_stats.set_job_filter('nonsub')
	total_tasks = workflow_stats.get_total_tasks_status()
	total_succeeded_tasks = workflow_stats.get_total_succeeded_tasks_status()
	total_failed_tasks = workflow_stats.get_total_failed_tasks_status()
	total_unsubmitted_tasks = total_tasks -(total_succeeded_tasks + total_failed_tasks )
	total_task_retries =  workflow_stats.get_total_tasks_retries()
	total_task_invocations = total_succeeded_tasks + total_failed_tasks + total_task_retries
	content =["","Tasks",  convert_to_str(total_succeeded_tasks) , convert_to_str(total_failed_tasks), convert_to_str(total_unsubmitted_tasks) , convert_to_str(total_tasks) ,"||",convert_to_str(total_task_retries), convert_to_str(total_task_invocations) ,""]
	tasks_status_str =  print_row(content,worklow_status_col_size)
	
	# job status
	workflow_stats.set_job_filter('nonsub')
	total_jobs = workflow_stats.get_total_jobs_status()
	total_succeeded_jobs = workflow_stats.get_total_succeeded_jobs_status()
	total_failed_jobs = workflow_stats.get_total_failed_jobs_status()
	total_unsubmitted_jobs = total_jobs - (total_succeeded_jobs + total_failed_jobs )
	total_job_retries = workflow_stats.get_total_jobs_retries()
	total_job_invocations =  total_succeeded_jobs + total_failed_jobs + total_job_retries
	content = ["","Jobs",convert_to_str(total_succeeded_jobs), convert_to_str(total_failed_jobs) , convert_to_str(total_unsubmitted_jobs), convert_to_str(total_jobs) ,"||",convert_to_str(total_job_retries), convert_to_str(total_job_invocations) ,"" ]
	jobs_status_str = print_row(content,worklow_status_col_size)
	
	# sub workflow
	workflow_stats.set_job_filter('subwf')
	total_sub_wfs = workflow_stats.get_total_jobs_status()
	total_succeeded_sub_wfs = workflow_stats.get_total_succeeded_jobs_status()
	total_failed_sub_wfs = workflow_stats.get_total_failed_jobs_status()
	total_unsubmitted_sub_wfs = total_sub_wfs - (total_succeeded_sub_wfs + total_failed_sub_wfs )
	total_sub_wfs_retries = workflow_stats.get_total_jobs_retries()
	total_sub_wfs_invocations =  total_succeeded_sub_wfs + total_failed_sub_wfs + total_sub_wfs_retries
	content = ["","Sub Workflows",convert_to_str(total_succeeded_sub_wfs), convert_to_str(total_failed_sub_wfs) , convert_to_str(total_unsubmitted_sub_wfs), convert_to_str(total_sub_wfs) ,"||",convert_to_str(total_sub_wfs_retries), convert_to_str(total_sub_wfs_invocations) ,"" ]
	sub_wf_status_str = print_row(content,worklow_status_col_size)
	
	
	content_str += "".center(sum(worklow_status_col_size), '-') +"\n"
	content_str += wf_status_str +"\n"
	content_str += tasks_status_str +"\n"
	content_str += jobs_status_str +"\n"
	content_str += sub_wf_status_str +"\n"
	
	return content_str
	

def print_individual_wf_job_stats(workflow_stats , title):
	"""
	Prints the job statistics of workflow
	@param workflow_stats :  workflow statistics object reference
	@param title  : title for the table
	"""	
	job_stats_dict={}
	job_stats_list=[]
	job_retry_count_dict={}
	job_status_str = "\n\n# " +  title +"\n"
	for index in range(len(job_stats_col_name)):
			job_status_str += (job_stats_col_name[index].ljust(job_stats_col_size[index]))
	job_status_str +="\n"
			
	wf_job_stats_list =  workflow_stats.get_job_statistics()
	
	for job in wf_job_stats_list:
		job_stats = JobStatistics()
		job_stats.name = job.job_name
		job_stats.site = job.site
		job_stats.kickstart = job.kickstart
		job_stats.post = job.post_time
		job_stats.runtime = job.runtime
		job_stats.condor_delay = job.condor_q_time
		job_stats.resource = job.resource_delay
		job_stats.seqexec = job.seqexec
		if job_stats.seqexec is not None and job_stats.kickstart is not None:
			job_stats.seqexec_delay = (float(job_stats.seqexec) - float(job_stats.kickstart))
		if job_retry_count_dict.has_key(job.job_name):
			job_retry_count_dict[job.job_name] +=1
		else:
			job_retry_count_dict[job.job_name] = 1
		job_stats.retry_count = job_retry_count_dict[job.job_name]
		job_stats_list.append(job_stats)
	
	# printing
	content_list = []
	# find the pretty print length
	for job_stat in job_stats_list:
		job_det =job_stat.getFormattedJobStatistics()
		index = 0
		for content in job_det:
			job_status_str += str(content).ljust(job_stats_col_size[index])
			index = index + 1
		job_status_str +=NEW_LINE_STR
	return job_status_str


def round_to_str(value , to=3):
	"""
Utility method for rounding the float value to rounded string
@param value :  value to round 
@param to    :  how many decimal points to round to
	"""
	return stats_utils.round_decimal_to_str(value,to)
	

def convert_to_date_format(value):
	"""
Utility method for converting the value to date format
@param value :  value to format 
	"""
	return stats_utils.convert_to_date_format(value, time_filter)
	
	

def print_wf_transformation_stats(workflow_stats , title):
	"""
Prints the transformation statistics of workflow
@param workflow_stats :  workflow statistics object reference
@param title  : title of the transformation statistics
	"""
	transformation_status_str = "\n\n# " +  title +"\n"
	transformation_status_str += print_row(transformation_stats_col_name , transformation_stats_col_size)
	for transformation in workflow_stats.get_transformation_statistics():
		transformation_status_str += "\n"
		content = [transformation.transformation ,str(transformation.count),str(transformation.success) , str(transformation.failure), round_to_str(transformation.min),round_to_str(transformation.max),round_to_str(transformation.avg),round_to_str(transformation.sum)]
		transformation_status_str += print_row(content , transformation_stats_col_size)
	return transformation_status_str
	

def print_statistics_by_time_and_host(workflow_stats ):
	"""
	Prints the job instance and invocation statistics sorted by time
	@param workflow_stats :  workflow statistics object reference
	"""
	statistics_by_time_str = NEW_LINE_STR
	workflow_stats.set_job_filter('all')
	workflow_stats.set_time_filter(time_filter)
	
	statistics_by_time_str +="#Job instances statistics per " + time_filter
	statistics_by_time_str += NEW_LINE_STR
	statistics_by_time_str +=print_row(time_stats_col_name , time_stats_col_size)
	statistics_by_time_str += NEW_LINE_STR
	stats_by_time = workflow_stats.get_jobs_run_by_time()
	for stats in stats_by_time:
		content = [convert_to_date_format(stats.date_format) ,str(stats.count),round_to_str(stats.total_runtime)]
		statistics_by_time_str += print_row(content , time_stats_col_size)
		statistics_by_time_str += NEW_LINE_STR
	
	statistics_by_time_str += NEW_LINE_STR	
	statistics_by_time_str +="#Invocation statistics run per " + time_filter
	statistics_by_time_str += NEW_LINE_STR
	statistics_by_time_str +=print_row(time_stats_col_name , time_stats_col_size)
	statistics_by_time_str += NEW_LINE_STR
	stats_by_time = workflow_stats.get_invocation_by_time()
	for stats in stats_by_time:
		content = [convert_to_date_format(stats.date_format) ,str(stats.count),round_to_str(stats.total_runtime)]
		statistics_by_time_str += print_row(content , time_stats_col_size)
		statistics_by_time_str += NEW_LINE_STR
	
	statistics_by_time_str += NEW_LINE_STR
	statistics_by_time_str +="#Job instances statistics on host per " + time_filter
	statistics_by_time_str += NEW_LINE_STR
	statistics_by_time_str +=print_row(time_host_stats_col_name , time_host_stats_col_size)
	statistics_by_time_str += NEW_LINE_STR
	stats_by_time = workflow_stats.get_jobs_run_by_time_per_host()
	for stats in stats_by_time:
		content = [convert_to_date_format(stats.date_format) ,str(stats.host_name) , str(stats.count),round_to_str(stats.total_runtime)]
		statistics_by_time_str += print_row(content , time_host_stats_col_size)
		statistics_by_time_str += NEW_LINE_STR
	
	statistics_by_time_str += NEW_LINE_STR
	statistics_by_time_str +="#Invocation statistics on host per " + time_filter
	statistics_by_time_str += NEW_LINE_STR
	statistics_by_time_str +=print_row(time_host_stats_col_name , time_host_stats_col_size)
	statistics_by_time_str += NEW_LINE_STR
	stats_by_time = workflow_stats.get_invocation_by_time_per_host()
	for stats in stats_by_time:
		content = [convert_to_date_format(stats.date_format) ,str(stats.host_name) , str(stats.count),round_to_str(stats.total_runtime)]
		statistics_by_time_str += print_row(content , time_host_stats_col_size)
		statistics_by_time_str += NEW_LINE_STR
	
	return statistics_by_time_str
	
"""
Sets the statistics level 
@param stats_level
"""
def set_statistics_level(stats_level):
	global calc_wf_stats
	global calc_wf_summary
	global calc_jb_stats
	global calc_tf_stats
	global calc_ti_stats
	if stats_level =='all':
		calc_wf_stats = True
		calc_wf_summary = True
		calc_jb_stats = True
		calc_tf_stats = True
		calc_ti_stats = True
	elif stats_level =='summary':
		calc_wf_summary = True
	elif stats_level =='wf_stats':
		calc_wf_stats = True
	elif stats_level == 'jb_stats':
		calc_jb_stats = True
	elif stats_level == 'tf_stats':
		calc_tf_stats = True
	else:
		calc_ti_stats = True
	
	
# ---------main----------------------------------------------------------------------------

def main():
	# Configure command line option parser
	prog_usage = prog_base +" [options] SUBMIT DIRECTORY" 
	parser = optparse.OptionParser(usage=prog_usage)
	parser.add_option("-o", "--output", action = "store", dest = "output_dir",
			help = "writes the output to given directory.")
	parser.add_option("-c","--conf", action = "store", type = "string", dest = "config_properties",
			  help = "specifies the properties file to use. This option overrides all other property files.")
	parser.add_option("-s", "--statistics_level", action = "store", dest = "statistics_level",
			choices=['all', 'summary', 'wf_stats','jb_stats','tf_stats' ,'ti_stats' ],
			help = "Valid levels are: all,summary,wf_stats,jb_stats,tf_stats , ti_stats; Default is summary.")
	parser.add_option("-t", "--time_filter", action = "store", dest = "time_filter",
			choices=['month', 'week', 'day','hour' ],
			help = "Valid levels are: month,week,day,hour; Default is day.")
	parser.add_option("-i", "--ignore_db_inconsistency", action = "store_const", const = 0, dest = "ignore_db_inconsistency",
                  help = "turn off the check for db consistency")
	parser.add_option("-v", "--verbose", action="count", default=0, dest="verbose", 
			help="Increase verbosity, repeatable")
	parser.add_option("-q", "--quiet", action="count", default=0, dest="quiet", 
			help="Decrease verbosity, repeatable")
	# Parse command line options
	(options, args) = parser.parse_args()
	
	if len(args) < 1:
		parser.error("Please specify Submit Directory")
		sys.exit(1)
	
	if len(args) > 1:
		parser.error("Invalid argument")
		sys.exit(1) 
	
	submit_dir = os.path.abspath(args[0])
	
	# Copy options from the command line parser
	
	# default is info
	log_level = 1 
	log_level_str = "info"
	log_level += (options.verbose - options.quiet)
	if log_level <=0:
		log_level_str = "error"
	elif log_level ==1:
		log_level_str = "warning"
	elif log_level ==2:
		log_level_str = "info"
	elif log_level >=3:
		log_level_str = "debug"
	setup_logger(log_level_str)
	logger.info(prog_base +" : initializing...")
	
	if options.ignore_db_inconsistency is None:
		if not utils.loading_completed(submit_dir):
			if utils.monitoring_running(submit_dir):
				logger.warning("pegasus-monitord still running. Please wait for it to complete. ")
			else:
				logger.warning("Please run pegasus monitord in replay mode. ")
			sys.exit(1)
	else:
		logger.warning("The tool is meant to be run after the completion of workflow run.")
				
	
	if options.statistics_level is not None:
		 statistics_level = options.statistics_level
	else:
		statistics_level =  'summary'
	set_statistics_level(statistics_level)
	
	global time_filter
	if options.time_filter is not None:
		 time_filter = options.time_filter
	else:
		time_filter =  'day'
	# Change the legend to show the time filter format
	time_stats_col_name[0]+= str(stats_utils.get_date_print_format(time_filter))
	time_host_stats_col_name[0] += str(stats_utils.get_date_print_format(time_filter))
	
	global output_dir
	if options.output_dir is not None:
		output_dir = options.output_dir
		if not os.path.isdir(output_dir):
			logger.warning("Output directory doesn't exists. Creating directory... ")
			try:
				os.mkdir(output_dir)
			except:
				logger.error("Unable to create output directory."+output_dir)
				sys.exit(1) 	
	else :
		output_dir = os.path.join(submit_dir, DEFAULT_OUTPUT_DIR)
		stats_utils.create_directory(output_dir, True)
		
	
	output_db_url , wf_uuid = stats_utils.get_db_url_wf_uuid(submit_dir , options.config_properties)
	if output_db_url is not None:
		print_workflow_details(output_db_url,wf_uuid ,output_dir)
	sys.exit(0)

if __name__ == '__main__':
	main()

