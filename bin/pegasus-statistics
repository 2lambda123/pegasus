#!/usr/bin/env perl
# author Prasanth Thomas
# propose: Runs statistics tools on pegasus workflow execution

use strict; # Perl pragma to restrict unsafe constructs
use FileHandle; #  Supply object methods for filehandles
use Getopt::Long qw(:config no_ignore_case bundling); #  Extended processing of command line options
use Term::ANSIColor qw(:constants colored); # Color screen output using ANSI escape sequences
$Term::ANSIColor::AUTORESET = 1;
use Data::Dumper; # help debug
use File::Basename;
use File::Path qw(rmtree);
use File::Copy;
use File::Spec;
use File::Temp qw/ tempfile tempdir  /;
use Cwd;

use lib dirname($0);
use common ':all'; 

# use local modules
use Work::Common;

#global variables
my $MAX_LOG_FILE = 1000;
my $job_state_file_name = 'jobstate.log';
my $brain_db_file_name ='braindump.txt';
my ($log_dir) = undef;
my($keep) = undef;
my ($base_submit_dir) = undef;
my ($base_path_len) =0;
# list of sub workflows
my (%sub_workflow_submit_hash) = ();
my (@workflow_statistics) =();
my $contains_dag_job = undef;
sub clean_exit();		
$SIG{'INT'} = \&clean_exit; 
$main::DEBUG =0;

END{
	remove_logs();
}

&main();


sub print_usage() {
	print "\n";
	print "Usage : pegasus-statistics <submit directory> \n";
	print GREEN "Optional :\n";
	print "\t -o|--output: write outputs in given directory\n";
	print "\t -m|--monitord: invokes pegasus-monitord before running pegasus-statistics\n";
	print "\t -c|--condor: specifies a pure condor run with no GRID_SUBMIT events\n";
	print "\t -k|--keep: keeps the logs of the pegasus-statistics run  \n";
	print "\t -v|--verbose: enter verbose mode \n";
	print "\t -h|--help: display this help message\n";
}

sub remove_logs(){
# purpose : removes logs if keep option is not set.
	if($log_dir){
		if(!$keep){
			rmtree($log_dir) or print "Unable to delete $log_dir . $! \n";
		}else{
			print "Logs are stored at $log_dir \n";
		}
	}
}

sub clean_exit(){
# purpose : makes a clean exit in case of interrupt event.
	exit(1);
}

sub main(){
	my ($help,$monitord, $condor)=(undef,undef,undef);
	my ($output_dir) =(undef);
	
	if(!GetOptions(
		"output|o=s"=>\$output_dir,
		"monitord|m"=>\$monitord,
		"condor|c"=>\$condor,
		"keep|k"=>\$keep,
		"verbose|v" => \$main::DEBUG,
		"help|h"=>\$help)){
			 print "Unable to parse the options .$!\n";
			 print_usage();
			 exit 1;
		}
		
		
	if ($help){
		print_usage();
		exit 0;
	}
	
	if(!$ARGV[0]){
		print "You need to provide submit directory. \n";
		print_usage();
		exit 1;
	}
	my ($submit_dir) = $ARGV[0];
	
	#sanity check first                                                                                                                            
	if( !defined $ENV{PEGASUS_HOME} ){
	    print STDERR "ERROR: PEGASUS_HOME is not set. Please source the setup-with-pegasus script.\n";
	    exit 1;
	}
	if($output_dir){
		$output_dir = File::Spec->rel2abs($output_dir);
	}
	($submit_dir) = File::Spec->rel2abs($submit_dir);
	$base_submit_dir = $submit_dir;
	my (%config) = slurp_braindb( $submit_dir ) or die "ERROR: open braindump.txt in directory $submit_dir: $!\n";
	# Gets the new and old submit directory value
	my ($bd_submit_dir,$bd_run_dir) = parse_run_dir_db_file(File::Spec->catfile( $submit_dir, $brain_db_file_name ));
	if(!$bd_submit_dir){
			$bd_submit_dir = $bd_run_dir;
			if(!$bd_submit_dir){
				print STDERR "ERROR: Unable to find the submit directory in the braindump.txt file.\n";
				exit 1;
			}
	}
	$base_path_len = length($bd_submit_dir);
	my($dag_file_name , $jsd_file_path) =  ($config{'dag'} ,$config{'jsd'});
	
	if(!$dag_file_name){
			print STDERR "ERROR: Unable to find the dag file name in the braindump.txt file.\n";
			exit 1;
	}
	if(!$jsd_file_path){
			print STDERR "ERROR: Unable to find the jobstate file path in the braindump.txt file.\n";
			exit 1;
	}
	$log_dir = tempdir('pegasus_statistics_XXXXXX' , TMPDIR=> 1) or die "Unable to create temp directory. $!";
	chmod(0755, $log_dir) or die "Couldn't chmod $log_dir: $!";
	if($monitord){
		my $job_state_backup = undef;
		my ($job_state_base_file)= File::Spec->catfile( $submit_dir, $job_state_file_name);
		if(-e "$job_state_base_file"){
			# Invoke only if jobstate.log is there
			$job_state_backup = setup_monitord($job_state_base_file);
		}
		my($dagman_out_file) = File::Spec->catfile( $submit_dir ,$dag_file_name .".dagman.out");
		my($monitord_status , $monitord_summary_msg)  = invoke_monitord($dagman_out_file,rlb($jsd_file_path) );
		if($monitord_status !=0 ){
			if($job_state_backup){
			# Revert back the jobstate file
				if(!move($job_state_backup ,$job_state_base_file)) {
					print STDERR "ERROR : Failed to create a revert back the jobstate file . $! \n";
				}
			}
			
			exit 1;
		}
		
	}
	
	if(!$output_dir){
		$output_dir =  File::Spec->catdir($submit_dir,"statistics");
		if (-d "$output_dir") {
			print STDERR "WARNING: Output directory $output_dir exists. Overwriting the contents ....\n";
			rmtree($output_dir) or die "Unable to delete $output_dir . $! \n";
		}
		mkdir ($output_dir) or die "Unable to create $output_dir . $! \n";
	}else{
		if (!(-d "$output_dir")) {
			print STDERR "WARNING: Output directory $output_dir doesn't exists. Creating the directory ....\n";
			mkdir ($output_dir) or die "Unable to create $output_dir . $! \n";
		}
	}
	
	my($dag_file_path) =  File::Spec->catfile("$submit_dir","$dag_file_name");
	print STDOUT "\n******  calculating workflow statistics for workflow : $submit_dir  ***** \n";
	my($workflow_statistics_status) = print_workflow_statistics($submit_dir,rlb($jsd_file_path),$dag_file_path,$condor);
	print STDOUT "******  Finished workflow statistics for workflow : $submit_dir  ***** \n\n";
	
	# Adding legends
	my ($genstats_cumulative_job_stats_path) =  File::Spec->catfile($output_dir ,'jobs.txt');
	open(JOBS_OUT,">$genstats_cumulative_job_stats_path") || die("Failed to open file $genstats_cumulative_job_stats_path");
	print JOBS_OUT "#Job - the name of the job. \n";
	print JOBS_OUT "#Num - the number of times JOB_TERMINATED event was seen. \n";
    print JOBS_OUT "#Site - the site where the job ran. \n";
    print JOBS_OUT "#Kickstart(sec.) - the actual duration of the job in seconds on the remote compute node. In case of retries the value is the cumulative of all retries. \n";
    print JOBS_OUT "#Post(sec.) - the postscript time as reported by DAGMan .In case of retries the value is the cumulative of all retries.\n";
    print JOBS_OUT "#DAGMan(sec.) - the time between the last parent job  of a job completes and the job gets submitted. In case of retries the value of the last retry is used for calculation.\n";
    print JOBS_OUT "#CondorQTime(sec.) - the time between submission by DAGMan and the remote Grid submission. It is an estimate of the time spent in the condor q on the submit node .In case of retries the value is the cumulative of all retries. \n";
    print JOBS_OUT "#Resource(sec.) - the time between the remote Grid submission and start of remote execution . It is an estimate of the time job spent in the remote queue .In case of retries the value is the cumulative of all retries. \n";
    print JOBS_OUT "#Runtime(sec.) - the time spent on the resource as seen by Condor DAGMan . Is always >=kickstart .In case of retries the value is the cumulative of all retries.\n";
    print JOBS_OUT "#Seqexec(sec.) -  the time taken for the completion of a clustered job .In case of retries the value is the cumulative of all retries.\n";
    print JOBS_OUT "#Seqexec-Delay(sec.) - the time difference between the time for the completion of a clustered job and sum of all the individual tasks kickstart time .In case of retries the value is the cumulative of all retries.\n";
	close(JOBS_OUT);
	print STDOUT "\n******  calculating job statistics for workflow : $submit_dir  ***** \n";
	my($genstats_status ,$genstats_summary_msg) =run_genstats($dag_file_path ,rlb($jsd_file_path),$output_dir ,$condor ,$submit_dir, undef);
	print STDOUT "******  Finished calculating job statistics for workflow : $submit_dir  ***** \n\n";
	## Executing genstats-breakdown command
	%sub_workflow_submit_hash = get_sub_workflow_list($dag_file_path);
	my ($number_of_elements) = scalar(keys %sub_workflow_submit_hash);
	my ($sub_wf_id ,$sub_submit_dir ) ;
	my ($genstats_error) = undef;
	my $workflow_events_dir  = undef;
	if($number_of_elements > 0){
	 	$workflow_events_dir  = File::Spec->catdir($output_dir,"workflow_events");
	 	if (-d "$workflow_events_dir") {
			print STDERR "WARNING: Output directory $workflow_events_dir exists. Overwriting the contents ....\n";
			rmtree($workflow_events_dir) or die "Unable to delete $workflow_events_dir . $! \n";
		}
		mkdir ($workflow_events_dir) or die "Unable to create $workflow_events_dir . $! \n";
		if($monitord){
			invoke_monitord_for_sub_workflow(values %sub_workflow_submit_hash);
		}
		while (($sub_wf_id, $sub_submit_dir) = each(%sub_workflow_submit_hash)){
			my (%sub_config) = slurp_braindb( $sub_submit_dir ) or die "ERROR: open braindump.txt in directory $sub_submit_dir: $!\n";
			my($sub_dag_file_name , $sub_jsd_file_path) =  ($sub_config{'dag'} ,$sub_config{'jsd'});
			my($sub_dag_file_path) =  File::Spec->catfile("$sub_submit_dir","$sub_dag_file_name");
			if(!$sub_dag_file_name){
					print STDERR "ERROR: Unable to find the dag file name in the braindump.txt file.\n";
					exit 1;
			}
			if(!$sub_jsd_file_path){
					print STDERR "ERROR: Unable to find the jobstate file path in the braindump.txt file.\n";
					exit 1;
			}
			print STDOUT "\n******  calculating workflow statistics for workflow : $sub_submit_dir  ***** \n";
			my($sub_workflow_statistics_status) = print_workflow_statistics($sub_submit_dir,rlb($sub_jsd_file_path),$sub_dag_file_path,$condor);
			print STDOUT "******  Finished calculating workflow statistics for workflow : $sub_submit_dir  ***** \n\n";
			print STDOUT "\n******  calculating job statistics for workflow : $sub_submit_dir  ***** \n";
			my($genstats_status_sub ,$genstats_summary_msg_sub) =run_genstats($sub_dag_file_path ,rlb($sub_jsd_file_path),$output_dir ,$condor ,$sub_submit_dir, $sub_wf_id."_");
			if($genstats_status_sub != 0){
				print STDERR "$genstats_summary_msg_sub \n";
				$genstats_error = 1;
			}
			print STDOUT "******  Finished calculating job statistics for workflow : $sub_submit_dir  ***** \n\n";
		}
	}
	if($number_of_elements > 0){
		$genstats_summary_msg .= "Directory that has the workflow events files for all sub workflows : \n$workflow_events_dir\n";
	}
	if(defined ($genstats_error)){
		$genstats_summary_msg .= "Failed to generate job statistics or Workflow events with time starting from zero for sub workflows.You can find details from the STDOUT.\n";
	}
	
	my($genstats_breakdown_status ,$genstats_breakdown_summary_msg) =run_genstats_breakdown($dag_file_path ,$submit_dir ,$output_dir  );
	
	my $wf_summary_msg;
	my ($all_exec_time) =((defined $workflow_statistics[0]{'total_execution_time'})?$workflow_statistics[0]{'total_execution_time'}:undef);
	my( $all_cpu_time,$all_total_jobs,$all_total_tasks,$all_succceeded,$all_failed,$all_unsubmitted, $all_unknown) =(0,0,0,0,0,0,0);
	my( $sub_dax_total_jobs,$sub_dax_total_tasks,$sub_dax_succceeded,$sub_dax_failed,$sub_dax_unsubmitted, $sub_dax_unknown) =(0,0,0,0,0,0);
	my( $sub_dag_total_jobs,$sub_dag_total_tasks,$sub_dag_succceeded,$sub_dag_failed,$sub_dag_unsubmitted, $sub_dag_unknown) =(0,0,0,0,0,0);
	for my $wstats (@workflow_statistics){
    	$wf_summary_msg .= "\n#$wstats->{'submit_dir'} \n";
    	if((defined $wstats->{'total_execution_time'})){
    		$wf_summary_msg .= sprintf("%-30s % 12d min. %2d sec.\n","Workflow runtime                   :",  ($wstats->{'total_execution_time'})/60 ,($wstats->{'total_execution_time'})%60);
    	}else{
    		$wf_summary_msg .= sprintf("%-30s % 12s \n","Workflow runtime                   :",  '-');
    	}
    	if(defined $wstats->{'total_cpu_time'}){ 
			$wf_summary_msg .= sprintf("%-30s % 12d min. %2d sec.\n","Cumulative workflow runtime        :",($wstats->{'total_cpu_time'})/60 ,($wstats->{'total_cpu_time'})%60);
		}else{
			$wf_summary_msg .= sprintf("%-30s % 12s \n"  ,"Cumulative workflow runtime        :",'-');
		}
		$wf_summary_msg .= sprintf("%-30s % 12d \n","Total jobs                         :", $wstats->{'total_jobs'} );
		
		$wf_summary_msg .= sprintf("%-30s % 12d \n","# jobs succeeded                   :", $wstats->{'succeeded'} );
   		$wf_summary_msg .= sprintf("%-30s % 12d \n","# jobs failed                      :", $wstats->{'failed'} );
   		$wf_summary_msg .= sprintf("%-30s % 12d \n","# jobs unsubmitted                 :", $wstats->{'unsubmitted'} );
   		$wf_summary_msg .= sprintf("%-30s % 12d \n","# jobs unknown                     :", $wstats->{'unknown'});
   		#if((defined $wstats->{'total_tasks'})){
		#	$wf_summary_msg .= sprintf("%-30s % 12d \n","Total tasks                        :", $wstats->{'total_tasks'});
		#}else{
		#	$wf_summary_msg .= sprintf("%-30s % 12s \n","Total tasks                        :", '-');
		#}
   		
   		# Non sub workflow jobs calculation
   		if(defined $all_cpu_time ){
   			if(defined $wstats->{'total_cpu_time_non_sub_wf'}){
   				$all_cpu_time += $wstats->{'total_cpu_time_non_sub_wf'};
   			}else{
   				$all_cpu_time = undef;
   			}
   			
   		}
   		
   		$all_total_jobs +=$wstats->{'total_jobs_non_sub_wf'};
   		if(defined $all_total_tasks ){
   			if(defined $wstats->{'total_tasks_non_sub_wf'}){
   				$all_total_tasks += $wstats->{'total_tasks_non_sub_wf'};
   			}else{
   				$all_total_tasks = undef;
   			}
   			
   		}
   		$all_succceeded += $wstats->{'non_sub_wf_succeeded'};
   		$all_failed +=$wstats->{'non_sub_wf_failed'};
   		$all_unsubmitted +=$wstats->{'non_sub_wf_unsubmitted'};
   		$all_unknown +=$wstats->{'non_sub_wf_unknown'};
   		
   		# dax job calculation
   		$sub_dax_total_jobs +=$wstats->{'total_jobs_sub_dax'};
   		$sub_dax_succceeded += $wstats->{'sub_dax_succeeded'};
   		$sub_dax_failed +=$wstats->{'sub_dax_failed'};
   		$sub_dax_unsubmitted +=$wstats->{'sub_dax_unsubmitted'};
   		$sub_dax_unknown +=$wstats->{'sub_dax_unknown'};
   		
   		# dag job calculation
   		
   		$sub_dag_total_jobs +=$wstats->{'total_jobs_sub_dag'};
   		$sub_dag_succceeded += $wstats->{'sub_dag_succeeded'};
   		$sub_dag_failed +=$wstats->{'sub_dag_failed'};
   		$sub_dag_unsubmitted +=$wstats->{'sub_dag_unsubmitted'};
   		$sub_dag_unknown +=$wstats->{'sub_dag_unknown'};
   	}
	my $summary_msg = "";
	if (defined $all_exec_time){
		$summary_msg .= sprintf("%-30s % 12d min. %2d sec.\n","Workflow runtime                   :", ($all_exec_time)/60 ,($all_exec_time)%60 );
	}else{
		$summary_msg .= sprintf("%-30s % 12s \n","Workflow runtime                   :", '-' );
	} 
	if(defined $all_cpu_time){
		$summary_msg .= sprintf("%-30s % 12d min. %2d sec.\n","Cumulative workflow runtime        :",($all_cpu_time)/60 ,($all_cpu_time)%60  );
	}else{
		$summary_msg .= sprintf("%-30s % 12s \n"  ,"Cumulative workflow runtime        :",'-' );
	}
	#if(defined $all_total_tasks){
	#	$summary_msg .= sprintf("%-30s % 12d \n","Total tasks                        :", $all_total_tasks);
	#}else{
	#	$summary_msg .= sprintf("%-30s % 12s \n","Total tasks                        :", '-');
	#}
	$summary_msg .= sprintf("\n          %-12s %-12s %-12s %-12s %-12s   \n","Total","Succeeded","Failed","Unsubmitted","Unknown");
	$summary_msg .= sprintf("Jobs      %-12s %-12s %-12s %-12s %-12s   \n",$all_total_jobs,$all_succceeded,$all_failed ,$all_unsubmitted,$all_unknown);
	$summary_msg .= sprintf("SUBDAX    %-12s %-12s %-12s %-12s %-12s   \n",$sub_dax_total_jobs,$sub_dax_succceeded,$sub_dax_failed ,$sub_dax_unsubmitted,$sub_dax_unknown);
	#$summary_msg .= sprintf("SUBDAG    %-12s %-12s %-12s %-12s %-12s   \n",$sub_dag_total_jobs,$sub_dag_succceeded,$sub_dag_failed ,$sub_dag_unsubmitted,$sub_dag_unknown);
	
	
	
	
	
	my $legends_msg_summary = "#Legends\n";
	$legends_msg_summary.= "#Workflow runtime (min,sec) - the waltime from the start of the workflow execution to the end as reported by the DAGMAN.In case of rescue dag the value is the cumulative of all retries.\n";
	$legends_msg_summary.= "#Cumulative workflow runtime (min,sec) - the sum of the walltime of all jobs as reported by the DAGMan .In case of job retries the value is the cumulative of all retries.\n";
	#$legends_msg_summary.= "#Total tasks - the total number of tasks in the workflow .\n";
	$legends_msg_summary.= "Job summary\n";
	$legends_msg_summary.="\t Total - the total number of jobs in the workflow. The total number of jobs is calculated by parsing the .dag file. For workflows having SUBDAX jobs , the SUDBAX job is skipped , but the calculation takes into account all the jobs that make up the SUBDAX sub workflow.For workflows having SUBDAG jobs , the SUBDAG jobs are treated like regular jobs.\n";
	$legends_msg_summary.="\t Succeeded - the total number of succeeded jobs in the workflow . \n";
	$legends_msg_summary.="\t Failed - the total number of failed jobs in the workflow .\n";
	$legends_msg_summary.="\t Unsubmitted - the total number of unsubmitted jobs in the workflow .\n";
	$legends_msg_summary.="\t Unknown - the total number of jobs that are submitted, but has not completed execution or the state is unknown in the workflow.\n";
	$legends_msg_summary.= "SUBDAX summary\n";
	$legends_msg_summary.="\t Total - the total number of SUBDAX jobs in the workflow \n";
	$legends_msg_summary.="\t Succeeded - the total number of succeeded SUBDAX jobs in the workflow.\n";
	$legends_msg_summary.="\t Failed - the total number of failed SUBDAX jobs in the workflow.\n";
	$legends_msg_summary.="\t Unsubmitted - the total number of unsubmitted SUBDAX jobs in the workflow.\n";
	$legends_msg_summary.="\t Unknown - the total number of SUBDAX jobs that are submitted, but has not completed execution or the state is unknown in the workflow.\n";
	#$legends_msg_summary.= "SUBDAG summary\n";
	#$legends_msg_summary.="\t Total - the total number of SUBDAG jobs in the workflow \n";
	#$legends_msg_summary.="\t Succeeded - the total number of succeeded SUBDAG jobs in the workflow.\n";
	#$legends_msg_summary.="\t Failed - the total number of failed SUBDAG jobs in the workflow.\n";
	#$legends_msg_summary.="\t Unsubmitted - the total number of unsubmitted SUBDAG jobs in the workflow.\n";
	#$legends_msg_summary.="\t Unknown - the total number of SUBDAG jobs that are submitted, but has not completed execution or the state is unknown in the workflow.\n";
	
	
	my $legends_msg = "#Legends\n";
	$legends_msg.= "#Workflow runtime (min,sec) - the waltime from the start of the workflow execution to the end as reported by the DAGMAN.In case of rescue dag the value is the cumulative of all retries.\n";
	$legends_msg.= "#Cumulative workflow runtime (min,sec) - the sum of the walltime of all jobs as reported by the DAGMan .In case of job retries the value is the cumulative of all retries.For workflows having sub workflow jobs (i.e SUBDAG and SUBDAX jobs) , the sub workflow jobs are treated like regular jobs. \n";
	$legends_msg.= "#Total jobs - the total number of jobs in the workflow.The total number of jobs is calculated by parsing the .dag file.For workflows having sub workflow jobs (i.e SUBDAG and SUBDAX jobs) , the sub workflow jobs are treated like regular jobs. \n";
	#$legends_msg.= "#Total tasks - the total number of tasks.Every job other than clustered job is considered as a single task.For workflows having sub workflow jobs (i.e SUBDAG and SUBDAX jobs) , the sub workflow jobs are treated like regular jobs. \n";
	$legends_msg.= "# jobs succeeded - the total number of jobs succeeded.\n";
	$legends_msg.= "# jobs failed - the total number of jobs failed.\n";
	$legends_msg.= "# jobs unsubmitted - the total number of jobs that are not submitted.\n";
	$legends_msg.= "# jobs unknown - the total number of jobs that are submitted, but has not completed execution or the state is unknown.\n";
	
	my $workflow_stats_file_path =  File::Spec->catfile("$output_dir","workflow.txt");
	open(STATSOUT,">$workflow_stats_file_path") || die("Unable to open file $workflow_stats_file_path.\n$!" );
	print STATSOUT "$legends_msg";
	print STATSOUT "$wf_summary_msg\n";
	#print STATSOUT "#All \n$summary_msg";
	close STATSOUT;
	
	$summary_msg .= "\nWorkflow execution statistics :\n$workflow_stats_file_path\n";
	# Executing genstat command
	 
	
	$legends_msg_summary.= "\n";
	my $dag_job_warning_msg ="";
	if (defined $contains_dag_job){
		$dag_job_warning_msg = "\n[WARNING]: This workflow has $sub_dag_total_jobs SUBDAG jobs.SUBDAG jobs are treated like regular jobs.The jobs making up the SUBDAG are not included in the cummulative numbers.\n\n";
	}
	print "\n\n******************************************** SUMMARY ********************************************\n";
	print $legends_msg_summary.$dag_job_warning_msg.$summary_msg.$genstats_summary_msg.$genstats_breakdown_summary_msg;
	
	print "**************************************************************************************************\n";
	
	exit 0;
}

sub parse_run_dir_db_file($){
	# purpose : parses the brain dump file to find the  'run'(old format) or 'submit_dir' config value in the brain dump file.
	#returns : submit,run directory if it exists, undef otherwise
	my ($braindump_file)=(@_);
	my ($run_dir,$sub_dir) = (undef,undef);
	open CONFIG, "$braindump_file" or die "Unable to open brain dump file: $braindump_file . $! \n";
	for (<CONFIG>) {
    	chomp;
    	s/\#.*//;
    	s/^\s+//;
    	s/\s+$//;
    	next unless length;
    	my ($var, $val) = split(/\s/, $_, 2);
    	# new format
		if ($var eq 'submit_dir'){
    		$sub_dir =  $val;
		}
    	# old format
    	if ($var eq 'run'){
    		$run_dir =  $val;
		}
	}	
	close(CONFIG);
	return ($sub_dir ,$run_dir);
}

sub rlb($){
	# purpose : changes the path relative to the base directory
	# returns : path relative to the base directory
	my( $file_path) =@_;
	chomp($file_path);
    $file_path =~ s/^\s+//;
	$file_path =~ s/\s+$//;
	my $file_relative_path = substr($file_path ,$base_path_len );
	# Removing '/' from the path
	$file_relative_path =~ s/^\///;
	my $relative_to_base_path =  File::Spec->catfile("$base_submit_dir" , "$file_relative_path" );
	return $relative_to_base_path;
}

sub get_job_list($){
	# purpose : parses dag file to find all the jobs
	# returns : the job list
	my($dagPath) =@_;
	open(DAGIN, "$dagPath") || die "$dagPath not found\n";
	my @job_list;
	foreach my $line (<DAGIN>) {
        chomp($line);
        $line =~ s/\#.*//;
	    $line =~ s/^\s+//;
	    $line =~ s/\s+$//;
	    next unless length($line);
	    if ($line =~ '^JOB') {
	    	my @tokens = split(' ', $line);
            my $job = $tokens[1];
            push(@job_list,$job);
        }elsif($line =~ '^SUBDAG.*EXTERNAL'){
        	my @tokens = split(' ', $line);
            my $job = $tokens[2];
            push(@job_list,$job);
        }
    }
    
   return @job_list; 
}

sub get_task_count($){
	# purpose : parses the clustered job .in file to find the task count
	# returns : the number of tasks ,undef in case of error 
	my ($in_path) =@_;
	my $task_count =0;
	my @job_list;
	if(open(CLUSTERJOBIN,"$in_path")){
		foreach my $line (<CLUSTERJOBIN>) {
	        chomp($line);
	        $line =~ s/\#.*//;
		    $line =~ s/^\s+//;
		    $line =~ s/\s+$//;
		    next unless length($line);
		   	++$task_count;
	    }
	    
    }
    return $task_count ==0?undef:$task_count;
}

sub print_workflow_statistics($$$$){
	# purpose : determine workflow duration command
	# returns : status code
	my($submit_dir,$jsd,$dag_file_path,$condor) =@_;
    my $startTime = undef;
    my $endTime = undef;
    my $total_execution_time = undef;
    my $status = 1;
    my %job_status_hash =();
    my %job_state_hoh = (); 
	open(JOBSTATE, "$jsd") || die "Unable to open $jsd .$!\n";
	foreach my $line (<JOBSTATE>) {
        if ($line =~ /INTERNAL/) {
            if ($line =~ /DAGMAN_STARTED*/) {
            	my @tokens = split(' ', $line);
                if($startTime){
                	if ($main::DEBUG){
        	 			print STDERR "WARNING: Mismatch between DAGMAN_STARTED and DAGMAN_FINISHED event in the jobstate file $jsd  ....\n";
        	 		}
                }else{
        	 		$startTime = $tokens[0];
        	 	}
            } else {
            	if ($line =~ /DAGMAN_FINISHED*/) {
            		my @tokens = split(' ', $line);
            	 	if($startTime){
            	 		$endTime = $tokens[0];
            	 		$total_execution_time += $endTime- $startTime;
            	 		$startTime = undef;
            	 		$endTime = undef;
            	 	}else{
            	 		if ($main::DEBUG){
            	 			print STDERR "WARNING: Mismatch between DAGMAN_STARTED and DAGMAN_FINISHED event in the jobstate file $jsd  ....\n";
            	 		}
            	 	}
            	}
            	
            	if ($line =~ /DAGMAN_FINISHED ([0-9])+/) {
            		if ($1 != 0) {
                    	if ($main::DEBUG){
                        	print STDERR "Warning: Workflow execution failed/restarted.\n";
                        }
                    }
                }
            }
        }else{
        	chomp($line);
            my @tokens = split(' ', $line);
            my $timestamp = $tokens[0];
            my $job = $tokens[1];
            my $event = $tokens[2];
            if($event eq 'SUBMIT') {
            	$job_status_hash{$job} = $event;
            }elsif($event eq 'JOB_SUCCESS'){
            	$job_status_hash{$job} = $event;
            }elsif($event eq 'JOB_FAILURE'){
            	$job_status_hash{$job} = $event;
            }
            $job_state_hoh{$job}{$event} = $timestamp;
            # Finding the run time of each from the job state.log
            #sanity test just to make sure the SUBMIT/EXECUTE and JOB_TERMINATED comes in pair
            if($event eq 'SUBMIT' or $event eq 'EXECUTE'){
            	$job_state_hoh{$job}{'JOB_TERMINATED'} = undef;
            }
            if($event eq 'JOB_TERMINATED'){
            	# if EXECUTE event is not present then runtime is calculated by subtracting from the SUBMIT event
				if (defined($job_state_hoh{$job}{'EXECUTE'}) or defined($job_state_hoh{$job}{'SUBMIT'}) ){
					my $exec_start_time =  (defined($job_state_hoh{$job}{'EXECUTE'}))?$job_state_hoh{$job}{'EXECUTE'}:$job_state_hoh{$job}{'SUBMIT'};
					if(defined ($job_state_hoh{$job}{'cumulative_job_runtime'})){
						if(!($job_state_hoh{$job}{'cumulative_job_runtime'} eq '-')){
							if ($main::DEBUG){
	            	 			print STDOUT "INFO: The job '$job' is retried.\n";
	            	 		}
							$job_state_hoh{$job}{'cumulative_job_runtime'} += $job_state_hoh{$job}{'JOB_TERMINATED'}  - $exec_start_time;
						}	
					}else{
						$job_state_hoh{$job}{'cumulative_job_runtime'}= $job_state_hoh{$job}{'JOB_TERMINATED'}  - $exec_start_time;
					}
				}else{
					$job_state_hoh{$job}{'cumulative_job_runtime'} ='-';
					if ($main::DEBUG){
            	 			print STDERR "WARNING: Absent or mismatch between the order of the events JOB_TERMINATED and EXECUTE/SUBMIT $job ....\n";
            	 	}
				}
			}
        }
    }
    
    # Getting the job list
    my @job_list = get_job_list($dag_file_path);
    my $total_job_count = scalar(@job_list); # total jobs in the dag file 
    my $current_dir = getcwd();
    # Change the directory to submit directory
    chdir $submit_dir || die "Cannot chdir to $submit_dir\n";
    my ($total_cpu_time ,$total_cpu_time_status_msg) = (0.0, ' ');
    foreach my $job (@job_list) {
		if(defined( $job_state_hoh{$job})){
			if (defined($job_state_hoh{$job}{'cumulative_job_runtime'}) and defined($job_state_hoh{$job}{'JOB_TERMINATED'}) and !($job_state_hoh{$job}{'cumulative_job_runtime'} eq '-')){
				$total_cpu_time += $job_state_hoh{$job}{'cumulative_job_runtime'};
			}else{
				$total_cpu_time = undef;
				$total_cpu_time_status_msg .= "Failed to find the runtime for the job '$job' \n";
				last;
			}
		}else{
			$total_cpu_time = undef;
			$total_cpu_time_status_msg .= "Failed to find the runtime for the job '$job' \n";
			last;
		}
    }
	if(!defined($total_cpu_time)){ 
			print STDERR "****** WARNING: Failed to calculate the total cpu time.$total_cpu_time_status_msg ******\n";
    }
    
    #calculating the workflow time without the sub workflow details

    my ($total_cpu_time_non_sub_wf ,$total_cpu_time_non_sub_wf_status_msg) = (0.0, ' ');
  	foreach my $job (@job_list) {
		my $is_sub_wf_job = (($job =~m/^pegasus-plan.*/i) || ($job  =~m/^subdax_.*/i))?1:0;
		# sub dag job is considered as a regular job
		if(!$is_sub_wf_job){  		
			if(defined( $job_state_hoh{$job})){
				if (defined($job_state_hoh{$job}{'cumulative_job_runtime'}) and defined($job_state_hoh{$job}{'JOB_TERMINATED'}) and !($job_state_hoh{$job}{'cumulative_job_runtime'} eq '-')){
					$total_cpu_time_non_sub_wf += $job_state_hoh{$job}{'cumulative_job_runtime'};
				}else{
					$total_cpu_time_non_sub_wf = undef;
					$total_cpu_time_non_sub_wf_status_msg .= "Failed to find the runtime for the job '$job' \n";
					last;
				}
			}else{
				$total_cpu_time_non_sub_wf = undef;
				$total_cpu_time_non_sub_wf_status_msg .= "Failed to find the runtime for the job '$job' \n";
				last;
			}
		}	
    }
    
    if(!(defined $total_cpu_time_non_sub_wf)){ 
			print STDERR "****** WARNING: Failed to calculate the total cpu time without the sub workflow run time .$total_cpu_time_non_sub_wf_status_msg ******\n";
    }
   
    # Finding the total job count without the sub dax name
    my $total_job_count_non_sub_wf =0;
    my $total_dax_job_count =0;
    my $total_dag_job_count =0;
  	foreach my $job (@job_list) {
  		my $is_dax_job = (($job =~m/^pegasus-plan.*/i) || ($job  =~m/^subdax_.*/i))?1:0;
    	my $is_dag_job = (($job  =~m/^subdag_.*/i))?1:0;
  		if($is_dax_job){
  			++$total_dax_job_count;
  		}else{
  			# sub dag job is considered as a regular job
  			++ $total_job_count_non_sub_wf;
  			if($is_dag_job){
  				++$total_dag_job_count;
  				$contains_dag_job = 1;
  			}
  		}
  	}
    
   #Finding total task count
  	my $total_task_count =0;
  	my $total_task_count_non_sub_wf =0;
  	my $total_task_count_msg ='';
	print STDOUT "******  calculating total tasks  ***** \n";
    print STDOUT "Please wait, this may take a few minutes ...\n";
	my $task_count = 0; 
	foreach my $job (@job_list) {
		$job =~ s/^\s+//; 
		my $is_sub_wf_job = (($job =~m/^pegasus-plan.*/i) || ($job  =~m/^subdax_.*/i))?1:0;
		if($job =~ /^merge_/){
			my($in_file) = File::Spec->catfile( $submit_dir ,$job .".in");
			$task_count = get_task_count($in_file);
			if($task_count){
				$total_task_count += $task_count;
				$total_task_count_non_sub_wf += $task_count;
			}else{
				$total_task_count = undef;
				$total_task_count_non_sub_wf = undef;
				$total_task_count_msg = "Failed to find the task count for the job '$job' \n";
				last;
			}
		
		}else{
			++$total_task_count;
			if(!$is_sub_wf_job){
				++$total_task_count_non_sub_wf;
			}
		}
	}
	if(!defined($total_task_count)){ 
		print STDERR "****** WARNING: Failed to find the total tasks .$total_task_count_msg ******\n";
    }else{
    	print STDOUT "****** Finished calculating total tasks    ***** \n";
    }
    # Revert back to the old directory
    chdir $current_dir || die "Cannot chdir to $current_dir\n";
    
    my $job_state_count = scalar(keys %job_status_hash); # total jobs that have been submitted as per the jobstate file
    my ($job_succeeded_count,$job_failed_count, $job_unknown_count,$job_unsubmitted_count) =(0,0,0,0);
    my ($non_sub_wf_job_succeeded_count,$non_sub_wf_job_failed_count, $non_sub_wf_job_unknown_count,$non_sub_wf_job_unsubmitted_count) =(0,0,0,0);
    my ($sub_dax_job_succeeded_count,$sub_dax_job_failed_count, $sub_dax_job_unknown_count,$sub_dax_job_unsubmitted_count) =(0,0,0,0);
    my ($sub_dag_job_succeeded_count,$sub_dag_job_failed_count, $sub_dag_job_unknown_count,$sub_dag_job_unsubmitted_count) =(0,0,0,0);
    while (my($job, $value) = each(%job_status_hash)){
    	my $is_dax_job = (($job =~m/^pegasus-plan.*/i) || ($job  =~m/^subdax_.*/i))?1:0;
    	my $is_dag_job = (($job  =~m/^subdag_.*/i))?1:0;
     	if($value eq 'JOB_SUCCESS') {
     		++$job_succeeded_count;
     		if($is_dax_job){
     			++$sub_dax_job_succeeded_count;
     		}else{
     			++$non_sub_wf_job_succeeded_count;
     			# sub dag job is considered as a regular job
     			if($is_dag_job){
     				++$sub_dag_job_succeeded_count;
     			}
     		}
     	}elsif($value eq 'JOB_FAILURE') {
     		++$job_failed_count;
     		if($is_dax_job){
     			++$sub_dax_job_failed_count;
     		}else{
     			++$non_sub_wf_job_failed_count;
     			# sub dag job is considered as a regular job
     			if($is_dag_job){
     				++$sub_dag_job_failed_count;
     			}
     		}
     	}elsif($value eq 'SUBMIT') {
     		++$job_unknown_count;
     		if($is_dax_job){
     			++$sub_dax_job_unknown_count;
     		}else{
     			++$non_sub_wf_job_unknown_count;
     			# sub dag job is considered as a regular job
     			if($is_dag_job){
     				++$sub_dag_job_unknown_count;
     			}
     		}
     	}
   }
   	# Assigning values
   	my $workflow_stats_hash ={};
   	$workflow_stats_hash->{'submit_dir'} =$submit_dir;
   	if($startTime || $endTime){
    	print STDERR "****** WARNING: Failed to find the workflow execution wall time ****** \n";
    }else{
    	if($total_execution_time){
    		$workflow_stats_hash->{'total_execution_time'} = $total_execution_time;
    	}else{
    		#case where it doesn't find start or end time
    		print STDERR "****** WARNING: Failed to find the workflow execution wall time ****** \n";
    	}
    }
    # Assigning 'All' jobs
    if(defined( $total_cpu_time)){
    	$workflow_stats_hash->{'total_cpu_time'} =  $total_cpu_time; 
    }
    
    $job_unsubmitted_count = $total_job_count - $job_state_count;
   	$workflow_stats_hash->{'total_jobs'} =  $total_job_count;
   	if(defined ($total_task_count)){
   	 	$workflow_stats_hash->{'total_tasks'} =  $total_task_count;
    }
    
    $workflow_stats_hash->{'succeeded'} = $job_succeeded_count;
    $workflow_stats_hash->{'failed'} = $job_failed_count;
    $workflow_stats_hash->{'unsubmitted'} = $job_unsubmitted_count;
    $workflow_stats_hash->{'unknown'} = $job_unknown_count;
   
	# Assigning now sub wf jobs
	if(defined ($total_cpu_time_non_sub_wf)){
    	$workflow_stats_hash->{'total_cpu_time_non_sub_wf'} =  $total_cpu_time_non_sub_wf; 
    }   
   	$non_sub_wf_job_unsubmitted_count = $total_job_count_non_sub_wf -($non_sub_wf_job_succeeded_count + $non_sub_wf_job_failed_count + $non_sub_wf_job_unknown_count);
    $workflow_stats_hash->{'total_jobs_non_sub_wf'} =  $total_job_count_non_sub_wf;
    if(defined ($total_task_count_non_sub_wf)){
   	 	$workflow_stats_hash->{'total_tasks_non_sub_wf'} =  $total_task_count_non_sub_wf;
    }
    $workflow_stats_hash->{'non_sub_wf_succeeded'} = $non_sub_wf_job_succeeded_count;
    $workflow_stats_hash->{'non_sub_wf_failed'} = $non_sub_wf_job_failed_count;
    $workflow_stats_hash->{'non_sub_wf_unsubmitted'} = $non_sub_wf_job_unsubmitted_count;
    $workflow_stats_hash->{'non_sub_wf_unknown'} = $non_sub_wf_job_unknown_count;
    
    
    # Assigning dax jobs
    $workflow_stats_hash->{'total_jobs_sub_dax'} =  $total_dax_job_count;
    $sub_dax_job_unsubmitted_count = $total_dax_job_count -($sub_dax_job_succeeded_count + $sub_dax_job_failed_count + $sub_dax_job_unknown_count);
    $workflow_stats_hash->{'sub_dax_succeeded'} = $sub_dax_job_succeeded_count;
    $workflow_stats_hash->{'sub_dax_failed'} = $sub_dax_job_failed_count;
    $workflow_stats_hash->{'sub_dax_unsubmitted'} = $sub_dax_job_unsubmitted_count;
    $workflow_stats_hash->{'sub_dax_unknown'} = $sub_dax_job_unknown_count;
    
    # Assigning dag jobs
    $workflow_stats_hash->{'total_jobs_sub_dag'} =  $total_dag_job_count;
    $sub_dag_job_unsubmitted_count =$total_dag_job_count -($sub_dag_job_succeeded_count+$sub_dag_job_failed_count + $sub_dag_job_unknown_count);
    $workflow_stats_hash->{'sub_dag_succeeded'} = $sub_dag_job_succeeded_count;
    $workflow_stats_hash->{'sub_dag_failed'} = $sub_dag_job_failed_count;
    $workflow_stats_hash->{'sub_dag_unsubmitted'} = $sub_dag_job_unsubmitted_count;
    $workflow_stats_hash->{'sub_dag_unknown'} = $sub_dag_job_unknown_count;
    
    
    
   	push(@workflow_statistics,$workflow_stats_hash);
   	return ($status );
}




sub invoke_monitord($$){
# purpose : executing pegasus-monitord command
# returns : status code and status message

	my($dagman_out_file,$job_state_file) = @_;
	my(@args);
	my $summary_msg;
	print STDOUT "******  pegasus-monitord  ***** \n";
	# Adding arguments
	push(@args ,"-r "); # add replay option
	push(@args ,"-j ".$job_state_file); # adding job state option
	push(@args ,"$dagman_out_file"); #/adding show-jobnames option
	
	# create pegasus-monitord command
	my (@pegasus_monitord_directory_path) = ("$ENV{PEGASUS_HOME}","bin");
	my ($pegasus_monitord_path) =  File::Spec->catfile( @pegasus_monitord_directory_path, 'pegasus-monitord');
	my ($pegasus_monitord_command) = "$pegasus_monitord_path ".join(" ",@args);
	
	if ($main::DEBUG){
		print STDOUT "Executing command :-\n$pegasus_monitord_command \n";
	}
	my($status,$exec_msg) = execute_command($pegasus_monitord_command,'pegasus-monitord');
	if($status == 0  ){
		print STDOUT "****** Finished executing pegasus-monitord  ***** \n";
		$summary_msg = "Finished executing pegasus-monitord\n";		
	}else{
		print STDERR "ERROR : Failed to execute pegasus-monitord command . $exec_msg\n";
		$summary_msg ="ERROR : Failed to execute pegasus-monitord  . $exec_msg\n";
	}
	return ($status, $summary_msg);
	
}


sub setup_monitord($){
	#purpose : makes set up for pegasus-monitord command, by creating a backup of jobstate.log file
	#returns : job state backup file path
	my ($job_state_base_file) =@_;
	my ($count) =0;
	my ($job_state_tmp_file);
	while($count < $MAX_LOG_FILE){
		if($count <10){
			$job_state_tmp_file = $job_state_base_file.".00".$count;
		}elsif($count <100){
			$job_state_tmp_file = $job_state_base_file.".0".$count;
		}else{
			$job_state_tmp_file = $job_state_base_file.".".$count;
		}
		if(-e "$job_state_tmp_file"){
			$count++;
		}else{
			if(!move($job_state_base_file,$job_state_tmp_file)) {
				print STDERR "ERROR : Failed to create a backup of jobstate file  $job_state_base_file . $! \n";
				exit 1;
			}
			return $job_state_tmp_file;
		}	
	}
	print STDERR "ERROR : Failed to create a backup of jobstate file  $job_state_base_file . Exceeded the rotation limit. \n";
	exit 1;
}


sub run_genstats($$$$$$){
# purpose : executing genstats command
# returns : status code and status message
	my(  $dag_file_path , $jobstate_log, $output_dir ,$condor, $submit_dir, $prefix )=(@_);
	my(@args);
	my $summary_msg;
	print STDOUT "******  genstats *****  \n";
	#Adding arguments
	push(@args ,"--dag ".$dag_file_path); # adding dag file argument
	push(@args ,"--output ".$output_dir); #/adding output dir argument
	push(@args ,"--jobstate-log ".$jobstate_log); # adding job state argument	
	if($condor){
		push(@args ,"--condor"); #adding pure condor run argument
	}
	# creating genstats command 
	my (@genstats_directory_path) = ("$ENV{PEGASUS_HOME}","libexec","statistics" );
	my ($genstats_path) =  File::Spec->catfile( @genstats_directory_path, 'genstats');
	my ($genstats_command) = "$genstats_path ".join(" ",@args);
	
	if ($main::DEBUG){
		print STDOUT "Executing command :-\n $genstats_command \n";
	}
	print STDOUT "Please wait, this may take a few minutes ...\n";
	my($status,$exec_msg) = execute_command($genstats_command,'genstats');
	if($status == 0  ){
		print STDOUT "****** Finished executing genstats  ***** \n";
		my (@genstats_misc_files) =  (File::Spec->catfile($output_dir ,'dag'),File::Spec->catfile($output_dir ,'dax'),File::Spec->catfile($output_dir ,'files'));
		foreach my $file (@genstats_misc_files) {
	 		if (!unlink($file)) {
	 			if ($main::DEBUG){
	    			print STDERR "WARNING : Failed to remove file . $file\n";
	    		}
			}
		}
		
		my ($genstats_job_path) =  File::Spec->catfile($output_dir ,'jobs');
		my ($genstats_out_path) =  File::Spec->catfile($output_dir ,'out');
		my ($genstats_cumulative_job_stats_path) =  File::Spec->catfile($output_dir ,'jobs.txt');
		my ($genstats_out_path_prefixed);
		#sub workflow has a prefix
		if(defined ($prefix)){ 
			my $workflow_events_dir  = File::Spec->catdir($output_dir,"workflow_events");
			$genstats_out_path_prefixed =  File::Spec->catfile($workflow_events_dir ,$prefix.'jobstate.txt');	
		}else{
			$genstats_out_path_prefixed =  File::Spec->catfile($output_dir ,"jobstate.txt");
		}
		
		open(JOBS_IN,"$genstats_job_path") || die("Failed to open file $genstats_job_path");
		open(JOBS_OUT,">>$genstats_cumulative_job_stats_path") || die("Failed to open file $genstats_cumulative_job_stats_path");
		print JOBS_OUT "\n#$submit_dir\n";
		foreach my $line (<JOBS_IN>) {
			print JOBS_OUT "$line";
		}
		close(JOBS_IN);
		close(JOBS_OUT);
		if(!unlink($genstats_job_path)){
    		if ($main::DEBUG){
				print STDERR  "ERROR : Failed to delete the jobs file.$genstats_job_path $! \n";
			}
		}
    	
    	
		if(!move($genstats_out_path ,$genstats_out_path_prefixed)) {
			print STDERR "ERROR : Failed to move file from $genstats_job_path to $genstats_out_path_prefixed. $! \n";
			$summary_msg = "\nJob statistics : \n$genstats_cumulative_job_stats_path\n";
			$summary_msg .= "\nFailed to move file from $genstats_job_path to $genstats_out_path_prefixed. $! \n";
			$status = 1;
			return ($status,$summary_msg);
		}
		if ($main::DEBUG){
			print STDOUT "The genstats result - $output_dir \n************ \n";
		}
		$summary_msg = "\nJob statistics : \n$genstats_cumulative_job_stats_path\n";
		$summary_msg .= "\nWorkflow events with time starting from zero : \n$genstats_out_path_prefixed\n";
	}else{
		print STDERR "ERROR: Failed to execute genstats command. $exec_msg\n";
		$summary_msg = "\nERROR: Failed to generate job statistics. $exec_msg\n";
	}
	return ($status,$summary_msg);
	
}

sub run_genstats_breakdown($$$){
# purpose : executing genstats-breakdown command
# returns : status code and status message
	my ($dag_file_path , $submit_dir , $output_dir )=(@_);
	my (@args);
	my ($submit_dirs) = $submit_dir;
	my ($number_of_elements) = scalar(keys %sub_workflow_submit_hash);
	my $summary_msg;
	
	if($number_of_elements > 0){
		$submit_dirs .= " ".join(" ",values %sub_workflow_submit_hash);
	}
	my $submit_dirs_file =&create_submit_directories_file($submit_dirs);
    print STDOUT "******  genstats-breakdown *****  \n";
    my $output_file = 	File::Spec->catfile($output_dir ,"breakdown.txt");
	#Adding arguments
	push(@args ,"--output ".$output_file); #/adding output file argument
	push(@args ,"-f ".$submit_dirs_file); #adding submit directories
	
	# creating genstats-breakdown command 
	my (@genstats_breakdown_directory_path) = ("$ENV{PEGASUS_HOME}","libexec","statistics");
	my ($genstats_breakdown_path) =  File::Spec->catfile( @genstats_breakdown_directory_path, 'genstats-breakdown');
	my ($genstats_breakdown_command) = "$genstats_breakdown_path ".join(" ", @args);
	if ($main::DEBUG){
		print STDOUT "Executing command :-\n $genstats_breakdown_command \n";
	}
	print STDOUT "Please wait, this may take a few minutes ...\n";
	my($status,$exec_msg) = execute_command($genstats_breakdown_command, 'genstats_breakdown');
	if($status == 0  ){
		print STDOUT "****** Finished executing genstats-breakdown  ***** \n";
		if ($main::DEBUG){
			print STDOUT "The genstats breakdown result - $output_file \n************ \n";
		}
		$summary_msg = "\nLogical transformation statistics :\n$output_file\n";
	}else{
		print STDERR "ERROR: Failed to execute genstats-breakdown command . $exec_msg\n";
		$summary_msg ="\nERROR: Failed to generate logical transformations statistics. $exec_msg\n";
	}
	return ($status ,$summary_msg);
	
}

sub invoke_monitord_for_sub_workflow(@){
	# purpose : Invoke pegasus monitord for sub workflow
	
	my(@sub_workflow_submit_dirs) = @_;
	print STDOUT "******  invoking monitord for sub workflows *****  \n";
	print STDOUT "Please wait, this may take a few minutes ...\n";
	foreach my $submit_dir (@sub_workflow_submit_dirs) {
		my (%config) = slurp_braindb( $submit_dir ) or die "ERROR: open braindump.txt in directory $submit_dir: $!\n";
		my($dag_file_name , $jsd_file_path) =  ($config{'dag'} ,$config{'jsd'});
		if(!$dag_file_name){
				print STDERR "ERROR: Unable to find the dag file name in the braindump.txt file in $submit_dir.\n";
				exit 1;
		}
		if(!$jsd_file_path){
				print STDERR "ERROR: Unable to find the jobstate file path in the braindump.txt file in $submit_dir.\n";
				exit 1;
		}
		my $job_state_backup = undef;
		my ($job_state_base_file)= File::Spec->catfile( $submit_dir, $job_state_file_name);
		if(-e "$job_state_base_file"){
			# Invoke only if jobstate.log is there
			$job_state_backup = setup_monitord($job_state_base_file);
		}
		my($dagman_out_file) = File::Spec->catfile( $submit_dir ,$dag_file_name .".dagman.out");
		my($monitord_status , $monitord_summary_msg)  = invoke_monitord($dagman_out_file,rlb($jsd_file_path) );
		if($monitord_status !=0 ){
			if($job_state_backup){
			# Revert back the jobstate file
				if(!move($job_state_backup ,$job_state_base_file)) {
					print STDERR "ERROR: Failed to create a revert back the jobstate file . $! \n";
				}
			}
			
			exit 1;
		}
	}
	print STDOUT "****** Finished invoking monitord for sub workflows  ***** \n";
}

sub create_submit_directories_file($){
	# purpose : creates a file with all the submit directories path.
	#returns : file path
	my($submit_dirs) =@_;
	my ($out_fh,$out_fn) = tempfile("submit_directory_XXXX",SUFFIX => '.in',DIR =>$log_dir);
	
	open ($out_fh, ">$out_fn") or die "Unable to write to file $out_fn. $! \n";
	my @submit_directories_arr = split(' ',$submit_dirs);
	foreach my $submit_dir (@submit_directories_arr) {
		print $out_fh "$submit_dir\n";
	}
	close $out_fh; 
	return $out_fn;
}

sub get_sub_workflow_list($){
	# purpose : find the list of sub workflows from the dag file
	# returns : list of sub workflows
	my($root_dag_file_path) =(@_);
	my($job_name ,$submit_file_name);
	my(@sub_workflow_list) ;
	my (@dag_path_list);
	my $dag_file_path;
	my ($dagbase,$submit_dir ,$type ) ;
    my ($submit_file_path,$sub_wf_submit_dir);
    my($sub_wf_submit_file_name);
    my (%config);
    my($sub_dag_file_name);
    my ($sub_dag_file_path);
    my(%sub_wf_name_submit_dir_hash);
	
	push(@dag_path_list,$root_dag_file_path);
	while(@dag_path_list){
		$dag_file_path = shift(@dag_path_list);
		@sub_workflow_list =();
		open CONFIG, "$dag_file_path" or die "Unable to open dag file: $dag_file_path . $! \n";
		# Getting sub workflows from dag file
		while (my $line = <CONFIG>) {
	    	chomp $line;
	    	$line =~ s/\#.*//;
	    	$line =~ s/^\s+//; 
	    	$line =~ s/^\s+//;
	    	next unless length($line);
	    	if( $line =~m/^JOB\s.*/i ){
	    		my @values = split(/\s+/, $line);
	    		$job_name =$values[1];
	    		$submit_file_name = $values[2];
	    		if(($job_name  =~m/^pegasus-plan.*/i) || ($job_name  =~m/^subdax_.*/i) ){
	  				push(@sub_workflow_list,$submit_file_name);
	  			}
	    	}
	    }
	    close CONFIG;
	 	
	 	($dagbase,$submit_dir ,$type) = fileparse( $dag_file_path  ,qr{\..*});
	 	foreach $sub_wf_submit_file_name (@sub_workflow_list){
	 		 $submit_file_path = File::Spec->catfile($submit_dir,$sub_wf_submit_file_name);
		     $sub_wf_submit_dir = &parse_submit_file($submit_file_path);
		     if($sub_wf_submit_dir){
		     	$sub_wf_submit_dir = rlb($sub_wf_submit_dir);
		        %config = slurp_braindb( $sub_wf_submit_dir ) or die "ERROR: open braindump.txt in directory $sub_wf_submit_dir: $!\n";
	     	    $sub_dag_file_name = $config{'dag'};
	     	    #Striping of the .sub extension
	     	    my $sub_wf_id = substr($sub_wf_submit_file_name,0,-4);
	     	    $sub_wf_name_submit_dir_hash{$sub_wf_id} = $sub_wf_submit_dir;
	     		if($sub_dag_file_name){
	     			$sub_dag_file_path = File::Spec->catfile($sub_wf_submit_dir,$sub_dag_file_name);
	     			push(@dag_path_list,$sub_dag_file_path);
		     	}else{
		     		if ($main::DEBUG){
		     			print STDERR "WARNING: Unable to find dag file name in the braindump file in $sub_wf_submit_dir.\n";
		     		}
		     	}
		     }else{
		     	if ($main::DEBUG){
		     		print STDERR "Skipping sub workflow $sub_wf_submit_file_name \n"
		     	}
		     }
		     
		}   
	}
	return %sub_wf_name_submit_dir_hash;
}

sub parse_submit_file($){
	# purpose : parses the submit file to find the 'initialdir' configuration value.
	# returns : 'initialdir' configuration value if present , undef otherwise
	
	my ($submit_file)=(@_);
	if(open CONFIG, "$submit_file"){ 
		for (<CONFIG>) {
	    	chomp;
	    	s/\#.*//;
	    	s/^\s+//;
	    	s/\s+$//;
	    	next unless length;
	    	my ($var, $val) = split(/\s*=\s*/, $_, 2);
	    	if ($var eq 'initialdir'){
	    		close(CONFIG);
				return $val;
			}
	    	
		}
		close(CONFIG);
		if ($main::DEBUG){
			print STDERR "WARNING: Unable to find 'initialdir' configuration value in the submit file $submit_file.\n";
		}
		return undef;
	}else{
		if ($main::DEBUG){
			print STDERR "WARNING: Unable to open submit file: $submit_file . $! \n" ;
		}
		return undef;
	}
	
}


sub  execute_command($$){
# purpose : executes command and returns the status and error message
	# returns : status and error message
	my ($command , $prefix)=(@_);
	my ($error_msg) = (undef);
	my ($out_fh,$out_fn) = tempfile($prefix."_XXXX",SUFFIX => '.out',DIR =>$log_dir);
	close($out_fh);
	my ($err_fn) = substr($out_fn,0,-4).".err";
	open( ERROUT, ">$err_fn" ) || die "ERROR: open $err_fn: $!\n";
    close(ERROUT);
    chmod(0755, $out_fn) or die "Couldn't chmod $out_fn: $!";
    chmod(0755, $err_fn) or die "Couldn't chmod $err_fn: $!";
	my($ret) = system("$command  1>$out_fn 2>$err_fn");
	$error_msg = "";
	if($ret == -1){
		#Fatal error : error running the system command
		$error_msg .= $!;
	}elsif($ret != 0){
		if ($main::DEBUG){
			if(open OUTPUT ,"$out_fn"){ 
				while(<OUTPUT>){
					 print STDOUT "$_ \n";
				}
			}
			close(OUTPUT);
			if(open ERR ,"$err_fn"){
				while(<ERR>){
					 print STDERR "$_ \n";
				}
			}
			close(ERR);
		}else{
			$keep = 1;
			print STDERR  "Failed to execute command '$command'.\n";
			print STDERR  "The STDOUT file is stored at '$out_fn'.\n";
			print STDERR  "The STDERR file is stored at '$err_fn'.\n";
		}
	}
	return ($ret , $error_msg);
}

