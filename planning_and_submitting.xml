<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN"
"http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<chapter id="planning_and_submitting">
  <title>Planning and Submitting</title>

  <section>
    <title>Refinement Steps Explained</title>

    <para>During the mapping process, the abstract workflow undergoes a series
    of refinement steps that converts it to an executable form.</para>

    <section>
      <title>Data Reuse</title>

      <para>The abstract workflow after parsing is optionally handed over to
      the Data Reuse Module. The Data Reuse Algorithm in Pegasus attempts to
      prune all the nodes in the abstract workflow for which the output files
      exist in the Replica Catalog. It also attempts to cascade the deletion
      to the parents of the deleted node for e.g if the output files for the
      leaf nodes are specified, Pegasus will prune out all the workflow as the
      output files in which a user is interested in already exist in the
      Replica Catalog.</para>

      <para>The Data Reuse Algorithm works in two passes</para>

      <para><emphasis role="bold">First Pass</emphasis> - Determine all the
      jobs whose output files exist in the Replica Catalog. An output file
      with the transfer flag set to false is treated equivalent to the file
      existing in the Replica Catalog , if the output file is not an input to
      any of the children of the job X.</para>

      <para><emphasis role="bold">Second Pass</emphasis> - The algorithm
      removes the job whose output files exist in the Replica Catalog and
      tries to cascade the deletion upwards to the parent jobs. We start the
      breadth first traversal of the workflow bottom up.</para>

      <programlisting>( It is already marked for deletion in Pass 1
     OR
      ( ALL of it's children have been marked for deletion
        AND
        Node's output files have transfer flags set to false
       )
 )</programlisting>

      <tip>
        <para>The Data Reuse Algorithm can be disabled by passing the
        <emphasis role="bold">--force</emphasis> option to
        pegasus-plan.</para>
      </tip>
    </section>

    <section>
      <title>Site Selection</title>

      <para>The abstract workflow is then handed over to the Site Selector
      module where the abstract jobs in the pruned workflow are mapped to the
      various sites passed by a user. The target sites for planning are
      specified on the command line using the<emphasis role="bold">
      --sites</emphasis> option to pegasus-plan. If not specified, then
      Pegasus picks up all the sites in the Site Catalog as candidate sites.
      Pegasus will map a compute job to a site only if Pegasus can</para>

      <itemizedlist>
        <listitem>
          <para>find an INSTALLED executable on the site</para>
        </listitem>

        <listitem>
          <para>OR find a STAGEABLE executable that can be staged to the site
          as part of the workflow execution.</para>

          <para>Pegasus supports variety of site selectors with Random being
          the default</para>

          <itemizedlist>
            <listitem>
              <para><emphasis role="bold">Random</emphasis></para>

              <para>The jobs will be randomly distributed among the sites that
              can execute them.</para>
            </listitem>

            <listitem>
              <para><emphasis role="bold">RoundRobin</emphasis></para>

              <para>The jobs will be assigned in a round robin manner amongst
              the sites that can execute them. Since each site cannot execute
              every type of job, the round robin scheduling is done per level
              on a sorted list. The sorting is on the basis of the number of
              jobs a particular site has been assigned in that level so far.
              If a job cannot be run on the first site in the queue (due to no
              matching entry in the transformation catalog for the
              transformation referred to by the job), it goes to the next one
              and so on. This implementation defaults to classic round robin
              in the case where all the jobs in the workflow can run on all
              the sites.</para>
            </listitem>

            <listitem>
              <para><emphasis role="bold">Group</emphasis></para>

              <para>Group of jobs will be assigned to the same site that can
              execute them. The use of the<emphasis role="bold"> PEGASUS
              profile key group</emphasis> in the DAX, associates a job with a
              particular group. The jobs that do not have the profile key
              associated with them, will be put in the default group. The jobs
              in the default group are handed over to the "Random" Site
              Selector for scheduling.</para>
            </listitem>

            <listitem>
              <para><emphasis role="bold">Heft</emphasis></para>

              <para>A version of the HEFT processor scheduling algorithm is
              used to schedule jobs in the workflow to multiple grid sites.
              The implementation assumes default data communication costs when
              jobs are not scheduled on to the same site. Later on this may be
              made more configurable.</para>

              <para>The runtime for the jobs is specified in the
              transformation catalog by associating the <emphasis
              role="bold">pegasus profile key runtime</emphasis> with the
              entries.</para>

              <para>The number of processors in a site is picked up from the
              attribute <emphasis role="bold">idle-nodes</emphasis> associated
              with the vanilla jobmanager of the site in the site
              catalog.</para>
            </listitem>

            <listitem>
              <para><emphasis role="bold">NonJavaCallout</emphasis></para>

              <para>Pegasus will callout to an external site selector.In this
              mode a temporary file is prepared containing the job information
              that is passed to the site selector as an argument while
              invoking it. The path to the site selector is specified by
              setting the property pegasus.site.selector.path. The environment
              variables that need to be set to run the site selector can be
              specified using the properties with a pegasus.site.selector.env.
              prefix. The temporary file contains information about the job
              that needs to be scheduled. It contains key value pairs with
              each key value pair being on a new line and separated by a
              =.</para>

              <para>The following pairs are currently generated for the site
              selector temporary file that is generated in the
              NonJavaCallout.</para>

              <table>
                <title>Table 1: Key Value Pairs that are currently generated
                for the site selector temporary file that is generated in the
                NonJavaCallout.</title>

                <tgroup cols="2">
                  <tbody>
                    <row>
                      <entry><emphasis role="bold">Key</emphasis></entry>

                      <entry><emphasis role="bold">Value</emphasis></entry>
                    </row>

                    <row>
                      <entry>version</entry>

                      <entry>is the version of the site selector api,currently
                      2.0.</entry>
                    </row>

                    <row>
                      <entry>transformation</entry>

                      <entry>is the fully-qualified definition identifier for
                      the transformation (TR) namespace::name:version.</entry>
                    </row>

                    <row>
                      <entry>derivation</entry>

                      <entry>is the fully qualified definition identifier for
                      the derivation (DV), namespace::name:version.</entry>
                    </row>

                    <row>
                      <entry>job.level</entry>

                      <entry>is the job's depth in the tree of the workflow
                      DAG.</entry>
                    </row>

                    <row>
                      <entry>job.id</entry>

                      <entry>is the job's ID, as used in the DAX file.</entry>
                    </row>

                    <row>
                      <entry>resource.id</entry>

                      <entry>is a pool handle, followed by whitespace,
                      followed by a gridftp server. Typically, each gridftp
                      server is enumerated once, so you may have multiple
                      occurances of the same site. There can be multiple
                      occurances of this key.</entry>
                    </row>

                    <row>
                      <entry>input.lfn</entry>

                      <entry>is an input LFN, optionally followed by a
                      whitespace and file size. There can be multiple
                      occurances of this key,one for each input LFN required
                      by the job.</entry>
                    </row>

                    <row>
                      <entry>wf.name</entry>

                      <entry>label of the dax, as found in the DAX's root
                      element. wf.index is the DAX index, that is incremented
                      for each partition in case of deferred planning.</entry>
                    </row>

                    <row>
                      <entry>wf.time</entry>

                      <entry>is the mtime of the workflow.</entry>
                    </row>

                    <row>
                      <entry>wf.manager</entry>

                      <entry>is the name of the workflow manager being used
                      .e.g condor</entry>
                    </row>

                    <row>
                      <entry>vo.name</entry>

                      <entry>is the name of the virtual organization that is
                      running this workflow. It is currently set to
                      NONE</entry>
                    </row>

                    <row>
                      <entry>vo.group</entry>

                      <entry>unused at present and is set to NONE.</entry>
                    </row>
                  </tbody>
                </tgroup>
              </table>
            </listitem>
          </itemizedlist>
        </listitem>
      </itemizedlist>

      <tip>
        <para>The site selector to use for site selection can be specified by
        setting the property <emphasis
        role="bold">pegasus.selector.site</emphasis></para>
      </tip>
    </section>

    <section>
      <title>Job Clustering</title>

      <para>After site selection, the workflow is optionally handed for to the
      job clustering module, which clusters jobs that are scheduled to the
      same site. Clustering is usually done on short running jobs in order to
      reduce the remote execution overheads associated with a job. Clustering
      is described in detail in the <link
      linkend="advanced_concepts_job_clustering">Chapter on Advanced Concepts
      - Job Clustering.</link></para>

      <tip>
        <para>The job clustering is turned on by passing the <emphasis
        role="bold">--cluster</emphasis> option to pegasus-plan.</para>
      </tip>
    </section>

    <section>
      <title>Addition of Data Transfer Nodes</title>

      <para>After job clustering, the workflow is handed to the Data Transfer
      module that adds data stage-in and stage-out nodes to the workflow. Data
      Stage-in Nodes transfer input data required by the workflow from the
      locations specified in the Replica Catalog to a directory on the
      execution site where the job executes. In case, multiple locations are
      specified for the same input file, the location from where to stage the
      data is selected using a <emphasis role="bold">Replica
      Selector</emphasis> . Replica Selection is described in detail in the
      <link linkend="advanced_concepts_replica_selection">Chapter on Advanced
      Concepts - Replica Selection.</link></para>

      <para>The process of adding the data stage-in and data stage-out nodes
      is handled by Transfer Refiners. All data transfer jobs in Pegasus are
      executed using <emphasis role="bold">pegasus-transfer</emphasis> . The
      pegasus-transfer client is a python based wrapper around various
      transfer clients like globus-url-copy, lcg-copy, wget, cp, ln . It looks
      at source and destination url and figures out automatically which
      underlying client to use. pegasus-transfer is distributed with the
      PEGASUS and can be found in the bin subdirectory . Pegasus Transfer
      Refiners are are described in the detail in the <link
      linkend="advanced_concepts_transfers">Chapter on Advanced Concepts -
      Transfers</link>. The default transfer refiner that is used in Pegasus
      is the <emphasis role="bold">Bundle</emphasis> Transfer Refiner, that
      bundles data stage-in nodes and data stage-out nodes on the basis of
      certain pegasus profile keys associated with the workflow.</para>

      <para>The data staged-in and staged-out from a directory that is created
      on the head node by a create dir job in the workflow. In the vanilla
      case, the directory is visible to all the worker nodes and compute jobs
      are launched in this directory on the shared filesystem. In the case
      where there is no shared filesystem, users can turn on worker node
      execution, where the data is staged from the head node directory to a
      directory on the worker node filesystem. This feature will be refined
      further for Pegasus 3.1. To use it with Pegasus 3.0 send email to
      <emphasis role="bold">pegasus-support aT isi.edu</emphasis>.</para>

      <tip>
        <para>The replica selector to use for replica selection can be
        specified by setting the property <emphasis
        role="bold">pegasus.selector.replica</emphasis></para>
      </tip>
    </section>

    <section>
      <title>Addition of Create Dir and Cleanup Jobs</title>

      <para>After the data transfer nodes have been added to the workflow,
      Pegasus adds a create dir jobs to the workflow. Pegasus usually ,
      creates one workflow specific directory per compute site , that is on
      the shared filesystem of compute site. This directory is visible to all
      the worker nodes and that is where the data is staged-in by the data
      stage-in jobs.</para>

      <para>After addition of the create dir jobs, the workflow is optionally
      handed to the cleanup module. The cleanup module adds cleanup nodes to
      the workflow that remove data from the directory on the shared
      filesystem when it is no longer required by the workflow. This is useful
      in reducing the peak storage requirements of the workflow.</para>

      <tip>
        <para>The addition of the cleanup nodes to the workflow can be
        disabled by passing the <emphasis role="bold">--nocleanup</emphasis>
        option to pegasus-plan.</para>
      </tip>
    </section>

    <section>
      <title>Code Generation</title>

      <para>The last step of refinement process, is the code generation where
      Pegasus writes out the executable workflow in a form understandable by
      the underlying workflow executor. At present Pegasus supports the
      following code generators</para>

      <orderedlist>
        <listitem>
          <para><emphasis role="bold">Condor</emphasis></para>

          <para>This is the default code generator for Pegasus . This
          generator generates the executable workflow as a Condor DAG file and
          associated job submit files. The Condor DAG file is passed as input
          to Condor DAGMan for job execution.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Shell</emphasis></para>

          <para>This Code Generator generates the executable workflow as a
          shell script that can be executed on the submit host. While using
          this code generator, all the jobs should be mapped to site local i.e
          specify <emphasis role="bold">--sites local </emphasis> to
          pegasus-plan.</para>

          <tip>
            <para>To use the Shell code Generator set the property <emphasis
            role="bold">pegasus.code.generator</emphasis> Shell</para>
          </tip>
        </listitem>
      </orderedlist>
    </section>
  </section>

  <section>
    <title>pegasus-plan</title>

    <para>pegasus-plan is the main executable that takes in the abstract
    workflow ( DAX ) and generates an executable workflow ( usually a Condor
    DAG ) by querying various catalogs and performing several refinement
    steps. Before users can run pegasus plan the following needs to be
    done</para>

    <orderedlist>
      <listitem>
        <para>Populate the various catalogs</para>

        <orderedlist>
          <listitem>
            <para><emphasis role="bold">Replica Catalog</emphasis></para>

            <para>The Replica Catalog needs to be catalogued with the
            locations of the input files required by the workflows. This can
            be done by using pegasus-rc-client that is explained in the
            Catalogs Chapter <link
            linkend="pegasus-rc-client">here</link>.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Transformation
            Catalog</emphasis></para>

            <para>The Transformation Catalog needs to be catalogued with the
            locations of the executables that the workflows will use. This can
            be done by using pegasus-tc-client that is explained in the
            Catalogs Chapter <link
            linkend="pegasus-tc-client">here</link>.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Site Catalog</emphasis></para>

            <para>The Site Catalog needs to be catalogued with the site layout
            of the various sites that the workflows can execute on. A site
            catalog can be generated for OSG by using the client
            pegasus-sc-client that is explained in the Catalogs Chapter <link
            linkend="pegasus_sc_client">here</link>.</para>
          </listitem>
        </orderedlist>
      </listitem>

      <listitem>
        <para>Configure Properties</para>

        <para>After the catalogs have been configured, the user properties
        file need to be updated with the types and locations of the catalogs
        to use. These properties are explained in the basic.properties files
        in the etc sub directory of the Pegasus installation.</para>

        <para>The basic properties that need to be set usually are listed
        below</para>

        <table>
          <title>Table2: Basic Properties that need to be set</title>

          <tgroup cols="1">
            <tbody>
              <row>
                <entry>pegasus.catalog.replica</entry>
              </row>

              <row>
                <entry>pegasus.catalog.replica.file |
                pegasus.catalog.replica.url</entry>
              </row>

              <row>
                <entry>pegasus.catalog.transformation</entry>
              </row>

              <row>
                <entry>pegasus.catalog.transformation.file</entry>
              </row>

              <row>
                <entry>pegasus.catalog.site</entry>
              </row>

              <row>
                <entry>pegasus.catalog.site.file</entry>
              </row>
            </tbody>
          </tgroup>
        </table>
      </listitem>
    </orderedlist>

    <para>To execute pegasus-plan user usually requires to specify the
    following options</para>

    <orderedlist>
      <listitem>
        <para><emphasis role="bold">--dax </emphasis> the path to the DAX file
        that needs to be mapped.</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">--dir </emphasis> the base directory where
        the executable workflow is generated</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">--sites </emphasis> comma separated list
        of execution sites.</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">--output</emphasis> the output site where
        to transfer the materialized output files.</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">--submit </emphasis> boolean value whether
        to submit the planned workflow for execution after planning is
        done.</para>
      </listitem>
    </orderedlist>

    <para>For a list of all options to pegasus-plan refer to the man
    page.</para>
  </section>

  <section>
    <title>Resource Configurations</title>

    <para>This section discusses the various resource configurations that can
    be used with Pegasus when Condor DAGMan is used as the workflow execution
    engine. It is assumed that there is a submit host where the workflow is
    submitted for execution. The following classification is done based the
    mechanism used for submitting jobs to the Grid resources and monitoring
    them. The classifications explored in this document are using Globus GRAM
    and using a Condor pool. Both of the configurations use Condor DAGMan to
    maintain the dependencies between the jobs, but differ in the manner as to
    how the jobs are launched. A combination of the above mentioned approach
    is also possible where some of the tasks in the workflow are executed in
    the Condor pool and the rest are executed on remote resources using
    Condor-G.</para>

    <section>
      <title>Locally on the Submit Host</title>

      <para>In this configuration , Pegasus schedules the jobs to run locally
      on the submit host. This is achieved by executing the workflow on site
      local ( <emphasis role="bold">--sites local </emphasis>option to
      pegasus-plan ). The <emphasis role="bold">site "local" is a reserved
      site</emphasis> in Pegasus and results in the jobs to run on the submit
      host in condor universe local.</para>
    </section>

    <section>
      <title>Using Globus GRAM</title>

      <para>In this configuration, it is assumed that the target execution
      system consists of one or more Grid resources. These resources may be
      geographically distributed and under various administrative domains.
      Each resource might be a single desktop computer or a network of
      workstations (NOW) or a cluster of dedicated machines. However, each
      resource must provide a Globus GRAM interface which allows the users to
      submit jobs remotely. In case the Grid resource consists of multiple
      compute nodes, e.g. a cluster or a network of workstations, there is a
      central entity called the head node that acts as the single point of job
      submissions to the resource. It is generally possible to specify whether
      the submitted job should run on the head node of the resource or a
      worker node in the cluster or NOW. In the latter case, the head node is
      responsible for submitting the job to a local resource management system
      (PBS, LSF, Condor etc) which controls all the machines in the resource.
      Since, the head node is the central point of job submissions to the
      resource it should not be used for job execution since that can overload
      the head node delaying further job submissions. Pegasus does not make
      any assumptions about the configuration of the remote resource; rather
      it provides the mechanisms by which such distinctions can be
      made.</para>

      <figure>
        <title>Resource Configuration using GRAM</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="./images/gram_layout.png" />
          </imageobject>
        </mediaobject>
      </figure>

      <para>In this configuration, Condor-G is used for submitting jobs to
      these resources. Condor-G is an extension to Condor that allows the jobs
      to be described in a Condor submit file and when the job is submitted to
      Condor for execution, it uses the Globus GRAM interface to submit the
      job to the remote resource and monitor its execution. The distinction is
      made in the Condor submit files by specify the universe as Grid and the
      grid_resource or globusscheduler attribute is used to indicate the
      location of the head node for the remote resource. Thus, Condor DAGMan
      is used for maintaining the dependencies between the jobs and Condor-G
      is used to launch the jobs on the remote resources using GRAM. The
      implicit assumption in this case is that all the worker nodes on a
      remote resource have access to a shared file system that can be used for
      data transfer between the tasks mapped on that resource. This data
      transfer is done using files.</para>

      <para>Below is an example of a site configured to use Globus GRAM2|
      GRAM5</para>

      <programlisting> &lt;site  handle="isi" arch="x86" os="LINUX" osrelease="" osversion="" glibc=""&gt;
       &lt;grid  type="gt2" contact="smarty.isi.edu/jobmanager-pbs" scheduler="PBS" jobtype="auxillary"/&gt;
       &lt;grid  type="gt2" contact="smarty.isi.edu/jobmanager-pbs" scheduler="PBS" jobtype="compute"/&gt;
       &lt;head-fs&gt;
               &lt;scratch&gt;
                   &lt;shared&gt;
                         &lt;file-server protocol="gsiftp" url="gsiftp://skynet-data.isi.edu" \
                                                     mount-point="/nfs/scratch01"/&gt;
                         &lt;internal-mount-point mount-point="/nfs/scratch01"/&gt;
                    &lt;/shared&gt;
               &lt;/scratch&gt;
               &lt;storage&gt;
                   &lt;shared&gt;
                         &lt;file-server protocol="gsiftp" url="gsiftp://skynet-data.isi.edu" \
                                                     mount-point="/exports/storage01"/&gt;
                         &lt;internal-mount-point mount-point="/exports/storage01"/&gt;
                    &lt;/shared&gt;
               &lt;/storage&gt;
        &lt;/head-fs&gt;
        &lt;replica-catalog  type="LRC" url="rlsn://smarty.isi.edu" /&gt;
  &lt;/site&gt;
</programlisting>
    </section>

    <section>
      <title>Condor Pool</title>

      <para>A Condor pool is a set of machines that use Condor for resource
      management. A Condor pool can be a cluster of dedicated machines or a
      set of distributively owned machines. Pegasus can generate concrete
      workflows that can be executed on a Condor pool.</para>

      <figure>
        <title>The distributed resources appear to be part of a Condor
        pool.</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="images/condor_layout.png" />
          </imageobject>
        </mediaobject>
      </figure>

      <para>The workflow is submitted using DAGMan from one of the job
      submission machines in the Condor pool. It is the responsibility of the
      Central Manager of the pool to match the task in the workflow submitted
      by DAGMan to the execution machines in the pool. This matching process
      can be guided by including Condor specific attributes in the submit
      files of the tasks. If the user wants to execute the workflow on the
      execution machines (worker nodes) in a Condor pool, there should be a
      resource defined in the site catalog which represents these execution
      machines. The universe attribute of the resource should be vanilla.
      There can be multiple resources associated with a single Condor pool,
      where each resource identifies a subset of machine (worker nodes) in the
      pool. Pegasus currently uses the FileSystemDomain classad[] attribute to
      restrict the set of machines that make up a single resource. To clarify
      this point, suppose there are certain execution machines in the Condor
      pool whose FileSystemDomain is set to “viz.isi.edu”. If the user wants
      to execute the workflow on these machines, then there should be a site,
      say “isi_viz” defined in the site catalog and the FileSystemDomain and
      universe attributes for this resource should be defined as “viz.isi.edu”
      and “vanilla” respectively. When invoking Pegasus, the user should
      select isi_viz as the compute resource.</para>

      <programlisting> &lt;site  handle="isi_viz" arch="x86" os="LINUX" osrelease="" osversion="" glibc=""&gt;
       &lt;grid  type="gt2" contact="smarty.isi.edu/jobmanager-pbs" scheduler="PBS" jobtype="auxillary"/&gt;
       &lt;grid  type="gt2" contact="smarty.isi.edu/jobmanager-pbs" scheduler="PBS" jobtype="compute"/&gt;
       &lt;head-fs&gt;
              &lt;scratch&gt;
                    &lt;shared&gt;
                           &lt;file-server protocol="gsiftp" url="gsiftp://viz-login.isi.edu" \
                                                              mount-point="/nfs/scratch01"/&gt;
                           &lt;internal-mount-point mount-point="/nfs/scratch01"/&gt;
                    &lt;/shared&gt;
              &lt;/scratch&gt;
              &lt;storage&gt;
                    &lt;shared&gt;
                           &lt;file-server protocol="gsiftp" url="gsiftp://viz-login.isi.edu" \
                                                           mount-point="/exports/storage01"/&gt;
                           &lt;internal-mount-point mount-point="/exports/storage01"/&gt;
                           &lt;/shared&gt;
               &lt;/storage&gt;
       &lt;/head-fs&gt;
       &lt;replica-catalog  type="LRC" url="rlsn://smarty.isi.edu"/&gt;
       <emphasis role="bold">&lt;profile namespace="condor" key="universe"&gt;vanilla&lt;/profile&gt; 
       &lt;profile namespace="condor" key="FileSystemDomain"&gt;viz.isi.edu&lt;/profile&gt;</emphasis>
 &lt;/site&gt;
</programlisting>
    </section>

    <section>
      <title>Condor Glideins</title>

      <para>As mentioned Pegasus can execute workflows over Condor pool. This
      pool can contain machines managed by a single institution or department
      and belonging to a single administrative domain. This is the case for
      most of the Condor pools. In this section we describe how machines from
      different administrative domains and supercomputing centers can be
      dynamically added to a Condor pool for certain timeframe. These machines
      join the Condor pool temporarily and can be used to execute jobs in a
      non preemptive manner. This functionality is achieved using a Condor
      feature called Glide-in http://cs.wisc.edu/condor/glidein that uses
      Globus GRAM interface for migrating machines from different domains to a
      Condor pool. The number of machines and the duration for which they are
      required can be specified</para>

      <para>In this case, we use the abstraction of a local Condor pool to
      execute the jobs in the workflow over remote resources that have joined
      the pool for certain timeframe. Details about the use of this feature
      can be found in the condor manual
      (http://cs.wisc.edu/condor/manual/).</para>

      <para>A basic step to migrate in a job to a local condor pool is
      described below.</para>

      <programlisting><emphasis role="bold">$condor_glidein -count 10 gatekeeper.site.edu/jobmanager-pbs</emphasis></programlisting>

      <para><emphasis role="bold">GlideIn of Remote Globus
      Resources</emphasis></para>

      <para>The above step glides in 10 nodes to the user’s local condor pool,
      from the remote pbs scheduler running at gatekeeper.site.edu. By
      default, the glide in binaries are installed in the users home directory
      on the remote host.</para>

      <para>It is possible that the Condor pool can contain resources from
      multiple Grid sites. It is normally the case that the resources from a
      particular site share the same file system and thus use the same
      FileSystemDomain attribute while advertising their presence to the
      Central Manager of the pool. If the user wants to run his jobs on
      machines from a particular Grid site, he has to specify the
      FileSystemDomain attribute in the requirements classad expression in the
      submit files with a value matching the FileSystemDomain of the machines
      from that site. For example, the user migrates nodes from the ISI
      cluster (with FileSystemDomain viz.isi.org) into a Condor pool and
      specifies FileSystemDomain == “viz.isi.edu”. Condor would then schedule
      the jobs only on the nodes from the ISI VIZ cluster in the local condor
      pool. The FileSystemDomain can be specified for an execution site in the
      site catalog with condor profile namespace as follows</para>

      <programlisting> &lt;site  handle="isi_viz" arch="x86" os="LINUX" osrelease="" osversion="" glibc=""&gt;
       &lt;grid  type="gt2" contact="smarty.isi.edu/jobmanager-pbs" scheduler="PBS" jobtype="auxillary"/&gt;
       &lt;grid  type="gt2" contact="smarty.isi.edu/jobmanager-pbs" scheduler="PBS" jobtype="compute"/&gt;
       &lt;head-fs&gt;
              &lt;scratch&gt;
                    &lt;shared&gt;
                         &lt;file-server protocol="gsiftp" url="gsiftp://viz-login.isi.edu" \
                                                                 mount-point="/nfs/scratch01"/&gt;
                         &lt;internal-mount-point mount-point="/nfs/scratch01"/&gt;
                    &lt;/shared&gt;
              &lt;/scratch&gt;
              &lt;storage&gt;
                     &lt;shared&gt;
                          &lt;file-server protocol="gsiftp" url="gsiftp://viz-login.isi.edu" \
                                                                      mount-point="/exports/storage01"/&gt;
                          &lt;internal-mount-point mount-point="/exports/storage01"/&gt;
                     &lt;/shared&gt;
              &lt;/storage&gt;
       &lt;/head-fs&gt;
       &lt;replica-catalog  type="LRC" url="rlsn://smarty.isi.edu"/&gt;
       <emphasis role="bold">&lt;profile namespace="pegasus" key="style"&gt;glidein&lt;/profile&gt;</emphasis>
       <emphasis role="bold">&lt;profile namespace="condor" key="universe"&gt;vanilla&lt;/profile&gt; 
       &lt;profile namespace="condor" key="FileSystemDomain"&gt;viz.isi.edu&lt;/profile&gt;</emphasis>
 &lt;/site&gt;
</programlisting>

      <para>Specifying the FileSystemDomain key in condor namespace for a
      site, triggers Pegasus into generating the requirements classad
      expression in the submit file for all the jobs scheduled on that
      particular site. For example, in the above case all jobs scheduled on
      site isi_condor would have the following expression in the submit
      file.</para>

      <programlisting>requirements = FileSystemDomain == “viz.isi.edu”</programlisting>
    </section>

    <section>
      <title>Condor Glidein's using CorralWMS</title>

      <para>Mats to add description here.</para>
    </section>

    <section>
      <title>Glite</title>

      <para>This section describes the various changes required in the site
      catalog for Pegasus to generate an executable workflow that uses gLite
      blahp to directly submit to PBS on the local machine. This mode of
      submission should only be used when the condor on the submit host can
      directly talk to scheduler running on the cluster. It is recommended
      that the cluster that gLite talks to is designated as a separate compute
      site in the Pegasus site catalog. To tag a site as a gLite site the
      following two profiles need to be specified for the site in the site
      catalog</para>

      <orderedlist>
        <listitem>
          <para><emphasis role="bold">pegasus</emphasis> profile <emphasis
          role="bold">style</emphasis> with value set to <emphasis
          role="bold">glite</emphasis>.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">condor</emphasis> profile <emphasis
          role="bold">grid_resource</emphasis> with value set to <emphasis
          role="bold">pbs|lsf</emphasis></para>
        </listitem>
      </orderedlist>

      <para>An example site catalog entry for a glite site looks as follows in
      the site catalog</para>

      <programlisting> &lt;site  handle="isi_viz_glite" arch="x86" os="LINUX" osrelease="" osversion="" glibc=""&gt;
       &lt;grid  type="gt2" contact="smarty.isi.edu/jobmanager-pbs" scheduler="PBS" jobtype="auxillary"/&gt;
       &lt;grid  type="gt2" contact="smarty.isi.edu/jobmanager-pbs" scheduler="PBS" jobtype="compute"/&gt;
       &lt;head-fs&gt;
             &lt;scratch&gt;
                   &lt;shared&gt;
                         &lt;file-server protocol="gsiftp" url="gsiftp://viz-login.isi.edu" \
                                                       mount-point="/nfs/scratch01"\&gt;
                         &lt;internal-mount-point mount-point="/nfs/scratch01"/&gt;
                   &lt;/shared&gt;
             &lt;/scratch&gt;
             &lt;storage&gt;
                   &lt;shared&gt;
                         &lt;file-server protocol="gsiftp" url="gsiftp://viz-login.isi.edu" \
                                                mount-point="/exports/storage01"\&gt;
                         &lt;internal-mount-point mount-point="/exports/storage01"/&gt;
                    &lt;/shared&gt;
             &lt;/storage&gt;
       &lt;/head-fs&gt;
       &lt;replica-catalog  type="LRC" url="rlsn://smarty.isi.edu" /&gt;
               
       <emphasis role="bold">&lt;!-- following profiles reqd for glite grid style--&gt;
       &lt;profile namespace="pegasus" key="style"&gt;glite&lt;/profile&gt; 
       &lt;profile namespace="condor" key="grid_resource"&gt;pbs&lt;/profile&gt;</emphasis>
 &lt;/site&gt;
</programlisting>

      <para></para>

      <section>
        <title>Changes to Jobs</title>

        <para>As part of applying the style to the job, this style adds the
        following classads expressions to the job description.</para>

        <orderedlist>
          <listitem>
            <para>+remote_queue - value picked up from globus profile
            queue</para>
          </listitem>

          <listitem>
            <para>+remote_cerequirements - See below</para>
          </listitem>
        </orderedlist>

        <para></para>

        <section>
          <title>Remote CE Requirements</title>

          <para>The remote CE requirements are constructed from the following
          profiles associated with the job. The profiles for a job are derived
          from various sources</para>

          <orderedlist>
            <listitem>
              <para>transformation catalog</para>
            </listitem>

            <listitem>
              <para>site catalog</para>
            </listitem>

            <listitem>
              <para>DAX</para>
            </listitem>

            <listitem>
              <para>user properties</para>
            </listitem>
          </orderedlist>

          <para>The following globus profiles if associated with the job are
          picked up and translated to corresponding glite key</para>

          <orderedlist>
            <listitem>
              <para>hostcount -&gt; PROCS</para>
            </listitem>

            <listitem>
              <para>count -&gt; NODES</para>
            </listitem>

            <listitem>
              <para>maxwalltime -&gt; WALLTIME</para>
            </listitem>
          </orderedlist>

          <para>The following condor profiles if associated with the job are
          picked up and translated to corresponding glite key</para>

          <orderedlist>
            <listitem>
              <para>priority -&gt; PRIORITY</para>
            </listitem>
          </orderedlist>

          <para>All the env profiles are translated to MYENV</para>

          <para>The remote_cerequirements expression is constructed on the
          basis of the profiles associated with job . An example
          +remote_cerequirements classad expression in the submit file is
          listed below</para>

          <programlisting><emphasis role="bold">+remote_cerequirements = "PROCS==18 &amp;&amp; NODES==1 &amp;&amp; PRIORITY==10 &amp;&amp; WALLTIME==3600 \
   &amp;&amp; PASSENV==1 &amp;&amp; JOBNAME==\"TEST JOB\" &amp;&amp; MYENV ==\"JAVA_HOME=/bin/java,APP_HOME=/bin/app\""</emphasis></programlisting>
        </section>

        <section>
          <title>Specifying directory for the jobs</title>

          <para>gLite blahp does not follow the remote_initialdir or
          initialdir classad directives. Hence, all the jobs that have this
          style applied don't have a remote directory specified in the submit
          directory. Instead, Pegasus relies on kickstart to change to the
          working directory when the job is launched on the remote
          node.</para>
        </section>
      </section>
    </section>
  </section>
</chapter>