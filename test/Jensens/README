See chapter 8 of the user guide. Here is the ASCII-fied and newer
version for your convenience.


8	Test Scripts

There are a couple of test scripts available. This section will deal
with some integration tests that the user should run to verify that
Chimera can indeed be run successfully.

8.1	Verifying Your Foundation

Before you try to run any Chimera software, it is strongly recommended
that you verify your foundation software. The foundation software
consists of Globus, Condor-G and Java.

The picture to the right is an approximation of the interactions between
the foundation layers of software. The directory $VDS_HOME/test/test0
contains the verification software for the foundation layers.

In order to run the verification script test.pl, you will need to have
set up your Globus, Condor and Java. It is assumed that you run the
verification on the submit host. Thus, network access is mandatory. Also
remember to initialize your user certificate before running the test
script.

The test script is a Perl script. It is assumed that at least version
5.006 is installed, though version 5.6.1 is recommended. If you have any
problems with missing modules, you might want to refer to the contrib
directory for the missing module File::Temp. The test script is on
purpose kept compatible to older versions of Perl, thus not featuring
many convenience modules.

To find out about the options to steer the test script, run it with the
--help option. By default, the test assumes that the machine you run the
test upon has a gatekeeper with a regular jobmanager installed, and that
the host also features a grid-enabled ftp server. If you want to contact
other hosts, please look into the options --contact and --gridftp. You
can repeat these options to test multiple destinations. Personal
gatekeepers are also permissible.

Most tests are fairly thorough. There are three report levels. "OK"
means that the test ran all right. "Warning" constitutes an unsure
situation. The test is sure that something is not as it should be, but
it will try to continue testing, though it may fail later on. The
"error" message leads to a termination of the tests. Usually, some
diagnostic messages should help you to locate the problem with the
software or configuration underneath.

   Here are your results:

	   test suite globus......: OK
	   test suite gridftp.....: OK
	   test suite condor......: OK
	   test suite javavm......: OK
	   test suite chimera.....: OK

After running the test script, the final output should look like the
output above. Please note that in this release, some tests are not very
elaborate, and contain FIXME sections to remind the authors of tests to
be written. You can safely ignore the FIXME sections.

8.2	Running Chimera samples

The next samples run the kanonical executable for grids (keg) on remote
pools. The tests in the directories test1 and test2 are similarly built.
Both times, a script test.sh needs to be run, which requires a similar
set of command line arguments. The setup of the pool.config file in the
parent directory of the tests needs to be set up correctly. This section
describes the common setup, while the subsections describe the tests
themselves. They also deal with the tc.data setup.

The keg binary is provided as statically linked version for Linux in the
bin directory for your convenience. In order to run the tests, you need
to copy the keg binary into the pool. Please run it as follows on one
node of the pool to verify that indeed it is runnable in your pool. Mine
yields the following results - yours should look similar, but not
identical.

   $ bin/keg -o /dev/fd/1
   Timestamp Today: 20020828T132047-05:00 (1030558847.754;0.004)
   Applicationname: keg @ hamachi.cs.uchicago.edu (128.135.11.143)
   Current Workdir: /home/voeckler
   Systemenvironm.: i686-Linux 2.4.10-4GB-SMP
   Processor Info.: 2 x Pentium II (Deschutes) @ 400.916
   Output Filename: /dev/fd/1

The purpose of the keg binary is to copy all its input files onto all
its output files while appending each output file with the above
information stamp. Thus, keg can be used as a stand-in for real
applications, and helps to track the flow through a DAG. Please refer to
its manual page for more information.

While you are at it, please remember that you must pre-stage each
application. The kickstart and transfer applications are used by the
automatically generated transfer nodes. A statically linked version for
Linux can be found in the $VDS_HOME/bin/linux directory for your
convenience. While you are still logged into the remote pool, you might
as well want to check, if the binaries work for you. First, you have to
have a valid grid user proxy certificate - if not, run grid-proxy-init.
Then, try to obtain a file from some grid ftp service that you have
access to. Please note, even though we talk about grid ftp, the URL
schema is gsiftp.

Since you are in the remote pool anyway, create a directory
$HOME/vdldemo. The tests will stage into this directory, compare with
the pool.config file in the next section.

In the next step, you need to adjust the pool.config.in file in the
parent directory of the testX directories. Certain variables between at
signs, e.g. @USER@, are rewritten by the test scripts when generating a
finalized version. The pool file must contain the pool handles that you
intend to use. It is recommend to supply information pertaining to the
local pool handle, too, describing your submit machine. The format of
the pool.config file is described in section xx.x. The site handles are
case-sensitive.

In a similar fashion, for each site that you intend to use, you need to
enter the mapping from the abstract transformation name to the concrete
application name into the transformation catalog. The catalog, for the
purposes of tests, resides in a multi-column textual file, described in
section xx.x. You may want to edit the tc.data.in to point, for each
site, the eight transformations for the tests to the remote "keg"
installation. You must point, for each site, the location of
globus-url-copy, part of the Globus installation, and transfer, where
kickstart is installed.

The test scripts of the following sections have a uniform invocation
behavior. The script takes at least two mandatory parameters, and may be
augmented with further parameters:

o The --rls parameter takes the name of a replica location service. You 
  need to contact the administrators to create a replica catalog for 
  your perusal, see section xx. This is a mandatory parameter.

o The --run parameter takes one or more site handles as argument. The 
  site will be used to run the transformation(s) at. This is a mandatory
  parameter.

o The --src parameter takes a site handle as argument. The site handle
  usually describes your own submit host. The source will solely be used
  to create required initial data files, and to stage the file to the
  execution pool. The default for the source handle is local. It is
  highly recommended to use local -- if your local hosts happens to run
  a gridftp server. 

o The --dst site handle argument describes the final resting place for
  data. The data files will be staged out into this site, and
  registered in the replica catalog for this site. The default for the
  destination pool is local. It is highly recommended to use local -- if
  your local hosts happens to run a gridftp server.  

o The --stop-after-cplan argument is intended for debugging purposes. It
  stops the script after the concrete planner generated the necessary
  information for Condor DAGMan, but before submitting the DAG to
  Condor.

The handles for source, execution and destination pool may at your
choice be all the same, or differ in any manner. It is recommended,
however, to use the submit host as destination site, and set up the site
handle local accordingly, if applicable.

Before running any of the further tests, it is highly recommended to
clean out your portion of the replica catalog.

8.2.1	Hello World

The hello world example is a very simple one job, one input file, one
output file simulation. The test can be found in the
$VDS_HOME/test/test1 directory. The hello world example runs the keg
binary to convert the input data into the output data.

   TR world( input a, output b )
   {
     argument = "-a hello";
     argument = " -i "${input:a};
     argument = " -o "${output:b};
   }
   DV hello->world( a=@{input:"data.in"}, b=@{output:"data.out"} );

Please refer to appendix 9 for a description of VDLt.

The transformation in this case is called world. It takes an input file
argument a and an output file argument b to run. Within, the command
line of the application linked to the transformation by the
transformation catalog in the tc.data file, will be invoked with a
string argument, the input filename and the output filename. The
transformation data is show in black in the dataflow chart.

The derivation hello instantiates the world transformation. It passes
the file data.in for argument a and data.out for argument b. The
derivation is shown in blue (gray) in above diagram.

The filenames refer to logical files. The replica catalog provides the
mapping between these logical filenames and their physical counterparts.
The test.sh script populates the replica catalog with the logical file
data.in, and removes any occurances of data.out.

Before running the hello world test from its test script test.sh, it is
strongly recommended that you open the supplied tc.data in an editor,
and modify it to suit your own configuration. Its syntax is described
elsewhere in this document. Make sure that you have an entry for the
world transformation and the pool you chose to run in. It should map to
the pre-staged keg executable. Also make sure that the pool has entries
for gsincftpget and gsincftpput, and that these applications are
available in the run pool. Finally, make sure that the submit site is
the local pool handle, has an entry for GriphynRC, and that it maps to
the replica-catalog wrapper for the submit host.

A sample minimal tc.data is shown below, with the entry for the RC
abbreviated:

The output file data.out will be displayed after a successful run. If
the host chalant is the local pool, which is also the recommended
destination pool, you might be able to see something similar to the
following result. The input file data.in is repeated in the result. It
is just a concatenation of the current date output with the output from
hostname -f.

   chalant:~/vdldemo $ head data.in data.out 
   ==> data.in <==
   Tue Aug 27 14:41:09 CDT 2002    chalant.mcs.anl.gov
   ==> data.out <==
   --- start data.in ----
     Tue Aug 27 14:41:09 CDT 2002  chalant.mcs.anl.gov
   --- final data.in ----
   Timestamp Today: 20020827T194153+00:00 (1030477313.408;0.009)
   Applicationname: hello @ grinux55.testux (172.16.17.200)
   Current Workdir: /users/voeckler/vdldemo
   Systemenvironm.: i686-Linux 2.2.19-6.2.16smp
   Processor Info.: 2 x Pentium III (Coppermine) @ 999.783
   Output Filename: data.out
   Input Filenames: data.in

8.2.2	A More Complex Example - The Black Diamond

The black diamond is a more complex version of the diamond DAG. The
black diamond is the final litmus test for ensuring that you will be
able to run your own Chimera examples successfully. The black diamond
test can be found in the $VDS_HOME/test/test2 directory.

Each transformation in the black diamond will run the keg binary. The
dataflow chart to the right only shows the derivation chain:

o The derivation top consumes one file f.a, and produces two files f.b1
  and f.b1.

o The derivations left and right both consume f.b1 and f.b2. The output
  files f.c1 and f.c2 are produced by these derivations respectively. As
  with the regular diamond, both left and right instantiate the same
  transformation.

o The derivation bottom takes the previously produced files, and
  combines them into the final result file f.d.

Again, when the test.sh script is run, the replica catalog is emptied of
any produced file. The local file $HOME/vdldemo/f.a will be produced and
registered with the RC for the source pool (this is the reason why we
recommend to use local as source pool). All other product files are
removed from the replica catalog. In order to create a location entry in
the RC for the destination pool, a dummy file is entered.

Before running the black diamond test from its test script test.sh, it
is strongly recommended that you open the supplied tc.data file in an
editor, and modify them to suit your own configuration. Its syntax is
described elsewhere in this document. Please make sure, as before, that
the transformations analyze, findrange and generate map to the keg
binary for your run pool, that it contains gsincftpget and gsincftpput
for the run pool, and a GriphynRC for the local pool.

The results from a successful run should display plenty of keg
information. It is too long to repeat here, but it should enable you to
trace the hosts that the jobs ran on.
