pegasus-mpi-cluster(1)
======================
:doctype: manpage


Name
----
pegasus-mpi-cluster - a tool for running computational workflows expressed as DAGs
(Directed Acyclic Graphs) on computational clusters using MPI.


Synopsis
--------
[verse]
*pegasus-mpi-cluster* [*-h*][*-v*][*-q*][*-s*][*-o* 'path'][*-e* 'path'] 
                    [*-m* 'M'][*-t* 'T'][*-n*][*-r*] 'workflow.dag'


Description
-----------
*pegasus-mpi-cluster* is a tool used to run HTC (High Throughput Computing) scientific
workflows on systems designed for HPC (High Performance Computing).
Many HPC systems have custom architectures that are optimized for
tightly-coupled, parallel applications. These systems commonly have
exotic, low-latency networks that are designed for passing short 
messages very quickly between compute nodes. Many of these networks are so
highly optimized that the compute nodes do not even support a TCP/IP
stack. This makes it impossible to run HTC applications using software
that was designed for commodity clusters, such as Condor.

*pegasus-mpi-cluster* was developed to enable loosely-coupled HTC applications such as
scientific workflows to take advantage of HPC systems. In order to get
around the network issues outlined above, *pegasus-mpi-cluster* uses MPI (Message
Passing Interface), a commonly used API for writing SPMD (Single
Process, Multiple Data) parallel applications. Most HPC systems have an
MPI implementation that works on whatever exotic network architecture
the system uses.

An *pegasus-mpi-cluster* job consists of a single master process (this process is rank
0 in MPI parlance) and several worker processes. The master process
manages the workflow and assigns workflow tasks to workers for execution. 
The workers execute the tasks and return the results to the master. Any 
output written to stdout or stderr by the tasks is captured (see 
<<TASK_STDIO,*TASK STDIO*>>).

*pegasus-mpi-cluster* applications are expressed as DAGs (Directed Acyclic Graphs)
(see <<DAG_FILES,*DAG FILES*>>). Each node in the graph represents a task, 
and the edges represent dependencies between the tasks that constrain the 
order in which the tasks are executed. Each task is a program and a set of
parameters that need to be run (i.e. a command and some optional 
arguments). The dependencies typically represent data flow dependencies in
the application, where the output files produced by one task are needed
as inputs for another.

If an error occurs while executing a DAG that causes the workflow to
stop, it can be restarted using a rescue file, which records the
progress of the workflow (see <<RESCUE_FILES,*RESCUE FILES*>>). This 
enables *pegasus-mpi-cluster* to pick up running the workflow where it stopped.

*pegasus-mpi-cluster* allows applications expressed as a DAG to be executed in 
parallel on a large number of compute nodes. It is designed to be simple,
lightweight and robust.


Options
-------
*-h*::
*--help*::
    Print help message

*-v*::
*--verbose*::
    Increase logging verbosity. Adding multiple *-v* increases the
    level more. The default log level is 'INFO'. (see <<LOGGING,
    *LOGGING*>>)

*-q*::
*--quiet*::
    Decrease logging verbosity. Adding multiple *-q* decreases the
    level more. The default log level is 'INFO'. (see <<LOGGING,
    *LOGGING*>>)

*-s*::
*--skip-rescue*::
    Ignore the rescue file for 'workflow.dag' if it exists. Note that
    *pegasus-mpi-cluster* will still create a new rescue file for the 
    current run. The default behavior is to use the rescue file if one 
    is found. (see <<RESCUE_FILES,*RESCUE FILES*>>)

*-o* 'path'::
*--stdout* 'path'::
    Path to file for task stdout. (see <<TASK_STDIO,*TASK STDIO*>>)

*-e* 'path'::
*--stderr* 'path'::
    Path to file for task stderr. (see <<TASK_STDIO,*TASK STDIO*>>)

*-m* 'M'::
*--max-failures* 'M'::
    Stop submitting new tasks after 'M' tasks have failed. Once 'M' 
    has been reached, *pegasus-mpi-cluster* will finish running any tasks that have
    been started, but will not start any more tasks. This option is
    used to prevent *pegasus-mpi-cluster* from continuing to run a workflow that 
    is suffering from a systematic error, such as a missing binary or
    an invalid path. The default for 'M' is 0, which means unlimited 
    failures are allowed.

*-t* 'T'::
*--tries* 'T'::
    Attempt to run each task 'T' times before marking the task as failed.
    Note that the 'T' tries do not count as failures for the purposes of the
    *-m* option. A task is only considered failed if it is tried 'T' times
    and all 'T' attempts result in a non-zero exitcode. The value of 'T' must 
    be at least 1. The default is 1.

*-n*::
*--nolock*::
    Do not lock DAGFILE. By default, *pegasus-mpi-cluster* will attempt to acquire
    an exclusive lock on DAGFILE to prevent multiple MPI jobs from running the same
    DAG at the same time. If this option is specified, then the lock will not be
    acquired.

*-r*::
*--rescue* 'path'::
    Path to rescue log. If the file exists, and *-s* is not specified, then 
    the log will be used to recover the state of the workflow. The file is truncated
    after it is read and a new rescue log is created in its place. The default is to
    append '.rescue' to the DAG file name. (see <<RESCUE_FILES,*RESCUE FILES*>>)

*--host-script* 'path'::
    Path to a script or executable to launch on each unique host that 
    *pegasus-mpi-cluster* is running on. (see <<HOST_SCRIPTS,*HOST SCRIPTS*>>)

*--host-memory* 'size'::
    Amount of memory available on each host in MB. The default is to determine the
    amount of physical RAM automatically. (see <<RESOURCE_SCHED, *RESOURCE-BASED 
    SCHEDULING*>>)

*--host-cpus* 'cpus'::
    Number of CPUs available on each host. The default is to determine the number of
    CPU cores automatically. (see <<RESOURCE_SCHED, *RESOURCE-BASED SCHEDULING*>>)

*--strict-limits*::
    This enables strict memory usage limits for tasks. When this option is specified,
    and a task tries to allocate more memory than was requested in the DAG, the memory
    allocation operation will fail.


[[DAG_FILES]]
DAG Files
---------
*pegasus-mpi-cluster* workflows are expressed using a simple text-based format similar
to that used by Condor DAGMan. There are only two record types allowed
in a DAG file: *TASK* and *EDGE*. Any blank lines in the DAG (lines with
all whitespace characters) are ignored, as are any lines beginning with
# (note that # can only appear at the beginning of a line, not in the
middle).

The format of a *TASK* record is:
[verse]
    *TASK* 'id' ['options...'] 'executable' ['arguments...']

Where 'id' is the ID of the task, 'options' is a list of task options, 'executable' is 
the path to the executable or script to run, and 'arguments...' is a space-separated 
list of arguments to pass to the task. An example is:
------------
    TASK t01 -m 10 /bin/program -a -b
------------
This example specifies a task 't01' that requires 10 MB memory to run '/bin/program'
with the arguments '-a' and '-b'. The available task options are:

    *-m* 'M'::
    *--request-memory* 'M'::
        The amount of memory required by the task in MB. (see <<RESOURCE_SCHED, 
        *RESOURCE-BASED SCHEDULING*>>)
    
    *-c* 'N'::
    *--request-cpus* 'N'::
        The number of CPUs required by the task. (see <<RESOURCE_SCHED,
        *RESOURCE-BASED SCHEDULING*>>)

The format of an *EDGE* record is:
[verse]
    *EDGE* 'parent' 'child'

Where 'parent' is the ID of the parent task, and 'child' is the ID of the
child task. An example *EDGE* record is:
------------
    EDGE t01 t02
------------
A simple diamond-shaped workflow would look like this:
------------
    # diamond.dag
    TASK A /bin/echo "I am A"
    TASK B /bin/echo "I am B"
    TASK C /bin/echo "I am C"
    TASK D /bin/echo "I am D"

    EDGE A B
    EDGE A C
    EDGE B D
    EDGE C D
------------


[[RESCUE_FILES]]
Rescue Files
------------
Many different types of errors can occur when running a DAG. One or
more of the tasks may fail, the MPI job may run out of wall time,
*pegasus-mpi-cluster* may segfault (we hope not), the system may crash, etc. In order
to ensure that the DAG does not need to be restarted from the beginning
after an error, *pegasus-mpi-cluster* generates a rescue file for each workflow.

The rescue file is a simple text file that lists all of the tasks in
the workflow that have finished successfully. This file is updated each
time a task finishes, and is flushed periodically so that if the work-
flow fails and the user restarts it, *pegasus-mpi-cluster* can determine which tasks
still need to be executed. As such, the rescue file is a sort-of trans-
action log for the workflow.

The rescue file contains zero or more DONE records. The format of these
records is:
[verse]
    *DONE* 'taskid'

Where 'taskid' is the ID of the task that finished successfully.

By default, rescue files are named 'DAGNAME.rescue' where 'DAGNAME' is the
path to the input DAG file. The file name can be changed by specifying the
*-r* argument.


[[LOGGING]]
Logging
-------
By default, all logging messages are printed to stderr. If you turn up
the logging using *-v* then you may end up with a lot of stderr being
forwarded from the workers to the master.

The log levels in order of severity are: FATAL, ERROR, WARN, INFO, DEBUG, 
and TRACE.

The default logging level is INFO. The logging levels can be
increased with *-v* and decreased with *-q*.


[[TASK_STDIO]]
Task STDIO
----------
By default the stdout and stderr of tasks will be redirected to the master's
stdout and stderr. You can change the path of these files with the *-o* and 
*-e* arguments. Note that the stdio of all workers will be merged into one 
out and one err file by the master at the end, so I/O from different workers 
will not be interleaved, but I/O from each worker will appear in the order 
that it was generated. Also note that, if the job fails for any reason, the outputs 
will not be merged, but instead there will be one file for each worker 
named DAGFILE.MPID.out.X and DAGFILE.MPID.err.X, where DAGFILE is the path to
the input DAG, MPID is the master's process ID, and 'X' is the worker's rank.


[[HOST_SCRIPTS]]
Host Scripts
------------
A host script is a shell script or executable that *pegasus-mpi-cluster*
launches on each unique host on which it is running. They can be used to 
start auxilliary services, such as memcached, that the tasks in a workflow
require.

Host scripts are specified using either the *--host-script* argument or
the *PMC_HOST_SCRIPT* environment variable.

The host script is started when *pegasus-mpi-cluster* starts and must exit 
with an exitcode of 0 before any tasks can be executed. If it the host script
returns a non-zero exitcode, then the workflow is aborted. The host script
is given 60 seconds to do any setup that is required. If it doesn't exit
in 60 seconds then a SIGALRM signal is delivered to the process, which, if not
handled, will cause the process to terminate.

When the workflow finishes, *pegasus-mpi-cluster* will deliver a SIGTERM 
signal to the host script's process group. Any child processes left running
by the host script will receive this signal unless they created their own
process group. If there were any processes left to receive this signal, then
they will be given a few seconds to exit, then they will be sent SIGKILL. 
This is the mechanism by which processes started by the host script can be 
informed of the termination of the workflow.


[[RESOURCE_SCHED]]
Resource-Based Scheduling
-------------------------

High-performance computing resources often have a low ratio of memory to CPUs.
At the same time, workflow tasks often have high memory requirements. Often,
the memory requirements of a workflow task exceed the amount of memory
available to each CPU on a given host. As a result, it may be necessary to
disable some CPUs in order to free up enough memory to run the tasks. Similarly,
many codes have support for multicore hosts. In that case it is necessary for
efficiency to ensure that the number of cores required by the tasks running on 
a host do not exceed the number of cores available on that host.

In order to make this process more efficient, *pegasus-mpi-cluster* supports
resource-based scheduling. In resource-based scheduling the tasks in the
workflow can specify how much memory and how many CPUs they require, and 
*pegasus-mpi-cluster* will schedule them so that the tasks running on a given 
host do not exceed the amount of physical memory and CPUs available. This 
enables *pegasus-mpi-cluster* to take advantage of all the CPUs available 
when the tasks' memory requirement is low, but also disable some CPUs when 
the tasks' memory requirement is higher. It also enables workflows with a 
mixture of single core and multi-core tasks to be executed on a heterogenous
pool.

If there are no hosts available that have enough memory and CPUs to execute
one of the tasks in a workflow, then the workflow is aborted.

Memory
~~~~~~
Users can specify both the amount of memory required per task, and the amount
of memory available per host. If the amount of memory required by any task
exceeds the available memory of all the hosts, then the workflow will be
aborted. By default, the host memory is determined automatically, however the
user can specify *--host-memory* to "lie" to *pegasus-mpi-cluster*. The amount
of memory required for each task is specified in the DAG using the 
*-m*/*--request-memory* argument (see <<DAG_FILES, *DAG Files*>>).

CPUs
~~~~
Users can specify the number of CPUs required per task, and the total number of
CPUs available on each host. If the number of CPUs required by a task exceeds
the available CPUs on all hosts, then the workflow will be aborted. By default,
the number of CPUs on a host is determined automatically, but the user can 
specify *--host-cpus* to over- or under-subscribe the host. The number of 
CPUs required for each task is specified in the DAG using the 
*-c*/*--request-cpus* argument (see <<DAG_FILES, *DAG Files*>>).


Misc
----
Resource Utilization
~~~~~~~~~~~~~~~~~~~~
At the end of the workflow run, the master will report the resource
utilization of the job. This is done by adding up the total runtimes
of all the tasks executed (including failed tasks) and dividing by
the total wall time of the job times N, where N is both the total 
number of processes including the master, and the total number of
workers. These two resource utilization values are provided so that
users can get an idea about how efficiently they are making use of
the resources they allocated. Low resource utilization values suggest
that the user should use fewer cores, and longer wall time, on future 
runs, while high resource utilization values suggest that the user 
could use more cores for future runs and get a shorter wall time.


Known Issues
------------
fork() and exec()
~~~~~~~~~~~~~~~~~
In order for the worker processes to start tasks on the compute
node the compute nodes must support the *fork()* and *exec()* system
calls. If your target machine runs a stripped-down OS on the
compute nodes that does not support these system calls, then
*pegasus-mpi-cluster* will not work.

CPU Usage
~~~~~~~~~
Many MPI implementations are optimized so that message sends and
receives do not block. The reasoning is that blocking adds over-
head and, since many HPC systems use space sharing on dedicated
hardware, there are no other processes competing, so spinning
instead of blocking can produce better performance. On those
MPI implementations the master and worker processes will run at
100% CPU usage even when they are waiting. If this is a problem
on your system, then there are some MPI implementations that
'do' block on message send and receive. To test *pegasus-mpi-cluster*, for
example, we use MPICH2 with the ch3:sock device instead of the
ch3:nemesis device to avoid this issue.


Environment Variables
---------------------

*PMC_HOST_SCRIPT*::
    Path to a script to launch on each host. This variable is an alias
    for *--host-script*. If it is defined then it is used as the default
    value for *--host-script*. If both are defined then the value for
    *--host-script* is used.

Author
------
Gideon Juve `<gideon@isi.edu>`

Mats Rynge `<rynge@isi.edu>`
