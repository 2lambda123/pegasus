<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN"
"http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<chapter id="quickstart">
  <title>Pegasus Quick Start Guide</title>

  <section>
    <title>Getting Started</title>

    <para>This document is structured to get you and up running with the
    Pegasus planner with minimal effort.</para>

    <para>Download the latest release of Pegasus WMS 3.0 from <ulink
    url="http://pegasus.isi.edu/wms/download.php">http://pegasus.isi.edu/wms/download.php</ulink></para>

    <para>For convenience, there are Pegasus WMS tarballs which has Condor
    included. This way you can instlal Pegasus and Condor at the same time,
    with just minor configuration to get up and running.</para>

    <para>To run Pegasus you will need</para>

    <itemizedlist>
      <listitem>
        <para>JAVA 1.6</para>
      </listitem>

      <listitem>
        <para>Python 2.6</para>
      </listitem>

      <listitem>
        <para>Perl</para>
      </listitem>
    </itemizedlist>

    <para>Check Java version and ensure it is JAVA 1.6.x</para>

    <screen><command>$ java -version</command><computeroutput>

java version "1.5.0_07"
Java(TM) 2 Runtime Environment, Standard Edition (build 1.5.0_07-164)
Java HotSpot(TM) Client VM (build 1.5.0_07-87, mixed mode, sharing)</computeroutput></screen>

    <para>To install Pegsus</para>

    <itemizedlist>
      <listitem>
        <para>Untar the tarball. The code will be untarred in a directory
        pegasus-&lt;version&gt;. Lets call this PEGASUS_HOME. Change
        PEGASUS_HOME on the command line to the directory where you untarred
        the code.</para>

        <para><screen><code><command>$ tar zxf pegasus-wms-*.tar.gz</command></code></screen></para>
      </listitem>

      <listitem>
        <para>Edit the Condor Configuration file located at
        PEGASUS_HOME/etc/condor_config (Additional configuration may be
        required for Production Environment).</para>

        <orderedlist>
          <listitem>
            <para>Change !!PEGASUS_HOME!! to the actual path where PEGASUS_WMS
            is installed</para>
          </listitem>

          <listitem>
            <para>CHANGE !!USER!! to the user who will receive email in case
            of error. (This can generally be just your username )</para>
          </listitem>
        </orderedlist>

        <para><screen><command>$ vim $PEGASUS_HOME/etc/condor_config</command><computeroutput>

RELEASE_DIR = !!PEGASUS_HOME!!    # CHANGE THIS TO PATH OF PEGASUS_WMS INSTALLATION

CONDOR_ADMIN = !!USER!! # CHANGE THIS To THE USER WHO wILL GET EMAIL INCASE OF ERROR.</computeroutput></screen></para>
      </listitem>

      <listitem>
        <para>Set up the environment:</para>

        <para><screen><command>$ export PATH=PEGASUS_HOME/bin:PEGASUS_HOME/condor/bin:$PATH
$ export CONDOR_CONFIG=PEGASUS_HOME/etc/condor_config</command></screen></para>
      </listitem>

      <listitem>
        <para>Start Condor by running ./sbin/condor_master</para>

        <para><screen><command>$ PEGASUS_HOME/sbin/condor_master</command></screen></para>
      </listitem>

      <listitem>
        <para>Check if condor is up by running the condor_q command</para>

        <para><screen><command>$ condor_q</command>

<computeroutput>-- Submitter: gmehta@smarty.isi.edu : &lt;128.9.72.26:60126&gt; : smarty.isi.edu
 ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD               

0 jobs; 0 idle, 0 running, 0 held</computeroutput></screen></para>
      </listitem>
    </itemizedlist>

    <para>This guide assumes you are using a Bourne Shell derived shell. e.g.
    /bin/sh or /bin/bash.</para>
  </section>

  <section>
    <title>Abstract Workflow (DAX)</title>

    <para>Pegasus uses an XML format to describe the input abstract workflow
    (DAX). The DAX is divided into 5 sections.</para>

    <itemizedlist>
      <listitem>
        <para><emphasis role="bold">File List:</emphasis> This section lists
        all the files used in the workflow. They can be of type input, output,
        inout or executable</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">Tasks:</emphasis> This section lists all
        the tasks or jobs in the workflow. Along with the tasks are arguments
        required to invoke the tasks, any profiles that may be associated with
        the tasks and any files used as input or output by the task.</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">Dependencies:</emphasis> This section
        lists all the dependencies betweeen the tasks in the workflow.</para>
      </listitem>
    </itemizedlist>

    <para>For an example DAX see the file
    $PEGASUS_HOME/examples/blackdiamond.dax</para>

    <para>A document describing various ways to generate a DAX and the DAX
    schema can be found on the Pegaus website at
    http://pegasus.isi.edu/doc.php</para>
  </section>

  <section>
    <title>Catalogs</title>

    <para>Pegasus uses several catalogs to help in its planning. These
    catalogs need to be setup before Pegasus can plan a workflow. The catalogs
    used by Pegasus are :</para>

    <itemizedlist>
      <listitem>
        <para><emphasis role="bold">Replica Catalog</emphasis><emphasis
        role="bold">:</emphasis> This catalog is used to lookup the location
        of input data as well as any existing output data referenced in the
        abstract workflow.</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">Site Catalog</emphasis><emphasis
        role="bold">:</emphasis> This catalog is used to track information
        about the various sites and their layout on the grid.</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">Transformation Catalog</emphasis><emphasis
        role="bold">:</emphasis> This catalog is used to lookup information
        about the location of executables (transformation) installed or
        available for staging on the Grid.</para>
      </listitem>
    </itemizedlist>

    <section>
      <title>Replica Catalog (RC)</title>

      <para>The Replica Catalog keeps mappings of logical file ids/names
      (LFN's) to physical file ids/names (PFN's). A single LFN can map to
      several PFN's. A PFN consists of a URL with protocol, host and port
      information and a path to a file. Along with the PFN one can also store
      additional key/value attributes to be associated with a PFN.</para>

      <para>Pegasus supports 3 different implemenations of the Replica
      Catalog.</para>

      <orderedlist>
        <listitem>
          <para>Simple File</para>
        </listitem>

        <listitem>
          <para>Database via JDBC</para>
        </listitem>

        <listitem>
          <para>Replica Location Service (RLS)</para>
        </listitem>
      </orderedlist>

      <para>In this guide we will only use the Simple File. The same client
      will however work for all 3 implementations. The implementation that the
      client talks to is configured using Pegasus properties.</para>

      <para>The black diamond example mentioned in Section 2 takes one input
      file, which we will have to tell the replica catalog. Create a file f.a
      by running the date command as shown</para>

      <programlisting>$ <emphasis>date &gt; $HOME/f.a </emphasis></programlisting>

      <para>We now need to register this file in the replica catalog using the
      rc-client. Replace the gsiftp://url with the appropriate parameters for
      your grid site.</para>

      <programlisting><emphasis>$ rc-client -Dpegasus.catalog.replica=SimpleFile -Dpegasus.catalog.replica.file=$HOME/rc \
insert f.a gsiftp://somehost:port/path/to/file/f.a pool=local</emphasis></programlisting>

      <para></para>

      <para>You may first want to check, if the file registeration made it
      into the replica catalog. Since we are using a Simple File catalog we
      can just go look at the file $HOME/rc to see if there are any entries in
      there.</para>

      <programlisting>$ <emphasis>cat $HOME/rc</emphasis>

# file-based replica catalog: 2007-07-10T17:52:53.405-07:00
f.a gsiftp://smarty.isi.edu/nfs/asd2/gmehta/f.a pool="local"</programlisting>

      <para>The above line shows that entry for file f.a was made
      correctly.</para>

      <para>If you are using some other mode of RC you can also use the
      rc-client to look for entries.</para>

      <programlisting>$ <emphasis>rc-client -Dpegasus.catalog.replica=SimpleFile -Dpegasus.catalog.replica.file=$HOME/rc \
lookup LFN f.a
</emphasis>
f.a gsiftp://smarty.isi.edu/nfs/asd2/gmehta/f.a pool="local"</programlisting>

      <para>Your Replica Catalog is now set.</para>
    </section>

    <section>
      <title>Site Catalog (SC)</title>

      <para>The Site Catalog describes the compute resources (which are often
      clusters) that we intend to run the workflow upon. A site is a
      homogeneous part of a cluster that has at least a single GRAM gatekeeper
      with a jobmanager-fork and jobmanager-&lt;scheduler&gt; interface and at
      least one gridftp server along with a shared file system. The GRAM
      gatekeeper can be either WS GRAM or Pre-WS GRAM. A site can also be a
      condor pool or glidein pool with a shared file system.</para>

      <para>Pegasus currently supports two implementation of the Site
      Catalog</para>

      <orderedlist>
        <listitem>
          <para>XML</para>
        </listitem>

        <listitem>
          <para>File</para>
        </listitem>
      </orderedlist>

      <para>The format for the File is as follows</para>

      <programlisting>site site_id { 
  #required. Can be a dummy value if using Simple File RC
  lrc "rls://someurl"
  
  #required on a shared file system
  workdir "path/to/a/tmp/shared/file/sytem/" 

  #required one or more entries
  gridftp "gsiftp://hostname/mountpoint” "GLOBUS VERSION"
  
  #required one or more entries
  universe transfer "hostname/jobmanager-&lt;scheduler&gt;" "GLOBUS VERSION"
  
  #reqired one or more entries 
  universe vanilla "hostname/jobmanager-&lt;scheduler&gt;" "GLOBUS VERSION"
  
  #optional
  sysinfo  "ARCH::OS:OSVER:GLIBC"

  #optional 
  gridlaunch "/path/to/gridlaunch/executable" 
  
  #optional zero or more entries
  profile namespace "key" "value"
} </programlisting>

      <para>The gridlaunch and profile entries are optional. All the rest are
      required for each pool. Also the transfer and vanilla universe are
      mandatory. You can add multiple transfer and vanilla universe if you
      have more then one head node on the cluster. The entries in the Site
      Catalog have the following meaning:</para>

      <orderedlist>
        <listitem>
          <para>site - A site identifier.</para>
        </listitem>

        <listitem>
          <para>lrc - URL for a local replica catalog (LRC) to register your
          files in. Only used for RLS implementation of the RC</para>
        </listitem>

        <listitem>
          <para>workdir - A remote working directory (Should be on a shared
          file system)</para>
        </listitem>

        <listitem>
          <para>gridftp - A URL prefix for a remote storage location.</para>
        </listitem>

        <listitem>
          <para>universe - Different universes are supported which map to
          different batch jobmanagers.</para>

          <para>"vanilla" for compute jobs and "transfer" for transfer jobs
          are mandatory. Generally a transfer universe should map to the fork
          jobmanager.</para>
        </listitem>

        <listitem>
          <para>gridlaunch - Path to the remote kickstart tool (provenance
          tracking)</para>
        </listitem>

        <listitem>
          <para>sysinfo - The arch/os/osversion/glibc of the site. The format
          is ARCH::OS:OSVER:GLIBC where OSVERSION and GLIBC are
          optiona.</para>

          <para>ARCH can have one of the following values INTEL32, INTEL64,
          SPARCV7, SPARCV9, AIX, AMD64. OS can have one of the following
          values LINUX,SUNOS. The default value for sysinfo if none specified
          is INTEL32::LINUX</para>
        </listitem>

        <listitem>
          <para>Profiles - One or many profiles can be attached to a
          pool.</para>

          <para>One example is the environments to be set on a remote
          pool.</para>
        </listitem>
      </orderedlist>

      <para>Create a file called sites.txt and edit the contents. Replace
      ‘$HOME’ in the example below to your home directory.</para>

      <programlisting>$ emacs $HOME/sites.txt </programlisting>

      <para>Let’s say you have one cluster available to run your jobs called
      clus1. You need to add 2 sections in the pool.config.txt file, one for
      the cluster and one for the local machine. The local site entry is
      mandatory. Change all the entries below to reflect your local host and
      cluster setting including all environment variables</para>

      <programlisting>site local { 
lrc "rlsn://localhost" 
workdir "$HOME/workdir"
gridftp "gsiftp://localhost/$HOME/storage" "4.0.5" 
universe transfer "localhost/jobmanager-fork" "4.0.5" 
universe vanilla  "localhost/jobmanager-fork" "4.0.5" 
gridlaunch "/nfs/vdt/pegasus/bin/kickstart"
sysinfo "INTEL32::LINUX"
profile env "PEGASUS_HOME" "/nfs/vdt/pegasus" 
profile env "GLOBUS_LOCATION" "/vdt/globus" 
profile env "LD_LIBRARY_PATH" "/vdt/globus/lib" 
profile env "JAVA_HOME" /vdt/java 
} 

site clus1 { 
lrc "rlsn://clus1.com" 
workdir "$HOME/workdir-clus1" 
gridftp "gsiftp://clus1.com/jobmanager-fork" "4.0.3" 
universe transfer "clus1.com/jobmanager-fork" "4.0.3"  
universe vanilla  "clus1.com/jobmanager-pbs" "4.0.3" 
sysinfo "INTEL32::LINUX"
gridlaunch "/opt/nfs/vdt/pegasus/bin/kickstart"
profile env "PEGASUS_HOME" "/opt/nfs/vdt/pegasus"
profile env "GLOBUS_LOCATION" "/opt/vdt/globus" 
profile env "LD_LIBRARY_PATH" "/opt/vdt/globus/lib"  
} </programlisting>

      <para>This file can be used as is by Pegasus but we prefer the XML
      version of the Site Catalog because it allows for a richer description
      of the site. The tool to convert the text site catalog to XML is called
      sc-client</para>

      <programlisting>$ <emphasis>sc-client --files $HOME/sites.txt --output $HOME/sites.xml

</emphasis>
2007.07.10 19:04:34.799 PDT: [INFO] Reading $HOME/sites.txt
2007.07.10 19:04:34.844 PDT: [INFO] Reading $HOME/sites.txt (completed)
2007.07.10 19:04:34.851 PDT: [INFO] Written xml output to file : $HOME/sites.xml</programlisting>

      <para>Cat the sites.xml file and just take a look.</para>

      <programlisting>$ <emphasis>cat $HOME/sites.xml</emphasis>

&lt;sitecatalog xmlns="http://pegasus.isi.edu/schema/sitecatalog" 
  xsi:schemaLocation="http://pegasus.isi.edu/schema/sitecatalog 
  http://pegasus.isi.edu/schema/sc-2.0.xsd" 
  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" version="2.0"&gt;
  &lt;site handle="local" gridlaunch="/nfs/vdt/pegasus/bin/kickstart" 
   sysinfo="INTEL32::LINUX"&gt;
    &lt;profile namespace="env" key="PEGASUS_HOME" &gt;/nfs/vdt/pegasus&lt;/profile&gt;
    &lt;profile namespace="env" key="GLOBUS_LOCATION" &gt;/vdt/globus&lt;/profile&gt;
    &lt;profile namespace="env" key="LD_LIBRARY_PATH" &gt;/vdt/globus/lib&lt;/profile&gt;
    &lt;profile namespace="env" key="JAVA_HOME" &gt;/vdt/java&lt;/profile&gt;
    &lt;lrc url="rlsn://localhost" /&gt;
    &lt;gridftp  url="gsiftp://localhost" storage="/$HOME/storage" major="4" minor="0" 
     patch="5"&gt; 
    &lt;/gridftp&gt;
    &lt;jobmanager universe="transfer" url="localhost/jobmanager-fork" major="4" minor="0"
     patch="5" /&gt;
    &lt;jobmanager universe="vanilla" url="localhost/jobmanager-fork" major="4" minor="0"
     patch="5" /&gt;
    &lt;workdirectory &gt;$HOME/workdir&lt;/workdirectory&gt;
  &lt;/site&gt;
  &lt;site handle="clus1" gridlaunch="/opt/nfs/vdt/pegasus/bin/kickstart"
   sysinfo="INTEL32::LINUX"&gt;
    &lt;profile namespace="env" key="PEGASUS_HOME" &gt;/opt/nfs/vdt/pegasus&lt;/profile&gt;
    &lt;profile namespace="env" key="GLOBUS_LOCATION" &gt;/opt/vdt/globus&lt;/profile&gt;
    &lt;profile namespace="env" key="LD_LIBRARY_PATH" &gt;/opt/vdt/globus/lib&lt;/profile&gt;
    &lt;lrc url="rlsn://clus1.com" /&gt;
    &lt;gridftp  url="gsiftp://clus1.com" storage="/jobmanager-fork" major="4" minor="0"
     patch="3"&gt; 
    &lt;/gridftp&gt;
    &lt;jobmanager universe="transfer" url="clus1.com/jobmanager-fork" major="4" minor="0"
     patch="3" /&gt;
    &lt;jobmanager universe="vanilla" url="clus1.com/jobmanager-pbs" major="4" minor="0"
     patch="3" /&gt;
    &lt;workdirectory &gt;$HOME/workdir-clus1&lt;/workdirectory&gt;
  &lt;/site&gt;
&lt;/sitecatalog&gt;</programlisting>

      <para>The site catalog can also be generated using the pegasus-get-sites
      command for the OSG grid using the VORS catalog.</para>

      <para>To configure Pegasus to pick up this site catalog file we will add
      some entries in the Pegasus properties file in Section 4.</para>
    </section>

    <section>
      <title>Transformation Catalog (TC)</title>

      <para>The Transformation Catalog maps logical transformations to
      physical executables on the system. It also provides additional
      information about the transformation as to what system they are compiled
      for, what profiles or environment variables need to be set when the
      transformation is invoked etc.</para>

      <para>Pegasus currently supports two implementations of the
      Transformation Catalog</para>

      <orderedlist>
        <listitem>
          <para><emphasis role="bold">File: </emphasis>A simple multi column
          text file</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Database:</emphasis> A database backend
          (MySQL or PostgreSQL) via JDBC</para>
        </listitem>
      </orderedlist>

      <para>In this guide we will look at the format of the File based TC. The
      File based TC is also used as a bulk input for populating the Database
      implementation.</para>

      <para>The format of the File is as follows</para>

      <programlisting>#site  logicaltr   physicaltr   type  system  profiles(NS::KEY="VALUE")

site1 sys::date:1.0 /usr/bin/date  INSTALLED INTEL32::LINUX:FC4.2:3.6
                   ENV::PATH="/usr/bin";PEGASUS_HOME="/usr/local/pegasus"</programlisting>

      <para>The system and profile entries are optional and will use default
      values if not specified. The entries in the file format have the
      following meaning:</para>

      <orderedlist>
        <listitem>
          <para>site - A site identifier.</para>
        </listitem>

        <listitem>
          <para>logicaltr - The logical transformation name. The format is
          NAMESPACE::NAME:VERSION where NAMESPACE and NAME are
          optional.</para>
        </listitem>

        <listitem>
          <para>physicaltr - The physical transformation path or URL.</para>

          <para>If the transformation type is INSTALLED then it needs to be an
          absolute path to the executable. If the type is STATIC_BINARY then
          the path needs to be a HTTP, FTP or gsiftp URL</para>
        </listitem>

        <listitem>
          <para>type - The type of transformation. Can have on of two
          values</para>

          <itemizedlist>
            <listitem>
              <para>INSTALLED: This means that the transformation is installed
              on the remote site</para>
            </listitem>

            <listitem>
              <para>STATIC_BINARY: This means that the transformation is
              available as a static binary and can be staged to a remote
              site.</para>
            </listitem>
          </itemizedlist>
        </listitem>

        <listitem>
          <para>system - The system for which the transformation is
          compiled.</para>

          <para>The formation of the sytem is ARCH::OS:OSVERSION:GLIBC where
          the GLIBC and OS VERSION are optional. ARCH can have one of the
          following values INTEL32, INTEL64, SPARCV7, SPARCV9, AIX, AMD64. OS
          can have one of the following values LINUX,SUNOS. The default value
          for system if none specified is INTEL32::LINUX</para>
        </listitem>

        <listitem>
          <para>Profiles - The profiles associated with the transformation.
          For indepth information about profiles and their priorities read the
          Profile Guide.</para>

          <para>The format for profiles is NS::KEY="VALUE" where NS is the
          namespace of the profile e.g. Pegasus,condor,DAGMan,env,globus. The
          key and value can be any strings. Remember to quote the value with
          double quotes. If you need to specify several profiles you can do it
          in several ways</para>

          <itemizedlist>
            <listitem>
              <para>NS1::KEY1="VALUE1",KEY2="VALUE2";NS2::KEY3="VALUE3",KEY4="VALUE4"</para>

              <para>This is the most optimized form. Multiple key values for
              the same namespace are separated by a comma "," and different
              namespaces are separated by a semicolon ";"</para>
            </listitem>

            <listitem>
              <para>NS1::KEY1="VALUE1";NS1::KEY2="VALUE2";NS2::KEY3="VALUE3";NS2::KEY4="VALUE4"</para>

              <para>You can also just repeat the triple of NS::KEY="VALUE"
              separated by semicolons for a simple format;</para>
            </listitem>
          </itemizedlist>
        </listitem>
      </orderedlist>

      <para>We need to map our declared transformations (preprocess,
      findranage, and analyze) from the example DAX above to a simple "mock
      application" name "keg" ("canonical example for the grid") which reads
      input files designated by arguments, writes them back onto output files,
      and produces on STDOUT a summary of where and when it was run. Keg ships
      with Pegasus in the bin directory. Run keg on the command line to see
      how it works.</para>

      <programlisting>$<emphasis> keg -o /dev/fd/1</emphasis> 

Timestamp Today: 20040624T054607-05:00 (1088073967.418;0.022) 
Applicationname: keg @ 10.10.0.11 (VPN) 
Current Workdir: /home/unique-name
Systemenvironm.: i686-Linux 2.4.18-3 
Processor Info.: 1 x Pentium III (Coppermine) @ 797.425 
Output Filename: /dev/fd/1</programlisting>

      <para>Now we need to map all 3 transformations onto the "keg"
      executable. We place these mappings in our File transformation catalog
      for site clus1. In earlier version of Pegasus one had to define entries
      for Pegasus executables like transfer, replica client, dirmanager etc on
      each site as well as site "local". This is no longer required. Pegasus
      2.0 and later automatically picks up the paths for these binaries from
      the environment profile PEGASUS_HOME set in the site catalog for each
      site.</para>

      <para>Create a file called tc.data and edit the contents</para>

      <programlisting><emphasis>$ vim $HOME/tc</emphasis>


# Site  LFN  PFN  TYPE  SYSTEM PROFILE

clus1 black::preprocess:1.0  gsiftp://clus1.com/opt/nfs/vdt/pegasus/bin/keg
                     STATIC_BINARY INTEL32::LINUX ENV::KEY1="VALUE1"
clus1 black::findrange:1.0  gsiftp://clus1.com/opt/nfs/vdt/pegasus/bin/keg  
                     STATIC_BINARY INTEL32::LINUX ENV::KEY2="VALUE2"

</programlisting>

      <para>Note: A single entry needs to be on one line. The above example is
      just formatted for convenience.</para>

      <para>Alternatively you can also use the tc-client to add entries to any
      implementation of the transformation catalog. The following eg: shows us
      adding the last entry in the File based transformation catalog.</para>

      <programlisting>$ <emphasis>tc-client -Dpegasus.catalog.transformation=File \
-Dpegasus.catalog.transformation.file=$HOME/tc -a -r clus1 -l black::analyze:1.0 \
-p gsiftp://clus1.com/opt/nfs/vdt/pegasus/bin/keg  -t STATIC_BINARY -s INTEL32::LINUX \
-e ENV::KEY3="VALUE3"</emphasis>

2007.07.11 16:12:03.712 PDT: [INFO] Added tc entry sucessfully</programlisting>

      <para>To verify if the entry was correctly added to the transformation
      catalog you can use the tc-client to query.</para>

      <programlisting>$ <emphasis>tc-client -Dpegasus.catalog.transformation=File \
-Dpegasus.catalog.transformation.file=$HOME/tc -q -P -l black::analyze:1.0
</emphasis>
#RESID     LTX          PFN                  TYPE              SYSINFO

clus1    black::analyze:1.0    gsiftp://clus1.com/opt/nfs/vdt/pegasus/bin/keg    
                STATIC_BINARY    INTEL32::LINUX</programlisting>

      <para>We are now done with setting up all the catalogs.</para>
    </section>
  </section>

  <section>
    <title>Pegasus Properties</title>

    <para>Properties allow you to configure Pegasus in various ways by
    selection various catalog implementations, enabling and disabling various
    features or just additionally tuning them. Pegasus utilizes Java
    properties technology. Pegasus properties can be defined in several
    ways</para>

    <orderedlist>
      <listitem>
        <para>System Property File - Found in $PEGASUS_HOME/etc/properties.
        Lowest Priority</para>

        <para>These properties are generally set by the administrator to be
        shared by all users of the installation. They default location of
        $PEGASUS_HOME/etc/properties can be overridden by using the property
        pegasus.system.properties=&lt;path to a file&gt; on the command line
        or in the user properties file.</para>
      </listitem>

      <listitem>
        <para>User Property File - Found in the $HOME/.pegasusrc. Higher
        Priority</para>

        <para>Thse properties are generally set by a user to augment existing
        properties from the system property file or override a few setting.
        The default location of $HOME/.pegasusrc can be overridden by using
        the property pegasus.user.properties on the command line or in the
        system property file.</para>
      </listitem>

      <listitem>
        <para>Command Line Properties - Provided on the command line of any
        Pegasus executable. Highest Priority</para>

        <para>The command line properties are specified on the command line of
        any Pegasus clients by using -D&lt;property&gt;=&lt;value&gt;. These
        properties need to be defined before any arguments to the
        clients.</para>

        <para><emphasis role="bold">Note:</emphasis> There is no space between
        -D and the property name</para>
      </listitem>
    </orderedlist>

    <para>There is a priority to the order of reading and evaluating
    properties from these sources. The properties in the system file have the
    lowest priority, user properties override system properties and command
    line properties have the highest priority. Note that the values rely on
    proper capitalization, unless explicitly noted otherwise. Some properties
    if not specified may have default values. You can use variable
    substitution to use the value of one property in a second property. For
    instance, ${pegasus.home} means that the value depends on the value of the
    pegasus.home property plus any noted additions. You can use this notation
    to refer to other properties, though the extent of the substitutions are
    limited. Usually, you want to refer to a set of the standard system
    properties. Nesting is not allowed. Substitutions will only be done
    once.</para>

    <para>If you are in doubt which properties are actually visible, a sample
    application called show-properties dumps all properties after reading and
    prioritizing them.</para>

    <para>For running the above example we shall create a user properties file
    in $HOME/.pegasusrc and specify properties for Pegasus to find various
    catalogs</para>

    <programlisting><emphasis>$ vim $HOME/.pegasusrc</emphasis>


pegasus.catalog.replica=SimpleFile
pegasus.catalog.replica.file=$HOME/rc
pegasus.catalog.site=XML
pegasus.catalog.site.file=$HOME/sites.xml
pegasus.catalog.transformation=File
pegasus.catalog.transformation.file=$HOME/tc
pegasus.catalog.transformation.mapper=Staged
pegasus.dir.storage=pegasusstorage
pegasus.dir.exec=pegasusexec
pegasus.exitcode.scope=all</programlisting>

    <para>Substitute all $HOME and $USER variables above with actual values
    for your site.</para>

    <para>For a complete list of Pegasus properties refer to the
    properties.pdf in $PEGASUS_HOME/doc or on the Pegasus webiste at
    http://pegasus.isi.edu/doc.php</para>
  </section>

  <section>
    <title>Running Pegasus</title>

    <para>Pegasus plans an abstract workflow into a concrete/executable
    workflow by querying various catalogs and performing several refinement
    steps. We have already setup all the catalogs required by Pegasus in the
    previous sections and also configured properties that will affect how
    Pegasus plans the workflow.</para>

    <para>Pegasus planner is invoked on the command line by running the
    pegasus-plan command. The client takes several parameters including one or
    more sites where to compute the workflow, an optional single site if the
    output data needs to be staged and stored and the input DAX file.</para>

    <section>
      <title>Running Locally</title>

      <para>In this section we will be planning the blackdiamond workflow to
      run locally on the submit machine and store the data on the submit
      machine also. Invoke the pegasus-plan command with the --sites option
      set to local and also the --output option set to local. The -D dags
      option specifies to Pegasus to create the Dag and submit files in a
      directory called dags inside the current directory. Lastly specify the
      blackdiamond.dax file as the input DAX to the client.</para>

      <programlisting><emphasis>$ pegasus-plan --sites local --output local -D dags \
--dax $PEGASUS_HOME/examples/blackdiamond.dax</emphasis>

2007.07.11 17:55:28.118 PDT: [INFO] Parsing the DAX
2007.07.11 17:55:28.668 PDT: [INFO] Parsing the DAX (completed)
2007.07.11 17:55:28.725 PDT: [INFO] Parsing the site catalog 
2007.07.11 17:55:28.926 PDT: [INFO] Parsing the site catalog (completed)
2007.07.11 17:55:28.985 PDT: [INFO] Doing site selection
2007.07.11 17:55:29.025 PDT: [INFO] Doing site selection (completed)
2007.07.11 17:55:29.025 PDT: [INFO] Grafting transfer nodes in the workflow
2007.07.11 17:55:29.137 PDT: [INFO] Grafting transfer nodes in the workflow (completed)
2007.07.11 17:55:29.142 PDT: [INFO] Grafting the remote workdirectory creation jobs 
                                    in the workflow
2007.07.11 17:55:29.159 PDT: [INFO] Grafting the remote workdirectory creation jobs 
                                    in the workflow (completed)
2007.07.11 17:55:29.159 PDT: [INFO] Generating the cleanup workflow
2007.07.11 17:55:29.164 PDT: [INFO] Generating the cleanup workflow (completed)
2007.07.11 17:55:29.164 PDT: [INFO] Adding cleanup jobs in the workflow
2007.07.11 17:55:29.183 PDT: [INFO] Parsing the site catalog 
2007.07.11 17:55:29.223 PDT: [INFO] Parsing the site catalog (completed)
2007.07.11 17:55:29.248 PDT: [INFO] Adding cleanup jobs in the workflow (completed)
2007.07.11 17:55:29.280 PDT: [INFO] Generating codes for the concrete workflow
2007.07.11 17:55:29.476 PDT: [INFO] Generating codes for the concrete workflow(completed)
2007.07.11 17:55:29.477 PDT: [INFO] Generating code for the cleanup workflow
2007.07.11 17:55:29.515 PDT: [INFO] Generating code for the cleanup workflow (completed)
2007.07.11 17:55:29.520 PDT: [INFO] 


I have concretized your abstract workflow. The workflow has been entered 
into the workflow database with a state of "planned". The next step is 
to start or execute your workflow. The invocation required is


pegasus-run -Dpegasus.user.properties=/nfs/asd2/gmehta/PEGASUS\
/dags/gmehta/pegasus/black-diamond/run0001/pegasus.61698.properties \
--nodatabase /nfs/asd2/gmehta/PEGASUS/dags/gmehta/pegasus/black-diamond/run0001


2007.07.11 17:55:29.521 PDT: [INFO] Time taken to execute is 1.71 seconds</programlisting>

      <para>Looks like the planning worked. pegasus-plan tells you what
      command to use next to submit your workflow. Just copy the pegasus-run
      line from your output and run the command. pegasus-plan submits your
      planned workflow to Condor-G for execution either locally or on the
      grid.</para>

      <programlisting>$<emphasis> pegasus-run -Dpegasus.user.properties=/nfs/asd2/gmehta/PEGASUS/dags\
/gmehta/pegasus/black-diamond/run0001/pegasus.61698.properties \
--nodatabase /nfs/asd2/gmehta/PEGASUS/dags/gmehta/pegasus/black-diamond/run0001
</emphasis>
Checking all your submit files for log file names.
This might take a while... 
Done.
-----------------------------------------------------------------------
File for submitting this DAG to Condor           : black-diamond-0.dag.condor.sub
Log of DAGMan debugging messages                 : black-diamond-0.dag.dagman.out
Log of Condor library output                     : black-diamond-0.dag.lib.out
Log of Condor library error messages             : black-diamond-0.dag.lib.err
Log of the life of condor_dagman itself          : black-diamond-0.dag.dagman.log

Condor Log file for all jobs of this DAG         : /tmp/black-diamond-061699.log
-no_submit given, not submitting DAG to Condor.  You can do this with:
"condor_submit black-diamond-0.dag.condor.sub"
-----------------------------------------------------------------------
Submitting job(s).
Logging submit event(s).
1 job(s) submitted to cluster 22402.

I have started your workflow, committed it to DAGMan, and updated its
state in the work database. A separate daemon was started to collect
information about the progress of the workflow. The job state will soon
be visible. Your workflow runs in base directory. 

cd /nfs/asd2/gmehta/PEGASUS/dags/gmehta/pegasus/black-diamond/run0001

*** To monitor the workflow you can run ***

pegasus-status -w black-diamond-0 -t 20070711T175528-0700 
or
pegasus-status /nfs/asd2/gmehta/PEGASUS/dags/gmehta/pegasus/black-diamond/run0001

*** To remove your workflow run ***

pegasus-remove /nfs/asd2/gmehta/PEGASUS/dags/gmehta/pegasus/black-diamond/run0001</programlisting>

      <para>The workflow was successfully submitted for execution to the local
      submit host. The planned workflow Dag and submit files reside in the
      directory
      /nfs/asd2/gmehta/PEGASUS/dags/gmehta/pegasus/black-diamond/run0001 as
      mentioned by the output of pegasus-run. Pegasus run also prints out a
      few monitoring commands that we will use in Section 6.</para>

      <para>If you have a grid site configured the proceed to Section 5.2 else
      proceed to Section 6.</para>
    </section>

    <section>
      <title>Running on the Grid</title>

      <para>While the local workflow is executing let us submit another
      workflow to run on the Grid. We have a grid site clus1 set up for use.
      Replace the --sites local option in the previous run with --sites clus1.
      We will also need to add a a --force flag because we already ran the
      same workflow earlier locally and some out of the output products of the
      workflow may be registered in the Replica Catalog. Pegasus does workflow
      reduction based on existence of data products in the Replica Catalog.
      The --force option is a way to tell Pegasus to ignore existing data
      products and compute the entire workflow again.</para>

      <programlisting>$ <emphasis>pegasus-plan --sites clus1 --output local -D dags \
--dax $PEGASUS_HOME/examples/blackdiamond.dax --force</emphasis>          
2007.07.11 18:12:14.541 PDT: [INFO] Parsing the DAX
2007.07.11 18:12:15.287 PDT: [INFO] Parsing the DAX (completed)

...
...

2007.07.11 18:12:15.847 PDT: [INFO] Generating codes for the concrete workflow
2007.07.11 18:12:16.008 PDT: [INFO] Generating codes for the concrete workflow(completed)
2007.07.11 18:12:16.008 PDT: [INFO] Generating code for the cleanup workflow
2007.07.11 18:12:16.049 PDT: [INFO] Generating code for the cleanup workflow (completed)
2007.07.11 18:12:16.054 PDT: [INFO] 


I have concretized your abstract workflow. The workflow has been entered 
into the workflow database with a state of "planned". The next step is 
to start or execute your workflow. The invocation required is


pegasus-run -Dpegasus.user.properties=/nfs/asd2/gmehta/PEGASUS/dags\
/gmehta/pegasus/black-diamond/run0002/pegasus.7398.properties \
--nodatabase /nfs/asd2/gmehta/PEGASUS/dags/gmehta/pegasus/black-diamond/run0002


2007.07.11 18:12:16.055 PDT: [INFO] Time taken to execute is 1.918 seconds</programlisting>

      <para>Submit the planned workflow also to Condor-G using the pegasus-run
      command printed by the previous step.</para>

      <programlisting>$ <emphasis>pegasus-run -Dpegasus.user.properties=/nfs/asd2/gmehta/PEGASUS/dags\
/gmehta/pegasus/black-diamond/run0002/pegasus.7398.properties \
--nodatabase /nfs/asd2/gmehta/PEGASUS/dags/gmehta/pegasus/black-diamond/run0002
</emphasis>
...
...
 

cd /nfs/asd2/gmehta/PEGASUS/dags/gmehta/pegasus/black-diamond/run0002

*** To monitor the workflow you can run ***

pegasus-status -w black-diamond-0 -t 20070711T181215-0700 
or
pegasus-status /nfs/asd2/gmehta/PEGASUS/dags/gmehta/pegasus/black-diamond/run0002

*** To remove your workflow run ***

pegasus-remove -d 22417.0
or
pegasus-remove /nfs/asd2/gmehta/PEGASUS/dags/gmehta/pegasus/black-diamond/run0002</programlisting>
    </section>
  </section>

  <section>
    <title>Monitoring and Debugging</title>

    <para>The output in the previous section showed that the workflow was
    submitted for execution. The output also gave the relevant commands to use
    to monitor the progress of the workflow or to terminate it. Lets go to the
    directory where Pegasus generated the Condor Dag and submit files. Use the
    directory name from the output of your pegasus-run command, either the
    local run or the grid run.</para>

    <programlisting>$ <emphasis>cd /nfs/asd2/gmehta/PEGASUS/dags/gmehta/pegasus/black-diamond/run0002</emphasis></programlisting>

    <para>If you do a listing of the files in the directory you will see
    several .sub files where are the job submit files. The output of each job
    is written in the .out.* file and the error goes in the .err file.</para>

    <para>To monitor the execution of the workflow lets run the pegasus-status
    command as suggested by the output of the pegasus-run command
    above.</para>

    <programlisting>$ <emphasis>pegasus-status /nfs/asd2/gmehta/PEGASUS/dags/gmehta/pegasus/black-diamond/run0002</emphasis>


-- Submitter: smarty.isi.edu : &lt;128.9.72.26:53194&gt; : smarty.isi.edu
 ID      OWNER/NODENAME   SUBMITTED     RUN_TIME ST PRI SIZE CMD               
22417.0   gmehta          7/11 18:13   0+00:03:58 R  0   9.8  condor_dagman -f -
22423.0    |-rc_tx_analy  7/11 18:16   0+00:00:54 R  2   0.0  kickstart -n pegas
22424.0    |-rc_tx_findr  7/11 18:16   0+00:00:00 I  2   0.0  kickstart -n pegas
22425.0    |-rc_tx_prepr  7/11 18:16   0+00:00:00 I  2   0.0  kickstart -n pegas</programlisting>

    <para>The above output shows that several jobs are running under the main
    DAGMan process. Keep a lookout to track whether a workflow is running or
    not. If you do not see any of your job in the output for sometime (say 30
    seconds), we know the workflow has finished. We need to wait, as there
    might be delay in CondorDAGMAN releasing the next job into the queue after
    a job has finished successfully.</para>

    <para>If output of pegasus-status is empty, then either your workflow
    has</para>

    <itemizedlist>
      <listitem>
        <para>successfully completed</para>
      </listitem>

      <listitem>
        <para>stopped midway due to non recoverable error.</para>
      </listitem>
    </itemizedlist>

    <para>Another way to monitor the workflow is to check the jobstate.log
    file. This is the output file of the monitoring daemon that is parsing all
    the condor log files to determine the status of the jobs. It logs the
    events seen by Condor into a more readable form for us.</para>

    <programlisting>$ <emphasis>less jobstate.log</emphasis>

1184202818 INTERNAL *** DAGMAN_STARTED ***
1184202831 black-diamond_0_viz_cdir SUBMIT 22418.0 clus1 -
1184202846 black-diamond_0_viz_cdir EXECUTE 22418.0 clus1 -
1184202846 black-diamond_0_viz_cdir GLOBUS_SUBMIT 22418.0 clus1 -
1184202846 black-diamond_0_viz_cdir GRID_SUBMIT 22418.0 clus1 -
1184202977 black-diamond_0_viz_cdir JOB_TERMINATED 22418.0 clus1 -
1184202977 black-diamond_0_viz_cdir POST_SCRIPT_STARTED - clus1 -
1184202982 black-diamond_0_viz_cdir POST_SCRIPT_TERMINATED 22418.0 clus1 -
1184202982 black-diamond_0_viz_cdir POST_SCRIPT_SUCCESS - clus1 -

...
...

1184205172 new_rc_register_analyze_ID000004 POST_SCRIPT_SUCCESS - local -
1184205302 cln_analyze_ID000004 JOB_TERMINATED 22436.0 clus1 -
1184205302 cln_analyze_ID000004 POST_SCRIPT_STARTED - clus1 -
1184205307 cln_analyze_ID000004 POST_SCRIPT_TERMINATED 22436.0 clus1 -
1184205307 cln_analyze_ID000004 POST_SCRIPT_SUCCESS - clus1 -
1184205307 INTERNAL *** DAGMAN_FINISHED ***
1184205311 INTERNAL *** TAILSTATD_FINISHED 0 **</programlisting>

    <para>The above shows the create dir job being submitted and then executed
    on the grid. In addition it lists that job is being run on the grid site
    clus1 The various states of the job while it goes through submission to
    execution to postprocessing are in UPPERCASE.</para>

    <para>At the bottom of the output we see that DAGMAN and TAILSTATD have
    FINISHED and with and exit code of zero "0" which signifies that the
    workflow ran successfully. If there were any errors then the TAILSTATD
    would exit with a non zero exitcode and the failed jobs would have a job
    state of FAILURE next to it.</para>

    <para>If your workflow fails then you can look at the job name (second
    column in the output) which has failed and check the contents of the
    kickstart record output stored in the jobname.out.NNN file where NNN can
    be 000 to 999 or the jobname.err file</para>

    <para>One can also monitor the status of the running workflow by looking
    at the output of the Condor DAGMan output file.</para>

    <programlisting>$ <emphasis>tail -f black-diamond-0.dag.dagman.out

7/11 18:52:27 Node new_rc_tx_analyze_ID000004_0 job proc (22435.0) completed suc
cessfully.
7/11 18:52:27 Node new_rc_tx_analyze_ID000004_0 job completed
7/11 18:52:27 Running POST script of Node new_rc_tx_analyze_ID000004_0...
7/11 18:52:27 Number of idle job procs: 0
7/11 18:52:27 Of 17 nodes total:
7/11 18:52:27  Done     Pre   Queued    Post   Ready   Un-Ready   Failed
7/11 18:52:27   ===     ===      ===     ===     ===        ===      ===
7/11 18:52:27    14       0        0       1       0          2        0</emphasis></programlisting>

    <para>You will see lines like the one above scrolling by giving you
    statistics of home many jobs have been finished, failed, running or
    queued. It will also tell if you the workflow has finished if the DAGMan
    finishes and the name of any jobs that failed.</para>

    <para>If you want to abort your workflow for any reason you can use the
    pegasus-remove command listed in the output of pegasus-run invocation or
    by specifiying the Dag directory for the workflow you want to
    terminate.</para>

    <programlisting>$ <emphasis>pegasus-remove /nfs/asd2/gmehta/PEGASUS/dags/gmehta/pegasus/black-diamond/run0001</emphasis></programlisting>

    <para>Pegasus will remove the DAGMan and all the jobs related to the
    DAGMan from the condor queue. A rescue DAG will be generated in case you
    want to resubmit the same workflow and continue execution from where it
    last stopped. A rescue DAG only skips jobs that have completely finished.
    It does not continue a partially running job unless the executable
    supports checkpointing.</para>

    <para>To resubmit an aborted or failed workflow with the same submit files
    and rescue Dag just rerun the pegasus-run command</para>

    <programlisting>$ <emphasis>pegasus-run -Dpegasus.user.properties=/nfs/asd2/gmehta/PEGASUS/dags\
/gmehta/pegasus/black-diamond/run0001/pegasus.61698.properties \
--nodatabase /nfs/asd2/gmehta/PEGASUS/dags/gmehta/pegasus/black-diamond/run0001
</emphasis></programlisting>
  </section>

  <section>
    <title>Monitoring and Debugging</title>

    <para></para>
  </section>

  <section>
    <title>Monitoring, Debugging and Statistics</title>

    <section>
      <title>pegasus-status</title>

      <para>To monitor the execution of the workflow lets run the
      pegasus-status command as suggested by the output of the pegasus-run
      command above.</para>

      <screen><emphasis role="bold">$ pegasus-status -l</emphasis> <replaceable>/Workflow/dags/directory</replaceable>

<computeroutput>Workflow_1-0.dag succeeded
10/01/10 12:24:38  Done     Pre   Queued    Post   Ready   Un-Ready   Failed
10/01/10 12:24:38   ===     ===      ===     ===     ===        ===      ===
10/01/10 12:24:38    48       0        5       0       5          0        0

WORKFLOW STATUS : RUNNING | 48/58 ( 83% ) | (workflow is running)</computeroutput></screen>

      <para>After the workflow finishes the status will change to COMPLETED or
      FAILED..</para>

      <screen><emphasis role="bold">$ pegasus-status -l</emphasis> <replaceable>/Workflow/dags/directory</replaceable>

<computeroutput>Workflow_1-0.dag succeeded
10/01/10 12:24:38  Done     Pre   Queued    Post   Ready   Un-Ready   Failed
10/01/10 12:24:38   ===     ===      ===     ===     ===        ===      ===
10/01/10 12:24:38    58       0        0       0       0          0        0

WORKFLOW STATUS : COMPLETED | 5/58 ( 100% ) | (Workflow finished successfully)</computeroutput></screen>

      <para></para>
    </section>

    <section>
      <title>pegasus-analyzer</title>

      <para>If the workflow fails the you can anlyze the run using the tool
      pegasus-analyzer.</para>

      <para>The simplest way to invoke pegasus-analyzer is to simply give it a
      workflow run directory, like in the example below:</para>

      <para><screen><command>$ pegasus-analyzer -d </command><replaceable>/home/user/run0004</replaceable>

<computeroutput>pegasus-analyzer: initializing...

************************************Summary*************************************

 Total jobs         :     26 (100.00%)
 # jobs succeeded   :     25 (96.15%)
 # jobs failed      :      1 (3.84%)
 # jobs unsubmitted :      0 (0.00%)

******************************Failed jobs' details******************************

============================register_viz_glidein_7_0============================

 last state: POST_SCRIPT_FAILURE
       site: local
submit file: /home/user/run0004/register_viz_glidein_7_0.sub
output file: /home/user/run0004/register_viz_glidein_7_0.out.002
 error file: /home/user/run0004/register_viz_glidein_7_0.err.002

-------------------------------Task #1 - Summary--------------------------------

site        : local
executable  : /lfs1/software/install/pegasus/default/bin/rc-client
arguments   : -Dpegasus.user.properties=/lfs1/work/pegasus/run0004/pegasus.15181.properties \
-Dpegasus.catalog.replica.url=rlsn://smarty.isi.edu --insert register_viz_glidein_7_0.in
exitcode    : 1
working dir : /lfs1/work/pegasus/run0004

---------Task #1 - pegasus::rc-client - pegasus::rc-client:1.0 - stdout---------

2009-02-20 16:25:13.467 ERROR [root] You need to specify the pegasus.catalog.replica property
2009-02-20 16:25:13.468 WARN  [root] non-zero exit-code 1</computeroutput></screen>In
      the case above, pegasus-analyzer's output contains a brief summary
      section, showing how many jobs have succeeded and how many have failed.
      After that, pegasus-analyzer will print information about each job that
      failed, showing its last known state, along with the location of its
      submit, output, and error files. pegasus-analyzer will also display any
      stdout and stderr from the job. </para>
    </section>

    <section>
      <title>pegasus-remove</title>

      <para>If you want to abort your workflow for any reason you can use the
      pegasus-remove command listed in the output of pegasus-run invocation or
      by specifiying the Dag directory for the workflow you want to
      terminate.</para>

      <programlisting><emphasis role="bold">$ <emphasis role="bold">pegasus-remove </emphasis></emphasis><replaceable>/PATH/To/WORKFLOW DIRECTORY</replaceable></programlisting>
    </section>

    <section>
      <title>pegasus-analyzer</title>

      <para>pegasus-analyzer is a command-line utility for parsing several
      files in the workflow directory and summarizing useful information to
      the user. It should be used after the workflow has already finished
      execution. pegasus-analyzer quickly goes through the jobstate.log file,
      and isolates jobs that did not complete successfully. It then parses
      their submit, and kickstart output files, printing to the user detailed
      information for helping the user debug what happened to his/her
      workflow.</para>

      <para>The simplest way to invoke pegasus-analyzer is to simply give it a
      workflow run directory, like in the example below:</para>

      <para><programlisting>$ pegasus-analyzer -d /home/user/run0004
pegasus-analyzer: initializing...

************************************Summary*************************************

 Total jobs         :     26 (100.00%)
 # jobs succeeded   :     25 (96.15%)
 # jobs failed      :      1 (3.84%)
 # jobs unsubmitted :      0 (0.00%)

******************************Failed jobs' details******************************

============================register_viz_glidein_7_0============================

 last state: POST_SCRIPT_FAILURE
       site: local
submit file: /home/user/run0004/register_viz_glidein_7_0.sub
output file: /home/user/run0004/register_viz_glidein_7_0.out.002
 error file: /home/user/run0004/register_viz_glidein_7_0.err.002

-------------------------------Task #1 - Summary--------------------------------

site        : local
executable  : /lfs1/software/install/pegasus/default/bin/rc-client
arguments   : -Dpegasus.user.properties=/lfs1/work/pegasus/run0004/pegasus.15181.properties \
-Dpegasus.catalog.replica.url=rlsn://smarty.isi.edu --insert register_viz_glidein_7_0.in
exitcode    : 1
working dir : /lfs1/work/pegasus/run0004

---------Task #1 - pegasus::rc-client - pegasus::rc-client:1.0 - stdout---------

2009-02-20 16:25:13.467 ERROR [root] You need to specify the pegasus.catalog.replica property
2009-02-20 16:25:13.468 WARN  [root] non-zero exit-code 1</programlisting>In
      the case above, pegasus-analyzer's output contains a brief summary
      section, showing how many jobs have succeeded and how many have failed.
      After that, pegasus-analyzer will print information about each job that
      failed, showing its last known state, along with the location of its
      submit, output, and error files. pegasus-analyzer will also display any
      stdout and stderr from the job, as recorded in its kickstart record.
      Please consult pegasus-analyzer's man page for more examples and a
      detailed description of its various command-line options.</para>
    </section>

    <section>
      <title></title>

      <para></para>
    </section>

    <section>
      <title>pegasus-analyzer</title>

      <para>If your workflow fails you can run the pegasus-analyzer command to
      see which job/jobs failed.</para>
    </section>
  </section>

  <section>
    <title>Plotting and Statistics</title>

    <section>
      <title>pegasus-statistics</title>

      <para>pegasus-statistics generates workflow execution statistics. To
      generate statistics run the command as shown below.</para>

      <screen><command>$ pegasus-statistics</command> <replaceable>/scratch/grid-setup/run0001/</replaceable>


...

******************************************** SUMMARY ********************************************
Total workflow execution time      :         1741 
Total workflow execution wall time :      276.963 
Total jobs                         :           17 
Total tasks                        :           17 
# jobs succeeded                   :           17 
# jobs failed                      :            0 
# jobs unsubmitted                 :            0 
# jobs unknown                     :            0 

Workflow execution statistics created at :
/scratch/grid-setup/run0001/statistics/workflow

Workflow events with time starting with zero is created at :
/scratch/grid-setup/run0001/statistics/out

Job statistics is created at : 
/scratch/grid-setup/run0001/statistics/jobs

Logical transformation statistics is created at :
/scratch/grid-setup/run0001/statistics/breakdown.txt
**************************************************************************************************</screen>

      <para>By default the output gets generated to statistics folder inside
      the submit directory. pegasus-statistics generates the following
      statistics list and tables.</para>

      <para><emphasis role="bold">Workflow statistics table</emphasis></para>

      <para>Workflow statistics table contains information about the workflow
      run like total execution time, job's failed etc. A sample table is shown
      below.<link linkend="???"></link></para>

      <table>
        <title>Table 3.1</title>

        <tgroup cols="2">
          <tbody>
            <row>
              <entry>Total workflow execution time</entry>

              <entry>1741</entry>
            </row>

            <row>
              <entry>Total workflow execution wall time</entry>

              <entry>276.963</entry>
            </row>

            <row>
              <entry>Total jobs</entry>

              <entry>17</entry>
            </row>

            <row>
              <entry>Total tasks</entry>

              <entry>17</entry>
            </row>

            <row>
              <entry># jobs succeeded</entry>

              <entry>17</entry>
            </row>

            <row>
              <entry># jobs failed</entry>

              <entry>0</entry>
            </row>

            <row>
              <entry># jobs unsubmitted</entry>

              <entry>0</entry>
            </row>

            <row>
              <entry># jobs unknown</entry>

              <entry>0</entry>
            </row>
          </tbody>
        </tgroup>
      </table>

      <para><emphasis role="bold">Logical transformation statistics
      table</emphasis></para>

      <para>Logical transformation statistics table contains information about
      each type of transformation in the workflow. A sample table is shown
      below.</para>

      <table>
        <title>Table 3.5</title>

        <tgroup align="center" cols="7">
          <thead>
            <row>
              <entry align="center">Transformation</entry>

              <entry align="center">Count</entry>

              <entry align="center">Mean</entry>

              <entry align="center">Variance</entry>

              <entry align="center">Min</entry>

              <entry align="center">Max</entry>

              <entry align="center">Total</entry>
            </row>
          </thead>

          <tbody>
            <row>
              <entry>pegasus::dirmanager</entry>

              <entry>1</entry>

              <entry>0.33</entry>

              <entry>0.00</entry>

              <entry>0.33</entry>

              <entry>0.33</entry>

              <entry>0.33</entry>
            </row>

            <row>
              <entry>diamond::analyze:2.0</entry>

              <entry>1</entry>

              <entry>60.16</entry>

              <entry>0.00</entry>

              <entry>60.16</entry>

              <entry>60.16</entry>

              <entry>60.16</entry>
            </row>

            <row>
              <entry>diamond::findrange:2.0</entry>

              <entry>2</entry>

              <entry>60.31</entry>

              <entry>0.01</entry>

              <entry>60.25</entry>

              <entry>60.37</entry>

              <entry>120.62</entry>
            </row>

            <row>
              <entry>diamond::preprocess:2.0</entry>

              <entry>1</entry>

              <entry>60.48</entry>

              <entry>0.00</entry>

              <entry>60.48</entry>

              <entry>60.48</entry>

              <entry>60.48</entry>
            </row>

            <row>
              <entry>pegasus::cleanup</entry>

              <entry>6</entry>

              <entry>3.74</entry>

              <entry>18.15</entry>

              <entry>0.97</entry>

              <entry>10.28</entry>

              <entry>22.46</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>

    <section>
      <title>pegasus-plots</title>

      <para>pegasus-plots generates graphs and charts to visualize workflow
      execution. To generate graphs and charts run the command as shown
      below.</para>

      <screen><command>$ pegasus-plots</command> &lt;submit dag directory&gt;


<computeroutput>******************************************** SUMMARY ********************************************
The workflow execution Gantt chart is created at -
png format :- /scratch/grid-setup/run0001/graph/diamond-2.png 
eps format :- /scratch/grid-setup/run0001/graph/diamond-2.eps 

The host over time chart is created at -
png format :-/scratch/grid-setup/run0001/graph/diamond-host.png 
eps format :-/scratch/grid-setup/run0001/graph/diamond-host.eps

JPEG file corresponding to the dag is created at: 
/scratch/grid-setup/run0001/graph/diamond-dag.jpg 

JPEG file corresponding to the dax is created at: 
/scratch/grid-setup/run0001/graph/blackdiamond-dax.jpg 
**************************************************************************************************</computeroutput></screen>

      <para>By default the output gets generated to graph folder inside the
      submit directory. pegasus-plots generates the following graphs and
      charts.</para>

      <para><emphasis role="bold">Gantt workflow execution
      chart</emphasis></para>

      <para>Gantt chart of the workflow execution run. A sample chart is shown
      below.</para>

      <figure>
        <title>Figure 4.1</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="images/diamond-2.png" scalefit="true" />
          </imageobject>
        </mediaobject>
      </figure>

      <para><emphasis role="bold"></emphasis></para>

      <para><emphasis role="bold">Dag Graph</emphasis></para>

      <para>Graph representation of the DAG file. A sample graph is shown
      below.</para>

      <figure>
        <title>Figure 4.3</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="images/diamond-dag.jpg" scale="50%" />
          </imageobject>
        </mediaobject>
      </figure>

      <para><emphasis role="bold">Dax Graph</emphasis></para>

      <para>Graph representation of the DAX file. A sample graph is shown
      below.</para>

      <figure>
        <title>Figure 4.4</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="images/blackdiamond-dax.jpg" scale="50%" />
          </imageobject>
        </mediaobject>
      </figure>

      <para>For more options see the man page for pegasus-plots or the <xref
      linkend="monitoring_debugging_stats" /> section</para>
    </section>
  </section>
</chapter>
