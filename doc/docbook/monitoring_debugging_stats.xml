<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN"
"http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<chapter id="monitoring_debugging_stats">
  <title>Monitoring, Debugging and Statistics</title>

  <section>
    <title>Introduction</title>

    <para>Pegasus comes bundled with useful tools that help users debug
    workflows and generate useful statistics and plots about their workflow
    runs. These tools internally parse the Condor log files and have a similar
    interface. With the exception of pegasus-monitord (see below), all tools
    take in the submit directory as an argument. Users can invoke the tools
    listed in this chapter as follows:</para>

    <programlisting>$ pegasus-[toolname]   &lt;path to the submit directory&gt;</programlisting>
  </section>

  <section>
    <title>Monitoring and Debugging</title>

    <para>As the number of jobs and tasks in workflows increase, the ability
    to track the progress and quickly debug a workflow becomes more and more
    important. Pegasus comes with a series of utilities that can be used to
    monitor and debug workflows both in real-time as well as after execution
    is already completed.</para>

    <section>
      <title>pegasus-monitord</title>

      <para>Pegasus-monitord is used to follow a workflow, parsing the output
      of DAGMan's dagman.out file. In addition to generating the jobstate.log
      file, which contains the various states that a job goes through during
      the workflow execution, pegasus-monitord can also be used to mine
      information from jobs' submit and output files, and either populate a
      database, or write a file with NetLogger events containing this
      information.</para>

      <para>Pegasus-monitord is automatically invoked by pegasus-run, and
      tracks workflows in real-time. By default, it produces the jobstate.log
      file, and a SQLite database, which contains all the information listed
      in the Stampede schema. When a workflow fails, and is re-submitted with
      a rescue DAG, pegasus-monitord will automatically pick up from where it
      left previously and continue to write the jobstate.log file and populate
      the database.</para>

      <para>If, after the workflow has already finished, users need to
      re-create the jobstate.log file, or re-populate the database from
      scratch, pegasus-monitord's --replay option should be used when running
      it manually. In addition to SQLite, pegasus-monitord supports other
      types of databases, such as MySQL and Postgres. Users will need to
      install the low-level database drivers, and should consult
      pegasus-monitord's man page for more detailed information.</para>

      <para>As an example, the command:</para>

      <para><programlisting>$ pegasus-monitord -r diamond-0.dag.dagman.out</programlisting>will
      launch pegasus-monitord in replay mode. In this case, if a jobstate.log
      file already exists, it will be rotated and a new file will be created.
      It will also create/use a SQLite database in the workflow's run
      directory, with the name of diamond-0.stampede.db. If the database
      already exists, it will make sure to remove any references to the
      current workflow before it populates the database. In this case,
      pegasus-monitord will process the workflow information from start to
      finish, including any restarts that may have happened.</para>

      <para>One important detail is that while processing a workflow,
      pegasus-monitord will automatically detect if/when sub-workflows are
      initiated, and will automatically track those sub-workflows as well. In
      this case, although pegasus-monitord will create a separate jobstate.log
      file in each workflow directory, the database at the top-level workflow
      will contain the information from not only the main workflow, but also
      from all sub-workflows.</para>
    </section>

    <section>
      <title>pegasus-status</title>

      <para>To monitor the execution of the workflow run the pegasus-status
      command as suggested by the output of the pegasus-run command
      above.</para>

      <programlisting><emphasis role="bold">$ pegasus-status -l</emphasis> <replaceable>/Workflow/dags/directory</replaceable>

Workflow_1-0.dag succeeded
10/01/10 12:24:38  Done     Pre   Queued    Post   Ready   Un-Ready   Failed
10/01/10 12:24:38   ===     ===      ===     ===     ===        ===      ===
10/01/10 12:24:38    48       0        5       0       5          0        0

WORKFLOW STATUS : RUNNING | 48/58 ( 83% ) | (workflow is running)</programlisting>

      <para>To see the jobs in the queue while the workflow is running, run
      the pegasus-status without the -l option as shown below:</para>

      <programlisting>$ <emphasis>pegasus-status /nfs/asd2/gmehta/PEGASUS/dags/gmehta/pegasus/black-diamond/run0002</emphasis>


-- Submitter: smarty.isi.edu : &lt;128.9.72.26:53194&gt; : smarty.isi.edu
 ID      OWNER/NODENAME   SUBMITTED     RUN_TIME ST PRI SIZE CMD
22417.0   gmehta          7/11 18:13   0+00:03:58 R  0   9.8  condor_dagman -f -
22423.0    |-rc_tx_analy  7/11 18:16   0+00:00:54 R  2   0.0  kickstart -n pegas
22424.0    |-rc_tx_findr  7/11 18:16   0+00:00:00 I  2   0.0  kickstart -n pegas
22425.0    |-rc_tx_prepr  7/11 18:16   0+00:00:00 I  2   0.0  kickstart -n pegas</programlisting>

      <para>If you do not see any of your job in the output of pegasus status
      or if the the pegasus-status says FAILED or Completed 100% then the
      workflow has</para>

      <itemizedlist>
        <listitem>
          <para>successfully completed</para>
        </listitem>

        <listitem>
          <para>stopped midway due to non recoverable error.</para>
        </listitem>
      </itemizedlist>
    </section>

    <section>
      <title>pegasus-analyzer</title>

      <para>Pegasus-analyzer is a command-line utility for parsing several
      files in the workflow directory and summarizing useful information to
      the user. It should be used after the workflow has already finished
      execution. pegasus-analyzer quickly goes through the jobstate.log file,
      and isolates jobs that did not complete successfully. It then parses
      their submit, and kickstart output files, printing to the user detailed
      information for helping the user debug what happened to his/her
      workflow.</para>

      <para>The simplest way to invoke pegasus-analyzer is to simply give it a
      workflow run directory, like in the example below:</para>

      <para><programlisting>$ pegasus-analyzer -d /home/user/run0004
pegasus-analyzer: initializing...

************************************Summary*************************************

 Total jobs         :     26 (100.00%)
 # jobs succeeded   :     25 (96.15%)
 # jobs failed      :      1 (3.84%)
 # jobs unsubmitted :      0 (0.00%)

******************************Failed jobs' details******************************

============================register_viz_glidein_7_0============================

 last state: POST_SCRIPT_FAILURE
       site: local
submit file: /home/user/run0004/register_viz_glidein_7_0.sub
output file: /home/user/run0004/register_viz_glidein_7_0.out.002
 error file: /home/user/run0004/register_viz_glidein_7_0.err.002

-------------------------------Task #1 - Summary--------------------------------

site        : local
executable  : /lfs1/software/install/pegasus/default/bin/rc-client
arguments   : -Dpegasus.user.properties=/lfs1/work/pegasus/run0004/pegasus.15181.properties \
-Dpegasus.catalog.replica.url=rlsn://smarty.isi.edu --insert register_viz_glidein_7_0.in
exitcode    : 1
working dir : /lfs1/work/pegasus/run0004

---------Task #1 - pegasus::rc-client - pegasus::rc-client:1.0 - stdout---------

2009-02-20 16:25:13.467 ERROR [root] You need to specify the pegasus.catalog.replica property
2009-02-20 16:25:13.468 WARN  [root] non-zero exit-code 1</programlisting>In
      the case above, pegasus-analyzer's output contains a brief summary
      section, showing how many jobs have succeeded and how many have failed.
      After that, pegasus-analyzer will print information about each job that
      failed, showing its last known state, along with the location of its
      submit, output, and error files. pegasus-analyzer will also display any
      stdout and stderr from the job, as recorded in its kickstart record.
      Please consult pegasus-analyzer's man page for more examples and a
      detailed description of its various command-line options.</para>
    </section>

    <section>
      <title>pegasus-remove</title>

      <para>If you want to abort your workflow for any reason you can use the
      pegasus-remove command listed in the output of pegasus-run invocation or
      by specifiying the Dag directory for the workflow you want to
      terminate.</para>

      <programlisting><emphasis role="bold">$ <emphasis role="bold">pegasus-remove </emphasis></emphasis><replaceable>/PATH/To/WORKFLOW DIRECTORY</replaceable></programlisting>
    </section>

    <section>
      <title>Resubmitting failed workflows</title>

      <para>Pegasus will remove the DAGMan and all the jobs related to the
      DAGMan from the condor queue. A rescue DAG will be generated in case you
      want to resubmit the same workflow and continue execution from where it
      last stopped. A rescue DAG only skips jobs that have completely
      finished. It does not continue a partially running job unless the
      executable supports checkpointing.</para>

      <para>To resubmit an aborted or failed workflow with the same submit
      files and rescue Dag just rerun the pegasus-run command</para>

      <programlisting>$ <emphasis>pegasus-run -Dpegasus.user.properties=/nfs/asd2/gmehta/PEGASUS/dags\
/gmehta/pegasus/black-diamond/run0001/pegasus.61698.properties \
--nodatabase /nfs/asd2/gmehta/PEGASUS/dags/gmehta/pegasus/black-diamond/run0001
</emphasis></programlisting>
    </section>
  </section>

  <section>
    <title>Plotting and Statistics</title>

    <para></para>

    <section>
      <title>pegasus-statistics</title>

      <para>Pegasus-statistics generates workflow execution statistics. To
      generate statistics run the command as shown below.</para>

      <programlisting>$ <emphasis>pegasus-statistics /scratch/grid-setup/run0001/ -s all </emphasis>


...

******************************************** SUMMARY ********************************************
...

-------------------------------------------------------------------------------------------------------------------------------------------------
Type                Succeeded           Failed              Unsubmitted         Total                    Retries             Total Run (Retries Included)
Tasks               8                   0                   0                   8                   ||   0                   8                   
Jobs                27                  0                   0                   27                  ||   0                   27                  
Sub Workflows       2                   0                   0                   2                   ||   0                   2                   
-------------------------------------------------------------------------------------------------------------------------------------------------

Workflow wall time                               : 21 mins, 9 secs,     (total 1269 seconds)

Workflow cumulative job wall time                : 8 mins, 4 secs,      (total 484 seconds)

Cumulative job walltime as seen from submit side : 8 mins, 0 secs,      (total 480 seconds)

Workflow execution statistics     : /scratch/grid-setup/run0001/statistics/workflow.txt

Job instance statistics           : /scratch/grid-setup/run0001/statistics/jobs.txt

Transformation statistics         : /scratch/grid-setup/run0001/statistics/breakdown.txt

Time statistics                   : /scratch/grid-setup/run0001/statistics/time.txt

**************************************************************************************************</programlisting>

      <para>By default the output gets generated to a statistics folder inside
      the submit directory. The output that is generated by pegasus-statistics
      is based on the value set for command line option 's'(statistics_level).
      In the sample run the command line option 's' is set to 'all' to generate
      all the statistics information for the workflow run. Please consult the
      pegasus-statistics man page to find a detailed description of various 
      command line options.</para>

      <note>
        <para>In case of hierarchal workflows, the metrics that are displayed
        on stdout take into account all the jobs/tasks/sub workflows that make 
        up the workflow by recursively iterating through each sub workflow. 
        </para>
      </note>

      <para>pegasus-statistics summary which is printed on the stdout contains the 
      following information.</para>

      <itemizedlist>
      
      	<listitem>
          <para><emphasis role="bold">Workflow summary</emphasis> -
          Summary of the workflow execution. In case of hierarchical
          workflow the calculation shows the statistics across all the 
          sub workflows.It shows the following statistics about tasks,
          jobs and sub workflows.</para>
          
          <itemizedlist>
          
      		<listitem>
      		<para><emphasis role="bold">Succeeded</emphasis> -
      		total count of succeeded tasks/jobs/sub workflows.
      		</para>
      		</listitem>
      		
      		<listitem>
      		<para><emphasis role="bold">Failed</emphasis> -
      		total count of failed tasks/jobs/sub workflows.
      		</para>
      		</listitem>
      		
      		<listitem>
      		<para><emphasis role="bold">Unsubmitted</emphasis> -
      		total count of unsubmitted tasks/jobs/sub workflows.This is
      		the difference between 'total' count and sum of 'succeeded'
      		and 'failed' count. 
      		</para>
      		</listitem>
      		
      		<listitem>
      		<para><emphasis role="bold">Total</emphasis> -
      		total count of tasks/jobs/sub workflows.
      		</para>
      		</listitem>
      		
      		<listitem>
      		<para><emphasis role="bold">Retries</emphasis> -
      		total retry count of tasks/jobs/sub workflows.
      		</para>
      		</listitem>
      		
      		<listitem>
      		<para><emphasis role="bold">Total Run</emphasis> -
      		total count of tasks/jobs/sub workflows executed during workflow
      		run. This is the cumulative of total retries, succeeded and
      		failed count.
      		</para>
      		</listitem>
      	   
      	   </itemizedlist>	
       
        </listitem>
        
        <listitem>
          <para><emphasis role="bold">Workflow wall time</emphasis> -
          The walltime from the start of the workflow execution to the  
          end as reported by the DAGMAN.In case of rescue dag the value
          is the cumulative of all retries.</para>
        </listitem>
        
        <listitem>
          <para><emphasis role="bold">Workflow cummulate job wall time</emphasis> -
          The sum of the walltime of all jobs as reported by kickstart. In case of 
          job retries the value is the cumulative of all retries. For workflows 
          having sub workflow jobs (i.e SUBDAG and SUBDAX jobs), the walltime value 
          includes jobs from the sub workflows as well.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Cumulative job walltime as seen from
          submit side</emphasis> - 
          The sum of the walltime of all jobs as reported by DAGMan. This is 
          similar to the regular cumulative job walltime, but includes job 
          management overhead and delays. In case of job retries the value is
          the cumulative of all retries. For workflows having sub workflow jobs
          (i.e SUBDAG and SUBDAX jobs), the walltime value includes jobs from 
          the sub workflows</para>
        </listitem>
        
	 </itemizedlist>
        
      <para>pegasus-statistics generates the following statistics files based
      on the command line options set.</para>

      <para><emphasis role="bold">Workflow statistics file per workflow
      [workflow.txt]</emphasis></para>

      <para>Workflow statistics file per workflow contains the following
      information about each workflow run. In case of hierarchal workflows,
      the file contains a table for each sub workflow. The file also contains
      a 'Total' table at the bottom which is the cummulative of all the individual
      statistics details.</para>

      <para>A sample table is shown below. It shows the following statistics about 
      tasks, jobs and sub workflows.</para>
      
      <itemizedlist>
          
  		<listitem>
  		<para><emphasis role="bold">Workflow retries</emphasis> -
  		number of times a workflow was retried.
  		</para>
  		</listitem>
  		
  		<listitem>
  		<para><emphasis role="bold">Succeeded</emphasis> -
  		total count of succeeded tasks/jobs/sub workflows.
  		</para>
  		</listitem>
  		
  		<listitem>
  		<para><emphasis role="bold">Failed</emphasis> -
  		total count of failed tasks/jobs/sub workflows.
  		</para>
  		</listitem>
  		
  		<listitem>
  		<para><emphasis role="bold">Unsubmitted</emphasis> -
  		total count of unsubmitted tasks/jobs/sub workflows.This is
  		the difference between 'total' count and sum of 'succeeded'
  		and 'failed' count. 
  		</para>
  		</listitem>
  		
  		<listitem>
  		<para><emphasis role="bold">Total</emphasis> -
  		total count of tasks/jobs/sub workflows.
  		</para>
  		</listitem>
  		
  		<listitem>
  		<para><emphasis role="bold">Retries</emphasis> -
  		total retry count of tasks/jobs/sub workflows.
  		</para>
  		</listitem>
  		
  		<listitem>
  		<para><emphasis role="bold">Total Run</emphasis> -
  		total count of tasks/jobs/sub workflows executed during workflow
  		run. This is the cumulative of total retries, succeeded and
  		failed count.
  		</para>
  		</listitem>
     
     </itemizedlist>

      <table>
        <title>Workflow Statistics</title>

        <tgroup align="center" cols="9">
          <thead>
            <row>
              <entry align="center">#</entry>
              
              <entry align="center">Type</entry>
              
              <entry align="center">Succeeded</entry>

              <entry align="center">Failed</entry>

              <entry align="center">Unsubmitted</entry>
              
              <entry align="center">Total</entry>
              
              <entry align="center">Retries</entry>

              <entry align="center">Total Run</entry>

              <entry align="center">Workflow Retries</entry>

            </row>
          </thead>

          <tbody>
            <row>
              <entry>2a6df11b-9972-4ba0-b4ba-4fd39c357af4</entry>

              <entry></entry>
              
              <entry></entry>
              
              <entry></entry>

              <entry></entry>

              <entry></entry>

              <entry></entry>

              <entry></entry>
              
              <entry>0</entry>
            </row>

            <row>
              <entry></entry>
              
              <entry>Tasks</entry>

              <entry>4</entry>
              
              <entry>0</entry>
              
              <entry>0</entry>

              <entry>4</entry>

              <entry>0</entry>

              <entry>4</entry>

              <entry></entry>
            </row>

            <row>
              <entry></entry>
              
              <entry>Jobs</entry>

              <entry>13</entry>
              
              <entry>0</entry>
              
              <entry>0</entry>

              <entry>13</entry>

              <entry>0</entry>

              <entry>13</entry>

              <entry></entry>
            </row>

            <row>
              <entry></entry>
              
              <entry>Sub Workflows</entry>

              <entry>0</entry>
              
              <entry>0</entry>
              
              <entry>0</entry>

              <entry>0</entry>

              <entry>0</entry>

              <entry>0</entry>

              <entry></entry>
            </row>
          </tbody>
        </tgroup>
      </table>


      <para><emphasis role="bold">Job statistics file per workflow
      [jobs.txt]</emphasis></para>

      <para>Job statistics file per workflow contains the following details
      about the job instances in each workflow. A sample file is shown below.</para>

      <itemizedlist>
        <listitem>
          <para><emphasis role="bold">Job</emphasis> - the name of the
          job instance</para>
        </listitem>
        
        <listitem>
          <para><emphasis role="bold">Try</emphasis> - the number representing 
          the job instance run count.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Site</emphasis> - the site where the job
          instance ran</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Kickstart(sec.)</emphasis> - the actual
          duration of the job instance in seconds on the remote compute node.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Post(sec.)</emphasis> - the postscript
          time as reported by DAGMan .</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">CondorQTime(sec.)</emphasis> - the time
          between submission by DAGMan and the remote Grid submission. It is
          an estimate of the time spent in the condor q on the submit node .</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Resource(sec.)</emphasis> - the time
          between the remote Grid submission and start of remote execution .
          It is an estimate of the time job instance spent in the remote 
          queue .</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Runtime(sec.)</emphasis> - the time
          spent on the resource as seen by Condor DAGMan . Is always
          &gt;=kickstart .</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Seqexec(sec.)</emphasis> - the time
          taken for the completion of a clustered job instance .</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Seqexec-Delay(sec.)</emphasis> - the
          time difference between the time for the completion of a clustered
          job instance and sum of all the individual tasks kickstart time .</para>
        </listitem>
      </itemizedlist>

      <table>
        <title>Job statistics</title>

        <tgroup align="center" cols="10">
          <thead>
            <row>
              <entry align="center">Job</entry>

              <entry align="center">Try</entry>
              
              <entry align="center">Site</entry>

              <entry align="center">Kickstart</entry>

              <entry align="center">Post</entry>

              <entry align="center">CondorQTime</entry>

              <entry align="center">Resource</entry>

              <entry align="center">Runtime</entry>

              <entry align="center">Seqexec</entry>

              <entry align="center">Seqexec-Delay</entry>
            </row>
          </thead>

          <tbody>
            <row>
              <entry>analyze_ID0000004</entry>

              <entry>1</entry>
              
              <entry>local</entry>

              <entry>60.002</entry>

              <entry>5.0</entry>

              <entry>0.0</entry>

              <entry>-</entry>

              <entry>62.0</entry>

              <entry>-</entry>

              <entry>-</entry>
            </row>

            <row>
              <entry>create_dir_diamond_0_local</entry>

              <entry>1</entry>
              
              <entry>local</entry>

              <entry>0.027</entry>

              <entry>5.0</entry>

              <entry>5.0</entry>

              <entry>-</entry>

              <entry>0.0</entry>

              <entry>-</entry>

              <entry>-</entry>
            </row>

            <row>
              <entry>findrange_ID0000002</entry>

              <entry>1</entry>
              
              <entry>local</entry>

              <entry>60.001</entry>

              <entry>5.0</entry>

              <entry>0.0</entry>

              <entry>-</entry>

              <entry>60.0</entry>

              <entry>-</entry>

              <entry>-</entry>
            </row>

            <row>
              <entry>findrange_ID0000003</entry>

              <entry>1</entry>
              
              <entry>local</entry>

              <entry>60.002</entry>

              <entry>5.0</entry>

              <entry>10.0</entry>

              <entry>-</entry>

              <entry>61.0</entry>

              <entry>-</entry>

              <entry>-</entry>
            </row>

            <row>
              <entry>preprocess_ID0000001</entry>

              <entry>1</entry>
              
              <entry>local</entry>

              <entry>60.002</entry>

              <entry>5.0</entry>

              <entry>5.0</entry>

              <entry>-</entry>

              <entry>60.0</entry>

              <entry>-</entry>

              <entry>-</entry>
            </row>

            <row>
              <entry>register_local_1_0</entry>

              <entry>1</entry>
              
              <entry>local</entry>

              <entry>0.459</entry>

              <entry>6.0</entry>

              <entry>5.0</entry>

              <entry>-</entry>

              <entry>0.0</entry>

              <entry>-</entry>

              <entry>-</entry>
            </row>

            <row>
              <entry>register_local_1_1</entry>

              <entry>1</entry>
              
              <entry>local</entry>

              <entry>0.338</entry>

              <entry>5.0</entry>

              <entry>5.0</entry>

              <entry>-</entry>

              <entry>0.0</entry>

              <entry>-</entry>

              <entry>-</entry>
            </row>

            <row>
              <entry>register_local_2_0</entry>

              <entry>1</entry>
              
              <entry>local</entry>

              <entry>0.348</entry>

              <entry>5.0</entry>

              <entry>5.0</entry>

              <entry>-</entry>

              <entry>0.0</entry>

              <entry>-</entry>

              <entry>-</entry>
            </row>

            <row>
              <entry>stage_in_local_local_0</entry>

              <entry>1</entry>
              
              <entry>local</entry>

              <entry>0.39</entry>

              <entry>5.0</entry>

              <entry>5.0</entry>

              <entry>-</entry>

              <entry>0.0</entry>

              <entry>-</entry>

              <entry>-</entry>
            </row>

            <row>
              <entry>stage_out_local_local_0_0</entry>

              <entry>1</entry>
              
              <entry>local</entry>

              <entry>0.165</entry>

              <entry>5.0</entry>

              <entry>10.0</entry>

              <entry>-</entry>

              <entry>0.0</entry>

              <entry>-</entry>

              <entry>-</entry>
            </row>

            <row>
              <entry>stage_out_local_local_1_0</entry>

              <entry>1</entry>
              
              <entry>local</entry>

              <entry>0.147</entry>

              <entry>7.0</entry>

              <entry>5.0</entry>

              <entry>-</entry>

              <entry>0.0</entry>

              <entry>-</entry>

              <entry>-</entry>
            </row>

            <row>
              <entry>stage_out_local_local_1_1</entry>

              <entry>1</entry>
              
              <entry>local</entry>

              <entry>0.139</entry>

              <entry>5.0</entry>

              <entry>6.0</entry>

              <entry>-</entry>

              <entry>0.0</entry>

              <entry>-</entry>

              <entry>-</entry>
            </row>

            <row>
              <entry>stage_out_local_local_2_0</entry>

              <entry>1</entry>
              
              <entry>local</entry>

              <entry>0.145</entry>

              <entry>5.0</entry>

              <entry>5.0</entry>

              <entry>-</entry>

              <entry>0.0</entry>

              <entry>-</entry>

              <entry>-</entry>
            </row>

          </tbody>
        </tgroup>
      </table>


      <para><emphasis role="bold">Transformation statistics file per
      workflow [breakdown.txt]</emphasis></para>

      <para>Transformation statistics file per workflow contains
      information about the invocations in each workflow grouped
      by transformation name. A sample file is shown below.</para>
      
      <itemizedlist>
        <listitem>
          <para><emphasis role="bold">Transformation</emphasis> - name of the
          transformation.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Count</emphasis> - the number of times
          invocations with a given transformation name was executed.</para>
        </listitem>
        
        <listitem>
          <para><emphasis role="bold">Succeeded</emphasis> - the count of  
          succeeded invocations with a  given logical transformation name .</para>
        </listitem>
        
        <listitem>
          <para><emphasis role="bold">Failed</emphasis> - the count of  
          failed invocations with a  given logical transformation name .</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Min (sec.)</emphasis> - the minimum 
          runtime value  of invocations with a given logical transformation name.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Max (sec.)</emphasis> - the minimum 
          runtime value  of invocations with a given logical transformation name.</para>
        </listitem>
        
        <listitem>
          <para><emphasis role="bold">Mean (sec.)</emphasis> - the mean of the 
          invocation runtimes  with a given logical transformation name.</para>
        </listitem>
        
        <listitem>
          <para><emphasis role="bold">Total (sec.)</emphasis> - the cumulative 
          of runtime value of invocations with a given logical transformation name</para>
        </listitem>
      </itemizedlist>

      <table>
        <title>Transformation Statistics</title>

        <tgroup align="center" cols="8">
          <thead>
            <row>
              <entry align="center">Transformation</entry>

              <entry align="center">Count</entry>

              <entry align="center">Succeeded</entry>
              
              <entry align="center">Failed</entry>
              
              <entry align="center">Min</entry>

              <entry align="center">Max</entry>
              
              <entry align="center">Mean</entry>

              <entry align="center">Total</entry>
            </row>
          </thead>

          <tbody>
            <row>
              <entry>dagman::post</entry>

              <entry>13</entry>
              
              <entry>13</entry>
              
              <entry>0</entry>

              <entry>5.0</entry>

              <entry>7.0</entry>

              <entry>5.231</entry>

              <entry>68.0 </entry>
            </row>

            <row>
              <entry>diamond::analyze</entry>

              <entry>1</entry>
              
              <entry>1</entry>
              
              <entry>0</entry>

              <entry>60.002</entry>

              <entry>60.002</entry>

              <entry>60.002</entry>

              <entry>60.002</entry>
            </row>

            <row>
              <entry>diamond::findrange</entry>

              <entry>2</entry>
              
              <entry>2</entry>
              
              <entry>0</entry>

              <entry>60.001</entry>

              <entry>60.002</entry>

              <entry>60.002</entry>

              <entry>120.003</entry>
            </row>

            <row>
              <entry>diamond::preprocess</entry>

              <entry>1</entry>
              
              <entry>1</entry>
              
              <entry>0</entry>

              <entry>60.002</entry>

              <entry>60.002</entry>

              <entry>60.002</entry>

              <entry>60.002</entry>
            </row>

            <row>
              <entry>pegasus::dirmanager</entry>

              <entry>1</entry>
              
              <entry>1</entry>
              
              <entry>0</entry>

              <entry>0.027</entry>

              <entry>0.027</entry>

              <entry>0.027</entry>

              <entry>0.027</entry>
            </row>
            
            <row>
              <entry>pegasus::pegasus-transfer</entry>

              <entry>5</entry>
              
              <entry>5</entry>
              
              <entry>0</entry>

              <entry>0.139</entry>

              <entry>0.39</entry>

              <entry>0.197</entry>

              <entry>0.986</entry>
            </row>
            
            
            <row>
              <entry>pegasus::rc-client</entry>

              <entry>3</entry>
              
              <entry>3</entry>
              
              <entry>0</entry>

              <entry>0.338</entry>

              <entry>0.459</entry>

              <entry>0.382</entry>

              <entry>1.145</entry>
            </row>
            
          </tbody>
        </tgroup>
      </table>
      
     <para><emphasis role="bold">Time statistics file [time.txt]</emphasis></para>

      <para>Time statistics file contains job instance and invocation statistics 
      information grouped by time and host. The time grouping can be on month/week/day/hour.
      The file contains the following tables Job instance statistics per day, Invocation 
      statistics per day, Job instance statistics by host per day and  Invocation by host 
      per day. A sample Invocation statistics by host per day table is shown below.</para>
      
      <itemizedlist>
        
        <listitem>
          <para><emphasis role="bold">Job instance statistics per day</emphasis> - 
          the number of job instances run, total runtime sorted by time filter</para>
        </listitem>
        
        <listitem>
          <para><emphasis role="bold">Invocation statistics per day</emphasis> - 
          the number of invocations , total runtime sorted by time filter.</para>
        </listitem>
        
        <listitem>
          <para><emphasis role="bold">Job instance statistics by host per day</emphasis> - 
          the number of job instances run, total runtime on each host sorted by time filter</para> 
        </listitem>
        
        <listitem>
          <para><emphasis role="bold">Invocation statistics by host per day</emphasis> - 
          the number of invocations , total runtime on each host sorted by time filter.</para>
        </listitem>
      </itemizedlist>
      
      <table>
        <title>Invocation statistics by host per day</title>

        <tgroup align="center" cols="4">
          <thead>
            <row>
              <entry align="center">Date [YYYY-MM-DD]</entry>

              <entry align="center">Host</entry>

              <entry align="center">Count</entry>
              
              <entry align="center">Runtime (Sec.)</entry>
              
             </row>
          </thead>

          <tbody>
            <row>
              <entry>2011-07-15</entry>

              <entry>butterfly.isi.edu</entry>
              
              <entry>54</entry>
              
              <entry>625.094</entry>

              </row>
           </tbody>
        </tgroup>
      </table> 
      
    </section>

    <section>
      <title>pegasus-plots</title>

      <para>Pegasus-plots generates graphs and charts to visualize workflow
      execution. To generate graphs and charts run the command as shown
      below.</para>

      <programlisting>$ <emphasis>pegasus-plots  -p all  /scratch/grid-setup/run0001/</emphasis>


...

******************************************** SUMMARY ********************************************

Graphs and charts generated by pegasus-plots can be viewed by opening the generated html file in the web browser  : 
/scratch/grid-setup/run0001/plots/index.html
 
**************************************************************************************************</programlisting>

      <para>By default the output gets generated to plots folder inside the
      submit directory. The output that is generated by pegasus-plots is based
      on the value set for command line option 'p'(plotting_level).In the 
      sample run the command line option 'p' is set to 'all' to generate all
      the charts and graphs for the workflow run. Please consult the
      pegasus-plots man page to find a detailed description of various 
      command line options.pegasus-plots generates an index.html file which 
      provides links to all the generated charts and plots. A sample index.html
      page is show below.
      </para> 
      <figure>
        <title>pegasus-plot index page</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="images/pegasus_plots_index.png" />
          </imageobject>
        </mediaobject>
      </figure>
      <para>
      pegasus-plots generates the following plots and charts. 
      </para>

      <para><emphasis role="bold">Dax Graph</emphasis></para>

      <para>Graph representation of the DAX file. A sample page is shown
      below. </para>

      <figure>
        <title>DAX Graph</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="images/dax_page.png" />
          </imageobject>
        </mediaobject>
      </figure>

	  
      <para><emphasis role="bold">Dag Graph</emphasis></para>

      <para>Graph representation of the DAG file. A sample page is shown
      below.</para>

      <figure>
        <title>DAG Graph</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="images/dag_page.png" />
          </imageobject>
        </mediaobject>
      </figure>

      
      <para><emphasis role="bold">Gantt workflow execution
      chart</emphasis></para>

      <para>Gantt chart of the workflow execution run. A sample page is shown
      below.</para>

      <figure>
        <title>Gantt Chart</title>

        <mediaobject>
          <imageobject>
                <imagedata fileref="images/gantt_chart_page.png" />
          </imageobject>
        </mediaobject>
      </figure>
      <para>The toolbar at the top provides zoom in/out , pan left/right/top/bottom 
      and show/hide job name functionality.The toolbar at the bottom can be used to 
      show/hide job states. Failed jobs are shown in red border in the chart. 
      Clicking on a sub workflow job will take you to the corresponding sub workflow
      chart.</para>

      <para><emphasis role="bold">Host over time chart</emphasis></para>

      <para>Host over time chart of the workflow execution run. A sample page
      is shown below.</para>

      <figure>
        <title>Host over time chart</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="images/host_chart_page.png" />
          </imageobject>
        </mediaobject>
      </figure>
      <para>The toolbar at the top provides zoom in/out , pan left/right/top/bottom 
      and show/hide host name functionality.The toolbar at the bottom can be used to 
      show/hide job states. Failed jobs are shown in red border in the chart. 
      Clicking on a sub workflow job will take you to the corresponding sub workflow
      chart.</para>
    </section>
  </section>
</chapter>
