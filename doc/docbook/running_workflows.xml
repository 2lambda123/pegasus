<?xml version="1.0" encoding="US-ASCII"?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN"
"http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<chapter id="running_workflows">
  <title>Running Workflows</title>

  <section>
    <title>Executable Workflows (DAG)</title>

    <para>The DAG an executable (concrete) workflow that can be executed over
    a variety of resources. When the workflow tasks are mapped to multiple
    resources that do not share a file system, explicit nodes are added to the
    workflow for orchestrating data. transfer between the tasks.</para>

    <para>When you take the DAX workflow created in <link linkend="creating_workflows">Creating Workflows</link>, and plan it for a single
    remote grid execution, here a site with handle <emphasis>hpcc</emphasis>,
    and plan the workflow without clean-up nodes, the following concrete
    workflow is built:</para>

    <para>
      <figure id="concepts-fig-dag">
        <title>Black Diamond DAG</title>

        <mediaobject>
          <imageobject>
            <imagedata contentdepth="100%" scalefit="1" width="100%" align="center"
                       fileref="images/concepts-diamond-dag.png"
                       valign="middle" />
          </imageobject>
        </mediaobject>
      </figure>
    </para>

    <para>Planning augments the original abstract workflow with ancillary
    tasks to facility the proper execution of the workflow. These tasks
    include:</para>

    <itemizedlist>
      <listitem>
        <para>the creation of remote working directories. These directories
        typically have name that seeks to avoid conflicts with other
        simultaneously running similar workflows. Such tasks use a job prefix
        of <code>create_dir</code>.</para>
      </listitem>

      <listitem>
        <para>the stage-in of input files before any task which requires these
        files. Any file consumed by a task needs to be staged to the task, if
        it does not already exist on that site. Such tasks use a job prefix of
        <code>stage_in</code>.If multiple files from various sources need to
        be transferred, multiple stage-in jobs will be created. Additional
        advanced options permit to control the size and number of these jobs,
        and whether multiple compute tasks can share stage-in jobs.</para>
      </listitem>

      <listitem>
        <para>the original DAX job is concretized into a compute task in the
        DAG. Compute jobs are a concatination of the job's
        <emphasis>name</emphasis> and <emphasis>id</emphasis> attribute from
        the DAX file.</para>
      </listitem>

      <listitem>
        <para>the stage-out of data products to a collecting site. Data
        products with their <emphasis>transfer</emphasis> flag set to
        <literal>false</literal> will not be staged to the output site.
        However, they may still be eligible for staging to other, dependent
        tasks. Stage-out tasks use a job prefix of
        <code>stage_out</code>.</para>
      </listitem>

      <listitem>
        <para>If compute jobs run at different sites, an intermediary staging
        task with prefix <code>stage_inter</code> is inserted between the
        compute jobs in the workflow, ensuring that the data products of the
        parent are available to the child job.</para>
      </listitem>

      <listitem>
        <para>the registration of data products in a replica catalog. Data
        products with their <emphasis>register</emphasis> flag set to
        <literal>false</literal> will not be registered.</para>
      </listitem>

      <listitem>
        <para>the clean-up of transient files and working directories. These
        steps can be omitted with the <command>--no-cleanup</command> option
        to the planner.</para>
      </listitem>
    </itemizedlist>

    <para>The <link linkend="reference">" Reference Manual"</link> Chapter
    details more about when and how staging nodes are inserted into the
    workflow.</para>

    <para>The DAG will be found in file <filename>diamond-0.dag</filename>,
    constructed from the <emphasis>name</emphasis> and
    <emphasis>index</emphasis> attributes found in the root element of the DAX
    file.</para>

    <programlisting>######################################################################
# PEGASUS WMS GENERATED DAG FILE
# DAG diamond
# Index = 0, Count = 1
######################################################################

JOB create_dir_diamond_0_hpcc create_dir_diamond_0_hpcc.sub
SCRIPT POST create_dir_diamond_0_hpcc /opt/pegasus/default/bin/pegasus-exitcode create_dir_diamond_0_hpcc.out

JOB stage_in_local_hpcc_0 stage_in_local_hpcc_0.sub
SCRIPT POST stage_in_local_hpcc_0 /opt/pegasus/default/bin/pegasus-exitcode stage_in_local_hpcc_0.out

JOB preprocess_ID000001 preprocess_ID000001.sub
SCRIPT POST preprocess_ID000001 /opt/pegasus/default/bin/pegasus-exitcode preprocess_ID000001.out

JOB findrange_ID000002 findrange_ID000002.sub
SCRIPT POST findrange_ID000002 /opt/pegasus/default/bin/pegasus-exitcode findrange_ID000002.out

JOB findrange_ID000003 findrange_ID000003.sub
SCRIPT POST findrange_ID000003 /opt/pegasus/default/bin/pegasus-exitcode findrange_ID000003.out

JOB analyze_ID000004 analyze_ID000004.sub
SCRIPT POST analyze_ID000004 /opt/pegasus/default/bin/pegasus-exitcode analyze_ID000004.out

JOB stage_out_local_hpcc_2_0 stage_out_local_hpcc_2_0.sub
SCRIPT POST stage_out_local_hpcc_2_0 /opt/pegasus/default/bin/pegasus-exitcode stage_out_local_hpcc_2_0.out

PARENT findrange_ID000002 CHILD analyze_ID000004
PARENT findrange_ID000003 CHILD analyze_ID000004
PARENT preprocess_ID000001 CHILD findrange_ID000002
PARENT preprocess_ID000001 CHILD findrange_ID000003
PARENT analyze_ID000004 CHILD stage_out_local_hpcc_2_0
PARENT stage_in_local_hpcc_0 CHILD preprocess_ID000001
PARENT create_dir_diamond_0_hpcc CHILD findrange_ID000002
PARENT create_dir_diamond_0_hpcc CHILD findrange_ID000003
PARENT create_dir_diamond_0_hpcc CHILD preprocess_ID000001
PARENT create_dir_diamond_0_hpcc CHILD analyze_ID000004
PARENT create_dir_diamond_0_hpcc CHILD stage_in_local_hpcc_0
######################################################################
# End of DAG
######################################################################
</programlisting>

    <para>The DAG file declares all jobs and links them to a Condor submit
    file that describes the planned, concrete job. In the same directory as
    the DAG file are all Condor submit files for the jobs from the picture
    plus a number of additional helper files.</para>

    <para>The various instructions that can be put into a DAG file are
    described in <ulink
    url="http://www.cs.wisc.edu/condor/manual/v7.5/2_10DAGMan_Applications.html">Condor's
    DAGMAN documentation</ulink>.The constituents of the submit directory are
    described in the<link linkend="submit_directory"> "Submit Directory
    Details"</link>chapter</para>
  </section>

  <section>
    <title>Mapping Refinement Steps</title>

    <para>During the mapping process, the abstract workflow undergoes a series
    of refinement steps that converts it to an executable form.</para>

    <section>
      <title>Data Reuse</title>

      <para>The abstract workflow after parsing is optionally handed over to
      the Data Reuse Module. The Data Reuse Algorithm in Pegasus attempts to
      prune all the nodes in the abstract workflow for which the output files
      exist in the Replica Catalog. It also attempts to cascade the deletion
      to the parents of the deleted node for e.g if the output files for the
      leaf nodes are specified, Pegasus will prune out all the workflow as the
      output files in which a user is interested in already exist in the
      Replica Catalog.</para>

      <para>The Data Reuse Algorithm works in two passes</para>

      <para><emphasis role="bold">First Pass</emphasis> - Determine all the
      jobs whose output files exist in the Replica Catalog. An output file
      with the transfer flag set to false is treated equivalent to the file
      existing in the Replica Catalog , if the output file is not an input to
      any of the children of the job X.</para>

      <para><emphasis role="bold">Second Pass</emphasis> - The algorithm
      removes the job whose output files exist in the Replica Catalog and
      tries to cascade the deletion upwards to the parent jobs. We start the
      breadth first traversal of the workflow bottom up.</para>

      <programlisting>( It is already marked for deletion in Pass 1
     OR
      ( ALL of it's children have been marked for deletion
        AND
        Node's output files have transfer flags set to false
       )
 )</programlisting>

      <tip>
        <para>The Data Reuse Algorithm can be disabled by passing the
        <emphasis role="bold">--force</emphasis> option to
        pegasus-plan.</para>
      </tip>

      <figure>
        <title>Workflow Data Reuse</title>

        <mediaobject>
          <imageobject>
            <imagedata contentdepth="100%"
                       fileref="./images/refinement-data-reuse.png"
                       scalefit="1" width="100%" />
          </imageobject>
        </mediaobject>
      </figure>
    </section>

    <section>
      <title>Site Selection</title>

      <para>The abstract workflow is then handed over to the Site Selector
      module where the abstract jobs in the pruned workflow are mapped to the
      various sites passed by a user. The target sites for planning are
      specified on the command line using the<emphasis role="bold">
      --sites</emphasis> option to pegasus-plan. If not specified, then
      Pegasus picks up all the sites in the Site Catalog as candidate sites.
      Pegasus will map a compute job to a site only if Pegasus can</para>

      <itemizedlist>
        <listitem>
          <para>find an INSTALLED executable on the site</para>
        </listitem>

        <listitem>
          <para>OR find a STAGEABLE executable that can be staged to the site
          as part of the workflow execution.</para>

          <para>Pegasus supports variety of site selectors with Random being
          the default</para>

          <itemizedlist>
            <listitem>
              <para>
                <emphasis role="bold">Random</emphasis>
              </para>

              <para>The jobs will be randomly distributed among the sites that
              can execute them.</para>
            </listitem>

            <listitem>
              <para>
                <emphasis role="bold">RoundRobin</emphasis>
              </para>

              <para>The jobs will be assigned in a round robin manner amongst
              the sites that can execute them. Since each site cannot execute
              every type of job, the round robin scheduling is done per level
              on a sorted list. The sorting is on the basis of the number of
              jobs a particular site has been assigned in that level so far.
              If a job cannot be run on the first site in the queue (due to no
              matching entry in the transformation catalog for the
              transformation referred to by the job), it goes to the next one
              and so on. This implementation defaults to classic round robin
              in the case where all the jobs in the workflow can run on all
              the sites.</para>
            </listitem>

            <listitem>
              <para>
                <emphasis role="bold">Group</emphasis>
              </para>

              <para>Group of jobs will be assigned to the same site that can
              execute them. The use of the<emphasis role="bold"> PEGASUS
              profile key group</emphasis> in the DAX, associates a job with a
              particular group. The jobs that do not have the profile key
              associated with them, will be put in the default group. The jobs
              in the default group are handed over to the "Random" Site
              Selector for scheduling.</para>
            </listitem>

            <listitem>
              <para>
                <emphasis role="bold">Heft</emphasis>
              </para>

              <para>A version of the HEFT processor scheduling algorithm is
              used to schedule jobs in the workflow to multiple grid sites.
              The implementation assumes default data communication costs when
              jobs are not scheduled on to the same site. Later on this may be
              made more configurable.</para>

              <para>The runtime for the jobs is specified in the
              transformation catalog by associating the <emphasis
              role="bold">pegasus profile key runtime</emphasis> with the
              entries.</para>

              <para>The number of processors in a site is picked up from the
              attribute <emphasis role="bold">idle-nodes</emphasis> associated
              with the vanilla jobmanager of the site in the site
              catalog.</para>
            </listitem>

            <listitem>
              <para>
                <emphasis role="bold">NonJavaCallout</emphasis>
              </para>

              <para>Pegasus will callout to an external site selector.In this
              mode a temporary file is prepared containing the job information
              that is passed to the site selector as an argument while
              invoking it. The path to the site selector is specified by
              setting the property pegasus.site.selector.path. The environment
              variables that need to be set to run the site selector can be
              specified using the properties with a pegasus.site.selector.env.
              prefix. The temporary file contains information about the job
              that needs to be scheduled. It contains key value pairs with
              each key value pair being on a new line and separated by a
              =.</para>

              <para>The following pairs are currently generated for the site
              selector temporary file that is generated in the
              NonJavaCallout.</para>

              <table>
                <title>Table 1: Key Value Pairs that are currently generated
                for the site selector temporary file that is generated in the
                NonJavaCallout.</title>

                <tgroup cols="2">
                  <tbody>
                    <row>
                      <entry>
                        <emphasis role="bold">Key</emphasis>
                      </entry>

                      <entry>
                        <emphasis role="bold">Value</emphasis>
                      </entry>
                    </row>

                    <row>
                      <entry>version</entry>

                      <entry>is the version of the site selector api,currently
                      2.0.</entry>
                    </row>

                    <row>
                      <entry>transformation</entry>

                      <entry>is the fully-qualified definition identifier for
                      the transformation (TR) namespace::name:version.</entry>
                    </row>

                    <row>
                      <entry>derivation</entry>

                      <entry>is the fully qualified definition identifier for
                      the derivation (DV), namespace::name:version.</entry>
                    </row>

                    <row>
                      <entry>job.level</entry>

                      <entry>is the job's depth in the tree of the workflow
                      DAG.</entry>
                    </row>

                    <row>
                      <entry>job.id</entry>

                      <entry>is the job's ID, as used in the DAX file.</entry>
                    </row>

                    <row>
                      <entry>resource.id</entry>

                      <entry>is a pool handle, followed by whitespace,
                      followed by a gridftp server. Typically, each gridftp
                      server is enumerated once, so you may have multiple
                      occurances of the same site. There can be multiple
                      occurances of this key.</entry>
                    </row>

                    <row>
                      <entry>input.lfn</entry>

                      <entry>is an input LFN, optionally followed by a
                      whitespace and file size. There can be multiple
                      occurances of this key,one for each input LFN required
                      by the job.</entry>
                    </row>

                    <row>
                      <entry>wf.name</entry>

                      <entry>label of the dax, as found in the DAX's root
                      element. wf.index is the DAX index, that is incremented
                      for each partition in case of deferred planning.</entry>
                    </row>

                    <row>
                      <entry>wf.time</entry>

                      <entry>is the mtime of the workflow.</entry>
                    </row>

                    <row>
                      <entry>wf.manager</entry>

                      <entry>is the name of the workflow manager being used
                      .e.g condor</entry>
                    </row>

                    <row>
                      <entry>vo.name</entry>

                      <entry>is the name of the virtual organization that is
                      running this workflow. It is currently set to
                      NONE</entry>
                    </row>

                    <row>
                      <entry>vo.group</entry>

                      <entry>unused at present and is set to NONE.</entry>
                    </row>
                  </tbody>
                </tgroup>
              </table>
            </listitem>
          </itemizedlist>
        </listitem>
      </itemizedlist>

      <tip>
        <para>The site selector to use for site selection can be specified by
        setting the property <emphasis
        role="bold">pegasus.selector.site</emphasis></para>
      </tip>

      <figure>
        <title>Workflow Site Selection</title>

        <mediaobject>
          <imageobject>
            <imagedata contentdepth="100%" scalefit="1" width="100%" fileref="./images/refinement-site-selection.png"/>
          </imageobject>
        </mediaobject>
      </figure>
    </section>

    <section>
      <title>Job Clustering</title>

      <para>After site selection, the workflow is optionally handed for to the
      job clustering module, which clusters jobs that are scheduled to the
      same site. Clustering is usually done on short running jobs in order to
      reduce the remote execution overheads associated with a job. Clustering
      is described in detail in the <link linkend="reference">Reference
      Manual</link>chapter.</para>

      <tip>
        <para>The job clustering is turned on by passing the <emphasis
        role="bold">--cluster</emphasis> option to pegasus-plan.</para>
      </tip>
    </section>

    <section>
      <title>Addition of Data Transfer and Registration Nodes</title>

      <para>After job clustering, the workflow is handed to the Data Transfer
      module that adds data stage-in , inter site and stage-out nodes to the
      workflow. Data Stage-in Nodes transfer input data required by the
      workflow from the locations specified in the Replica Catalog to a
      directory on the execution site where the job executes. In case,
      multiple locations are specified for the same input file, the location
      from where to stage the data is selected using a <emphasis
      role="bold">Replica Selector</emphasis> . Replica Selection is described
      in detail in the Replica Selection section of the <link
      linkend="reference">Reference Manual.</link></para>

      <para>The process of adding the data stage-in and data stage-out nodes
      is handled by Transfer Refiners. All data transfer jobs in Pegasus are
      executed using <emphasis role="bold">pegasus-transfer</emphasis> . The
      pegasus-transfer client is a python based wrapper around various
      transfer clients like globus-url-copy, lcg-copy, wget, cp, ln . It looks
      at source and destination url and figures out automatically which
      underlying client to use. pegasus-transfer is distributed with the
      PEGASUS and can be found in the bin subdirectory . Pegasus Transfer
      Refiners are are described in the detail in the Transfers section of the
      <link linkend="reference">Reference Manual</link>. The default transfer
      refiner that is used in Pegasus is the <emphasis
      role="bold">Bundle</emphasis> Transfer Refiner, that bundles data
      stage-in nodes and data stage-out nodes on the basis of certain pegasus
      profile keys associated with the workflow.</para>

      <figure>
        <title>Addition of Data Transfer Nodes to the Workflow</title>

        <mediaobject>
          <imageobject>
            <imagedata contentdepth="100%" scalefit="1" width="100%" fileref="./images/refinement-transfer-jobs.png"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>Data Registration Nodes may also be added to the final executable
      workflow to register the location of the output files on the final
      output site back in the Replica Catalog . An output file is registered
      in the Replica Catalog if the register flag for the file is set to true
      in the DAX.</para>

      <figure>
        <title>Addition of Data Registration Nodes to the Workflow</title>

        <mediaobject>
          <imageobject>
            <imagedata contentdepth="100%" scalefit="1" width="100%" fileref="./images/refinement-registration-jobs.png"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>The data staged-in and staged-out from a directory that is created
      on the head node by a create dir job in the workflow. In the vanilla
      case, the directory is visible to all the worker nodes and compute jobs
      are launched in this directory on the shared filesystem. In the case
      where there is no shared filesystem, users can turn on worker node
      execution, where the data is staged from the head node directory to a
      directory on the worker node filesystem. This feature will be refined
      further for Pegasus 3.1. To use it with Pegasus 3.0 send email to
      <emphasis role="bold">pegasus-support at isi.edu</emphasis>.</para>

      <tip>
        <para>The replica selector to use for replica selection can be
        specified by setting the property <emphasis
        role="bold">pegasus.selector.replica</emphasis></para>
      </tip>
    </section>

    <section>
      <title>Addition of Create Dir and Cleanup Jobs</title>

      <para>After the data transfer nodes have been added to the workflow,
      Pegasus adds a create dir jobs to the workflow. Pegasus usually ,
      creates one workflow specific directory per compute site , that is on
      the shared filesystem of compute site. This directory is visible to all
      the worker nodes and that is where the data is staged-in by the data
      stage-in jobs.</para>

      <para>After addition of the create dir jobs, the workflow is optionally
      handed to the cleanup module. The cleanup module adds cleanup nodes to
      the workflow that remove data from the directory on the shared
      filesystem when it is no longer required by the workflow. This is useful
      in reducing the peak storage requirements of the workflow.</para>

      <tip>
        <para>The addition of the cleanup nodes to the workflow can be
        disabled by passing the <emphasis role="bold">--nocleanup</emphasis>
        option to pegasus-plan.</para>
      </tip>

      <figure>
        <title>Addition of Directory Creation and File Removal Jobs</title>

        <mediaobject>
          <imageobject>
            <imagedata contentdepth="100%" scalefit="1" width="100%" fileref="./images/refinement-creadir-rm-jobs.png"/>
          </imageobject>
        </mediaobject>
      </figure>
    </section>

    <section>
      <title>Code Generation</title>

      <para>The last step of refinement process, is the code generation where
      Pegasus writes out the executable workflow in a form understandable by
      the underlying workflow executor. At present Pegasus supports the
      following code generators</para>

      <orderedlist>
        <listitem>
          <para>
            <emphasis role="bold">Condor</emphasis>
          </para>

          <para>This is the default code generator for Pegasus . This
          generator generates the executable workflow as a Condor DAG file and
          associated job submit files. The Condor DAG file is passed as input
          to Condor DAGMan for job execution.</para>
        </listitem>

        <listitem>
          <para>
            <emphasis role="bold">Shell</emphasis>
          </para>

          <para>This Code Generator generates the executable workflow as a
          shell script that can be executed on the submit host. While using
          this code generator, all the jobs should be mapped to site local i.e
          specify <emphasis role="bold">--sites local </emphasis> to
          pegasus-plan.</para>

          <tip>
            <para>To use the Shell code Generator set the property <emphasis
            role="bold">pegasus.code.generator</emphasis> Shell</para>
          </tip>
        </listitem>
      </orderedlist>

      <figure>
        <title>Final Executable Workflow</title>

        <mediaobject>
          <imageobject>
            <imagedata contentdepth="100%" scalefit="1" width="100%" fileref="./images/refinement-final-executable-wf.png"/>
          </imageobject>
        </mediaobject>
      </figure>
    </section>
  </section>

  <section>
    <title>Pegasus-Plan</title>

    <para>pegasus-plan is the main executable that takes in the abstract
    workflow ( DAX ) and generates an executable workflow ( usually a Condor
    DAG ) by querying various catalogs and performing several refinement
    steps. Before users can run pegasus plan the following needs to be
    done</para>

    <orderedlist>
      <listitem>
        <para>Populate the various catalogs</para>

        <orderedlist>
          <listitem>
            <para>
              <emphasis role="bold">Replica Catalog</emphasis>
            </para>

            <para>The Replica Catalog needs to be catalogued with the
            locations of the input files required by the workflows. This can
            be done by using pegasus-rc-client (See the Replica section of
            <link linkend="creating_workflows">Creating
            Workflows</link>).</para>
          </listitem>

          <listitem>
            <para>
              <emphasis role="bold">Transformation Catalog</emphasis>
            </para>

            <para>The Transformation Catalog needs to be catalogued with the
            locations of the executables that the workflows will use. This can
            be done by using pegasus-tc-client (See the Transformation section
            of <link linkend="creating_workflows">Creating
            Workflows</link>).</para>
          </listitem>

          <listitem>
            <para>
              <emphasis role="bold">Site Catalog</emphasis>
            </para>

            <para>The Site Catalog needs to be catalogued with the site layout
            of the various sites that the workflows can execute on. A site
            catalog can be generated for OSG by using the client
            pegasus-sc-client (See the Site section of the <link
            linkend="creating_workflows">Creating Workflows</link>).</para>
          </listitem>
        </orderedlist>
      </listitem>

      <listitem>
        <para>Configure Properties</para>

        <para>After the catalogs have been configured, the user properties
        file need to be updated with the types and locations of the catalogs
        to use. These properties are described in the
        <emphasis>basic.properties</emphasis> files in the
        <emphasis>etc</emphasis> sub directory of <link
        linkend="installation">Installation</link> chapter.</para>

        <para>The basic properties that need to be set usually are listed
        below</para>

        <table>
          <title>Table2: Basic Properties that need to be set</title>

          <tgroup cols="1">
            <tbody>
              <row>
                <entry>pegasus.catalog.replica</entry>
              </row>

              <row>
                <entry>pegasus.catalog.replica.file |
                pegasus.catalog.replica.url</entry>
              </row>

              <row>
                <entry>pegasus.catalog.transformation</entry>
              </row>

              <row>
                <entry>pegasus.catalog.transformation.file</entry>
              </row>

              <row>
                <entry>pegasus.catalog.site</entry>
              </row>

              <row>
                <entry>pegasus.catalog.site.file</entry>
              </row>
            </tbody>
          </tgroup>
        </table>
      </listitem>
    </orderedlist>

    <para>To execute pegasus-plan user usually requires to specify the
    following options</para>

    <orderedlist>
      <listitem>
        <para><emphasis role="bold">--dax </emphasis> the path to the DAX file
        that needs to be mapped.</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">--dir </emphasis> the base directory where
        the executable workflow is generated</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">--sites </emphasis> comma separated list
        of execution sites.</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">--output</emphasis> the output site where
        to transfer the materialized output files.</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">--submit </emphasis> boolean value whether
        to submit the planned workflow for execution after planning is
        done.</para>
      </listitem>
    </orderedlist>

    <para>For a list of all options to pegasus-plan refer to the man
    page.</para>
  </section>

  <!-- section is autogenerated from properties file -->

  <xi:include href="configuration.xml"
              xmlns:xi="http://www.w3.org/2001/XInclude" />

  <section>
    <title>Execution Environments</title>

    <para>Pegasus supports a number of execution environments. An execution
    environment is a setup where jobs from a workflow are running.</para>

    <itemizedlist>
      <listitem>
        <para>The simplest execution environment does not involve Condor.
        Pegasus is capable of planning small workflows for full-local
        execution using a shell planner. Please refer to the <filename
        class="directory">examples</filename> directory in your Pegasus
        installation, the shell planner's <link
        linkend="local_shell_examples">documentation section</link>, or the
        tutorials, for details.</para>
      </listitem>

      <listitem>
        <para>A slighly more challenging setup is still all-local, but Condor
        manages queued jobs and Condor DAGMan the workflow. This setup permits
        limited parallelism with full scalability. With proper setup, Condor
        can scavenge cycles from unused computers in your department, enabling
        a pool of more than just a single machine.</para>
      </listitem>

      <listitem>
        <para>The vanilla setup are workflows to be executed in a grid
        environment. This setup requires a number of configurations, and an
        understanding of remote sites.</para>
      </listitem>

      <listitem>
        <para>Various advanced execution environment setups deal with
        minimally using Globus to obtain remote resources (Glide-ins),
        by-passing Globus completely (Condor-C), or supporting cloud computing
        (Nimbus, Eucalyptus, EC2). You should be familiar with the Grid
        Execution Environment, as some concept are borrowed.</para>
      </listitem>
    </itemizedlist>

    <section>
      <title>The Grid Execution Environment</title>

      <para>The rest of this chapter will focus on the vanilla grid execution
      environment.</para>

      <para>
        <figure id="concepts-fig-site-layout">
          <title>Grid Sample Site Layout</title>

          <mediaobject>
            <imageobject>
              <imagedata contentdepth="100%" scalefit="1" width="100%" align="center"
                         fileref="images/concepts-site-layout.jpg"
                         valign="middle" />
            </imageobject>
          </mediaobject>
        </figure>
      </para>

      <para>The vanilla grid environment shown in the figure <link
      linkend="concepts-fig-site-layout">above</link>. We will work from the
      left to the right top, then the right bottom.</para>

      <para>On the left side, you have a submit machine where Pegasus runs,
      Condor schedules jobs, and workflows are executed. We call it the
      <emphasis>submit host</emphasis> (SH), though its functionality can be
      assumed by a virtual machine image. In order to properly communicate
      over secured channels, it is important that the submit machine has a
      proper notion of time, i.e. runs an NTP daemon to keep accurate time. To
      be able to connect to remote clusters and receive connections from the
      remote clusters, the submit host has a public IP address to facilitate
      this communication.</para>

      <para>In order to send a job request to the remote cluster, Condor wraps
      the job into Globus calls via Condor-G. Globus uses GRAM to manage jobs
      on remote sites. In terms of a software stack, Pegasus wraps the job
      into Condor. Condor wraps the job into Globus. Globus transports the job
      to the remote site, and unwraps the Globus component, sending it to the
      remote site's <emphasis>resource manager</emphasis> (RM).</para>

      <para>To be able to communicate using the Globus security infrastructure
      (GSI), the submit machine needs to have the certificate authority (CA)
      certificates configured, requires a host certificate in certain
      circumstances, and the user a user certificate that is enabled on the
      remote site. On the remote end, the remote gatekeeper node requires a
      host certificate, all signing CA certificate chains and policy files,
      and a goot time source.</para>

      <para>In a grid environment, there are one or more clusters accessible
      via grid middleware like the <ulink url="http://www.globus.org/">Globus
      Toolkit</ulink>. In case of Globus, there is the Globus gatekeeper
      listening on TCP port 2119 of the remote cluster. The port is opened to
      a single machine called <emphasis>head node</emphasis> (HN).The
      head-node is typically located in a de-militarized zone (DMZ) of the
      firewall setup, as it requires limited outside connectivity and a public
      IP address so that it can be contacted. Additionally, once the
      gatekeeper accepted a job, it passes it on to a jobmanager. Often, these
      jobmanagers require a limited port range, in the example TCP ports
      40000-41000, to call back to the submit machine.</para>

      <para>For the user to be able to run jobs on the remote site, the user
      must have some form of an account on the remtoe site. The user's grid
      identity is passed from the submit host. An entity called <emphasis>grid
      mapfile</emphasis> on the gatekeeper maps the user's grid identity into
      a remote account. While most sites do not permit account sharing, it is
      possible to map multiple user certificates to the same account.</para>

      <para>The gatekeeper is the interface through which jobs are submitted
      to the remote cluster's resource manager. A resource manager is a
      scheduling system like PBS, Maui, LSF, FBSNG or Condor that queues tasks
      and allocates worker nodes. The <emphasis>worker nodes</emphasis> (WN)
      in the remote cluster might not have outside connectivity and often use
      all private IP addresses. The Globus toolkit requires a shared
      filesystem to properly stage files between the head node and worker
      nodes.</para>

      <note>
        <para>The shared filesystem requirement is imposed by Globus. Pegasus
        is capable of supporting advanced site layouts that do not require a
        shared filesystem. Please contact us for details, should you require
        such a setup.</para>
      </note>

      <para>To stage data between external sites for the job, it is
      recommended to enable a GridFTP server. If a shared networked filesystem
      is involved, the GridFTP server should be located as close to the
      file-server as possible. The GridFTP server requires TCP port 2811 for
      the control channel, and a limited port range for data channels, here as
      an example the TPC ports from 40000 to 41000. The GridFTP server
      requires a host certificate, the signing CA chain and policy files, a
      stable time source, and a gridmap file that maps between a user's grid
      identify and the user's account on the remote site.</para>

      <para>The GridFTP server is often installed on the head node, the same
      as the gatekeeper, so that they can share the grid mapfile, CA
      certificate chains and other setups. However, for performance purposes
      it is recommended that the GridFTP server has its own machine.</para>
    </section>

    <section>
      <title>A Cloud Execution Environment</title>

      <para>
        <figure id="concepts-fig-cloud-layout">
          <title>Cloud Sample Site Layout</title>

          <mediaobject>
            <imageobject>
              <imagedata contentdepth="100%" scalefit="1" width="100%" align="center" fileref="images/fg-pwms-prefio.3.png"
                         valign="middle" />
            </imageobject>
          </mediaobject>
        </figure>
      </para>

      <para>The pevious figure shows a sample layout for sky computing (as in:
      multiple clouds) as supported by Pegasus. At this point, it is up to the
      user to provision the remote resources with a proper VM image that
      includes a Condor startd and proper Condor configuration to report back
      to a Condor collector that the Condor schedd has access to.</para>

      <para>In this discussion, the <emphasis>submit host</emphasis> (SH) is
      located logically external to the cloud provider(s). The SH is the point
      where a user submits Pegasus workflows for execution. This site
      typically runs a Condor collector to gather resource announcements, or
      is part of a larger Condor pool that collects these announcement. Condor
      makes the remote resources available to the submit host&amp;rsquor;s Condor
      installation.</para>

      <para>The <link linkend="concepts-fig-cloud-layout">figure above</link>
      shows the way Pegasus WMS is deployed in cloud computing resources,
      ignoring how these resources were provisioned. The provisioning request
      shows multiple resources per provisioning request.</para>

      <para>The provisioning broker -- Nimbus, Eucalyptus or EC2 -- at the
      remote site is responsible to allocate and set up the resources. For a
      multi-node request, the worker nodes often require access to a form of
      shared data storage. Concretely, either a POSIX-compliant shared file
      system (e.g. NFS, PVFS) is available to the nodes, or can be brought up
      for the lifetime of the application workflow. The task steps of the
      application workflow facilitate shared file systems to exchange
      intermediary results between tasks on the same cloud site. Pegasus also
      supports an S3 data mode for the application workflow data
      staging.</para>

      <para>The initial stage-in and final stage-out of application data into
      and out of the node set is part of any Pegasus-planned workflow. Several
      configuration options exist in Pegasus to deal with the dynamics of push
      and pull of data, and when to stage data. In many use-cases, some form
      of external access to or from the shared file system that is visible to
      the application workflow is required to facilitate successful data
      staging. However, Pegasus is prepared to deal with a set of boundary
      cases.</para>

      <para>The data server in the figure is shown at the submit host. This is
      not a strict requirement. The data server for consumed data and data
      products may both be different and external to the submit host.</para>

      <para>Once resources begin appearing in the pool managed by the submit
      machine&amp;rsquor;s Condor collector, the application workflow can be
      submitted to Condor. A Condor DAGMan will manage the application
      workflow execution. Pegasus run-time tools obtain timing-, performance
      and provenance information as the application workflow is executed. At
      this point, it is the user's responsibility to de-provision the
      allocated resources.</para>

      <para>In the figure, the cloud resources on the right side are assumed
      to have uninhibited outside connectivity. This enables the Condor I/O to
      communicate with the resources. The right side includes a setup where
      the worker nodes use all private IP, but have out-going connectivity and
      a NAT router to talk to the internet. The <emphasis>Condor connection
      broker</emphasis> (CCB) facilitates this setup almost
      effortlessly.</para>

      <para>The left side shows a more difficult setup where the connectivity
      is fully firewalled without any connectivity except to in-site nodes. In
      this case, a proxy server process, the <emphasis>generic connection
      broker</emphasis> (GCB), needs to be set up in the DMZ of the cloud site
      to facilitate Condor I/O between the submit host and worker
      nodes.</para>

      <para>If the cloud supports data storage servers, Pegasus is starting to
      support workflows that require staging in two steps: Consumed data is
      first staged to a data server in the remote site's DMZ, and then a
      second staging task moves the data from the data server to the worker
      node where the job runs. For staging out, data needs to be first staged
      from the job's worker node to the site's data server, and possibly from
      there to another data server external to the site. Pegasus is capable to
      plan both steps: Normal staging to the site's data server, and the
      worker-node staging from and to the site's data server as part of the
      job. We are working on expanding the current code to support a more
      generic set by Pegasus 3.1.</para>

      <section>
        <title>Shared File Systems</title>

        <para>Ideally, any allocation of multiple resources shares a file
        system among them. This can be either a globally shared file system
        like NFS, or a per-request file system dynamically allocated like PVFS
        or NFS. Often, for the sake of speed, it is advisable that each
        resource has its own fast local non-shared disk space that can be
        transiently used by application workflow jobs.</para>

        <para>Pegasus recommends the use of a shared file system, because it
        simplifies workflows and may even improve performance. Without shared
        file system, the same data may need to be staged to the same site but
        different resources multiple times. Without shared file systems, data
        products of a parent job need to be staged out, and staged back in to
        the child job, currently via an external data server, even if parent
        and child run on the same multi-resource allocation.</para>

        <para>The Pegasus team explores the option of dynamically bringing up
        an NFS server on one resource of a multi-resource allocation for the
        duration of the allocation. This option is part of the configuration
        for an experiment&amp;rsquor;s workflow&amp;rsquor;s provisioning stage. We do
        not expect cloud sites to ban NFS and RPC traffic between resources of
        the same multi-resource allocation.</para>
      </section>
    </section>
  </section>

  <section>
    <title>Resource Configurations</title>

    <para>This section discusses the various resource configurations that can
    be used with Pegasus when Condor DAGMan is used as the workflow execution
    engine. It is assumed that there is a submit host where the workflow is
    submitted for execution. The following classification is done based the
    mechanism used for submitting jobs to the Grid resources and monitoring
    them. The classifications explored in this document are using Globus GRAM
    and using a Condor pool. Both of the configurations use Condor DAGMan to
    maintain the dependencies between the jobs, but differ in the manner as to
    how the jobs are launched. A combination of the above mentioned approach
    is also possible where some of the tasks in the workflow are executed in
    the Condor pool and the rest are executed on remote resources using
    Condor-G.</para>

    <section>
      <title>Locally on the Submit Host</title>

      <para>In this configuration , Pegasus schedules the jobs to run locally
      on the submit host. This is achieved by executing the workflow on site
      local ( <emphasis role="bold">--sites local </emphasis>option to
      pegasus-plan ). The <emphasis role="bold">site "local" is a reserved
      site</emphasis> in Pegasus and results in the jobs to run on the submit
      host in condor universe local.</para>
    </section>

    <section>
      <title>Using Globus GRAM</title>

      <para>In this configuration, it is assumed that the target execution
      system consists of one or more Grid resources. These resources may be
      geographically distributed and under various administrative domains.
      Each resource might be a single desktop computer or a network of
      workstations (NOW) or a cluster of dedicated machines. However, each
      resource must provide a Globus GRAM interface which allows the users to
      submit jobs remotely. In case the Grid resource consists of multiple
      compute nodes, e.g. a cluster or a network of workstations, there is a
      central entity called the head node that acts as the single point of job
      submissions to the resource. It is generally possible to specify whether
      the submitted job should run on the head node of the resource or a
      worker node in the cluster or NOW. In the latter case, the head node is
      responsible for submitting the job to a local resource management system
      (PBS, LSF, Condor etc) which controls all the machines in the resource.
      Since, the head node is the central point of job submissions to the
      resource it should not be used for job execution since that can overload
      the head node delaying further job submissions. Pegasus does not make
      any assumptions about the configuration of the remote resource; rather
      it provides the mechanisms by which such distinctions can be
      made.</para>

      <figure>
        <title>Resource Configuration using GRAM</title>

        <mediaobject>
          <imageobject>
            <imagedata contentdepth="100%" scalefit="1" width="100%" fileref="./images/gram_layout.png"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>In this configuration, Condor-G is used for submitting jobs to
      these resources. Condor-G is an extension to Condor that allows the jobs
      to be described in a Condor submit file and when the job is submitted to
      Condor for execution, it uses the Globus GRAM interface to submit the
      job to the remote resource and monitor its execution. The distinction is
      made in the Condor submit files by specify the universe as Grid and the
      grid_resource or globusscheduler attribute is used to indicate the
      location of the head node for the remote resource. Thus, Condor DAGMan
      is used for maintaining the dependencies between the jobs and Condor-G
      is used to launch the jobs on the remote resources using GRAM. The
      implicit assumption in this case is that all the worker nodes on a
      remote resource have access to a shared file system that can be used for
      data transfer between the tasks mapped on that resource. This data
      transfer is done using files.</para>

      <para>Below is an example of a site configured to use Globus GRAM2|
      GRAM5</para>

      <programlisting> &lt;site  handle="isi" arch="x86" os="LINUX" osrelease="" osversion="" glibc=""&gt;
       &lt;grid  type="gt2" contact="smarty.isi.edu/jobmanager-pbs" scheduler="PBS" jobtype="auxillary"/&gt;
       &lt;grid  type="gt2" contact="smarty.isi.edu/jobmanager-pbs" scheduler="PBS" jobtype="compute"/&gt;
       &lt;head-fs&gt;
               &lt;scratch&gt;
                   &lt;shared&gt;
                         &lt;file-server protocol="gsiftp" url="gsiftp://skynet-data.isi.edu" \
                                                     mount-point="/nfs/scratch01"/&gt;
                         &lt;internal-mount-point mount-point="/nfs/scratch01"/&gt;
                    &lt;/shared&gt;
               &lt;/scratch&gt;
               &lt;storage&gt;
                   &lt;shared&gt;
                         &lt;file-server protocol="gsiftp" url="gsiftp://skynet-data.isi.edu" \
                                                     mount-point="/exports/storage01"/&gt;
                         &lt;internal-mount-point mount-point="/exports/storage01"/&gt;
                    &lt;/shared&gt;
               &lt;/storage&gt;
        &lt;/head-fs&gt;
        &lt;replica-catalog  type="LRC" url="rlsn://smarty.isi.edu" /&gt;
  &lt;/site&gt;
</programlisting>
    </section>

    <section>
      <title>Condor Pool</title>

      <para>A Condor pool is a set of machines that use Condor for resource
      management. A Condor pool can be a cluster of dedicated machines or a
      set of distributively owned machines. Pegasus can generate concrete
      workflows that can be executed on a Condor pool.</para>

      <figure>
        <title>The distributed resources appear to be part of a Condor
        pool.</title>

        <mediaobject>
          <imageobject>
            <imagedata contentdepth="100%" scalefit="1" width="100%" fileref="images/condor_layout.png"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>The workflow is submitted using DAGMan from one of the job
      submission machines in the Condor pool. It is the responsibility of the
      Central Manager of the pool to match the task in the workflow submitted
      by DAGMan to the execution machines in the pool. This matching process
      can be guided by including Condor specific attributes in the submit
      files of the tasks. If the user wants to execute the workflow on the
      execution machines (worker nodes) in a Condor pool, there should be a
      resource defined in the site catalog which represents these execution
      machines. The universe attribute of the resource should be vanilla.
      There can be multiple resources associated with a single Condor pool,
      where each resource identifies a subset of machine (worker nodes) in the
      pool. Pegasus currently uses the FileSystemDomain classad[] attribute to
      restrict the set of machines that make up a single resource. To clarify
      this point, suppose there are certain execution machines in the Condor
      pool whose FileSystemDomain is set to &amp;ldquo;viz.isi.edu&amp;rdquor;. If the
      user wants to execute the workflow on these machines, then there should
      be a site, say &amp;ldquo;isi_viz&amp;rdquor; defined in the site catalog and
      the FileSystemDomain and universe attributes for this resource should be
      defined as &amp;ldquo;viz.isi.edu&amp;rdquor; and &amp;ldquo;vanilla&amp;rdquor;
      respectively. When invoking Pegasus, the user should select isi_viz as
      the compute resource.</para>

      <programlisting> &lt;site  handle="isi_viz" arch="x86" os="LINUX" osrelease="" osversion="" glibc=""&gt;
       &lt;grid  type="gt2" contact="smarty.isi.edu/jobmanager-pbs" scheduler="PBS" jobtype="auxillary"/&gt;
       &lt;grid  type="gt2" contact="smarty.isi.edu/jobmanager-pbs" scheduler="PBS" jobtype="compute"/&gt;
       &lt;head-fs&gt;
              &lt;scratch&gt;
                    &lt;shared&gt;
                           &lt;file-server protocol="gsiftp" url="gsiftp://viz-login.isi.edu" \
                                                              mount-point="/nfs/scratch01"/&gt;
                           &lt;internal-mount-point mount-point="/nfs/scratch01"/&gt;
                    &lt;/shared&gt;
              &lt;/scratch&gt;
              &lt;storage&gt;
                    &lt;shared&gt;
                           &lt;file-server protocol="gsiftp" url="gsiftp://viz-login.isi.edu" \
                                                           mount-point="/exports/storage01"/&gt;
                           &lt;internal-mount-point mount-point="/exports/storage01"/&gt;
                           &lt;/shared&gt;
               &lt;/storage&gt;
       &lt;/head-fs&gt;
       &lt;replica-catalog  type="LRC" url="rlsn://smarty.isi.edu"/&gt;
       <emphasis role="bold">&lt;profile namespace="condor" key="universe"&gt;vanilla&lt;/profile&gt; 
       &lt;profile namespace="condor" key="FileSystemDomain"&gt;viz.isi.edu&lt;/profile&gt;</emphasis>
 &lt;/site&gt;
</programlisting>
    </section>

    <section>
      <title>Condor Glideins</title>

      <para>Pegasus WMS can execute workflows over Condor pool. This pool can
      contain machines managed by a single institution or department and
      belonging to a single administrative domain. This is the case for most
      of the Condor pools. In this section we describe how machines from
      different administrative domains and supercomputing centers can be
      dynamically added to a Condor pool for certain timeframe. These machines
      join the Condor pool temporarily and can be used to execute jobs in a
      non preemptive manner. This functionality is achieved using a Condor
      feature called Glide-in http://cs.wisc.edu/condor/glidein that uses
      Globus GRAM interface for migrating machines from different domains to a
      Condor pool. The number of machines and the duration for which they are
      required can be specified</para>

      <para>In this case, we use the abstraction of a local Condor pool to
      execute the jobs in the workflow over remote resources that have joined
      the pool for certain timeframe. Details about the use of this feature
      can be found in the condor manual
      (http://cs.wisc.edu/condor/manual/).</para>

      <para>A basic step to migrate in a job to a local condor pool is
      described below.</para>

      <programlisting><emphasis role="bold">$condor_glidein -count 10 gatekeeper.site.edu/jobmanager-pbs</emphasis></programlisting>

      <para>
        <emphasis role="bold">GlideIn of Remote Globus Resources</emphasis>
      </para>

      <para>The above step glides in 10 nodes to the user&amp;rsquor;s local
      condor pool, from the remote pbs scheduler running at
      gatekeeper.site.edu. By default, the glide in binaries are installed in
      the users home directory on the remote host.</para>

      <para>It is possible that the Condor pool can contain resources from
      multiple Grid sites. It is normally the case that the resources from a
      particular site share the same file system and thus use the same
      FileSystemDomain attribute while advertising their presence to the
      Central Manager of the pool. If the user wants to run his jobs on
      machines from a particular Grid site, he has to specify the
      FileSystemDomain attribute in the requirements classad expression in the
      submit files with a value matching the FileSystemDomain of the machines
      from that site. For example, the user migrates nodes from the ISI
      cluster (with FileSystemDomain viz.isi.org) into a Condor pool and
      specifies FileSystemDomain == &amp;ldquo;viz.isi.edu&amp;rdquor;. Condor would
      then schedule the jobs only on the nodes from the ISI VIZ cluster in the
      local condor pool. The FileSystemDomain can be specified for an
      execution site in the site catalog with condor profile namespace as
      follows</para>

      <programlisting> &lt;site  handle="isi_viz" arch="x86" os="LINUX" osrelease="" osversion="" glibc=""&gt;
       &lt;grid  type="gt2" contact="smarty.isi.edu/jobmanager-pbs" scheduler="PBS" jobtype="auxillary"/&gt;
       &lt;grid  type="gt2" contact="smarty.isi.edu/jobmanager-pbs" scheduler="PBS" jobtype="compute"/&gt;
       &lt;head-fs&gt;
              &lt;scratch&gt;
                    &lt;shared&gt;
                         &lt;file-server protocol="gsiftp" url="gsiftp://viz-login.isi.edu" \
                                                                 mount-point="/nfs/scratch01"/&gt;
                         &lt;internal-mount-point mount-point="/nfs/scratch01"/&gt;
                    &lt;/shared&gt;
              &lt;/scratch&gt;
              &lt;storage&gt;
                     &lt;shared&gt;
                          &lt;file-server protocol="gsiftp" url="gsiftp://viz-login.isi.edu" \
                                                                      mount-point="/exports/storage01"/&gt;
                          &lt;internal-mount-point mount-point="/exports/storage01"/&gt;
                     &lt;/shared&gt;
              &lt;/storage&gt;
       &lt;/head-fs&gt;
       &lt;replica-catalog  type="LRC" url="rlsn://smarty.isi.edu"/&gt;
       <emphasis role="bold">&lt;profile namespace="pegasus" key="style"&gt;glidein&lt;/profile&gt;</emphasis>
       <emphasis role="bold">&lt;profile namespace="condor" key="universe"&gt;vanilla&lt;/profile&gt; 
       &lt;profile namespace="condor" key="FileSystemDomain"&gt;viz.isi.edu&lt;/profile&gt;</emphasis>
 &lt;/site&gt;
</programlisting>

      <para>Specifying the FileSystemDomain key in condor namespace for a
      site, triggers Pegasus into generating the requirements classad
      expression in the submit file for all the jobs scheduled on that
      particular site. For example, in the above case all jobs scheduled on
      site isi_condor would have the following expression in the submit
      file.</para>

      <programlisting>requirements = FileSystemDomain == &amp;ldquo;viz.isi.edu&amp;rdquor;</programlisting>
    </section>

    <section>
      <title>Condor Glidein's using glideinWMS</title>

      <para><ulink url="???">glideinWMS</ulink> is a glidein system widely
      used on Open Science Grid. Pegasus has a convenience style named
      glideinWMS to make running workflows on top of glideinWMS easy. This
      style is similar to the Condor style, but provides some more defaults
      set up to work well with glideinWMS. All you have to do is specify the
      style profile in the site catalog:</para>

      <programlisting> &lt;site  handle="isi_viz" arch="x86" os="LINUX" osrelease="" osversion="" glibc=""&gt;
       &lt;grid  type="gt2" contact="smarty.isi.edu/jobmanager-pbs" scheduler="PBS" jobtype="auxillary"/&gt;
       &lt;grid  type="gt2" contact="smarty.isi.edu/jobmanager-pbs" scheduler="PBS" jobtype="compute"/&gt;
       &lt;head-fs&gt;
              &lt;scratch&gt;
                    &lt;shared&gt;
                         &lt;file-server protocol="gsiftp" url="gsiftp://viz-login.isi.edu" \
                                                                 mount-point="/nfs/scratch01"/&gt;
                         &lt;internal-mount-point mount-point="/nfs/scratch01"/&gt;
                    &lt;/shared&gt;
              &lt;/scratch&gt;
              &lt;storage&gt;
                     &lt;shared&gt;
                          &lt;file-server protocol="gsiftp" url="gsiftp://viz-login.isi.edu" \
                                                                      mount-point="/exports/storage01"/&gt;
                          &lt;internal-mount-point mount-point="/exports/storage01"/&gt;
                     &lt;/shared&gt;
              &lt;/storage&gt;
       &lt;/head-fs&gt;
       &lt;replica-catalog  type="LRC" url="rlsn://smarty.isi.edu"/&gt;
       <emphasis role="bold">&lt;profile namespace="pegasus" key="style"&gt;glideinwms&lt;/profile&gt;</emphasis>
 &lt;/site&gt;</programlisting>

      <para>Planned jobs will have universe, requirements and rank set:</para>

      <programlisting>universe = vanilla
rank = DaemonStartTime
requirements = ((GLIDEIN_Entry_Name == "isi_iz") || (TARGET.Pegasus_Site == "isi_viz")) \
               &amp;&amp; (IS_MONITOR_VM == False) &amp;&amp; (Arch != "") &amp;&amp; (OpSys != "") \
               &amp;&amp; (Disk != -42) &amp;&amp; (Memory &gt; 1) &amp;&amp; (FileSystemDomain != "")
</programlisting>
    </section>

    <section>
      <title>Glite</title>

      <para>This section describes the various changes required in the site
      catalog for Pegasus to generate an executable workflow that uses gLite
      blahp to directly submit to PBS on the local machine. This mode of
      submission should only be used when the condor on the submit host can
      directly talk to scheduler running on the cluster. It is recommended
      that the cluster that gLite talks to is designated as a separate compute
      site in the Pegasus site catalog. To tag a site as a gLite site the
      following two profiles need to be specified for the site in the site
      catalog</para>

      <orderedlist>
        <listitem>
          <para><emphasis role="bold">pegasus</emphasis> profile <emphasis
          role="bold">style</emphasis> with value set to <emphasis
          role="bold">glite</emphasis>.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">condor</emphasis> profile <emphasis
          role="bold">grid_resource</emphasis> with value set to <emphasis
          role="bold">pbs|lsf</emphasis></para>
        </listitem>
      </orderedlist>

      <para>An example site catalog entry for a glite site looks as follows in
      the site catalog</para>

      <programlisting> &lt;site  handle="isi_viz_glite" arch="x86" os="LINUX" osrelease="" osversion="" glibc=""&gt;
       &lt;grid  type="gt2" contact="smarty.isi.edu/jobmanager-pbs" scheduler="PBS" jobtype="auxillary"/&gt;
       &lt;grid  type="gt2" contact="smarty.isi.edu/jobmanager-pbs" scheduler="PBS" jobtype="compute"/&gt;
       &lt;head-fs&gt;
             &lt;scratch&gt;
                   &lt;shared&gt;
                         &lt;file-server protocol="gsiftp" url="gsiftp://viz-login.isi.edu" \
                                                       mount-point="/nfs/scratch01"\&gt;
                         &lt;internal-mount-point mount-point="/nfs/scratch01"/&gt;
                   &lt;/shared&gt;
             &lt;/scratch&gt;
             &lt;storage&gt;
                   &lt;shared&gt;
                         &lt;file-server protocol="gsiftp" url="gsiftp://viz-login.isi.edu" \
                                                mount-point="/exports/storage01"\&gt;
                         &lt;internal-mount-point mount-point="/exports/storage01"/&gt;
                    &lt;/shared&gt;
             &lt;/storage&gt;
       &lt;/head-fs&gt;
       &lt;replica-catalog  type="LRC" url="rlsn://smarty.isi.edu" /&gt;
               
       <emphasis role="bold">&lt;!-- following profiles reqd for glite grid style--&gt;
       &lt;profile namespace="pegasus" key="style"&gt;glite&lt;/profile&gt; 
       &lt;profile namespace="condor" key="grid_resource"&gt;pbs&lt;/profile&gt;</emphasis>
 &lt;/site&gt;
</programlisting>

      <para/>

      <section>
        <title>Changes to Jobs</title>

        <para>As part of applying the style to the job, this style adds the
        following classads expressions to the job description.</para>

        <orderedlist>
          <listitem>
            <para>+remote_queue - value picked up from globus profile
            queue</para>
          </listitem>

          <listitem>
            <para>+remote_cerequirements - See below</para>
          </listitem>
        </orderedlist>

        <para/>

        <section>
          <title>Remote CE Requirements</title>

          <para>The remote CE requirements are constructed from the following
          profiles associated with the job. The profiles for a job are derived
          from various sources</para>

          <orderedlist>
            <listitem>
              <para>transformation catalog</para>
            </listitem>

            <listitem>
              <para>site catalog</para>
            </listitem>

            <listitem>
              <para>DAX</para>
            </listitem>

            <listitem>
              <para>user properties</para>
            </listitem>
          </orderedlist>

          <para>The following globus profiles if associated with the job are
          picked up and translated to corresponding glite key</para>

          <orderedlist>
            <listitem>
              <para>hostcount -&gt; PROCS</para>
            </listitem>

            <listitem>
              <para>count -&gt; NODES</para>
            </listitem>

            <listitem>
              <para>maxwalltime -&gt; WALLTIME</para>
            </listitem>
          </orderedlist>

          <para>The following condor profiles if associated with the job are
          picked up and translated to corresponding glite key</para>

          <orderedlist>
            <listitem>
              <para>priority -&gt; PRIORITY</para>
            </listitem>
          </orderedlist>

          <para>All the env profiles are translated to MYENV</para>

          <para>The remote_cerequirements expression is constructed on the
          basis of the profiles associated with job . An example
          +remote_cerequirements classad expression in the submit file is
          listed below</para>

          <programlisting><emphasis role="bold">+remote_cerequirements = "PROCS==18 &amp;&amp; NODES==1 &amp;&amp; PRIORITY==10 &amp;&amp; WALLTIME==3600 \
   &amp;&amp; PASSENV==1 &amp;&amp; JOBNAME==\"TEST JOB\" &amp;&amp; MYENV ==\"JAVA_HOME=/bin/java,APP_HOME=/bin/app\""</emphasis></programlisting>
        </section>

        <section>
          <title>Specifying directory for the jobs</title>

          <para>gLite blahp does not follow the remote_initialdir or
          initialdir classad directives. Hence, all the jobs that have this
          style applied don't have a remote directory specified in the submit
          directory. Instead, Pegasus relies on kickstart to change to the
          working directory when the job is launched on the remote
          node.</para>
        </section>
      </section>
    </section>

    <section>
      <title>Condor Pools Without a Shared Filesystem and Using Condor File
      Transfers</title>

      <para>It is possible to run on a condor pool without the worker nodes
      sharing a filesystem and relying on Condor to do the file transfers.
      However, for the Condor file to transfer the input data for a job, all
      the input data</para>

      <orderedlist>
        <listitem>
          <para>Must be on the submit host</para>
        </listitem>

        <listitem>
          <para>The input files should be catalogued as file URL's in the
          Replica Catalog with the pool attribute set as local for all the
          locations. To make sure that local URL's are always picked up use
          the Local Replica Selector.</para>
        </listitem>
      </orderedlist>

      <para>To set up Pegasus to plan workflows for Amazon you need to create
      an 'local-condor' entry your site catalog. The site configuration is
      similar to what you would create for running on a local Condor pool (See
      previous section).</para>

      <programlisting> &lt;site  handle="local-condor" arch="x86" os="LINUX"&gt;
        &lt;grid  type="gt2" contact="localhost/jobmanager-fork" scheduler="Fork" jobtype="auxillary"/&gt;
        &lt;grid  type="gt2" contact="localhost/jobmanager-condor" scheduler="unknown" jobtype="compute"/&gt;
        &lt;head-fs&gt;

          <emphasis role="bold">&lt;!-- the paths for scratch filesystem are the paths on local site as we execute create dir job
               on local site. Improvements planned for 3.1 release.--&gt;</emphasis>
            &lt;scratch&gt;
                &lt;shared&gt;
                    &lt;file-server protocol="file" url="file:///" mount-point="/submit-host/scratch"/&gt;
                    &lt;internal-mount-point mount-point="/submit-host/scratch"/&gt;
                &lt;/shared&gt;
            &lt;/scratch&gt;
            &lt;storage&gt;
                &lt;shared&gt;
                    &lt;file-server protocol="file" url="file:///" mount-point="/glusterfs/scratch"/&gt;
                    &lt;internal-mount-point mount-point="/glusterfs/scratch"/&gt;
                &lt;/shared&gt;
            &lt;/storage&gt;
        &lt;/head-fs&gt;
        &lt;replica-catalog  type="LRC" url="rlsn://dummyValue.url.edu" /&gt;
        &lt;profile namespace="env" key="PEGASUS_HOME" &gt;/cluster-software/pegasus/2.4.1&lt;/profile&gt;
        &lt;profile namespace="env" key="GLOBUS_LOCATION" &gt;/cluster-software/globus/5.0.1&lt;/profile&gt;

        <emphasis role="bold">&lt;!-- profies for site to be treated as condor pool --&gt;</emphasis>
        &lt;profile namespace="pegasus" key="style" &gt;condor&lt;/profile&gt;
        &lt;profile namespace="condor" key="universe" &gt;vanilla&lt;/profile&gt;

        <emphasis role="bold">&lt;!-- required for Condor File IO and transferring stdout and stderr back --&gt;</emphasis>
        &lt;profile namespace="condor" key="should_transfer_files"&gt;YES&lt;/profile&gt;
        &lt;profile namespace="condor" key="transfer_output"&gt;true&lt;/profile&gt;
        &lt;profile namespace="condor" key="transfer_error"&gt;true&lt;/profile&gt;
        &lt;profile namespace="condor" key="WhenToTransferOutput"&gt;ON_EXIT&lt;/profile&gt;
        
        <emphasis role="bold">&lt;!-- to enable kickstart staging from local site--&gt;</emphasis>
        &lt;profile namespace="condor" key="transfer_executable"&gt;true&lt;/profile&gt;


    &lt;/site&gt;
</programlisting>

      <para>Next you need need to update pegasus.properties to tell Pegasus to
      turn on Condor File IO and execute jobs on the worker nodes
      filesystem.</para>

      <programlisting>pegasus.execute.*.filesystem.local = true       # Turn on second-level staging (SLS)
pegasus.transfer.refiner = Condor               # Condor Specific Transfer Refiner
pegasus.transfer.sls.*.impl = Condor            # Turn on Condor File Transfer to stage data to the worker nodes
pegasus.selector.replica =  Local               # Always prefer file URL's on the submit host with pool attribute set to local
</programlisting>

      <para>By default, Pegasus creates a workflow specific execution
      directory that is associated with the compute site ( in this case
      local-condor). However, in this scenario, the workflow specific
      execution directory needs to be created on the submit host, as native
      Condor File I/O only works against local directories. To ensure that the
      create dir job for the workflow for the local-condor site, you need to
      do the following</para>

      <orderedlist>
        <listitem>
          <para>the paths for the scratch filesystem in the site catalog entry
          for local-condor site should actually refer to paths on the submit
          host. It is illustrated in the example site catalog entry
          above.</para>
        </listitem>

        <listitem>
          <para>have an entry in the transformation catalog for
          pegasus::dirmanager for site local-condor that has a condor profile
          that makes the create dir job run in local universe instead of
          vanilla.</para>

          <programlisting># This entry for pegaasus-dirmanager is to force the create dir jobs to run in the local universe instead of
# the vanilla universe. The CONDOR transfer settings are so that stdout and stderr are
# not redirected to initialdir instead of the submit dir

local-condor pegasus::dirmanager /path/on/submithost/pegasus/bin/dirmanager INSTALLED INTEL32::LINUX CONDOR::universe="local"CONDOR::requirements="";CONDOR::transfer_output="";CONDOR::transfer_error="";CONDOR::whentotransferoutput=""
</programlisting>
        </listitem>
      </orderedlist>

      <note>
        <para>For Pegasus 3.1 running workflows in this setup will be made
        much simpler.</para>
      </note>
    </section>

    <section id="running_on_cloud">
      <title>Cloud</title>

      <section>
        <title>Using Amazon</title>

        <para>In order to use Amazon to execute workflows you need to a) set
        up an execution environment in EC2, and b) configure Pegasus to plan
        workflows for that environment.</para>

        <para>There are many different ways to set up the execution
        environment in Amazon. The easiest way is to use a submit machine
        outside the cloud, and to provision several worker nodes and a file
        server node in the cloud as shown here:</para>

        <para>
          <figure id="ec2">
            <title>Amazon EC2</title>

            <mediaobject>
              <imageobject>
                <imagedata contentdepth="100%" scalefit="1" width="100%" align="center" fileref="images/ec2.png"
                           valign="middle" />
              </imageobject>
            </mediaobject>
          </figure>
        </para>

        <para>The submit machine runs Pegasus and a Condor master (collector,
        schedd, negotiator),the workers run a Condor startd, and the file
        server node exports an NFS file system. The workers' startd is
        configured to connect to the master running outside the cloud. The
        worker also mounts the NFS file system. More information on setting up
        Condor for this environment can be found at <ulink
        url="http://www.isi.edu/~gideon/condor-ec2/">http://www.isi.edu/~gideon/condor-ec2/</ulink>.</para>

        <para>To set up Pegasus to plan workflows for Amazon you need to
        create an 'ec2' entry your site catalog. The site configuration is
        similar to what you would create for running on a local Condor pool
        (See previous section).</para>

        <programlisting> 
 &lt;site  handle="ec2" arch="x86" os="LINUX" osrelease="" osversion="" glibc=""&gt;
       &lt;!-- These entries are required, but not used --&gt;

       &lt;grid  type="gt2" contact="example.com/jobmanager-pbs" scheduler="PBS" jobtype="auxillary"/&gt;
       &lt;grid  type="gt2" contact="example.com/jobmanager-fork" scheduler="PBS" jobtype="compute"/&gt;
       &lt;head-fs&gt;
             &lt;scratch&gt;
                   &lt;shared&gt;
                         &lt;file-server protocol="gsiftp" url="gridftp://example.com" \
                                                       mount-point="/nfs"\&gt;
                         &lt;internal-mount-point mount-point="/nfs"/&gt;
                   &lt;/shared&gt;
             &lt;/scratch&gt;
             &lt;storage&gt;
                   &lt;shared&gt;
                         &lt;file-server protocol="gsiftp" url="gridftp://example.com" \
                                                mount-point=""\&gt;
                         &lt;internal-mount-point mount-point=""/&gt;
                    &lt;/shared&gt;
             &lt;/storage&gt;
       &lt;/head-fs&gt;
       &lt;replica-catalog  type="LRC" url="rlsn://smarty.isi.edu" /&gt;
               
       <emphasis role="bold">&lt;!-- following profiles reqd for ec2 --&gt;
       
       &lt;profile namespace="env" key="PEGASUS_HOME"&gt;/usr/local/pegasus/default&lt;/profile&gt;
       
       &lt;!-- The directory where a user wants to run the jobs on the
            nodes retrived from ec2 --&gt;
       &lt;profile namespace="env" key="wntmp"&gt;/mnt&lt;/profile&gt;

       &lt;!-- Use condor style vanilla universe jobs --&gt;
       &lt;profile namespace="pegasus" key="style"&gt;condor&lt;/profile&gt;
       &lt;profile namespace="condor" key="universe"&gt;vanilla&lt;/profile&gt;

       &lt;!-- This ensures that Condor sends stdout and stderr back to the submit host --&gt;	
       &lt;profile namespace="condor" key="should_transfer_files"&gt;YES&lt;/profile&gt;
       &lt;profile namespace="condor" key="transfer_output"&gt;true&lt;/profile&gt;
       &lt;profile namespace="condor" key="transfer_error"&gt;true&lt;/profile&gt;
       &lt;profile namespace="condor" key="WhenToTransferOutput"&gt;ON_EXIT&lt;/profile&gt;

       &lt;!-- This ensures that the jobs generated by Pegasus will run on the remote workers --&gt;
       &lt;profile namespace="condor" key="requirements"&gt;(Arch==Arch)&amp;amp;&amp;amp;(Disk!=0)&amp;amp;&amp;amp;(Memory!=0)&amp;amp;&amp;amp;(OpSys==OpSys)&amp;amp;&amp;amp;(FileSystemDomain!="")&lt;/profile&gt;

       </emphasis>
 &lt;/site&gt;
</programlisting>
      </section>

      <section>
        <title>Using S3 for intermediate files</title>

        <note>
          <para>This section describes how to use the legacy support for S3
          that exists in Pegasus 2.4.x and 3.0. In Pegasus 3.1, support for S3
          will be added to pegasus-transfer.</para>
        </note>

        <para>This section will show you how to use S3 to store intermediate
        files. In this mode, Pegasus transfers workflow inputs from the input
        site to S3 if they are not already in S3. When a job runs, the inputs
        for that job are fetched from S3 to the worker node, the job is
        executed, then the output files are transferred from the worker node
        back to S3. Finally, at the end of the workflow, the desired outputs
        are transferred from S3 to the output site. This is similar to how
        second-level staging works (SLS), and, in fact, the current S3
        implementation is an extension of SLS.</para>

        <para>In order to use S3 for your workflows, you need to first create
        a config file for the S3 transfer client, s3cmd. This config file
        needs to be installed on both the submit host, and the worker nodes.
        Save the file as 's3cmd.cfg'. This file will contain your Amazon
        security tokens, so you will want to set the permissions so that other
        users cannot read it (e.g. chmod 600 s3cmd.cfg). In the listing below,
        replace 'YOUR_AMAZON_ACCESS_KEY_HERE' and
        'YOUR_AMAZON_SECRET_KEY_HERE' with your Amazon access key and secret
        key.</para>

        <programlisting>[default]
access_key = YOUR_AMAZON_ACCESS_KEY_HERE
secret_key = YOUR_AMAZON_SECRET_KEY_HERE
acl_public = False
bucket_location = US
debug_syncmatch = False
default_mime_type = binary/octet-stream
delete_removed = False
dry_run = False
encrypt = False
force = False
gpg_command = /usr/bin/gpg
gpg_decrypt = %(gpg_command)s -d --verbose --no-use-agent --batch --yes --passphrase-fd %(passphrase_fd)s -o %(output_file)s %(input_file)s
gpg_encrypt = %(gpg_command)s -c --verbose --no-use-agent --batch --yes --passphrase-fd %(passphrase_fd)s -o %(output_file)s %(input_file)s
gpg_passphrase = 
guess_mime_type = False
host_base = s3.amazonaws.com
host_bucket = %(bucket)s.s3.amazonaws.com
human_readable_sizes = False
preserve_attrs = True
proxy_host = 
proxy_port = 0
recv_chunk = 4096
send_chunk = 4096
simpledb_host = sdb.amazonaws.com
use_https = False
verbosity = WARNING</programlisting>

        <para>Next, you need to modify your site catalog to tell s3cmd the
        path to your config file. You can do this by specifying an environment
        variable called S3CFG using an 'env' profile. This needs to be added
        to both the 'local' site, and any execution sites where you want to
        use S3. This environment variable will be used by s3cmd in the S3
        transfer jobs to locate the config file.</para>

        <programlisting>&lt;profile namespace="env" key="S3CFG"&gt;/local/path/to/s3cmd.cfg&lt;/profile&gt;</programlisting>

        <para>Next, you need to update your pegasus.properties file to tell
        Pegasus to turn on SLS, to use S3 for first and second-level staging,
        and to create and delete buckets. The pegasus.gridstart property tells
        pegasus to use seqexec, which allows the application jobs to be
        wrapped with S3 PUT and GET commands to retrieve and store input and
        output data.</para>

        <programlisting>pegasus.execute.*.filesystem.local = true       # Turn on second-level staging (SLS)
pegasus.transfer.*.impl = S3                    # Use S3 for first-level staging
pegasus.transfer.sls.*.impl = S3                # Use S3 for second-level staging
pegasus.transfer.sls.s3.stage.sls.file = false  # Do not transfer .sls files
pegasus.dir.create.impl = S3                    # Create buckets in S3
pegasus.file.cleanup.impl = S3                  # Delete buckets from S3
pegasus.gridstart = SeqExecOld                  # Use the old seqexec implementation</programlisting>

        <para>Next, you need to add an entry for amazon::s3cmd in your
        transformation catalog. The s3cmd executable is in the
        PEGASUS_HOME/bin directory.</para>

        <programlisting>local   amazon::s3cmd    /path/to/pegasus/bin/s3cmd    INSTALLED   INTEL32::LINUX  NULL
ec2     amazon::s3cmd    /path/to/pegasus/bin/s3cmd    INSTALLED   INTEL32::LINUX  NULL</programlisting>

        <para>Finally, if you want to use an existing bucket to store your
        data in S3 add this to your site catalog entry (replacing the existing
        workdirectory):</para>

        <programlisting>&lt;workdirectory&gt;existing-bucket&lt;/workdirectory&gt;</programlisting>

        <para>Otherwise, you can have Pegasus create a new bucket for each
        workflow by adding this to your site catalog entry (again, replacing
        any existing workdirectory):</para>

        <programlisting>&lt;workdirectory&gt;/&lt;/workdirectory&gt;</programlisting>
      </section>
    </section>
  </section>
</chapter>

