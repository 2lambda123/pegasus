<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE book PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN"
"http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<chapter id="example_workflows">
  <title>Example Workflows</title>

  <para>These examples are included in the Pegasus distribution and can be
  found under <filename>$PEGASUS_HOME/examples/</filename></para>

  <section>
    <title>Grid Examples</title>

    <para>These examples assumes you have access to a cluster with Globus
    installed. A pre-ws gatekeeper and gridftp server is required. You also
    need Globus and Pegasus installed, both on the machine you are submitting
    from, and the cluster.</para>

    <section id="example_black_diamond" xreflabel="Black Diamond">
      <title>Black Diamond</title>

      <para>Pegasus is shipped with 3 different Black Diamond examples for the
      grid. This is to highlight the available DAX APIs which are Java, Perl
      and Python. The examples can be found under:</para>

      <programlisting>examples/grid-blackdiamond-java/
examples/grid-blackdiamond-perl/
examples/grid-blackdiamond-python/</programlisting>

      <para>The workflow has 4 nodes, layed out in a diamond shape, with files
      being passed between them (f.*):</para>

      <mediaobject>
        <imageobject>
          <imagedata align="center" fileref="images/examples-diamond.jpg"
                     valign="middle" />
        </imageobject>
      </mediaobject>

      <para>The binary for the nodes is a simple "mock application" name
      <command>keg</command> ("canonical example for the grid") which reads
      input files designated by arguments, writes them back onto output files,
      and produces on STDOUT a summary of where and when it was run. Keg ships
      with Pegasus in the bin directory.</para>

      <para>This example ships with a "submit" script which will build the
      replica catalog, the transformation catalog, and the site catalog. When
      you create your own workflows, such a submit script is not needed if you
      want to maintain those catalogs manually.</para>

      <note>
        <para>The use of <filename>./submit</filename> scripts in these
        examples are just to make it more easy to run the examples out of the
        box. For a production site, the catalogs (transformation, replica,
        site) may or may not be static or generated by other tooling.</para>
      </note>

      <para>To test the examples, edit the <command>submit</command> script
      and change the cluster config to the setup and install locations for
      your cluster. Then run:</para>

      <para><programlisting>./submit</programlisting></para>

      <para>The workflow should now be submitted and in the output you should
      see a submit dir location for the instance. With that directory you can
      monitor the workflow with:</para>

      <para><programlisting>pegasus-status [submitdir]</programlisting></para>

      <para>Once the workflow is done, you can make sure it was sucessful
      with:</para>

      <para><programlisting>pegasus-analyzer -d [submitdir]</programlisting></para>
    </section>

    <section>
      <title>NASA/IPAC Montage</title>

      <para>This example can be found under</para>

      <programlisting><filename>examples/grid-montage/</filename></programlisting>

      <para>The NASA IPAC Montage (<ulink
      url="http://montage.ipac.caltech.edu/">http://montage.ipac.caltech.edu/</ulink>)
      workflow projects/montages a set of input images from telescopes like
      Hubble and end up with images like <ulink
      url="???">http://montage.ipac.caltech.edu/images/m104.jpg</ulink> . The
      test workflow is for a 1 by 1 degrees tile. It has about 45 input images
      which all have to be projected, background modeled and adjusted to come
      out as one seamless image.</para>

      <para>Just like the <xref linkend="example_black_diamond">
           black diamond example 
        </xref> above, this example uses a <filename>./submit</filename>
      script.</para>

      <para>The Montage DAX is generated with a tool called
      <filename>mDAG</filename> shipped with Montage.
      <filename>mDAG</filename> is generating a DAX in schema 2.1 and the
      workflow is then run with the 2.1 DAX parser using a property in
      pegasusrc:</para>

      <programlisting>pegasus.schema.dax=${pegasus.home}/etc/dax-2.1.xsd</programlisting>
    </section>

    <section>
      <title>Rosetta</title>

      <para>This example can be found under</para>

      <programlisting><filename>examples/grid-rosetta/</filename></programlisting>

      <para>Rosetta (<ulink
      url="http://www.rosettacommons.org/">http://www.rosettacommons.org/</ulink>)
      is a high resolution protein prediction and design software. Highlights
      in this example are:</para>

      <itemizedlist>
        <listitem>
          <para>Using the Pegasus Java API to generate the DAX</para>
        </listitem>

        <listitem>
          <para>The DAX generator loops over the input PDBs and creates a job
          for each input</para>
        </listitem>

        <listitem>
          <para>The jobs all have a dependency on a flatfile database. For
          simplicity, each job depends on all the files in the database
          directory.</para>
        </listitem>

        <listitem>
          <para>Job clustering is turned on to make each grid job run longer
          and better utilize the compute cluster</para>
        </listitem>
      </itemizedlist>

      <para>Just like the <xref linkend="example_black_diamond">
           black diamond example 
        </xref> above, this example uses a <filename>./submit</filename>
      script.</para>
    </section>

    <section>
      <title>Rosetta with Second Level Staging</title>

      <para>This example can be found under</para>

      <programlisting><filename>examples/grid-rosetta-sls/</filename></programlisting>

      <para><caution>
          <para>Second Level Staging is an experimental feature in Pegasus,
          and is only partially implemented in the 3.0 release. The first hop
          is not implemented, and therefore you either have to use the submit
          host for the data staging, or manually stage into the storage server
          you want to use</para>

          <para>Intermediate data is kept on the shared filesystem at the
          site, and outputs are staged out with separate stage out
          jobs.</para>
        </caution>This is similar to the previous example, except that the
      data files are staged directly from the submit host to a local
      filesystem on the compute node. The changes below are already in place,
      but described to highlight the difference between the standard Pegasus
      stageing strategy and the second level one.</para>

      <para>The second level staging is enabled by some properties in
      pegasusrc:</para>

      <programlisting># turn on second level staging
pegasus.execute.*.filesystem.local = true                                                                                     
pegasus.transfer.stage.sls.file = false                                                                                       
pegasus.gridstart = SeqExec          </programlisting>

      <para>And by setting the <command>wntmp</command> environment variable
      for the site in the site catalog, pointing to where the local directory
      to use on the worker nodes:</para>

      <programlisting>&lt;profile namespace="env" key="wmtmp" &gt;/condor/osg_wn_tmp&lt;/profile&gt;</programlisting>

      <para>You also need to set pool="[sitehandle]" in the replica catalog.
      When doing second level staging, this is a required hint to the Pegasus
      planner to know what files need to be staged. This is done automatically
      in the submit script with example result:</para>

      <programlisting>minirosetta_database/Rama08.dat
    gsiftp://yggdrasil.isi.edu/home/rynge/Pegasus/rosetta-sls/minirosetta_database/Rama08.dat
    pool="RENCI-Engagement"
</programlisting>

      <para>Some potential issues when using second level staging:</para>

      <itemizedlist>
        <listitem>
          <para>Not enough disk space on the local disk on the compute
          node</para>
        </listitem>

        <listitem>
          <para>Hard to control gridftp access pattern / bandwidth usage as
          the workflow scales up</para>
        </listitem>

        <listitem>
          <para>If a lot of jobs are using the same files, you will end up
          seeing many transfers of the same file over and over again</para>
        </listitem>
      </itemizedlist>

      <para>Just like the <xref linkend="example_black_diamond">
           black diamond example 
        </xref> above, this example uses a <filename>./submit</filename>
      script</para>
    </section>
  </section>

  <section>
    <title>Local Shell Examples</title>

    <section>
      <title>Black Diamond</title>

      <para>To aid in workflow development and debugging, Pegasus can now map
      a workflow to a local shell script.</para>
    </section>
  </section>

  <section>
    <title>glideinWMS Examples</title>

    <section>
      <title>Montage</title>

      <para>...</para>
    </section>
  </section>

  <section>
    <title>Workflow of Workflows</title>

    <section>
      <title>Galactic Plane</title>

      <para></para>
    </section>
  </section>
</chapter>
