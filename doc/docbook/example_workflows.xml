<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE book PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN"
"http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<chapter id="example_workflows">
  <title>Example Workflows</title>

  <para>These examples are included in the Pegasus distribution and can be
  found under <filename>$PEGASUS_HOME/examples/</filename></para>

  <section id="grid_examples">
    <title>Grid Examples</title>

    <para>These examples assumes you have access to a cluster with Globus
    installed. A pre-ws gatekeeper and gridftp server is required. You also
    need Globus and Pegasus installed, both on the machine you are submitting
    from, and the cluster.</para>

    <section id="example_black_diamond" xreflabel="Black Diamond">
      <title>Black Diamond</title>

      <para>Pegasus is shipped with 3 different Black Diamond examples for the
      grid. This is to highlight the available DAX APIs which are Java, Perl
      and Python. The examples can be found under:</para>

      <programlisting>examples/grid-blackdiamond-java/
examples/grid-blackdiamond-perl/
examples/grid-blackdiamond-python/</programlisting>

      <para>The workflow has 4 nodes, layed out in a diamond shape, with files
      being passed between them (f.*):</para>

      <mediaobject>
        <imageobject>
          <imagedata align="center" fileref="images/examples-diamond.jpg"
                     valign="middle" />
        </imageobject>
      </mediaobject>

      <para>The binary for the nodes is a simple "mock application" name
      <command>keg</command> ("canonical example for the grid") which reads
      input files designated by arguments, writes them back onto output files,
      and produces on STDOUT a summary of where and when it was run. Keg ships
      with Pegasus in the bin directory.</para>

      <para>This example ships with a "submit" script which will build the
      replica catalog, the transformation catalog, and the site catalog. When
      you create your own workflows, such a submit script is not needed if you
      want to maintain those catalogs manually.</para>

      <note>
        <para>The use of <filename>./submit</filename> scripts in these
        examples are just to make it more easy to run the examples out of the
        box. For a production site, the catalogs (transformation, replica,
        site) may or may not be static or generated by other tooling.</para>
      </note>

      <para>To test the examples, edit the <command>submit</command> script
      and change the cluster config to the setup and install locations for
      your cluster. Then run:</para>

      <para><programlisting>$ <emphasis role="bold">./submit</emphasis></programlisting></para>

      <para>The workflow should now be submitted and in the output you should
      see a work dir location for the instance. With that directory you can
      monitor the workflow with:</para>

      <para><programlisting>$ <emphasis role="bold">pegasus-status [workdir]</emphasis></programlisting></para>

      <para>Once the workflow is done, you can make sure it was sucessful
      with:</para>

      <para><programlisting>$ <emphasis role="bold">pegasus-analyzer -d [workdir]</emphasis></programlisting></para>
    </section>

    <section>
      <title>NASA/IPAC Montage</title>

      <para>This example can be found under</para>

      <programlisting><filename>examples/grid-montage/</filename></programlisting>

      <para>The NASA IPAC Montage (<ulink
      url="http://montage.ipac.caltech.edu/">http://montage.ipac.caltech.edu/</ulink>)
      workflow projects/montages a set of input images from telescopes like
      Hubble and end up with images like <ulink
      url="???">http://montage.ipac.caltech.edu/images/m104.jpg</ulink> . The
      test workflow is for a 1 by 1 degrees tile. It has about 45 input images
      which all have to be projected, background modeled and adjusted to come
      out as one seamless image.</para>

      <para>Just like the <xref linkend="example_black_diamond">
           black diamond example 
        </xref> above, this example uses a <filename>./submit</filename>
      script.</para>

      <para>The Montage DAX is generated with a tool called
      <filename>mDAG</filename> shipped with Montage.
      <filename>mDAG</filename> is generating a DAX in schema 2.1 and the
      workflow is then run with the 2.1 DAX parser using a property in
      pegasusrc:</para>

      <programlisting>pegasus.schema.dax=${pegasus.home}/etc/dax-2.1.xsd</programlisting>
    </section>

    <section>
      <title>Rosetta</title>

      <para>This example can be found under</para>

      <programlisting><filename>examples/grid-rosetta/</filename></programlisting>

      <para>Rosetta (<ulink
      url="http://www.rosettacommons.org/">http://www.rosettacommons.org/</ulink>)
      is a high resolution protein prediction and design software. Highlights
      in this example are:</para>

      <itemizedlist>
        <listitem>
          <para>Using the Pegasus Java API to generate the DAX</para>
        </listitem>

        <listitem>
          <para>The DAX generator loops over the input PDBs and creates a job
          for each input</para>
        </listitem>

        <listitem>
          <para>The jobs all have a dependency on a flatfile database. For
          simplicity, each job depends on all the files in the database
          directory.</para>
        </listitem>

        <listitem>
          <para>Job clustering is turned on to make each grid job run longer
          and better utilize the compute cluster</para>
        </listitem>
      </itemizedlist>

      <para>Just like the <xref linkend="example_black_diamond">
           black diamond example 
        </xref> above, this example uses a <filename>./submit</filename>
      script.</para>
    </section>

    <section>
      <title>Rosetta with Second Level Staging</title>

      <para>This example can be found under</para>

      <programlisting><filename>examples/grid-rosetta-sls/</filename></programlisting>

      <para><caution>
          <para>Second Level Staging is an experimental feature in Pegasus,
          and is only partially implemented in the 3.0 release. The first hop
          is not implemented, and therefore you either have to use the submit
          host for the data staging, or manually stage into the storage server
          you want to use</para>

          <para>Intermediate data is kept on the shared filesystem at the
          site, and outputs are staged out with separate stage out
          jobs.</para>
        </caution>This is similar to the previous example, except that the
      data files are staged directly from the submit host to a local
      filesystem on the compute node. The changes below are already in place,
      but described to highlight the difference between the standard Pegasus
      stageing strategy and the second level one.</para>

      <para>The second level staging is enabled by some properties in
      pegasusrc:</para>

      <programlisting># turn on second level staging
pegasus.execute.*.filesystem.local = true                                                                                     
pegasus.transfer.stage.sls.file = false                                                                                       
pegasus.gridstart = SeqExec          </programlisting>

      <para>And by setting the <command>wntmp</command> environment variable
      for the site in the site catalog, pointing to where the local directory
      to use on the worker nodes:</para>

      <programlisting>&lt;profile namespace="env" key="wmtmp" &gt;/condor/osg_wn_tmp&lt;/profile&gt;</programlisting>

      <para>You also need to set pool="[sitehandle]" in the replica catalog.
      When doing second level staging, this is a required hint to the Pegasus
      planner to know what files need to be staged. This is done automatically
      in the submit script with example result:</para>

      <programlisting>minirosetta_database/Rama08.dat
    gsiftp://yggdrasil.isi.edu/home/rynge/Pegasus/rosetta-sls/minirosetta_database/Rama08.dat
    pool="RENCI-Engagement"
</programlisting>

      <para>Some potential issues when using second level staging:</para>

      <itemizedlist>
        <listitem>
          <para>Not enough disk space on the local disk on the compute
          node</para>
        </listitem>

        <listitem>
          <para>Hard to control gridftp access pattern / bandwidth usage as
          the workflow scales up</para>
        </listitem>

        <listitem>
          <para>If a lot of jobs are using the same files, you will end up
          seeing many transfers of the same file over and over again</para>
        </listitem>
      </itemizedlist>

      <para>Just like the <xref linkend="example_black_diamond">
           black diamond example 
        </xref> above, this example uses a <filename>./submit</filename>
      script</para>
    </section>
  </section>

  <section id="condor_examples">
    <title>Condor Examples</title>

    <section>
      <title>Black Diamond</title>

      <para>This example can be found under</para>

      <programlisting><filename>examples/condor-blackdiamond/</filename></programlisting>

      <para>This example is using the same abstract workflow as the <xref
          linkend="example_black_diamond">
           Black Diamond 
        </xref> grid example above, and can be executed either on the submit
      machine (universe="local") or on a local Condor pool
      (universe="vanilla"). The latter requires a shared filesystem for the
      pool.</para>

      <para>You can run this example with the <filename>./submit</filename>
      script, but you have to pass an argument: <emphasis>condor</emphasis>
      for universe=local or <emphasis>condorpool</emphasis> for
      universe="vanilla". Example:</para>

      <programlisting>$ <emphasis role="bold">./submit condorpool</emphasis></programlisting>
    </section>
  </section>

  <section id="local_shell_examples">
    <title>Local Shell Examples</title>

    <section>
      <title>Black Diamond</title>

      <para>To aid in workflow development and debugging, Pegasus can now map
      a workflow to a local shell script. One advantage is that you do not
      need a remote compute resource.</para>

      <para>This example is using the same abstract workflow as the <xref
          linkend="example_black_diamond">
           Black Diamond 
        </xref> grid example above. The difference is that a property is set
      in pegasusrc to force shell execution:</para>

      <programlisting># tell pegasus to generate shell version of
# the workflow
pegasus.code.generator = Shell</programlisting>

      <para>You can run this example with the <filename>./submit</filename>
      script.</para>
    </section>
  </section>

  <section id="glideinwms_examples">
    <title>glideinWMS Examples</title>

    <section>
      <title>NASA/IPAC Montage</title>

      <para>By default Pegasus generates Condor-G compute jobs, but there are
      a couple of other job styles available. One is to run the workflow on a
      local Condor pool (<emphasis>pegasus.style=condor</emphasis>), and an
      extension of that is the glideinWMS style
      (<emphasis>pegasus.style=glideinwms</emphasis>).</para>

      <para>glideinWMS is a Condor glidein system which creates an Condor
      overlay over one or more grid resources. More information can be found
      at <ulink
      url="http://www.uscms.org/SoftwareComputing/Grid/WMS/glideinWMS/">http://www.uscms.org/SoftwareComputing/Grid/WMS/glideinWMS/</ulink>.
      A glidein system is usually better than Condor-G at handling short jobs.
      Because of the overlay effect, hides some of the issues with the low
      level grid protocol, and provides a more homogeneous user
      environment.</para>

      <para>In this example, Pegasus is told to use the glideinwms styple by
      setting the style property in the site catalog:</para>

      <programlisting>&lt;profile namespace="pegasus" key="style"&gt;glideinwms&lt;/profile&gt;</programlisting>

      <para>Once submitted, the jobs have attributes fitting for a glideinWMS
      setup:</para>

      <programlisting>universe = vanilla
rank = DaemonStartTime
requirements = ((GLIDEIN_Entry_Name == "ISIViz") || (TARGET.Pegasus_Site == "ISIViz")) \
               &amp;&amp; (IS_MONITOR_VM == False) &amp;&amp; (Arch != "") &amp;&amp; (OpSys != "") \
               &amp;&amp; (Disk != -42) &amp;&amp; (Memory &gt; 1) &amp;&amp; (FileSystemDomain != "")
</programlisting>

      <para>This example should work against other Condor based glidein
      systems as well by changing the style to condor.</para>
    </section>
  </section>

  <section>
    <title id="workflow_of_workflows">Workflow of Workflows</title>

    <section>
      <title>Galactic Plane</title>

      <para>The <ulink
      url="http://en.wikipedia.org/wiki/Galactic_plane">Galactic Plane</ulink>
      workflow is a workflow of many Montage workflows. The output is a set of
      tiles which can be used in software which takes the tiles and produces a
      seamless image which can be scrolled and zoomed into. As this is more of
      a production workflow than an example one, it can be a little bit harder
      to get running in your environment.</para>

      <para>Highlights of the example are:</para>

      <itemizedlist>
        <listitem>
          <para>The subworkflow DAXes are generated as jobs in the parent
          workflow - this is an example on how to make more dynamic workflows.
          For example, if you need a job in your workflow to determine the
          number of jobs in the next level, you can have the first job create
          a subworkflow with the right number of jobs.</para>
        </listitem>

        <listitem>
          <para>DAGMan job categories are used to limit the number of
          concurrant jobs in certain places. This is used to limit the number
          of concurrant connections to the data find service, as well limit
          the number of concurrant subworkflows to manage disk usage on the
          compute cluster.</para>
        </listitem>

        <listitem>
          <para>Job priorities are used to make sure we overlap staging and
          computation. Pegasus sets default priorities, which for most jobs
          are fine, but the priority of the data find job is set explicitly to
          a higher priority.</para>
        </listitem>

        <listitem>
          <para>A specific output site is defined the the site catalog and
          specified with the --output option of subworkflows.</para>
        </listitem>
      </itemizedlist>

      <para>The DAX API has support for sub workflows:</para>

      <programlisting>    remote_tile_setup = Job(namespace="gp", name="remote_tile_setup", version="1.0")
    remote_tile_setup.addArguments("%05d" % (tile_id))
    remote_tile_setup.addProfile(Profile("dagman", "CATEGORY", "remote_tile_setup"))
    remote_tile_setup.uses(params, link=Link.INPUT, register=False)
    remote_tile_setup.uses(mdagtar, link=Link.OUTPUT, register=False, transfer=True)
    uberdax.addJob(remote_tile_setup)
...
    subwf = DAX("%05d.dax" % (tile_id), "ID%05d" % (tile_id))
    subwf.addArguments("-Dpegasus.schema.dax=%s/etc/dax-2.1.xsd" %(os.environ["PEGASUS_HOME"]),
                       "-Dpegasus.catalog.replica.file=%s/rc.data" % (tile_work_dir),
                       "-Dpegasus.catalog.site.file=%s/sites.xml" % (work_dir),
                       "-Dpegasus.transfer.links=true",
                       "--sites", cluster_name,
                       "--cluster", "horizontal",
                       "--basename", "tile-%05d" % (tile_id),
                       "--force",
                       "--output", output_name)
    subwf.addProfile(Profile("dagman", "CATEGORY", "subworkflow"))
    subwf.uses(subdax_file, link=Link.INPUT, register=False)
    uberdax.addDAX(subwf)

</programlisting>

      <para/>
    </section>
  </section>
</chapter>

