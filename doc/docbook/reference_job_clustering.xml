<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE section PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN"
"http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<section id="job_clustering">
  <title>Job Clustering</title>

  <para>A large number of workflows executed through the Pegasus Workflow
  Management System, are composed of several jobs that run for only a few
  seconds or so. The overhead of running any job on the grid is usually 60
  seconds or more. Hence, it makes sense to cluster small independent jobs
  into a larger job. This is done while mapping an abstract workflow to an
  executable workflow. Site specific or transformation specific criteria are
  taken into consideration while clustering smaller jobs into a larger job in
  the executable workflow. The user is allowed to control the granularity of
  this clustering on a per transformation per site basis.</para>

  <section>
    <title>Overview</title>

    <para>The abstract workflow is mapped onto the various sites by the Site
    Selector. This semi executable workflow is then passed to the clustering
    module. The clustering of the workflow can be either be</para>

    <itemizedlist>
      <listitem>
        <para>level based (horizontal clustering )</para>
      </listitem>

      <listitem>
        <para>label based (label clustering)</para>
      </listitem>
    </itemizedlist>

    <para>The clustering module clusters the jobs into larger/clustered jobs,
    that can then be executed on the remote sites. The execution can either be
    sequential on a single node or on multiple nodes using MPI. To specify
    which clustering technique to use the user has to pass the <emphasis
    role="bold">--cluster</emphasis> option to <emphasis
    role="bold">pegasus-plan</emphasis> .</para>

    <section>
      <title>Generating Clustered Executable Workflow</title>

      <para>The clustering of a workflow is activated by passing the <emphasis
      role="bold">--cluster|-C</emphasis> option to <emphasis
      role="bold">pegasus-plan</emphasis>. The clustering granularity of a
      particular logical transformation on a particular site is dependant upon
      the clustering techniques being used. The executable that is used for
      running the clustered job on a particular site is determined as
      explained in section 7.<programlisting>#Running pegasus-plan to generate clustered workflows

$ pegasus-plan --dax example.dax --dir ./dags -p siteX --output local
               --cluster [comma separated list of clustering techniques]  -verbose

Valid clustering techniques are horizontal and label.</programlisting></para>

      <para>The naming convention of submit files of the clustered jobs
      is<emphasis role="bold"> merge_NAME_IDX.sub</emphasis> . The NAME is
      derived from the logical transformation name. The IDX is an integer
      number between 1 and the total number of jobs in a cluster. Each of the
      submit files has a corresponding input file, following the naming
      convention <emphasis role="bold">merge_NAME_IDX.in </emphasis>. The
      input file contains the respective execution targets and the arguments
      for each of the jobs that make up the clustered job.</para>

      <section id="horizontal_clustering">
        <title>Horizontal Clustering</title>

        <para>In case of horizontal clustering, each job in the workflow is
        associated with a level. The levels of the workflow are determined by
        doing a modified Breadth First Traversal of the workflow starting from
        the root nodes. The level associated with a node, is the furthest
        distance of it from the root node instead of it being the shortest
        distance as in normal BFS. For each level the jobs are grouped by the
        site on which they have been scheduled by the Site Selector. Only jobs
        of same type (txnamespace, txname, txversion) can be clustered into a
        larger job. To use horizontal clustering the user needs to set the
        <emphasis role="bold">--cluster</emphasis> option of <emphasis
        role="bold">pegasus-plan to horizontal</emphasis> .</para>

        <section>
          <title>Controlling Clustering Granularity</title>

          <para>The number of jobs that have to be clustered into a single
          large job, is determined by the value of two parameters associated
          with the smaller jobs. Both these parameters are specified by the
          use of a PEGASUS namespace profile keys. The keys can be specified
          at any of the placeholders for the profiles (abstract transformation
          in the DAX, site in the site catalog, transformation in the
          transformation catalog). The normal overloading semantics apply i.e.
          profile in transformation catalog overrides the one in the site
          catalog and that in turn overrides the one in the DAX. The two
          parameters are described below.</para>

          <itemizedlist>
            <listitem>
              <para><emphasis role="bold">clusters.size
              factor</emphasis></para>

              <para>The clusters.size factor denotes how many jobs need to be
              merged into a single clustered job. It is specified via the use
              of a PEGASUS namespace profile key
              &amp;ldquo;clusters.size&amp;rdquor;. for e.g. if at a
              particular level, say 4 jobs referring to logical transformation
              B have been scheduled to a siteX. The clusters.size factor
              associated with job B for siteX is say 3. This will result in 2
              clustered jobs, one composed of 3 jobs and another of 2 jobs.
              The clusters.size factor can be specified in the transformation
              catalog as follows</para>

              <programlisting><emphasis role="bold">#site   transformation   pfn            type               architecture  profiles
</emphasis>
siteX    B     /shared/PEGASUS/bin/jobB INSTALLED       INTEL32::LINUX  PEGASUS::clusters.size=3
siteX    C     /shared/PEGASUS/bin/jobC INSTALLED       INTEL32::LINUX  PEGASUS::clusters.size=2
</programlisting>

              <figure>
                <title>Clustering by clusters.size</title>

                <mediaobject>
                  <imageobject>
                    <imagedata contentdepth="8in"
                               fileref="images/advanced-clustering-1.png" />
                  </imageobject>
                </mediaobject>
              </figure>
            </listitem>

            <listitem>
              <para><emphasis role="bold">clusters.num
              factor</emphasis></para>

              <para>The clusters.num factor denotes how many clustered jobs
              does the user want to see per level per site. It is specified
              via the use of a PEGASUS namespace profile key
              &amp;ldquo;clusters.num&amp;rdquor;. for e.g. if at a particular
              level, say 4 jobs referring to logical transformation B have
              been scheduled to a siteX. The
              &amp;ldquo;clusters.num&amp;rdquor; factor associated with job B
              for siteX is say 3. This will result in 3 clustered jobs, one
              composed of 2 jobs and others of a single job each. The
              clusters.num factor in the transformation catalog can be
              specified as follows</para>

              <programlisting><emphasis role="bold">#site  transformation      pfn           type            architecture    profiles
</emphasis>
siteX    B     /shared/PEGASUS/bin/jobB INSTALLED       INTEL32::LINUX  PEGASUS::clusters.num=3
siteX    C     /shared/PEGASUS/bin/jobC INSTALLED       INTEL32::LINUX  PEGASUS::clusters.num=2
</programlisting>

              <para>In the case, where both the factors are associated with
              the job, the clusters.num value supersedes the clusters.size
              value.</para>

              <programlisting><emphasis role="bold">#site  transformation   pfn             type             architecture   profiles
</emphasis>
siteX    B     /shared/PEGASUS/bin/jobB INSTALLED       INTEL32::LINUX PEGASUS::clusters.size=3,clusters.num=3
</programlisting>

              <para>In the above case the jobs referring to logical
              transformation B scheduled on siteX will be clustered on the
              basis of &amp;ldquo;clusters.num&amp;rdquor; value. Hence, if
              there are 4 jobs referring to logical transformation B scheduled
              to siteX, then 3 clustered jobs will be created.</para>

              <figure>
                <title>Clustering by clusters.num</title>

                <mediaobject>
                  <imageobject>
                    <imagedata contentdepth="8in"
                               fileref="images/advanced-clustering-2.png" />
                  </imageobject>
                </mediaobject>
              </figure>
            </listitem>
          </itemizedlist>
        </section>
      </section>

      <section id="runtime_clustering">
        <title>Runtime Clustering</title>

        <para>Workflows often consist of jobs of same type, but have varying
        run times. Two or more instances of the same job, with varying inputs
        can differ significantly in their runtimes. A simple way to think
        about this is running the same program on two distinct input sets,
        where one input is smaller (1 MB) as compared to the other which is 10
        GB in size. In such a case the two jobs will having significantly
        differing run times. When such jobs are clustered using horizontal
        clustering, the benefits of job clustering may be lost if all smaller
        jobs get clustered together, while the larger jobs are clustered
        together. In such scenarios it would be beneficial to be able to
        cluster jobs together such that all clustered jobs have similar
        runtimes.</para>

        <para>In case of runtime clustering, jobs in the workflow are
        associated with a level. The levels of the workflow are determined in
        the same manner as in horizontal clustering. For each level the jobs
        are grouped by the site on which they have been scheduled by the Site
        Selector. Only jobs of same type (txnamespace, txname, txversion) can
        be clustered into a larger job. To use runtime clustering the user
        needs to set the <emphasis role="bold">--cluster</emphasis> option of
        <emphasis role="bold">pegasus-plan to horizontal</emphasis>.</para>

        <para>Basic Algorithm of grouping jobs into clusters is as
        follows</para>

        <programlisting>// cluster.maxruntime - Is the maximum runtime for which the clustered job should run.
// j.runtime - Is the runtime of the job j.
1. Create a set of jobs of the same type (txnamespace, txname, txversion), and that run on the same site.
2. Sort the jobs in decreasing order of their runtime.
3. For each job j, repeat
  a. If j.runtime &gt; cluster.maxruntime then 
        ignore j.
  // Sum of runtime of jobs already in the bin + j.runtime &lt;= cluster.maxruntime
  b. If j can be added to any existing bin (clustered job) then 
        Add j to bin
     Else
        Add a new bin
        Add job j to newly added bin</programlisting>

        <para>The runtime of a job, and maximum runtime for which a clustered
        jobs should run, is determined by the value of two parameters
        associated with the jobs.</para>

        <itemizedlist>
          <listitem>
            <para><emphasis role="bold">job.runtime</emphasis></para>

            <para>expected runtime for a job</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">clusters.maxruntime</emphasis></para>

            <para>maxruntime for the clustered job</para>
          </listitem>
        </itemizedlist>

        <para>Both these parameters are specified by the use of a PEGASUS
        namespace profile keys. The keys can be specified at any of the
        placeholders for the profiles (abstract transformation in the DAX,
        site in the site catalog, transformation in the transformation
        catalog). The normal overloading semantics apply i.e. profile in
        transformation catalog overrides the one in the site catalog and that
        in turn overrides the one in the DAX. The two parameters are described
        below.</para>

        <programlisting><emphasis role="bold">#site  transformation   pfn             type             architecture   profiles
</emphasis>
siteX    B     /shared/PEGASUS/bin/jobB INSTALLED       INTEL32::LINUX PEGASUS::clusters.maxruntime=250,job.runtime=100
siteX    C     /shared/PEGASUS/bin/jobC INSTALLED       INTEL32::LINUX PEGASUS::clusters.maxruntime=300,job.runtime=100</programlisting>

        <figure>
          <title>Clustering by runtime</title>

          <mediaobject>
            <imageobject>
              <imagedata contentdepth="8in"
                         fileref="images/advanced-clustering-5.png" />
            </imageobject>
          </mediaobject>
        </figure>

        <para>In the above case the jobs referring to logical transformation B
        scheduled on siteX will be clustered such that all clustered jobs will
        run approximately for the same duration specified by the
        clusters.maxruntime property. In the above case we assume all jobs
        referring to transformation B run for 100 seconds. For jobs with
        significantly differeing runtime, the job.runtime property will be
        associated with the jobs in the DAX.</para>

        <para>In addition to the above two profiles, we need to inform
        pegasus-plan to use runtime clustering. This is done by setting the
        following property .</para>

        <programlisting><emphasis role="bold"> pegasus.clusterer.preference          Runtime</emphasis> </programlisting>

        <para></para>
      </section>

      <section id="label_clustering">
        <title>Label Clustering</title>

        <para>In label based clustering, the user labels the workflow. All
        jobs having the same label value are clustered into a single clustered
        job. This allows the user to create clusters or use a clustering
        technique that is specific to his workflows. If there is no label
        associated with the job, the job is not clustered and is executed as
        is<figure>
            <title>Label-based clustering</title>

            <mediaobject>
              <imageobject>
                <imagedata contentdepth="8in"
                           fileref="images/advanced-clustering-3.png" />
              </imageobject>
            </mediaobject>
          </figure></para>

        <para>Since, the jobs in a cluster in this case are not independent,
        it is important the jobs are executed in the correct order. This is
        done by doing a topological sort on the jobs in each cluster. To use
        label based clustering the user needs to set the <emphasis
        role="bold">--cluster</emphasis> option of <emphasis
        role="bold">pegasus-plan</emphasis> to label.</para>

        <section>
          <title>Labelling the Workflow</title>

          <para>The labels for the jobs in the workflow are specified by
          associated <emphasis role="bold">pegasus</emphasis> profile keys
          with the jobs during the DAX generation process. The user can choose
          which profile key to use for labeling the workflow. By default, it
          is assumed that the user is using the PEGASUS profile key label to
          associate the labels. To use another key, in the <emphasis
          role="bold">pegasus</emphasis> namespace the user needs to set the
          following property</para>

          <itemizedlist>
            <listitem>
              <para>pegasus.clusterer.label.key</para>
            </listitem>
          </itemizedlist>

          <para>For example if the user sets <emphasis
          role="bold">pegasus.clusterer.label.key </emphasis>to <emphasis
          role="bold">user_label</emphasis> then the job description in the
          DAX looks as follows</para>

          <programlisting>&lt;adag &gt;
...
  &lt;job id="ID000004" namespace="app" name="analyze" version="1.0" level="1" &gt;
    &lt;argument&gt;-a bottom -T60  -i &lt;filename file="user.f.c1"/&gt;  -o &lt;filename file="user.f.d"/&gt;&lt;/argument&gt;
    &lt;profile namespace="pegasus" key="user_label"&gt;p1&lt;/profile&gt;
    &lt;uses file="user.f.c1" link="input" dontRegister="false" dontTransfer="false"/&gt;
    &lt;uses file="user.f.c2" link="input" dontRegister="false" dontTransfer="false"/&gt;
    &lt;uses file="user.f.d" link="output" dontRegister="false" dontTransfer="false"/&gt;
  &lt;/job&gt;
...
&lt;/adag&gt;</programlisting>

          <itemizedlist>
            <listitem>
              <para>The above states that the <emphasis
              role="bold">pegasus</emphasis> profiles with key as <emphasis
              role="bold">user_label</emphasis> are to be used for designating
              clusters.</para>
            </listitem>

            <listitem>
              <para>Each job with the same value for <emphasis
              role="bold">pegasus</emphasis> profile key <emphasis
              role="bold">user_label </emphasis>appears in the same
              cluster.</para>
            </listitem>
          </itemizedlist>
        </section>
      </section>

      <section>
        <title>Recursive Clustering</title>

        <para>In some cases, a user may want to use a combination of
        clustering techniques. For e.g. a user may want some jobs in the
        workflow to be horizontally clustered and some to be label clustered.
        This can be achieved by specifying a comma separated list of
        clustering techniques to the<emphasis role="bold">
        --cluster</emphasis> option of <emphasis
        role="bold">pegasus-plan</emphasis>. In this case the clustering
        techniques are applied one after the other on the workflow in the
        order specified on the command line.</para>

        <para>For example</para>

        <programlisting>$ <emphasis>pegasus-plan --dax example.dax --dir ./dags --cluster label,horizontal -s siteX --output local --verbose</emphasis></programlisting>

        <figure>
          <title>Recursive clustering</title>

          <mediaobject>
            <imageobject>
              <imagedata contentdepth="8in"
                         fileref="images/advanced-clustering-4.png" />
            </imageobject>
          </mediaobject>
        </figure>
      </section>
    </section>

    <section>
      <title>Execution of the Clustered Job</title>

      <para>The execution of the clustered job on the remote site, involves
      the execution of the smaller constituent jobs either</para>

      <itemizedlist>
        <listitem>
          <para><emphasis role="bold">sequentially on a single node of the
          remote site</emphasis></para>

          <para>The clustered job is executed using <emphasis
          role="bold">pegasus-cluster</emphasis>, a wrapper tool written in C
          that is distributed as part of the PEGASUS. It takes in the jobs
          passed to it, and ends up executing them sequentially on a single
          node. To use pegasus-cluster for executing any clustered job on a
          siteX, there needs to be an entry in the transformation catalog for
          an executable with the logical name seqexec and namespace as
          pegasus.</para>

          <programlisting><emphasis role="bold">#site  transformation   pfn            type                 architecture    profiles</emphasis>

siteX    pegasus::seqexec     /usr/pegasus/bin/pegasus-cluster INSTALLED       INTEL32::LINUX NULL</programlisting>

          <para>If the entry is not specified, Pegasus will attempt create a
          default path on the basis of the environment profile PEGASUS_HOME
          specified in the site catalog for the remote site.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">On multiple nodes of the remote site
          using MPI based task management tool called Pegasus MPI Cluster
          (PMC)</emphasis></para>

          <para>The clustered job is executed using <emphasis
          role="bold">pegasus-mpi-cluster</emphasis>, a wrapper MPI program
          written in C that is distributed as part of the PEGASUS. A PMC job
          consists of a single master process (this process is rank 0 in MPI
          parlance) and several worker processes. These processes follow the
          standard master-worker architecture. The master process manages the
          workflow and assigns workflow tasks to workers for execution. The
          workers execute the tasks and return the results to the master.
          Communication between the master and the workers is accomplished
          using a simple text-based protocol implemented using MPI_Send and
          MPI_Recv. PMC relies on a shared filesystem on the remote site to
          manage the individual tasks stdout and stderr and stage it back to
          the submit host as part of it's own stdout/stderr.</para>

          <para>The input format for PMC is a DAG based format similar to
          Condor DAGMan's. PMC follows the dependencies specified in the DAG
          to release the jobs in the right order and executes parallel jobs
          via the workers when possible. The input file for PMC is
          automatically generated by the Pegasus Planner when generating the
          executable workflow. PMC allows for a finer grained control on how
          each task is executed. This can be enabled by associating the
          following pegasus profiles with the jobs in the DAX</para>

          <table>
            <title>Table : Pegasus Profiles that can be associated with jobs
            in the DAX for PMC</title>

            <tgroup cols="2">
              <tbody>
                <row>
                  <entry><emphasis role="bold">Key</emphasis></entry>

                  <entry><emphasis role="bold">Description</emphasis></entry>
                </row>

                <row>
                  <entry>pmc_request_memory</entry>

                  <entry>This key is used to set the -m option for
                  pegasus-mpi-cluster. It specifies the amount of memory in MB
                  that a job requires. This profile is usually set in the DAX
                  for each job.</entry>
                </row>

                <row>
                  <entry>pmc_request_cpus</entry>

                  <entry>This key is used to set the -c option for
                  pegasus-mpi-cluster. It specifies the number of cpu's that a
                  job requires. This profile is usually set in the DAX for
                  each job.</entry>
                </row>

                <row>
                  <entry>pmc_priority</entry>

                  <entry>This key is used to set the -p option for
                  pegasus-mpi-cluster. It specifies the priority for a job .
                  This profile is usually set in the DAX for each job.
                  Negative values are allowed for priorities.</entry>
                </row>
              </tbody>
            </tgroup>
          </table>

          <para>Refer to the pegasus-mpi-cluster man page in the <link
          linkend="pegasus-cli-chapter">command line tools chapter</link> to
          know more about PMC and how it schedules individual tasks.</para>

          <para>It is recommended to have a pegasus::mpiexec entry in the
          transformation catalog to specify the path to PMC on the remote and
          specify the relevant globus profiles such as xcount, host_xcount and
          maxwalltime to control size of the MPI job.</para>

          <programlisting><emphasis role="bold">#site  transformation   pfn            type                 architecture    profiles</emphasis>

siteX    pegasus::mpiexec     /usr/pegasus/bin/pegasus-mpi-cluster INSTALLED       INTEL32::LINUX globus::xcount=32;globus::host_xcount=1</programlisting>

          <para>If the entry is not specified, Pegasus will attempt create a
          default path on the basis of the environment profile PEGASUS_HOME
          specified in the site catalog for the remote site.</para>

          <tip>
            <para>Users are encouraged to use label based clustering in
            conjunction with PMC</para>
          </tip>
        </listitem>
      </itemizedlist>

      <section>
        <title>Specification of Method of Execution for Clustered Jobs</title>

        <para>The method execution of the clustered job(whether to launch via
        mpiexec or seqexec) can be specified</para>

        <orderedlist>
          <listitem>
            <para><emphasis role="bold">globally in the properties
            file</emphasis></para>

            <para>The user can set a property in the properties file that
            results in all the clustered jobs of the workflow being executed
            by the same type of executable.</para>

            <programlisting><emphasis role="bold">#PEGASUS PROPERTIES FILE</emphasis>
pegasus.clusterer.job.aggregator seqexec|mpiexec</programlisting>

            <para>In the above example, all the clustered jobs on the remote
            sites are going to be launched via the property value, as long as
            the property value is not overridden in the site catalog.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">associating profile key collapser with
            the site in the site catalog</emphasis></para>

            <programlisting>&lt;site handle="siteX" gridlaunch = "/shared/PEGASUS/bin/kickstart"&gt;
    &lt;profile namespace="env" key="GLOBUS_LOCATION" &gt;/home/shared/globus&lt;/profile&gt;
    &lt;profile namespace="env" key="LD_LIBRARY_PATH"&gt;/home/shared/globus/lib&lt;/profile&gt;
    &lt;profile namespace="pegasus" key="collapser" &gt;seqexec&lt;/profile&gt;
    &lt;lrc url="rls://siteX.edu" /&gt;
    &lt;gridftp  url="gsiftp://siteX.edu/" storage="/home/shared/work" major="2" minor="4" patch="0" /&gt;
    &lt;jobmanager universe="transfer" url="siteX.edu/jobmanager-fork" major="2" minor="4" patch="0" /&gt;
    &lt;jobmanager universe="vanilla" url="siteX.edu/jobmanager-condor" major="2" minor="4" patch="0" /&gt;
    &lt;workdirectory &gt;/home/shared/storage&lt;/workdirectory&gt;
  &lt;/site&gt;</programlisting>

            <para>In the above example, all the clustered jobs on a siteX are
            going to be executed via seqexec, as long as the value is not
            overridden in the transformation catalog.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">associating profile key collapser with
            the transformation that is being clustered, in the transformation
            catalog</emphasis></para>

            <programlisting><emphasis role="bold">#site  transformation   pfn            type                architecture profiles
</emphasis>
siteX    B     /shared/PEGASUS/bin/jobB INSTALLED       INTEL32::LINUX pegasus::clusters.size=3,collapser=mpiexec</programlisting>

            <para>In the above example, all the clustered jobs that consist of
            transformation B on siteX will be executed via mpiexec.</para>

            <note>
              <para><emphasis role="bold"> The clustering of jobs on a site
              only happens only if </emphasis></para>
            </note>

            <itemizedlist>
              <listitem>
                <para>there exists an entry in the transformation catalog for
                the clustering executable that has been determined by the
                above 3 rules</para>
              </listitem>

              <listitem>
                <para>the number of jobs being clustered on the site are more
                than 1</para>
              </listitem>
            </itemizedlist>
          </listitem>
        </orderedlist>
      </section>
    </section>

    <section>
      <title>Outstanding Issues</title>

      <orderedlist>
        <listitem>
          <para><emphasis role="bold">Label Clustering</emphasis></para>

          <para>More rigorous checks are required to ensure that the labeling
          scheme applied by the user is valid.</para>
        </listitem>
      </orderedlist>
    </section>
  </section>
</section>