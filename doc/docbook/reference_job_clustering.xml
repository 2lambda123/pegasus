<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE section PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN"
"http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<section id="job_clustering">
  <title>Job Clustering</title>

  <para>A large number of workflows executed through the Pegasus Workflow
  Management System, are composed of several jobs that run for only a few
  seconds or so. The overhead of running any job on the grid is usually 60
  seconds or more. Hence, it makes sense to cluster small independent jobs
  into a larger job. This is done while mapping an abstract workflow to a
  concrete workflow. Site specific or transformation specific criteria are
  taken into consideration while clustering smaller jobs into a larger job in
  the concrete workflow. The user is allowed to control the granularity of
  this clustering on a per transformation per site basis.</para>

  <section>
    <title>Overview</title>

    <para>The abstract workflow is mapped onto the various sites by the Site
    Selector. This semi executable workflow is then passed to the clustering
    module. The clustering of the workflow can be either be</para>

    <itemizedlist>
      <listitem>
        <para>level based (horizontal clustering )</para>
      </listitem>

      <listitem>
        <para>label based (label clustering)</para>
      </listitem>
    </itemizedlist>

    <para>The clustering module clusters the jobs into larger/clustered jobs,
    that can then be executed on the remote sites. The execution can either be
    sequential on a single node or on multiple nodes using MPI. To specify
    which clustering technique to use the user has to pass the <emphasis
    role="bold">--cluster</emphasis> option to <emphasis
    role="bold">pegasus-plan</emphasis> .</para>

    <section>
      <title>Generating Clustered Concrete DAG</title>

      <para>The clustering of a workflow is activated by passing the <emphasis
      role="bold">--cluster|-C</emphasis> option to <emphasis
      role="bold">pegasus-plan</emphasis>. The clustering granularity of a
      particular logical transformation on a particular site is dependant upon
      the clustering techniques being used. The executable that is used for
      running the clustered job on a particular site is determined as
      explained in section 7.<programlisting>#Running pegasus-plan to generate clustered workflows

$ <emphasis>pegasus-plan &amp;ndash;-dax example.dax --dir ./dags &amp;ndash;p siteX &amp;ndash;-output local
               --cluster [ comma separated list of clustering techniques]  &amp;ndash;verbose
</emphasis>
Valid clustering techniques are horizontal and label.</programlisting></para>

      <para>The naming convention of submit files of the clustered jobs
      is<emphasis role="bold"> merge_NAME_IDX.sub</emphasis> . The NAME is
      derived from the logical transformation name. The IDX is an integer
      number between 1 and the total number of jobs in a cluster. Each of the
      submit files has a corresponding input file, following the naming
      convention <emphasis role="bold">merge_NAME_IDX.in </emphasis>. The
      input file contains the respective execution targets and the arguments
      for each of the jobs that make up the clustered job.</para>

      <section id="horizontal_clustering">
        <title>Horizontal Clustering</title>

        <para>In case of horizontal clustering, each job in the workflow is
        associated with a level. The levels of the workflow are determined by
        doing a modified Breadth First Traversal of the workflow starting from
        the root nodes. The level associated with a node, is the furthest
        distance of it from the root node instead of it being the shortest
        distance as in normal BFS. For each level the jobs are grouped by the
        site on which they have been scheduled by the Site Selector. Only jobs
        of same type (txnamespace, txname, txversion) can be clustered into a
        larger job. To use horizontal clustering the user needs to set the
        <emphasis role="bold">--cluster</emphasis> option of <emphasis
        role="bold">pegasus-plan to horizontal</emphasis> .</para>

        <section>
          <title>Controlling Clustering Granularity</title>

          <para>The number of jobs that have to be clustered into a single
          large job, is determined by the value of two parameters associated
          with the smaller jobs. Both these parameters are specified by the
          use of a PEGASUS namespace profile keys. The keys can be specified
          at any of the placeholders for the profiles (abstract transformation
          in the DAX, site in the site catalog, transformation in the
          transformation catalog). The normal overloading semantics apply i.e.
          profile in transformation catalog overrides the one in the site
          catalog and that in turn overrides the one in the DAX. The two
          parameters are described below.</para>

          <itemizedlist>
            <listitem>
              <para><emphasis role="bold">clusters.size
              factor</emphasis></para>

              <para>The clusters.size factor denotes how many jobs need to be
              merged into a single clustered job. It is specified via the use
              of a PEGASUS namespace profile key
              &amp;ldquo;clusters.size&amp;rdquor;. for e.g. if at a
              particular level, say 4 jobs referring to logical transformation
              B have been scheduled to a siteX. The clusters.size factor
              associated with job B for siteX is say 3. This will result in 2
              clustered jobs, one composed of 3 jobs and another of 2 jobs.
              The clusters.size factor can be specified in the transformation
              catalog as follows</para>

              <programlisting><emphasis role="bold">#site   transformation   pfn            type               architecture  profiles
</emphasis>
siteX    B     /shared/PEGASUS/bin/jobB INSTALLED       INTEL32::LINUX  PEGASUS::clusters.size=3
siteX    C     /shared/PEGASUS/bin/jobC INSTALLED       INTEL32::LINUX  PEGASUS::clusters.size=2
</programlisting>

              <figure>
                <title></title>

                <mediaobject>
                  <imageobject>
                    <imagedata fileref="images/advanced-clustering-1.png" />
                  </imageobject>
                </mediaobject>
              </figure>
            </listitem>

            <listitem>
              <para><emphasis role="bold">clusters.num
              factor</emphasis></para>

              <para>The clusters.num factor denotes how many clustered jobs
              does the user want to see per level per site. It is specified
              via the use of a PEGASUS namespace profile key
              &amp;ldquo;clusters.num&amp;rdquor;. for e.g. if at a particular
              level, say 4 jobs referring to logical transformation B have
              been scheduled to a siteX. The
              &amp;ldquo;clusters.num&amp;rdquor; factor associated with job B
              for siteX is say 3. This will result in 3 clustered jobs, one
              composed of 2 jobs and others of a single job each. The
              clusters.num factor in the transformation catalog can be
              specified as follows</para>

              <programlisting><emphasis role="bold">#site  transformation      pfn           type            architecture    profiles
</emphasis>
siteX    B     /shared/PEGASUS/bin/jobB INSTALLED       INTEL32::LINUX  PEGASUS::clusters.num=3
siteX    C     /shared/PEGASUS/bin/jobC INSTALLED       INTEL32::LINUX  PEGASUS::clusters.num=2
</programlisting>

              <para>In the case, where both the factors are associated with
              the job, the clusters.num value supersedes the clusters.size
              value.</para>

              <programlisting><emphasis role="bold">#site  transformation   pfn             type             architecture   profiles
</emphasis>
siteX    B     /shared/PEGASUS/bin/jobB INSTALLED       INTEL32::LINUX PEGASUS::clusters.size=3,clusters.num=3
</programlisting>

              <para>In the above case the jobs referring to logical
              transformation B scheduled on siteX will be clustered on the
              basis of &amp;ldquo;clusters.num&amp;rdquor; value. Hence, if
              there are 4 jobs referring to logical transformation B scheduled
              to siteX, then 3 clustered jobs will be created.</para>

              <figure>
                <title></title>

                <mediaobject>
                  <imageobject>
                    <imagedata fileref="images/advanced-clustering-2.png" />
                  </imageobject>
                </mediaobject>
              </figure>
            </listitem>
          </itemizedlist>
        </section>
      </section>

      <section id="runtime_clustering">
        <title>Runtime Clustering</title>

        <para>Workflows often consist of jobs of same type, but have varying
        run times. Two or more instances of the same job, with varying inputs
        can differ significantly in their runtimes. A simple way to think
        about this is running the same program on two distinct input sets,
        where one input is smaller (1 MB) as compared to the other which is 10
        GB in size. In such a case the two jobs will having significantly
        differing run times. When such jobs are clustered using horizontal
        clustering, the benefits of job clustering may be lost if all smaller
        jobs get clustered together, while the larger jobs are clustered
        together. In such scenarios it would be beneficial to be able to
        cluster jobs together such that all clustered jobs have similar
        runtimes.</para>

        <para>In case of runtime clustering, jobs in the workflow are
        associated with a level. The levels of the workflow are determined in
        the same manner as in horizontal clustering. For each level the jobs
        are grouped by the site on which they have been scheduled by the Site
        Selector. Only jobs of same type (txnamespace, txname, txversion) can
        be clustered into a larger job. To use runtime clustering the user
        needs to set the <emphasis role="bold">--cluster</emphasis> option of
        <emphasis role="bold">pegasus-plan to horizontal</emphasis>.</para>

        <para>Basic Algorithm of grouping jobs into clusters is as
        follows</para>

        <programlisting>// cluster.maxruntime - Is the maximum runtime for which the clustered job should run.
// j.runtime - Is the runtime of the job j.
1. Create a set of jobs of the same type (txnamespace, txname, txversion), and that run on the same site.
2. Sort the jobs in decreasing order of their runtime.
3. For each job j, repeat
  a. If j.runtime &gt; cluster.maxruntime then 
        ignore j.
  // Sum of runtime of jobs already in the bin + j.runtime &lt;= cluster.maxruntime
  b. If j can be added to any existing bin (clustered job) then 
        Add j to bin
     Else
        Add a new bin
        Add job j to newly added bin</programlisting>

        <para>The runtime of a job, and maximum runtime for which a clustered
        jobs should run, is determined by the value of two parameters
        associated with the jobs.</para>

        <itemizedlist>
          <listitem>
            <para><emphasis role="bold">job.runtime</emphasis></para>

            <para>expected runtime for a job</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">clusters.maxruntime</emphasis></para>

            <para>maxruntime for the clustered job</para>
          </listitem>
        </itemizedlist>

        <para>Both these parameters are specified by the use of a PEGASUS
        namespace profile keys. The keys can be specified at any of the
        placeholders for the profiles (abstract transformation in the DAX,
        site in the site catalog, transformation in the transformation
        catalog). The normal overloading semantics apply i.e. profile in
        transformation catalog overrides the one in the site catalog and that
        in turn overrides the one in the DAX. The two parameters are described
        below.</para>

        <programlisting><emphasis role="bold">#site  transformation   pfn             type             architecture   profiles
</emphasis>
siteX    B     /shared/PEGASUS/bin/jobB INSTALLED       INTEL32::LINUX PEGASUS::clusters.maxruntime=250,job.runtime=100
siteX    C     /shared/PEGASUS/bin/jobC INSTALLED       INTEL32::LINUX PEGASUS::clusters.maxruntime=300,job.runtime=100</programlisting>

        <figure>
          <title></title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="images/advanced-clustering-5.png" />
            </imageobject>
          </mediaobject>
        </figure>

        <para>In the above case the jobs referring to logical transformation B
        scheduled on siteX will be clustered such that all clustered jobs will
        run approximately for the same duration specified by the
        clusters.maxruntime property. In the above case we assume all jobs
        referring to transformation B run for 100 seconds. For jobs with
        significantly differeing runtime, the job.runtime property will be
        associated with the jobs in the DAX.</para>

        <para>In addition to the above two profiles, we need to inform
        pegasus-plan to use runtime clustering. This is done by setting the
        following property .</para>

        <programlisting><emphasis role="bold"> pegasus.clusterer.preference          Runtime</emphasis> </programlisting>

        <para></para>
      </section>

      <section id="label_clustering">
        <title>Label Clustering</title>

        <para>In label based clustering, the user labels the workflow. All
        jobs having the same label value are clustered into a single clustered
        job. This allows the user to create clusters or use a clustering
        technique that is specific to his workflows. If there is no label
        associated with the job, the job is not clustered and is executed as
        is<figure>
            <title></title>

            <mediaobject>
              <imageobject>
                <imagedata fileref="images/advanced-clustering-5.png" />
              </imageobject>
            </mediaobject>
          </figure></para>

        <para>Since, the jobs in a cluster in this case are not independent,
        it is important the jobs are executed in the correct order. This is
        done by doing a topological sort on the jobs in each cluster. To use
        label based clustering the user needs to set the <emphasis
        role="bold">--cluster</emphasis> option of <emphasis
        role="bold">pegasus-plan</emphasis> to label.</para>

        <section>
          <title>Labelling the Workflow</title>

          <para>The labels for the jobs in the workflow are specified by
          associated <emphasis role="bold">pegasus</emphasis> profile keys
          with the jobs during the DAX generation process. The user can choose
          which profile key to use for labeling the workflow. By default, it
          is assumed that the user is using the PEGASUS profile key label to
          associate the labels. To use another key, in the <emphasis
          role="bold">pegasus</emphasis> namespace the user needs to set the
          following property</para>

          <itemizedlist>
            <listitem>
              <para>pegasus.clusterer.label.key</para>
            </listitem>
          </itemizedlist>

          <para>For example if the user sets <emphasis
          role="bold">pegasus.clusterer.label.key </emphasis>to <emphasis
          role="bold">user_label</emphasis> then the job description in the
          DAX looks as follows</para>

          <programlisting>&lt;adag &gt;
&amp;mldr;
  &lt;job id="ID000004" namespace="app" name="analyze" version="1.0" level="1" &gt;
    &lt;argument&gt;-a bottom -T60  -i &lt;filename file="user.f.c1"/&gt;  -o &lt;filename file="user.f.d"/&gt;&lt;/argument&gt;
    &lt;profile namespace=&amp;ldquo;pegasus&amp;rdquor; key=&amp;ldquo;user_label&amp;rdquor;&gt;p1&lt;/profile&gt;
    &lt;uses file="user.f.c1" link="input" dontRegister="false" dontTransfer="false"/&gt;
    &lt;uses file="user.f.c2" link="input" dontRegister="false" dontTransfer="false"/&gt;
    &lt;uses file="user.f.d" link="output" dontRegister="false" dontTransfer="false"/&gt;
  &lt;/job&gt;
&amp;mldr;
&lt;/adag&gt;</programlisting>

          <itemizedlist>
            <listitem>
              <para>The above states that the <emphasis
              role="bold">pegasus</emphasis> profiles with key as <emphasis
              role="bold">user_label</emphasis> are to be used for designating
              clusters.</para>
            </listitem>

            <listitem>
              <para>Each job with the same value for <emphasis
              role="bold">pegasus</emphasis> profile key <emphasis
              role="bold">user_label </emphasis>appears in the same
              cluster.</para>
            </listitem>
          </itemizedlist>
        </section>
      </section>

      <section>
        <title>Recursive Clustering</title>

        <para>In some cases, a user may want to use a combination of
        clustering techniques. For e.g. a user may want some jobs in the
        workflow to be horizontally clustered and some to be label clustered.
        This can be achieved by specifying a comma separated list of
        clustering techniques to the<emphasis role="bold">
        &amp;ndash;-cluster</emphasis> option of <emphasis
        role="bold">pegasus-plan</emphasis>. In this case the clustering
        techniques are applied one after the other on the workflow in the
        order specified on the command line.</para>

        <para>For example</para>

        <programlisting>$ <emphasis>pegasus-plan &amp;ndash;-dax example.dax --dir ./dags --cluster label,horizontal &amp;ndash;s siteX &amp;ndash;-output local --verbose</emphasis></programlisting>

        <figure>
          <title></title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="images/advanced-clustering-4.png" />
            </imageobject>
          </mediaobject>
        </figure>
      </section>
    </section>

    <section>
      <title>Execution of the Clustered Job</title>

      <para>The execution of the clustered job on the remote site, involves
      the execution of the smaller constituent jobs either</para>

      <itemizedlist>
        <listitem>
          <para><emphasis role="bold">sequentially on a single node of the
          remote site</emphasis></para>

          <para>The clustered job is executed using <emphasis
          role="bold">seqexec</emphasis>, a wrapper tool written in C that is
          distributed as part of the PEGASUS. It takes in the jobs passed to
          it, and ends up executing them sequentially on a single node. To use
          &amp;ldquo;<emphasis role="bold">seqexec</emphasis>&amp;rdquor; for
          executing any clustered job on a siteX, there needs to be an entry
          in the transformation catalog for an executable with the logical
          name seqexec and namespace as pegasus.</para>

          <programlisting><emphasis role="bold">#site  transformation   pfn            type                 architecture    profiles</emphasis>

siteX    pegasus::seqexec     /shared/PEGASUS/bin/seqexec INSTALLED       INTEL32::LINUX NULL</programlisting>

          <para>By default, the entry for seqexec on a site is automatically
          picked up if $PEGASUS_HOME or $VDS_HOME is specified in the site
          catalog for that site.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">On multiple nodes of the remote site
          using MPI</emphasis></para>

          <para>The clustered job is executed using mpiexec, a wrapper mpi
          program written in C that is distributed as part of the PEGASUS. It
          is only distributed as source not as binary. The wrapper ends up
          being run on every mpi node, with the first one being the master and
          the rest of the ones as workers. The number of instances of mpiexec
          that are invoked is equal to the value of the globus rsl key
          nodecount. The master distributes the smaller constituent jobs to
          the workers.</para>

          <para>For e.g. If there were 10 jobs in the merged job and nodecount
          was 5, then one node acts as master, and the 10 jobs are distributed
          amongst the 4 slaves on demand. The master hands off a job to the
          slave node as and when it gets free. So initially all the 4 nodes
          are given a single job each, and then as and when they get done are
          handed more jobs till all the 10 jobs have been executed.</para>

          <para>To use &amp;ldquo;mpiexec&amp;rdquor; for executing the
          clustered job on a siteX, there needs to be an entry in the
          transformation catalog for an executable with the logical name
          mpiexec and namespace as pegasus.</para>

          <programlisting><emphasis role="bold">#site  transformation   pfn            type                 architecture    profiles</emphasis>

siteX    pegasus::seqexec     /shared/PEGASUS/bin/mpiexec INSTALLED       INTEL32::LINUX NULL</programlisting>

          <para>Another added advantage of using mpiexec, is that regular non
          mpi code can be run via MPI.</para>

          <para>Both the clustered job and the smaller constituent jobs are
          invoked via kickstart, unless the clustered job is being run via mpi
          (mpiexec). Kickstart is unable to launch mpi jobs. If kickstart is
          not installed on a particular site i.e. the gridlaunch attribute for
          site is not specified in the site catalog, the jobs are invoked
          directly.</para>
        </listitem>
      </itemizedlist>

      <section>
        <title>Specification of Method of Execution for Clustered Jobs</title>

        <para>The method execution of the clustered job(whether to launch via
        mpiexec or seqexec) can be specified</para>

        <orderedlist>
          <listitem>
            <para><emphasis role="bold">globally in the properties
            file</emphasis></para>

            <para>The user can set a property in the properties file that
            results in all the clustered jobs of the workflow being executed
            by the same type of executable.</para>

            <programlisting><emphasis role="bold">#PEGASUS PROPERTIES FILE</emphasis>
pegasus.clusterer.job.aggregator seqexec|mpiexec</programlisting>

            <para>In the above example, all the clustered jobs on the remote
            sites are going to be launched via the property value, as long as
            the property value is not overridden in the site catalog.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">associating profile key
            &amp;ldquo;collapser&amp;rdquor; with the site in the site
            catalog</emphasis></para>

            <programlisting>&lt;site handle="siteX" gridlaunch = "/shared/PEGASUS/bin/kickstart"&gt;
    &lt;profile namespace="env" key="GLOBUS_LOCATION" &gt;/home/shared/globus&lt;/profile&gt;
    &lt;profile namespace="env" key="LD_LIBRARY_PATH"&gt;/home/shared/globus/lib&lt;/profile&gt;
    &lt;profile namespace="pegasus" key="collapser" &gt;seqexec&lt;/profile&gt;
    &lt;lrc url="rls://siteX.edu" /&gt;
    &lt;gridftp  url="gsiftp://siteX.edu/" storage="/home/shared/work" major="2" minor="4" patch="0" /&gt;
    &lt;jobmanager universe="transfer" url="siteX.edu/jobmanager-fork" major="2" minor="4" patch="0" /&gt;
    &lt;jobmanager universe="vanilla" url="siteX.edu/jobmanager-condor" major="2" minor="4" patch="0" /&gt;
    &lt;workdirectory &gt;/home/shared/storage&lt;/workdirectory&gt;
  &lt;/site&gt;</programlisting>

            <para>In the above example, all the clustered jobs on a siteX are
            going to be executed via seqexec, as long as the value is not
            overridden in the transformation catalog.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">associating profile key
            &amp;ldquo;collapser&amp;rdquor; with the transformation that is
            being clustered, in the transformation catalog</emphasis></para>

            <programlisting><emphasis role="bold">#site  transformation   pfn            type                architecture profiles
</emphasis>
siteX    B     /shared/PEGASUS/bin/jobB INSTALLED       INTEL32::LINUX pegasus::clusters.size=3,collapser=mpiexec</programlisting>

            <para>In the above example, all the clustered jobs that consist of
            transformation B on siteX will be executed via mpiexec.</para>

            <note>
              <para><emphasis role="bold"> The clustering of jobs on a site
              only happens only if </emphasis></para>
            </note>

            <itemizedlist>
              <listitem>
                <para>there exists an entry in the transformation catalog for
                the clustering executable that has been determined by the
                above 3 rules</para>
              </listitem>

              <listitem>
                <para>the number of jobs being clustered on the site are more
                than 1</para>
              </listitem>
            </itemizedlist>
          </listitem>
        </orderedlist>
      </section>
    </section>

    <section>
      <title>Outstanding Issues</title>

      <orderedlist>
        <listitem>
          <para><emphasis role="bold">Label Clustering</emphasis></para>

          <para>More rigorous checks are required to ensure that the labeling
          scheme applied by the user is valid.</para>
        </listitem>
      </orderedlist>
    </section>
  </section>
</section>
