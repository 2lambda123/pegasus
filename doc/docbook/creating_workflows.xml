<?xml version="1.0" encoding="US-ASCII"?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN"
"http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<chapter id="creating_workflows">
  <title>Creating Workflows</title>

  <section>
    <title>Executable Workflow (DAG)</title>

    <para>Pegasus takes an abstract workflow in XML format called DAX as its
    fundamental input. Pegasus plans the abstract workflow provided by the DAX
    using the concrete information found in the replica catalog (RC), site
    catalog (SC), and transformation catalog (TC). The concrete behavior is
    controlled by the configuration (properties). Pegasus refines the abstract
    workflow to create a number of concretely runnable jobs that can be
    executed in the Grid, Clouds, locally, or remotely.</para>

    <para>The DAX format is described by the XML schema instance document
    <ulink
    url="http://pegasus.isi.edu/wms/docs/schemas/dax-3.2/dax-3.2.xsd">dax-3.2.xsd</ulink>.
    A local copy of the schema definition is provided in the
    <quote>etc</quote> directory. The documentation of the schema and its
    elements can be found in <ulink
    url="http://pegasus.isi.edu/wms/docs/schemas/dax-3.2/dax-3.2.html">dax-3.2.html</ulink>.
    The example below shows a workflow we call the "black diamond" for
    historical reasons and due to its shape. This simple workflow incorporates
    some of the elementary graph structures you will deal with in your own
    workflows:</para>

    <itemizedlist>
      <listitem>
        <para><emphasis>fan-out</emphasis>, <emphasis>scatter</emphasis>, and
        <emphasis>diverge</emphasis> all describe the fact that multiple
        siblings are dependent on fewer parents.</para>

        <para>The example shows how the <emphasis>findrange</emphasis> nodes
        depend on the <emphasis>preprocess</emphasis> node.</para>
      </listitem>

      <listitem>
        <para><emphasis>fan-in</emphasis>, <emphasis>gather</emphasis>,
        <emphasis>join</emphasis>, and <emphasis>converge</emphasis> describe
        how multiple siblings are merged into fewer dependent child
        nodes.</para>

        <para>The example shows how the <emphasis>analyze</emphasis> node
        depends on both <emphasis>findrange</emphasis> nodes.</para>
      </listitem>
    </itemizedlist>

    <itemizedlist>
      <listitem>
        <para><emphasis>serial execution</emphasis> implies that nodes are
        dependent on one another, like pearls on a string.</para>
      </listitem>

      <listitem>
        <para><emphasis>parallel execution</emphasis> implies that nodes can
        be executed in parallel, as shown by the
        <emphasis>findrange</emphasis> nodes in the example.</para>
      </listitem>
    </itemizedlist>

    <para><figure id="concepts-fig-dax">
        <title>Black Diamond Dax</title>

        <mediaobject>
          <imageobject>
            <imagedata align="center" fileref="images/concepts-diamond.jpg"
                       valign="middle" />
          </imageobject>
        </mediaobject>
      </figure></para>

    <para>The example diamond workflow consits of four nodes representing
    jobs, and are linked by six files.</para>

    <itemizedlist>
      <listitem>
        <para>If you follow the arrows, you realize that one arrow end in file
        <filename>f.d</filename>, but no arrows are leaving it. This indicates
        that <filename>f.d</filename> is a leaf file. It is a product or
        output of this workflow. Output files can be collected at a
        location.</para>
      </listitem>

      <listitem>
        <para>If you follow the arrows, you also realize that only one arrow
        comes from file <filename>f.a</filename>, but no arrows are leading to
        it. This indicates that <filename>f.a</filename> is a required input
        file to the workflow. Its location must be registered with the replica
        catalog in order for Pegasus to find it and integrate it into the
        workflow.</para>
      </listitem>

      <listitem>
        <para>The remaining files all have arrows leading to them and
        originating from them. These files are products of some job steps
        (arrows leading to them), and consumed by other job steps (arrows
        leading out of them). Often, these files represent intermediary
        results that can be cleaned.</para>
      </listitem>
    </itemizedlist>

    <para>There are two main ways of generating DAX's</para>

    <orderedlist>
      <listitem>
        <para>Using a DAX generating API in <link
        linkend="api-java">Java</link>, <link linkend="api-perl">Perl</link>
        or <link linkend="api-python">Python</link>.</para>

        <para><emphasis>Note</emphasis> This option is what we
        recommend.</para>
      </listitem>

      <listitem>
        <para>Generating XML directly from your script.</para>

        <para>This option should only be considered by advanced users who can
        also read XML schema definitions.</para>
      </listitem>
    </orderedlist>

    <para>One example for a DAX representing the example workflow can look
    like the following:</para>

    <programlisting>&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;!-- generated: 2010-11-22T22:55:08Z --&gt;
&lt;adag xmlns="http://pegasus.isi.edu/schema/DAX" 
      xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
      xsi:schemaLocation="http://pegasus.isi.edu/schema/DAX http://pegasus.isi.edu/schema/dax-3.2.xsd" 
      version="3.2" name="diamond" index="0" count="1"&gt;
  &lt;!-- part 2: definition of all jobs (at least one) --&gt;
  &lt;job namespace="diamond" name="preprocess" version="2.0" id="ID000001"&gt;
    &lt;argument&gt;-a preprocess -T60 -i &lt;file name="f.a" /&gt; -o &lt;file name="f.b1" /&gt; &lt;file name="f.b2" /&gt;&lt;/argument&gt;
    &lt;uses name="f.b2" link="output" register="false" transfer="false" /&gt;
    &lt;uses name="f.b1" link="output" register="false" transfer="false" /&gt;
    &lt;uses name="f.a" link="input" /&gt;
  &lt;/job&gt;
  &lt;job namespace="diamond" name="findrange" version="2.0" id="ID000002"&gt;
    &lt;argument&gt;-a findrange -T60 -i &lt;file name="f.b1" /&gt; -o &lt;file name="f.c1" /&gt;&lt;/argument&gt;
    &lt;uses name="f.b1" link="input" register="false" transfer="false" /&gt;
    &lt;uses name="f.c1" link="output" register="false" transfer="false" /&gt;
  &lt;/job&gt;
  &lt;job namespace="diamond" name="findrange" version="2.0" id="ID000003"&gt;
    &lt;argument&gt;-a findrange -T60 -i &lt;file name="f.b2" /&gt; -o &lt;file name="f.c2" /&gt;&lt;/argument&gt;
    &lt;uses name="f.c2" link="output" register="false" transfer="false" /&gt;
    &lt;uses name="f.b2" link="input" register="false" transfer="false" /&gt;
  &lt;/job&gt;
  &lt;job namespace="diamond" name="analyze" version="2.0" id="ID000004"&gt;
    &lt;argument&gt;-a analyze -T60 -i &lt;file name="f.c1" /&gt; &lt;file name="f.c2" /&gt; -o &lt;file name="f.d" /&gt;&lt;/argument&gt;
    &lt;uses name="f.c2" link="input" register="false" transfer="false" /&gt;
    &lt;uses name="f.d" link="output" register="false" transfer="true" /&gt;
    &lt;uses name="f.c1" link="input" register="false" transfer="false" /&gt;
  &lt;/job&gt;
  &lt;!-- part 3: list of control-flow dependencies --&gt;
  &lt;child ref="ID000002"&gt;
    &lt;parent ref="ID000001" /&gt;
  &lt;/child&gt;
  &lt;child ref="ID000003"&gt;
    &lt;parent ref="ID000001" /&gt;
  &lt;/child&gt;
  &lt;child ref="ID000004"&gt;
    &lt;parent ref="ID000002" /&gt;
    &lt;parent ref="ID000003" /&gt;
  &lt;/child&gt;
&lt;/adag&gt;</programlisting>

    <para>The example workflow representation in form of a DAX requires
    external catalogs, such as transformation catalog (TC) to resolve the
    logical job names (such as diamond::preprocess:2.0), and a replica catalog
    (RC) to resolve the input file <filename>f.a</filename>. The above
    workflow defines the four jobs just like the example picture, and the
    files that flow between the jobs. The intermediary files are neither
    registered nor staged out, and can be considered transient. Only the final
    result file <filename>f.d</filename> is staged out.</para>

    <para>The decision between not staging nor remembering intermediary files,
    and staging them, is a balance act of your confidence in the workflow. If
    it is a very large workflow that is likely to fail at some point due to a
    site problem, or even a transient problem, you may want to be able to
    restart your workflow without losing too much work. If intermediary files
    were saved and registered, you can reduce the number of repeated,
    previously computed tasks, especially when re-planning.</para>

    <para>When you take the workflow DAX above, and plan it for a single
    remote grid execution, here a site with handle <emphasis>hpcc</emphasis>,
    and plan the workflow without clean-up nodes, the following concrete
    workflow is built:</para>

    <para><figure id="concepts-fig-dag-example">
        <title>Black Diamond DAG</title>

        <mediaobject>
          <imageobject>
            <imagedata align="center"
                       fileref="images/concepts-diamond-dag.png"
                       valign="middle" />
          </imageobject>
        </mediaobject>
      </figure></para>

    <para>Planning augments the original abstract workflow with ancillary
    tasks to facility the proper execution of the workflow. These tasks
    include:</para>

    <itemizedlist>
      <listitem>
        <para>the creation of remote working directories. These directories
        typically have name that seeks to avoid conflicts with other
        simultaneously running similar workflows. Such tasks use a job prefix
        of <code>create_dir</code>.</para>
      </listitem>

      <listitem>
        <para>the stage-in of input files before any task which requires these
        files. Any file consumed by a task needs to be staged to the task, if
        it does not already exist on that site. Such tasks use a job prefix of
        <code>stage_in</code>.If multiple files from various sources need to
        be transferred, multiple stage-in jobs will be created. Additional
        advanced options permit to control the size and number of these jobs,
        and whether multiple compute tasks can share stage-in jobs.</para>
      </listitem>

      <listitem>
        <para>the original DAX job is concretized into a compute task in the
        DAG. Compute jobs are a concatination of the job's
        <emphasis>name</emphasis> and <emphasis>id</emphasis> attribute from
        the DAX file.</para>
      </listitem>

      <listitem>
        <para>the stage-out of data products to a collecting site. Data
        products with their <emphasis>transfer</emphasis> flag set to
        <literal>false</literal> will not be staged to the output site.
        However, they may still be eligible for staging to other, dependent
        tasks. Stage-out tasks use a job prefix of
        <code>stage_out</code>.</para>
      </listitem>

      <listitem>
        <para>If compute jobs run at different sites, an intermediary staging
        task with prefix <code>stage_inter</code> is inserted between the
        compute jobs in the workflow, ensuring that the data products of the
        parent are available to the child job.</para>
      </listitem>

      <listitem>
        <para>the registration of data products in a replica catalog. Data
        products with their <emphasis>register</emphasis> flag set to
        <literal>false</literal> will not be registered.</para>
      </listitem>

      <listitem>
        <para>the clean-up of transient files and working directories. These
        steps can be omitted with the <command>--no-cleanup</command> option
        to the planner.</para>
      </listitem>
    </itemizedlist>

    <para>See <link linkend="transfer">section on transfers</link> to find out
    more about when and how staging nodes are inserted into the
    workflow.</para>

    <para>The DAG will be found in file <filename>diamond-0.dag</filename>,
    constructed from the <emphasis>name</emphasis> and
    <emphasis>index</emphasis> attributes found in the root element of the DAX
    file.</para>

    <programlisting>######################################################################
# PEGASUS WMS GENERATED DAG FILE
# DAG diamond
# Index = 0, Count = 1
######################################################################

JOB create_dir_diamond_0_hpcc create_dir_diamond_0_hpcc.sub
SCRIPT POST create_dir_diamond_0_hpcc /opt/pegasus/default/bin/pegasus-exitcode create_dir_diamond_0_hpcc.out

JOB stage_in_local_hpcc_0 stage_in_local_hpcc_0.sub
SCRIPT POST stage_in_local_hpcc_0 /opt/pegasus/default/bin/pegasus-exitcode stage_in_local_hpcc_0.out

JOB preprocess_ID000001 preprocess_ID000001.sub
SCRIPT POST preprocess_ID000001 /opt/pegasus/default/bin/pegasus-exitcode preprocess_ID000001.out

JOB findrange_ID000002 findrange_ID000002.sub
SCRIPT POST findrange_ID000002 /opt/pegasus/default/bin/pegasus-exitcode findrange_ID000002.out

JOB findrange_ID000003 findrange_ID000003.sub
SCRIPT POST findrange_ID000003 /opt/pegasus/default/bin/pegasus-exitcode findrange_ID000003.out

JOB analyze_ID000004 analyze_ID000004.sub
SCRIPT POST analyze_ID000004 /opt/pegasus/default/bin/pegasus-exitcode analyze_ID000004.out

JOB stage_out_local_hpcc_2_0 stage_out_local_hpcc_2_0.sub
SCRIPT POST stage_out_local_hpcc_2_0 /opt/pegasus/default/bin/pegasus-exitcode stage_out_local_hpcc_2_0.out

PARENT findrange_ID000002 CHILD analyze_ID000004
PARENT findrange_ID000003 CHILD analyze_ID000004
PARENT preprocess_ID000001 CHILD findrange_ID000002
PARENT preprocess_ID000001 CHILD findrange_ID000003
PARENT analyze_ID000004 CHILD stage_out_local_hpcc_2_0
PARENT stage_in_local_hpcc_0 CHILD preprocess_ID000001
PARENT create_dir_diamond_0_hpcc CHILD findrange_ID000002
PARENT create_dir_diamond_0_hpcc CHILD findrange_ID000003
PARENT create_dir_diamond_0_hpcc CHILD preprocess_ID000001
PARENT create_dir_diamond_0_hpcc CHILD analyze_ID000004
PARENT create_dir_diamond_0_hpcc CHILD stage_in_local_hpcc_0
######################################################################
# End of DAG
######################################################################
</programlisting>

    <para>The DAG file declares all jobs and links them to a Condor submit
    file that describes the planned, concrete job. In the same directory as
    the DAG file are all Condor submit files for the jobs from the picture
    plus a number of additional helper files.</para>

    <para>The various instructions that can be put into a DAG file are
    described in <ulink
    url="http://www.cs.wisc.edu/condor/manual/v7.5/2_10DAGMan_Applications.html">Condor's
    DAGMAN documentation</ulink>.The constituents of the submit directory are
    shown in <link linkend="submit_directory-layout">chapter "submit
    directory"</link>.</para>
  </section>

  <section>
    <title>Execution Environments</title>

    <para>Pegasus supports a number of execution environments. An execution
    environment is a setup where jobs from a workflow are running.</para>

    <itemizedlist>
      <listitem>
        <para>The simplest execution environment does not involve Condor.
        Pegasus is capable of planning small workflows for full-local
        execution using a shell planner. Please refer to the <filename
        class="directory">examples</filename> directory in your Pegasus
        installation, the shell planner's <link
        linkend="local_shell_examples">documentation section</link>, or the
        tutorials, for details.</para>
      </listitem>

      <listitem>
        <para>A slighly more challenging setup is still all-local, but Condor
        manages queued jobs and Condor DAGMan the workflow. This setup permits
        limited parallelism with full scalability. With proper setup, Condor
        can scavenge cycles from unused computers in your department, enabling
        a pool of more than just a single machine.</para>
      </listitem>

      <listitem>
        <para>The vanilla setup are workflows to be executed in a grid
        environment. This setup requires a number of configurations, and an
        understanding of remote sites.</para>
      </listitem>

      <listitem>
        <para>Various advanced execution environment setups deal with
        minimally using Globus to obtain remote resources (Glide-ins),
        by-passing Globus completely (Condor-C), or supporting cloud computing
        (Nimbus, Eucalyptus, EC2). You should be familiar with the Grid
        Execution Environment, as some concept are borrowed.</para>
      </listitem>
    </itemizedlist>

    <section>
      <title>The Grid Execution Environment</title>

      <para>The rest of this chapter will focus on the vanilla grid execution
      environment.</para>

      <para><figure id="concepts-fig-site-layout">
          <title>Grid Sample Site Layout</title>

          <mediaobject>
            <imageobject>
              <imagedata align="center"
                         fileref="images/concepts-site-layout.jpg"
                         valign="middle" />
            </imageobject>
          </mediaobject>
        </figure></para>

      <para>The vanilla grid environment shown in the figure <link
      linkend="concepts-fig-site-layout">above</link>. We will work from the
      left to the right top, then the right bottom.</para>

      <para>On the left side, you have a submit machine where Pegasus runs,
      Condor schedules jobs, and workflows are executed. We call it the
      <emphasis>submit host</emphasis> (SH), though its functionality can be
      assumed by a virtual machine image. In order to properly communicate
      over secured channels, it is important that the submit machine has a
      proper notion of time, i.e. runs an NTP daemon to keep accurate time. To
      be able to connect to remote clusters and receive connections from the
      remote clusters, the submit host has a public IP address to facilitate
      this communication.</para>

      <para>In order to send a job request to the remote cluster, Condor wraps
      the job into Globus calls via Condor-G. Globus uses GRAM to manage jobs
      on remote sites. In terms of a software stack, Pegasus wraps the job
      into Condor. Condor wraps the job into Globus. Globus transports the job
      to the remote site, and unwraps the Globus component, sending it to the
      remote site's <emphasis>resource manager</emphasis> (RM).</para>

      <para>To be able to communicate using the Globus security infrastructure
      (GSI), the submit machine needs to have the certificate authority (CA)
      certificates configured, requires a host certificate in certain
      circumstances, and the user a user certificate that is enabled on the
      remote site. On the remote end, the remote gatekeeper node requires a
      host certificate, all signing CA certificate chains and policy files,
      and a goot time source.</para>

      <para>In a grid environment, there are one or more clusters accessible
      via grid middleware like the <ulink url="http://www.globus.org/">Globus
      Toolkit</ulink>. In case of Globus, there is the Globus gatekeeper
      listening on TCP port 2119 of the remote cluster. The port is opened to
      a single machine called <emphasis>head node</emphasis> (HN).The
      head-node is typically located in a de-militarized zone (DMZ) of the
      firewall setup, as it requires limited outside connectivity and a public
      IP address so that it can be contacted. Additionally, once the
      gatekeeper accepted a job, it passes it on to a jobmanager. Often, these
      jobmanagers require a limited port range, in the example TCP ports
      40000-41000, to call back to the submit machine.</para>

      <para>For the user to be able to run jobs on the remote site, the user
      must have some form of an account on the remtoe site. The user's grid
      identity is passed from the submit host. An entity called <emphasis>grid
      mapfile</emphasis> on the gatekeeper maps the user's grid identity into
      a remote account. While most sites do not permit account sharing, it is
      possible to map multiple user certificates to the same account.</para>

      <para>The gatekeeper is the interface through which jobs are submitted
      to the remote cluster's resource manager. A resource manager is a
      scheduling system like PBS, Maui, LSF, FBSNG or Condor that queues tasks
      and allocates worker nodes. The <emphasis>worker nodes</emphasis> (WN)
      in the remote cluster might not have outside connectivity and often use
      all private IP addresses. The Globus toolkit requires a shared
      filesystem to properly stage files between the head node and worker
      nodes.</para>

      <note>
        <para>The shared filesystem requirement is imposed by Globus. Pegasus
        is capable of supporting advanced site layouts that do not require a
        shared filesystem. Please contact us for details, should you require
        such a setup.</para>
      </note>

      <para>To stage data between external sites for the job, it is
      recommended to enable a GridFTP server. If a shared networked filesystem
      is involved, the GridFTP server should be located as close to the
      file-server as possible. The GridFTP server requires TCP port 2811 for
      the control channel, and a limited port range for data channels, here as
      an example the TPC ports from 40000 to 41000. The GridFTP server
      requires a host certificate, the signing CA chain and policy files, a
      stable time source, and a gridmap file that maps between a user's grid
      identify and the user's account on the remote site.</para>

      <para>The GridFTP server is often installed on the head node, the same
      as the gatekeeper, so that they can share the grid mapfile, CA
      certificate chains and other setups. However, for performance purposes
      it is recommended that the GridFTP server has its own machine.</para>
    </section>

    <section>
      <title>A Cloud Execution Environment</title>

      <para><figure id="concepts-fig-cloud-layout">
          <title>Cloud Sample Site Layout</title>

          <mediaobject>
            <imageobject>
              <imagedata align="center" fileref="images/fg-pwms-prefio.3.png"
                         valign="middle" />
            </imageobject>
          </mediaobject>
        </figure></para>

      <para>The pevious figure shows a sample layout for sky computing (as in:
      multiple clouds) as supported by Pegasus. At this point, it is up to the
      user to provision the remote resources with a proper VM image that
      includes a Condor startd and proper Condor configuration to report back
      to a Condor collector that the Condor schedd has access to.</para>

      <para>In this discussion, the <emphasis>submit host</emphasis> (SH) is
      located logically external to the cloud provider(s). The SH is the point
      where a user submits Pegasus workflows for execution. This site
      typically runs a Condor collector to gather resource announcements, or
      is part of a larger Condor pool that collects these announcement. Condor
      makes the remote resources available to the submit host&rsquor;s Condor
      installation.</para>

      <para>The <link linkend="concepts-fig-cloud-layout">figure above</link>
      shows the way Pegasus WMS is deployed in cloud computing resources,
      ignoring how these resources were provisioned. The provisioning request
      shows multiple resources per provisioning request.</para>

      <para>The provisioning broker -- Nimbus, Eucalyptus or EC2 -- at the
      remote site is responsible to allocate and set up the resources. For a
      multi-node request, the worker nodes often require access to a form of
      shared data storage. Concretely, either a POSIX-compliant shared file
      system (e.g. NFS, PVFS) is available to the nodes, or can be brought up
      for the lifetime of the application workflow. The task steps of the
      application workflow facilitate shared file systems to exchange
      intermediary results between tasks on the same cloud site. Pegasus also
      supports an S3 data mode for the application workflow data
      staging.</para>

      <para>The initial stage-in and final stage-out of application data into
      and out of the node set is part of any Pegasus-planned workflow. Several
      configuration options exist in Pegasus to deal with the dynamics of push
      and pull of data, and when to stage data. In many use-cases, some form
      of external access to or from the shared file system that is visible to
      the application workflow is required to facilitate successful data
      staging. However, Pegasus is prepared to deal with a set of boundary
      cases.</para>

      <para>The data server in the figure is shown at the submit host. This is
      not a strict requirement. The data server for consumed data and data
      products may both be different and external to the submit host.</para>

      <para>Once resources begin appearing in the pool managed by the submit
      machine&rsquor;s Condor collector, the application workflow can be
      submitted to Condor. A Condor DAGMan will manage the application
      workflow execution. Pegasus run-time tools obtain timing-, performance
      and provenance information as the application workflow is executed. At
      this point, it is the user's responsibility to de-provision the
      allocated resources.</para>

      <para>In the figure, the cloud resources on the right side are assumed
      to have uninhibited outside connectivity. This enables the Condor I/O to
      communicate with the resources. The right side includes a setup where
      the worker nodes use all private IP, but have out-going connectivity and
      a NAT router to talk to the internet. The <emphasis>Condor connection
      broker</emphasis> (CCB) facilitates this setup almost
      effortlessly.</para>

      <para>The left side shows a more difficult setup where the connectivity
      is fully firewalled without any connectivity except to in-site nodes. In
      this case, a proxy server process, the <emphasis>generic connection
      broker</emphasis> (GCB), needs to be set up in the DMZ of the cloud site
      to facilitate Condor I/O between the submit host and worker
      nodes.</para>

      <para>If the cloud supports data storage servers, Pegasus is starting to
      support workflows that require staging in two steps: Consumed data is
      first staged to a data server in the remote site's DMZ, and then a
      second staging task moves the data from the data server to the worker
      node where the job runs. For staging out, data needs to be first staged
      from the job's worker node to the site's data server, and possibly from
      there to another data server external to the site. Pegasus is capable to
      plan both steps: Normal staging to the site's data server, and the
      worker-node staging from and to the site's data server as part of the
      job. We are working on expanding the current code to support a more
      generic set by Pegasus 3.1.</para>

      <section>
        <title>Shared File Systems</title>

        <para>Ideally, any allocation of multiple resources shares a file
        system among them. This can be either a globally shared file system
        like NFS, or a per-request file system dynamically allocated like PVFS
        or NFS. Often, for the sake of speed, it is advisable that each
        resource has its own fast local non-shared disk space that can be
        transiently used by application workflow jobs.</para>

        <para>Pegasus recommends the use of a shared file system, because it
        simplifies workflows and may even improve performance. Without shared
        file system, the same data may need to be staged to the same site but
        different resources multiple times. Without shared file systems, data
        products of a parent job need to be staged out, and staged back in to
        the child job, currently via an external data server, even if parent
        and child run on the same multi-resource allocation.</para>

        <para>The Pegasus team explores the option of dynamically bringing up
        an NFS server on one resource of a multi-resource allocation for the
        duration of the allocation. This option is part of the configuration
        for an experiment&rsquor;s workflow&rsquor;s provisioning stage. We do
        not expect cloud sites to ban NFS and RPC traffic between resources of
        the same multi-resource allocation.</para>
      </section>
    </section>
  </section>

  <section>
    <title>Abstract Workflows (DAX)</title>

    <para>The DAX is a description of an abstract workflow in XML format that
    is used as the primary input into Pegasus. The DAX schema is described in
    <ulink
    url="http://pegasus.isi.edu/wms/docs/schemas/dax-3.2/dax-3.2.xsd">dax-3.2.xsd</ulink>
    The documentation of the schema and its elements can be found in <ulink
    url="http://pegasus.isi.edu/wms/docs/schemas/dax-3.2/dax-3.2.html">dax-3.2.html</ulink>.</para>

    <para>A DAX can be created by all users with the DAX generating API in
    Java, Perl, or Python format</para>

    <note>
      We highly recommend using the DAX API.
    </note>

    <para>Advanced users who can read XML schema definitions can generate a
    DAX directly from a script</para>

    <para>The sample workflow below incorporates some of the elementary graph
    structures used in all abstract workflows.</para>

    <itemizedlist>
      <listitem>
        <para><emphasis>fan-out</emphasis>, <emphasis>scatter</emphasis>, and
        <emphasis>diverge</emphasis> all describe the fact that multiple
        siblings are dependent on fewer parents.</para>

        <para>The example shows how the <emphasis> Job 2 and 3</emphasis>
        nodes depend on <emphasis>Job 1</emphasis> node.</para>
      </listitem>

      <listitem>
        <para><emphasis>fan-in</emphasis>, <emphasis>gather</emphasis>,
        <emphasis>join</emphasis>, and <emphasis>converge</emphasis> describe
        how multiple siblings are merged into fewer dependent child
        nodes.</para>

        <para>The example shows how the <emphasis>Job 4</emphasis> node
        depends on both <emphasis>Job 2 and Job 3</emphasis> nodes.</para>
      </listitem>
    </itemizedlist>

    <itemizedlist>
      <listitem>
        <para><emphasis>serial execution</emphasis> implies that nodes are
        dependent on one another, like pearls on a string.</para>
      </listitem>

      <listitem>
        <para><emphasis>parallel execution</emphasis> implies that nodes can
        be executed in parallel</para>
      </listitem>
    </itemizedlist>

    <para><figure id="components_blackdiamond">
        <title>Sample Workflow</title>

        <mediaobject>
          <imageobject>
            <imagedata align="center" fileref="images/DiamondWorkflow.png"
                       valign="middle" />
          </imageobject>
        </mediaobject>
      </figure></para>

    <para>The example diamond workflow consits of four nodes representing
    jobs, and are linked by six files.</para>

    <itemizedlist>
      <listitem>
        <para>Required input files must be registered with the Replica catalog
        in order for Pegasus to find it and integrate it into the
        workflow.</para>
      </listitem>

      <listitem>
        <para>Leaf files are a product or output of a workflow. Output files
        can be collected at a location.</para>
      </listitem>

      <listitem>
        <para>The remaining files all have lines leading to them and
        originating from them. These files are products of some job steps
        (lines leading to them), and consumed by other job steps (lines
        leading out of them). Often, these files represent intermediary
        results that can be cleaned.</para>
      </listitem>
    </itemizedlist>

    <para>There are two main ways of generating DAX's</para>

    <orderedlist>
      <listitem>
        <para>Using a DAX generating API in <link
        linkend="api-java">Java</link>, <link linkend="api-perl">Perl</link>
        or <link linkend="api-python">Python</link>.</para>

        <para><emphasis>Note</emphasis> This option is what we
        recommend.</para>
      </listitem>

      <listitem>
        <para>Generating XML directly from your script.</para>

        <para>This option should only be considered by advanced users who can
        also read XML schema definitions.</para>
      </listitem>
    </orderedlist>

    <para>One example for a DAX representing the example workflow can look
    like the following:</para>

    <programlisting>&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;!-- generated: 2010-11-22T22:55:08Z --&gt;
&lt;adag xmlns="http://pegasus.isi.edu/schema/DAX"
      xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
      xsi:schemaLocation="http://pegasus.isi.edu/schema/DAX http://pegasus.isi.edu/schema/dax-3.2.xsd"
      version="3.2" name="diamond" index="0" count="1"&gt;
  &lt;!-- part 2: definition of all jobs (at least one) --&gt;
  &lt;job namespace="diamond" name="preprocess" version="2.0" id="ID000001"&gt;
    &lt;argument&gt;-a preprocess -T60 -i &lt;file name="f.a" /&gt; -o &lt;file name="f.b1" /&gt; &lt;file name="f.b2" /&gt;&lt;/argument&gt;
    &lt;uses name="f.b2" link="output" register="false" transfer="false" /&gt;
    &lt;uses name="f.b1" link="output" register="false" transfer="false" /&gt;
    &lt;uses name="f.a" link="input" /&gt;
  &lt;/job&gt;
  &lt;job namespace="diamond" name="findrange" version="2.0" id="ID000002"&gt;
    &lt;argument&gt;-a findrange -T60 -i &lt;file name="f.b1" /&gt; -o &lt;file name="f.c1" /&gt;&lt;/argument&gt;
    &lt;uses name="f.b1" link="input" register="false" transfer="false" /&gt;
    &lt;uses name="f.c1" link="output" register="false" transfer="false" /&gt;
  &lt;/job&gt;
  &lt;job namespace="diamond" name="findrange" version="2.0" id="ID000003"&gt;
    &lt;argument&gt;-a findrange -T60 -i &lt;file name="f.b2" /&gt; -o &lt;file name="f.c2" /&gt;&lt;/argument&gt;
    &lt;uses name="f.c2" link="output" register="false" transfer="false" /&gt;
    &lt;uses name="f.b2" link="input" register="false" transfer="false" /&gt;
  &lt;/job&gt;
  &lt;job namespace="diamond" name="analyze" version="2.0" id="ID000004"&gt;
    &lt;argument&gt;-a analyze -T60 -i &lt;file name="f.c1" /&gt; &lt;file name="f.c2" /&gt; -o &lt;file name="f.d" /&gt;&lt;/argument&gt;
    &lt;uses name="f.c2" link="input" register="false" transfer="false" /&gt;
    &lt;uses name="f.d" link="output" register="false" transfer="true" /&gt;
    &lt;uses name="f.c1" link="input" register="false" transfer="false" /&gt;
  &lt;/job&gt;
  &lt;!-- part 3: list of control-flow dependencies --&gt;
  &lt;child ref="ID000002"&gt;
    &lt;parent ref="ID000001" /&gt;
  &lt;/child&gt;
  &lt;child ref="ID000003"&gt;
    &lt;parent ref="ID000001" /&gt;
  &lt;/child&gt;
  &lt;child ref="ID000004"&gt;
    &lt;parent ref="ID000002" /&gt;
    &lt;parent ref="ID000003" /&gt;
  &lt;/child&gt;
&lt;/adag&gt;</programlisting>

    <para>The example workflow representation in form of a DAX requires
    external catalogs, such as transformation catalog (TC) to resolve the
    logical job names (such as diamond::preprocess:2.0), and a replica catalog
    (RC) to resolve the input file <filename>f.a</filename>. The above
    workflow defines the four jobs just like the example picture, and the
    files that flow between the jobs. The intermediary files are neither
    registered nor staged out, and can be considered transient. Only the final
    result file <filename>f.d</filename> is staged out.</para>
  </section>

  <section id="replica">
    <title>Data Discovery (Replica Catalog)</title>

    <para>The Replica Catalog keeps mappings of logical file ids/names (LFN's)
    to physical file ids/names (PFN's). A single LFN can map to several PFN's.
    A PFN consists of a URL with protocol, host and port information and a
    path to a file. Along with the PFN one can also store additional key/value
    attributes to be associated with a PFN.</para>

    <para>Pegasus supports 3 different implemenations of the Replica
    Catalog.</para>

    <orderedlist>
      <listitem>
        <para>File <emphasis role="bold">(Default)</emphasis></para>
      </listitem>

      <listitem>
        <para>Database via JDBC</para>
      </listitem>

      <listitem>
        <para>Replica Location Service</para>

        <itemizedlist>
          <listitem>
            <para>RLS</para>
          </listitem>

          <listitem>
            <para>LRC</para>
          </listitem>
        </itemizedlist>
      </listitem>

      <listitem>
        <para>MRC</para>
      </listitem>
    </orderedlist>

    <section id="rc-FILE">
      <title>File</title>

      <para>In this mode, Pegasus queries a file based replica catalog. The
      file format is a simple multicolumn format. It is neither
      transactionally safe, nor advised to use for production purposes in any
      way. Multiple concurrent instances will clobber each other. The site
      attribute should be specified whenever possible. The attribute key for
      the site attribute is <emphasis role="bold">"pool".</emphasis></para>

      <programlisting>
LFN PFN
LFN PFN a=b [..]
LFN PFN a="b" [..]
"LFN w/LWS" "PFN w/LWS" [..]
      </programlisting>

      <para>The LFN may or may not be quoted. If it contains linear
      whitespace, quotes, backslash or an equality (equal?) sign, it must be
      quoted and escaped. The same conditions apply for the PFN. The attribute
      key-value pairs are separated by an equality sign without any
      whitespaces. The value may be in quoted. The LFN sentiments about
      quoting apply.</para>

      <para>The file mode is the Default mode. In order to use the File mode
      you have to set the following properties</para>

      <para><orderedlist>
          <listitem>
            <para><emphasis
            role="bold">pegasus.catalog.replica=File</emphasis></para>
          </listitem>

          <listitem>
            <para><emphasis
            role="bold">pegasus.catalog.replica.file=<replaceable>&lt;path to
            the replica catalog file&gt;</replaceable></emphasis></para>
          </listitem>
        </orderedlist></para>
    </section>

    <section id="rc-JDBCRC">
      <title>JDBCRC</title>

      <para>In this mode, Pegasus queries a SQL based replica catalog that is
      accessed via JDBC. The sql schema&rsquor;s for this catalog can be found
      at <emphasis role="bold">$PEGASUS_HOME/sql</emphasis> directory. You
      will have to install the schema into either PostgreSQL or MySQL by
      running the appropriate commands to load the two scheams <emphasis
      role="bold">create-XX-init.sql</emphasis> and <emphasis
      role="bold">create-XX-rc.sql</emphasis> where XX is either <emphasis
      role="bold">my</emphasis> (for MySQL) or <emphasis
      role="bold">pg</emphasis> (for PostgreSQL)</para>

      <para>To use JDBCRC, the user additionally needs to set the following
      properties</para>

      <orderedlist>
        <listitem>
          <para><emphasis
          role="bold">pegasus.catalog.replica.db.url=<replaceable>&lt;jdbc url
          to the databse&gt;</replaceable></emphasis></para>
        </listitem>

        <listitem>
          <para><emphasis
          role="bold">pegasus.catalog.replica.db.user=<replaceable>&lt;database
          user&gt;</replaceable></emphasis></para>
        </listitem>

        <listitem>
          <para><emphasis
          role="bold">pegasus.catalog.replica.db.password=<replaceable>&lt;database
          password&gt;</replaceable></emphasis></para>
        </listitem>
      </orderedlist>
    </section>

    <section id="rc-RLS">
      <title>Replica Location Service</title>

      <para>RLS (Replica Location Service) is a distributed replica catalog,
      which ships with Globus. There is an index service called Replica
      Location Index (RLI) to which 1 or more Local Replica Catalog (LRC)
      report. Each LRC can contain all or a subset of mappings.</para>

      <para>Details about RLS can be found at <ulink
      url="http://www.globus.org/toolkit/data/rls/">http://www.globus.org/toolkit/data/rls/</ulink></para>

      <section>
        <title>RLS</title>

        <para>In this mode, Pegasus queries the central RLI to discover in
        which LRC&rsquor;s the mappings for a LFN reside. It then queries the
        individual LRC&rsquor;s for the PFN&rsquor;s. To use this mode the
        following properties need to be set</para>

        <para><orderedlist>
            <listitem>
              <para><emphasis
              role="bold">pegasus.catalog.replica=RLS</emphasis></para>
            </listitem>

            <listitem>
              <para><emphasis
              role="bold">pegasus.catalog.replica.url=<replaceable>&lt;url to
              the globus LRC&gt;</replaceable></emphasis></para>
            </listitem>
          </orderedlist></para>
      </section>

      <section>
        <title>LRC</title>

        <para>This mode is availabe If the user does not want to query the RLI
        (Replica Location Index), but directly a single Local Replica Catalog.
        To use the LRC mode the follow properties need to be set</para>

        <para><orderedlist>
            <listitem>
              <para><emphasis
              role="bold">pegasus.catalog.replica=<replaceable>LRC</replaceable></emphasis></para>
            </listitem>

            <listitem>
              <para><emphasis
              role="bold">pegasus.catalog.replica.url=<replaceable>&lt;url to
              the globus LRC&gt;</replaceable></emphasis></para>
            </listitem>
          </orderedlist></para>

        <para>Details about Globus Replica Catalog and LRC can be found at
        <ulink
        url="http://www.globus.org/toolkit/data/rls/">http://www.globus.org/toolkit/data/rls/</ulink></para>
      </section>
    </section>

    <section id="rc-MRC">
      <title>MRC</title>

      <para>In this mode, Pegasus queries multiple replica catalogs to
      discover the file locations on the grid.</para>

      <para>To use it set</para>

      <para><orderedlist>
          <listitem>
            <para><emphasis
            role="bold">pegasus.catalog.replica=<replaceable>MRC</replaceable></emphasis></para>
          </listitem>
        </orderedlist></para>

      <para>Each associated replica catalog can be configured via properties
      as follows.</para>

      <para>The user associates a variable name referred to as [value] for
      each of the catalogs, where [value] is any legal identifier (concretely
      [A-Za-z][_A-Za-z0-9]*) For each associated replica catalogs the user
      specifies the following properties</para>

      <itemizedlist>
        <listitem>
          <para><emphasis role="bold">pegasus.catalog.replica.mrc.[value]
          </emphasis>- specifies the type of replica catalog.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">pegasus.catalog.replica.mrc.[value].key
          </emphasis>- specifies a property name key for a particular
          catalog</para>
        </listitem>
      </itemizedlist>

      <para>For example, if a user wants to query two lrc&rsquor;s at the same
      time he/she can specify as follows</para>

      <para><itemizedlist>
          <listitem>
            <para><emphasis
            role="bold">pegasus.catalog.replica.mrc.lrc1=LRC</emphasis></para>
          </listitem>

          <listitem>
            <para><emphasis
            role="bold">pegasus.catalog.replica.mrc.lrc1.url=<replaceable>&lt;url
            to the 1st globus LRC&gt;</replaceable></emphasis></para>
          </listitem>

          <listitem>
            <para><emphasis
            role="bold">pegasus.catalog.replica.mrc.lrc2=LRC</emphasis></para>
          </listitem>

          <listitem>
            <para><emphasis
            role="bold">pegasus.catalog.replica.mrc.lrc2.url=</emphasis><emphasis
            role="bold">&lt;url to the 2nd globus LRC&gt;</emphasis></para>
          </listitem>
        </itemizedlist>In the above example, lrc1, lrc2 are any valid
      identifier names and url is the property key that needed to be
      specified.</para>
    </section>

    <section id="pegasus-rc-client">
      <title>Replica Catalog Client pegasus-rc-client</title>

      <para>The client used to interact with the Replica Catalogs is
      pegasus-rc-client. The implementation that the client talks to is
      configured using Pegasus properties.</para>

      <para>Lets assume we create a file f.a in your home directory as shown
      below.</para>

      <screen><command>$ date &gt; $HOME/f.a </command></screen>

      <para>We now need to register this file in the <emphasis
      role="bold">File</emphasis> replica catalog located in <emphasis
      role="bold">$HOME/rc</emphasis> using the pegasus-rc-client. Replace the
      <emphasis role="bold">gsiftp://url</emphasis> with the appropriate
      parameters for your grid site.</para>

      <screen><emphasis>$<command> rc-client -Dpegasus.catalog.replica=File -Dpegasus.catalog.replica.file=$HOME/rc insert \
 f.a</command> <replaceable>gsiftp://somehost:port/path/to/file/f.a pool=local</replaceable></emphasis></screen>

      <para>You may first want to check, if the file registeration made it
      into the replica catalog. Since we are using a File catalog we can just
      go look at the file $HOME/rc to see if there are any entries in
      there.</para>

      <screen><command>$ cat $HOME/rc</command><computeroutput>
    
# file-based replica catalog: 2010-11-10T17:52:53.405-07:00
f.a gsiftp://somehost:port/path/to/file/f.a pool=local</computeroutput></screen>

      <para>The above line shows that entry for file f.a was made
      correctly.</para>

      <para>You can also use the pegasus-rc-client to look for entries.</para>

      <screen><command>$ pegasus-rc-client -Dpegasus.catalog.replica=File -Dpegasus.catalog.replica.file=$HOME/rc lookup LFN f.a</command><computeroutput>

f.a gsiftp://somehost:port/path/to/file/f.a pool=local</computeroutput></screen>
    </section>
  </section>

  <section id="site">
    <title>Resource Discover (Site Catalog)</title>

    <para>The Site Catalog describes the compute resources (which are often
    clusters) that we intend to run the workflow upon. A site is a homogeneous
    part of a cluster that has at least a single GRAM gatekeeper with a
    jobmanager-fork and jobmanager-&lt;scheduler&gt; interface and at least
    one gridftp server along with a shared file system. The GRAM gatekeeper
    can be either WS GRAM or Pre-WS GRAM. A site can also be a condor pool or
    glidein pool with a shared file system.</para>

    <para>Pegasus currently supports two implementation of the Site
    Catalog</para>

    <orderedlist>
      <listitem>
        <para>XML3 <emphasis role="bold">(Default)</emphasis></para>
      </listitem>

      <listitem>
        <para>XML <emphasis role="bold">(Deprecated)</emphasis></para>
      </listitem>

      <listitem>
        <para>File <emphasis role="bold">(Deprecated)</emphasis></para>
      </listitem>
    </orderedlist>

    <section id="sc-XML3">
      <title>XML3</title>

      <para>This is the default format for Pegasus 3.0. This format allows
      defining filesystem of shared as well as local type on the head node of
      the remote cluster as well as on the backend nodes</para>

      <figure>
        <title>Schema Image of the Site Catalog XML 3</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="images/sc-3.0_p2.png" />
          </imageobject>
        </mediaobject>
      </figure>

      <para>Below is an example of the XML3 site catalog</para>

      <programlisting>&lt;sitecatalog xmlns="http://pegasus.isi.edu/schema/sitecatalog" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://pegasus.isi.edu/schema/sitecatalog 
http://pegasus.isi.edu/schema/sc-3.0.xsd" version="3.0"&gt;
  &lt;site  handle="isi" arch="x86" os="LINUX" osrelease="" osversion="" glibc=""&gt;
      &lt;grid  type="gt2" contact="smarty.isi.edu/jobmanager-pbs" scheduler="PBS" jobtype="auxillary"/&gt;
      &lt;grid  type="gt2" contact="smarty.isi.edu/jobmanager-pbs" scheduler="PBS" jobtype="compute"/&gt;
          &lt;head-fs&gt;
               &lt;scratch&gt;
                  &lt;shared&gt;
                     &lt;file-server protocol="gsiftp" url="gsiftp://skynet-data.isi.edu"
                                  mount-point="/nfs/scratch01" /&gt;
                     &lt;internal-mount-point mount-point="/nfs/scratch01"/&gt;
                  &lt;/shared&gt;
               &lt;/scratch&gt;
               &lt;storage&gt;
                  &lt;shared&gt;
                     &lt;file-server protocol="gsiftp" url="gsiftp://skynet-data.isi.edu" 
                                  mount-point="/exports/storage01"/&gt;
                     &lt;internal-mount-point mount-point="/exports/storage01"/&gt;
                  &lt;/shared&gt;
               &lt;/storage&gt;
          &lt;/head-fs&gt;
      &lt;replica-catalog  type="LRC" url="rlsn://smarty.isi.edu"/&gt;
      &lt;profile namespace="env" key="PEGASUS_HOME" &gt;/nfs/vdt/pegasus&lt;/profile&gt;
      &lt;profile namespace="env" key="GLOBUS_LOCATION" &gt;/vdt/globus&lt;/profile&gt;
  &lt;/site&gt;
&lt;/sitecatalog&gt;</programlisting>

      <para>Described below are some of the entries in the site
      catalog.</para>

      <para><orderedlist>
          <listitem>
            <para>site - A site identifier.</para>
          </listitem>

          <listitem>
            <para>replica-catalog - URL for a local replica catalog (LRC) to
            register your files in. Only used for RLS implementation of the
            RC. This is optional</para>
          </listitem>

          <listitem>
            <para>File Systems - Info about filesystems mounted on the remote
            clusters head node or worker nodes. It has several
            configurations</para>

            <itemizedlist>
              <listitem>
                <para>head-fs/scratch - This describe the scratch file systems
                (temporary for execution) available on the head node</para>
              </listitem>

              <listitem>
                <para>head-fs/storage - This describes the storage file
                systems (long term) available on the head node</para>
              </listitem>

              <listitem>
                <para>worker-fs/scratch - This describe the scratch file
                systems (temporary for execution) available on the worker
                node</para>
              </listitem>

              <listitem>
                <para>worker-fs/storage - This describes the storage file
                systems (long term) available on the worker node</para>
              </listitem>
            </itemizedlist>

            <para>Each scratch and storage entry can contain two sub
            entries,</para>

            <itemizedlist>
              <listitem>
                <para>SHARED for shared file systems like NFS, LUSTRE
                etc.</para>
              </listitem>

              <listitem>
                <para>LOCAL for local file systems (local to the
                node/machine)</para>
              </listitem>
            </itemizedlist>

            <para>Each of the filesystems are defined by used a file-server
            element. Protocol defines the protocol uses to access the files,
            URL defines the url prefix to obtain the files from and
            mount-point is the mount point exposed by the file server.</para>

            <para>Along with this an internal-mount-point needs to defined to
            access the files directly from the machine without any file
            servers.</para>
          </listitem>

          <listitem>
            <para>arch,os,osrelease,osversion, glibc - The
            arch/os/osrelease/osversion/glibc of the site. OSRELEASE,
            OSVERSION and GLIBC are optional</para>

            <para>ARCH can have one of the following values X86, X86_64,
            SPARCV7, SPARCV9, AIX, PPC.</para>

            <para>OS can have one of the following values LINUX,SUNOS,MACOSX.
            The default value for sysinfo if none specified is
            X86::LINUX</para>
          </listitem>

          <listitem>
            <para>Profiles - One or many profiles can be attached to a
            pool.</para>

            <para>One example is the environments to be set on a remote
            pool.</para>
          </listitem>
        </orderedlist></para>

      <para>To use this site catalog the follow properties need to be
      set</para>

      <orderedlist>
        <listitem>
          <para><emphasis
          role="bold">pegasus.catalog.site=XML3</emphasis></para>
        </listitem>

        <listitem>
          <para><emphasis
          role="bold">pegasus.catalog.site.file=<replaceable>&lt;path to the
          site catalog file&gt;</replaceable></emphasis></para>
        </listitem>
      </orderedlist>
    </section>

    <section id="sc-XML">
      <title>XML</title>

      <para><warning>
          <para>This format is now deprecated in favor of the XML3 format. If
          you are still using the XML or File format you should convert it to
          XML3 formation using the pegasus-sc-converter client</para>
        </warning></para>

      <para><programlisting>$ <emphasis>cat $HOME/sites.xml</emphasis>

&lt;sitecatalog xmlns="http://pegasus.isi.edu/schema/sitecatalog"
  xsi:schemaLocation="http://pegasus.isi.edu/schema/sitecatalog
  http://pegasus.isi.edu/schema/sc-2.0.xsd"
  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" version="2.0"&gt;
  &lt;site handle="local" gridlaunch="/nfs/vdt/pegasus/bin/kickstart"
   sysinfo="INTEL32::LINUX"&gt;
    &lt;profile namespace="env" key="PEGASUS_HOME" &gt;/nfs/vdt/pegasus&lt;/profile&gt;
    &lt;profile namespace="env" key="GLOBUS_LOCATION" &gt;/vdt/globus&lt;/profile&gt;
    &lt;profile namespace="env" key="LD_LIBRARY_PATH" &gt;/vdt/globus/lib&lt;/profile&gt;
    &lt;profile namespace="env" key="JAVA_HOME" &gt;/vdt/java&lt;/profile&gt;
    &lt;lrc url="rlsn://localhost" /&gt;
    &lt;gridftp  url="gsiftp://localhost" storage="/$HOME/storage" major="4" minor="0"
     patch="5"&gt;
    &lt;/gridftp&gt;
    &lt;jobmanager universe="transfer" url="localhost/jobmanager-fork" major="4" minor="0"
     patch="5" /&gt;
    &lt;jobmanager universe="vanilla" url="localhost/jobmanager-fork" major="4" minor="0"
     patch="5" /&gt;
    &lt;workdirectory &gt;$HOME/workdir&lt;/workdirectory&gt;
  &lt;/site&gt;
  &lt;site handle="clus1" gridlaunch="/opt/nfs/vdt/pegasus/bin/kickstart"
   sysinfo="INTEL32::LINUX"&gt;
    &lt;profile namespace="env" key="PEGASUS_HOME" &gt;/opt/nfs/vdt/pegasus&lt;/profile&gt;
    &lt;profile namespace="env" key="GLOBUS_LOCATION" &gt;/opt/vdt/globus&lt;/profile&gt;
    &lt;profile namespace="env" key="LD_LIBRARY_PATH" &gt;/opt/vdt/globus/lib&lt;/profile&gt;
    &lt;lrc url="rlsn://clus1.com" /&gt;
    &lt;gridftp  url="gsiftp://clus1.com" storage="/jobmanager-fork" major="4" minor="0"
     patch="3"&gt;
    &lt;/gridftp&gt;
    &lt;jobmanager universe="transfer" url="clus1.com/jobmanager-fork" major="4" minor="0"
     patch="3" /&gt;
    &lt;jobmanager universe="vanilla" url="clus1.com/jobmanager-pbs" major="4" minor="0"
     patch="3" /&gt;
    &lt;workdirectory &gt;$HOME/workdir-clus1&lt;/workdirectory&gt;
  &lt;/site&gt;
&lt;/sitecatalog&gt;</programlisting><orderedlist>
          <listitem>
            <para>site - A site identifier.</para>
          </listitem>

          <listitem>
            <para>lrc - URL for a local replica catalog (LRC) to register your
            files in. Only used for RLS implementation of the RC</para>
          </listitem>

          <listitem>
            <para>workdirectory - A remote working directory (Should be on a
            shared file system)</para>
          </listitem>

          <listitem>
            <para>gridftp - A URL prefix for a remote storage location. and a
            path to the storage location</para>
          </listitem>

          <listitem>
            <para>jobmanager - Url to the jobmanager entrypoints for the
            remote grid. Different universes are supported which map to
            different batch jobmanagers.</para>

            <para>"vanilla" for compute jobs and "transfer" for transfer jobs
            are mandatory. Generally a transfer universe should map to the
            fork jobmanager.</para>
          </listitem>

          <listitem>
            <para>gridlaunch - Path to the remote kickstart tool (provenance
            tracking)</para>
          </listitem>

          <listitem>
            <para>sysinfo - The arch/os/osversion/glibc of the site. The
            format is ARCH::OS:OSVER:GLIBC where OSVERSION and GLIBC are
            optional.</para>

            <para>ARCH can have one of the following values INTEL32, INTEL64,
            SPARCV7, SPARCV9, AIX, AMD64. OS can have one of the following
            values LINUX,SUNOS. The default value for sysinfo if none
            specified is INTEL32::LINUX</para>
          </listitem>

          <listitem>
            <para>Profiles - One or many profiles can be attached to a
            pool.</para>

            <para>One example is the environments to be set on a remote
            pool.</para>
          </listitem>
        </orderedlist></para>

      <para>To use this format you need to set the following properties</para>

      <para><orderedlist>
          <listitem>
            <para><emphasis
            role="bold">pegasus.catalog.site=XML</emphasis></para>
          </listitem>

          <listitem>
            <para><emphasis
            role="bold">pegasus.catalog.site.file=<replaceable>&lt;path to the
            site catalog file&gt;</replaceable></emphasis></para>
          </listitem>
        </orderedlist></para>
    </section>

    <section id="sc-Text">
      <title>Text</title>

      <para><warning>
          <para>This format is now deprecated in favor of the XML3 format. If
          you are still using the File format you should convert it to XML3
          format using the client pegasus-sc-converter</para>
        </warning></para>

      <para>The format for the File is as follows</para>

      <programlisting>site site_id {
  #required. Can be a dummy value if using Simple File RC
  lrc "rls://someurl"

  #required on a shared file system
  workdir "path/to/a/tmp/shared/file/sytem/"

  #required one or more entries
  gridftp "gsiftp://hostname/mountpoint&rdquor; "GLOBUS VERSION"

  #required one or more entries
  universe transfer "hostname/jobmanager-&lt;scheduler&gt;" "GLOBUS VERSION"

  #reqired one or more entries
  universe vanilla "hostname/jobmanager-&lt;scheduler&gt;" "GLOBUS VERSION"

  #optional
  sysinfo  "ARCH::OS:OSVER:GLIBC"

  #optional
  gridlaunch "/path/to/gridlaunch/executable"

  #optional zero or more entries
  profile namespace "key" "value"
} </programlisting>

      <para>The gridlaunch and profile entries are optional. All the rest are
      required for each pool. Also the transfer and vanilla universe are
      mandatory. You can add multiple transfer and vanilla universe if you
      have more then one head node on the cluster. The entries in the Site
      Catalog have the following meaning:</para>

      <orderedlist>
        <listitem>
          <para>site - A site identifier.</para>
        </listitem>

        <listitem>
          <para>lrc - URL for a local replica catalog (LRC) to register your
          files in. Only used for RLS implementation of the RC</para>
        </listitem>

        <listitem>
          <para>workdir - A remote working directory (Should be on a shared
          file system)</para>
        </listitem>

        <listitem>
          <para>gridftp - A URL prefix for a remote storage location.</para>
        </listitem>

        <listitem>
          <para>universe - Different universes are supported which map to
          different batch jobmanagers.</para>

          <para>"vanilla" for compute jobs and "transfer" for transfer jobs
          are mandatory. Generally a transfer universe should map to the fork
          jobmanager.</para>
        </listitem>

        <listitem>
          <para>gridlaunch - Path to the remote kickstart tool (provenance
          tracking)</para>
        </listitem>

        <listitem>
          <para>sysinfo - The arch/os/osversion/glibc of the site. The format
          is ARCH::OS:OSVER:GLIBC where OSVERSION and GLIBC are
          optiona.</para>

          <para>ARCH can have one of the following values INTEL32, INTEL64,
          SPARCV7, SPARCV9, AIX, AMD64. OS can have one of the following
          values LINUX,SUNOS. The default value for sysinfo if none specified
          is INTEL32::LINUX</para>
        </listitem>

        <listitem>
          <para>Profiles - One or many profiles can be attached to a
          pool.</para>

          <para>One example is the environments to be set on a remote
          pool.</para>
        </listitem>
      </orderedlist>

      <para>To use this format you need to set the following properties</para>

      <para><orderedlist>
          <listitem>
            <para><emphasis
            role="bold">pegasus.catalog.site=Text</emphasis></para>
          </listitem>

          <listitem>
            <para><emphasis
            role="bold">pegasus.catalog.site.file=<replaceable>&lt;path to the
            site catalog file&gt;</replaceable></emphasis></para>
          </listitem>
        </orderedlist></para>
    </section>

    <section id="pegasus-sc-client">
      <title>Site Catalog Client pegasus-sc-client</title>

      <para>The pegasus-sc-client can be used to generate a site catalog for
      Open Science Grid (OSG) by querying their Monitoring Interface likes
      VORS or OSGMM. See pegasus-sc-client --help for more details</para>
    </section>

    <section>
      <title>Site Catalog Converter pegasus-sc-converter</title>

      <para>Pegasus 3.0 by default now parses Site Catalog format conforming
      to the SC schema 3.0 ( XML3 ) available <ulink role=""
      url="http://pegasus.isi.edu/wms/docs/schemas/dax-3.2/dax-3.2.xsd"
      userlevel="">here</ulink> and is explained in detail in the chapter on
      Catalogs.</para>

      <para>Pegasus 3.0 comes with a pegasus-sc-converter that will convert
      users old site catalog ( XML ) to the XML3 format. Sample usage is given
      below.</para>

      <programlisting><emphasis role="bold">$ pegasus-sc-converter -i sample.sites.xml -I XML -o sample.sites.xml3 -O XML3
</emphasis>
2010.11.22 12:55:14.169 PST:   Written out the converted file to sample.sites.xml3 
</programlisting>

      <para>To use the converted site catalog, in the properties do the
      following</para>

      <orderedlist>
        <listitem>
          <para>unset pegasus.catalog.site or set pegasus.catalog.site to
          XML3</para>
        </listitem>

        <listitem>
          <para>point pegasus.catalog.site.file to the converted site
          catalog</para>
        </listitem>
      </orderedlist>
    </section>
  </section>

  <section id="transformation">
    <title>Executable Discovery (Transformation Catalog)</title>

    <para>The Transformation Catalog maps logical transformations to physical
    executables on the system. It also provides additional information about
    the transformation as to what system they are compiled for, what profiles
    or environment variables need to be set when the transformation is invoked
    etc.</para>

    <para>Pegasus currently supports two implementations of the Transformation
    Catalog</para>

    <orderedlist>
      <listitem>
        <para><emphasis role="bold">Text: </emphasis>A multiline text based
        Transformation Catalog (DEFAULT)</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">File:</emphasis> A simple multi column
        text based Transformation Catalog</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">Database:</emphasis> A database backend
        (MySQL or PostgreSQL) via JDB</para>
      </listitem>
    </orderedlist>

    <para>In this guide we will look at the format of the Multiline Text based
    TC.</para>

    <section id="tc-Text">
      <title>MultiLine Text based TC (Text)</title>

      <para>The multile line text based TC is the new default TC in Pegasus.
      This format allows you to define the transformations</para>

      <para>The file is read and cached in memory. Any modifications, as
      adding or deleting, causes an update of the memory and hence to the file
      underneath. All queries are done against the memory representation. The
      file sample.tc.text in the etc directory contains an example</para>

      <para><programlisting>tr example::keg:1.0 { 

#specify profiles that apply for all the sites for the transformation 
#in each site entry the profile can be overriden 

  profile env "APP_HOME" "/tmp/myscratch"
  profile env "JAVA_HOME" "/opt/java/1.6"

  site isi {
    profile env "HELLo" "WORLD"
    profile condor "FOO" "bar"
    profile env "JAVA_HOME" "/bin/java.1.6"
    pfn "/path/to/keg"
    arch "x86"
    os "linux"
    osrelease "fc"
    osversion "4"
    type "INSTALLED"
  }

  site wind {
    profile env "CPATH" "/usr/cpath"
    profile condor "universe" "condor"
    pfn "file:///path/to/keg"
    arch "x86"
    os "linux"
    osrelease "fc"
    osversion "4"
    type "STAGEABLE"
  }
}</programlisting></para>

      <para>The entries in this catalog have the following meaning</para>

      <para><orderedlist>
          <listitem>
            <para>tr - A transformation identifier. (Normally a
            Namespace::Name:Version.. The Namespace and Version are
            optional.)</para>
          </listitem>

          <listitem>
            <para>pfn - URL or file path for the location of the executable.
            The pfn is a file path if the transformation is of type INSTALLED
            and generally a url (file:/// or http:// or gridftp://) if of type
            STAGEABLE</para>
          </listitem>

          <listitem>
            <para>site - The site identifier for the site where the
            transformation is available</para>
          </listitem>

          <listitem>
            <para>type - The type of transformation. Whether it is Iinstalled
            ("INSTALLED") on the remote site or is availabe to stage
            ("STAGEABLE").</para>
          </listitem>

          <listitem>
            <para>arch, os, osrelease, osversion - The
            arch/os/osrelease/osversion of the transformation. osrelease and
            osversion are optional.</para>

            <para>ARCH can have one of the following values x86, x86_64,
            sparcv7, sparcv9, ppc, aix. The default value for arch is
            x86</para>

            <para>OS can have one of the following values linux,sunos,macosx.
            The default value for OS if none specified is linux</para>
          </listitem>

          <listitem>
            <para>Profiles - One or many profiles can be attached to a
            transformation for all sites or to a transformation on a
            particular site.</para>
          </listitem>
        </orderedlist></para>

      <para>To use this format of the Transformation Catalog you need to set
      the following properties</para>

      <para><orderedlist>
          <listitem>
            <para><emphasis
            role="bold">pegasus.catalog.transformation=Text</emphasis></para>
          </listitem>

          <listitem>
            <para><emphasis
            role="bold">pegasus.catalog.transformation.file=<replaceable>&lt;path
            to the transformation catalog
            file&gt;</replaceable></emphasis></para>
          </listitem>
        </orderedlist></para>
    </section>

    <section id="tc-File">
      <title>Singleline Text based TC (File)</title>

      <warning>
        <para>This format is now deprecated in favor of the multiline TC. If
        you are still using the single line TC you should convert it to
        multiline using the tc-converter client.</para>
      </warning>

      <para>The format of the this TC is as follows.</para>

      <programlisting>#site  logicaltr   physicaltr   type  system  profiles(NS::KEY="VALUE")

site1 sys::date:1.0 /usr/bin/date  INSTALLED INTEL32::LINUX:FC4.2:3.6 ENV::PATH="/usr/bin";PEGASUS_HOME="/usr/local/pegasus"</programlisting>

      <para>The system and profile entries are optional and will use default
      values if not specified. The entries in the file format have the
      following meaning:</para>

      <orderedlist>
        <listitem>
          <para>site - A site identifier.</para>
        </listitem>

        <listitem>
          <para>logicaltr - The logical transformation name. The format is
          NAMESPACE::NAME:VERSION where NAMESPACE and NAME are
          optional.</para>
        </listitem>

        <listitem>
          <para>physicaltr - The physical transformation path or URL.</para>

          <para>If the transformation type is INSTALLED then it needs to be an
          absolute path to the executable. If the type is STAGEABLE then the
          path needs to be a HTTP, FTP or gsiftp URL</para>
        </listitem>

        <listitem>
          <para>type - The type of transformation. Can have on of two
          values</para>

          <itemizedlist>
            <listitem>
              <para>INSTALLED: This means that the transformation is installed
              on the remote site</para>
            </listitem>

            <listitem>
              <para>STAGEABLE: This means that the transformation is available
              as a static binary and can be staged to a remote site.</para>
            </listitem>
          </itemizedlist>
        </listitem>

        <listitem>
          <para>system - The system for which the transformation is
          compiled.</para>

          <para>The formation of the sytem is ARCH::OS:OSVERSION:GLIBC where
          the GLIBC and OS VERSION are optional. ARCH can have one of the
          following values INTEL32, INTEL64, SPARCV7, SPARCV9, AIX, AMD64. OS
          can have one of the following values LINUX,SUNOS. The default value
          for system if none specified is INTEL32::LINUX</para>
        </listitem>

        <listitem>
          <para>Profiles - The profiles associated with the transformation.
          For indepth information about profiles and their priorities read the
          Profile Guide.</para>

          <para>The format for profiles is NS::KEY="VALUE" where NS is the
          namespace of the profile e.g. Pegasus,condor,DAGMan,env,globus. The
          key and value can be any strings. Remember to quote the value with
          double quotes. If you need to specify several profiles you can do it
          in several ways</para>

          <itemizedlist>
            <listitem>
              <para>NS1::KEY1="VALUE1",KEY2="VALUE2";NS2::KEY3="VALUE3",KEY4="VALUE4"</para>

              <para>This is the most optimized form. Multiple key values for
              the same namespace are separated by a comma "," and different
              namespaces are separated by a semicolon ";"</para>
            </listitem>

            <listitem>
              <para>NS1::KEY1="VALUE1";NS1::KEY2="VALUE2";NS2::KEY3="VALUE3";NS2::KEY4="VALUE4"</para>

              <para>You can also just repeat the triple of NS::KEY="VALUE"
              separated by semicolons for a simple format;</para>
            </listitem>
          </itemizedlist>
        </listitem>
      </orderedlist>

      <para>To use this format of the Transformation Catalog you need to set
      the following properties</para>

      <para><orderedlist>
          <listitem>
            <para><emphasis
            role="bold">pegasus.catalog.transformation=File</emphasis></para>
          </listitem>

          <listitem>
            <para><emphasis
            role="bold">pegasus.catalog.transformation.file=<replaceable>&lt;path
            to the transformation catalog
            file&gt;</replaceable></emphasis></para>
          </listitem>
        </orderedlist></para>
    </section>

    <section id="tc-Database">
      <title>Database TC (Database)</title>

      <para>The database TC alows you to use a relational database. To use the
      database TC you need to have installed a MySQL or PostgreSQL server. The
      schema for the database is available in $PEGASUS_HOME/sql directory. You
      will have to install the schema into either PostgreSQL or MySQL by
      running the appropriate commands to load the two scheams <emphasis
      role="bold">create-XX-init.sql</emphasis> and <emphasis
      role="bold">create-XX-tc.sql</emphasis> where XX is either <emphasis
      role="bold">my</emphasis> (for MySQL) or <emphasis
      role="bold">pg</emphasis> (for PostgreSQL)</para>

      <para>To use the Database TC you need to set the following
      properties</para>

      <para><orderedlist>
          <listitem>
            <para><emphasis
            role="bold">pegasus.catalog.transformation.db.driver=MySQL |
            Postgres</emphasis></para>
          </listitem>

          <listitem>
            <para><emphasis
            role="bold">pegasus.catalog.transformation.db.url=<replaceable>&lt;jdbc
            url to the databse&gt;</replaceable></emphasis></para>
          </listitem>

          <listitem>
            <para><emphasis
            role="bold">pegasus.catalog.transformation.db.user=<replaceable>&lt;database
            user&gt;</replaceable></emphasis></para>
          </listitem>

          <listitem>
            <para><emphasis
            role="bold">pegasus.catalog.transformation.db.password=<replaceable>&lt;database
            password&gt;</replaceable></emphasis></para>
          </listitem>
        </orderedlist></para>
    </section>

    <section id="pegasus-tc-client">
      <title>TC Client pegasus-tc-client</title>

      <para>We need to map our declared transformations (preprocess,
      findranage, and analyze) from the example DAX above to a simple "mock
      application" name "keg" ("canonical example for the grid") which reads
      input files designated by arguments, writes them back onto output files,
      and produces on STDOUT a summary of where and when it was run. Keg ships
      with Pegasus in the bin directory. Run keg on the command line to see
      how it works.</para>

      <screen><command>$ keg -o /dev/fd/1</command>
<computeroutput>
Timestamp Today: 20040624T054607-05:00 (1088073967.418;0.022)
Applicationname: keg @ 10.10.0.11 (VPN)
Current Workdir: /home/unique-name
Systemenvironm.: i686-Linux 2.4.18-3
Processor Info.: 1 x Pentium III (Coppermine) @ 797.425
Output Filename: /dev/fd/1</computeroutput></screen>

      <para>Now we need to map all 3 transformations onto the "keg"
      executable. We place these mappings in our File transformation catalog
      for site clus1. In earlier version of Pegasus one had to define entries
      for Pegasus executables like transfer, replica client, dirmanager etc on
      each site as well as site "local". This is no longer required. Pegasus
      2.0 and later automatically picks up the paths for these binaries from
      the environment profile PEGASUS_HOME set in the site catalog for each
      site.</para>

      <para>Note: A single entry needs to be on one line. The above example is
      just formatted for convenience.</para>

      <para>Alternatively you can also use the pegasus-tc-client to add
      entries to any implementation of the transformation catalog. The
      following eg: shows us adding the last entry in the File based
      transformation catalog.</para>

      <screen><command>$ pegasus-tc-client -Dpegasus.catalog.transformation=Text \
-Dpegasus.catalog.transformation.file=$HOME/tc -a -r clus1 -l black::analyze:1.0 \
-p gsiftp://clus1.com/opt/nfs/vdt/pegasus/bin/keg  -t STAGEABLE -s INTEL32::LINUX \
-e ENV::KEY3="VALUE3"</command><computeroutput>

2007.07.11 16:12:03.712 PDT: [INFO] Added tc entry sucessfully</computeroutput></screen>

      <para>To verify if the entry was correctly added to the transformation
      catalog you can use the pegasus-tc-client to query.</para>

      <screen><command>$ pegasus-tc-client -Dpegasus.catalog.transformation=File \
-Dpegasus.catalog.transformation.file=$HOME/tc -q -P -l black::analyze:1.0</command>

<computeroutput>#RESID     LTX          PFN                  TYPE              SYSINFO

clus1    black::analyze:1.0    gsiftp://clus1.com/opt/nfs/vdt/pegasus/bin/keg
                STAGEABLE    INTEL32::LINUX</computeroutput></screen>

      <para></para>
    </section>

    <section>
      <title>TC Converter Client pegasus-tc-converter</title>

      <para>Pegasus 3.0 by default now parses a file based multiline textual
      format of a Transformation Catalog. The new Text format is explained in
      detail in the chapter on Catalogs.</para>

      <para>Pegasus 3.0 comes with a pegasus-tc-converter that will convert
      users old transformation catalog ( File ) to the Text format. Sample
      usage is given below.</para>

      <programlisting><emphasis role="bold">$ pegasus-tc-converter -i sample.tc.data -I File -o sample.tc.text -O Text
</emphasis>
2010.11.22 12:53:16.661 PST:   Successfully converted Transformation Catalog from File to Text 
2010.11.22 12:53:16.666 PST:   The output transfomation catalog is in file  /lfs1/software/install/pegasus/pegasus-3.0.0cvs/etc/sample.tc.text 
</programlisting>

      <para>To use the converted transformation catalog, in the properties do
      the following</para>

      <orderedlist>
        <listitem>
          <para>unset pegasus.catalog.transformation or set
          pegasus.catalog.transformation to Text</para>
        </listitem>

        <listitem>
          <para>point pegasus.catalog.transformation.file to the converted
          transformation catalog </para>
        </listitem>
      </orderedlist>
    </section>
  </section>
</chapter>
