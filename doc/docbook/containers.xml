<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN"
"http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<chapter id="containers">
  <title>Containers</title>

  <section id="container-overview">
    <title>Overview</title>

    <para>Application containers provides a solution to package software with
    complex dependencies to be used during workflow execution. Starting with
    Pegasus 4.8.0, Pegasus has support for application containers in the
    non-shared filesystem or condorio data configurations using PegasusLite.
    Users can specify with their transformations in the Transformation Catalog
    the container in which the the transformation should be executed. Pegasus
    currently has support for the following container technologies:</para>

    <orderedlist>
      <listitem>
        <para>Docker</para>
      </listitem>

      <listitem>
        <para>Singularity</para>
      </listitem>
    </orderedlist>

    <para>The worker package is not required to be pre-installed in images. If
    a matching worker package is not installed, Pegasus will try to determine
    which package is required and download it.</para>
  </section>

  <section id="container-configuration">
    <title>Configuring Workflows To Use Containers</title>

    <para>Containers currently can only be specified in the Transformation
    Catalog. Users have the option of either using a different container for
    each executable or same container for all executables. In the case, where
    you wants to use a container that does not have your executable
    pre-installed, you can mark the executable as STAGEABLE and Pegasus will
    stage the executable into the container, as part of executable
    staging.</para>

    <para>The DAX API extensions don't support references for
    containers.</para>

    <xi:include href="creating_workflows.xml" xpointer="tc-container"
                xmlns:xi="http://www.w3.org/2001/XInclude"/>
  </section>

  <section id="container-exec-model">
    <title>Container Execution Model</title>

    <para>User's containerized applications are launched as part of
    PegasusLite jobs. PegasusLite job when starting on a remote worker
    node.</para>

    <orderedlist>
      <listitem>
        <para>Sets up a directory to run a user job in.</para>
      </listitem>

      <listitem>
        <para>Pulls the container image to that directory</para>
      </listitem>

      <listitem>
        <para>Optionally, loads the container from the container image file
        and sets up the user to run as in the container (only applicable for
        Docker containers)</para>
      </listitem>

      <listitem>
        <para>Mounts the job directory into the container as /scratch for
        Docker containers, while as /srv for Singularity containers.</para>
      </listitem>

      <listitem>
        <para>Container will run a job specific script that figures created by
        PegasusLite that does the following:</para>

        <orderedlist numeration="loweralpha">
          <listitem>
            <para>Figures the appropriate Pegasus worker to use in the
            container if not already installed</para>
          </listitem>

          <listitem>
            <para>Sets up the job environment to use including transfer and
            setup of any credentials transferred as part of PegasusLite</para>
          </listitem>

          <listitem>
            <para>Pulls in all the relevant input data, executables required
            by the job</para>
          </listitem>

          <listitem>
            <para>Launches the user application using
            <emphasis>pegasus-kickstart.</emphasis></para>
          </listitem>
        </orderedlist>
      </listitem>

      <listitem>
        <para>Optionally, shuts down the container (only applicable for Docker
        containers)</para>
      </listitem>

      <listitem>
        <para>Ships out the output data to the staging site</para>
      </listitem>

      <listitem>
        <para>Cleans up the directory on the worker node.</para>
      </listitem>
    </orderedlist>

    <note>
      <para>Starting Pegasus 4.9.1 the container data transfer model has been
      changed. Instead of data transfers for the job occurring outside the
      container in the PegasusLite wrapper, they now happen when the user job
      starts in the container.</para>
    </note>

    <para>In versions of Pegasus &gt;= 4.9.1 the transfers are handled from
    within the container, and thus container recipes require some extra
    attention. A Dockerfile example that prepares a container for GridFTP
    transfers is provided below.</para>

    <para>In this example there are three sections. <itemizedlist>
        <listitem>
          <para>Essential Packages</para>
        </listitem>

        <listitem>
          <para>Install Globus Toolkit</para>
        </listitem>

        <listitem>
          <para>Install CA Certs</para>
        </listitem>
      </itemizedlist> From the "Essential Packages", <emphasis
    role="bold">python</emphasis> and either <emphasis
    role="bold">curl</emphasis> or <emphasis role="bold">wget</emphasis> have
    to be present. "Install Globus Toolkit", sets up the enviroment for
    GridFTP transfers. And "Install CA Certs" copies the grid certificates in
    the container.</para>

    <note>
      <para>Globus Toolkit introduced some breaking changes in August 2018 to
      its authentication module, and some sites haven't upgraded their
      installations (eg. NERSC). GridFTP in order to authenticate
      successfully, requires the libglobus-gssapi-gsi4 package to be pinned to
      the version 13.8-1. The code snipet below contains installation
      directives to handle this but they are commented out.</para>
    </note>

    <programlisting>
##########################################
#### This Container Supports GridFTP  ####
##########################################

FROM ubuntu:18.04

#### Essential Packages ####
RUN apt-get update &amp;&amp;\
apt-get install -y software-properties-common curl wget python unzip &amp;&amp;\
rm -rf /var/lib/apt/lists/*

#### Install Globus Toolkit ####
RUN wget -nv http://www.globus.org/ftppub/gt6/installers/repo/globus-toolkit-repo_latest_all.deb &amp;&amp;\
dpkg -i globus-toolkit-repo_latest_all.deb &amp;&amp;\
apt-get update &amp;&amp;\
# apt-get install -y libglobus-gssapi-gsi4=13.8-1+gt6.bionic &amp;&amp;\
# apt-mark hold libglobus-gssapi-gsi4 &amp;&amp;\
apt-get install -y globus-data-management-client &amp;&amp;\
rm -f globus-toolkit-repo_latest_all.deb &amp;&amp;\
rm -rf /var/lib/apt/lists/*

#### Install CA Certs ####
RUN mkdir -p /etc/grid-security &amp;&amp;\
cd /etc/grid-security &amp;&amp;\
wget -nv https://download.pegasus.isi.edu/containers/certificates.tar.gz &amp;&amp;\
tar xzf certificates.tar.gz &amp;&amp;\
rm -f certificates.tar.gz

##########################################
#### Your Container Specific Commands ####
##########################################
   </programlisting>
  </section>

  <section id="container-transfers">
    <title>Staging of Application Containers</title>

    <para>Pegasus treats containers as other files in terms of data
    management. Container to be used for a job is tracked as an input
    dependency that needs to be staged if it is not already there. Similar to
    executables, you specify the location for your container image in the
    Transformation Catalog. You can specify the source URL's for containers as
    the following.</para>

    <orderedlist>
      <listitem>
        <para>URL to a container hosted on a central hub repository</para>

        <para>Example of a docker hub URL is docker:///rynge/montage:latest,
        while for singularity shub://pegasus-isi/fedora-montage</para>
      </listitem>

      <listitem>
        <para>URL to a container image file on a file server.</para>

        <itemizedlist>
          <listitem>
            <para><emphasis role="bold">Docker - </emphasis>Docker supports
            loading of containers from a tar file, Hence, containers images
            can only be specified as tar files and the extension for the
            filename is not important.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Singularity -</emphasis> Singularity
            supports container images in various forms and relies on the
            extension in the filename to determine what format the file is in.
            Pegasus supports the following extensions for singularity
            container images</para>

            <itemizedlist>
              <listitem>
                <para>.img</para>
              </listitem>

              <listitem>
                <para>.tar</para>
              </listitem>

              <listitem>
                <para>.tar.gz</para>
              </listitem>

              <listitem>
                <para>.tar.bz2</para>
              </listitem>

              <listitem>
                <para>.cpio</para>
              </listitem>

              <listitem>
                <para>.cpio.gz</para>
              </listitem>
            </itemizedlist>

            <para>Singularity will fail to run the container if you don't
            specify the right extension , when specify the source URL for the
            image.</para>
          </listitem>
        </itemizedlist>
      </listitem>
    </orderedlist>

    <para>In both the cases, Pegasus will place the container image on the
    staging site used for the workflow, as part of the data stage-in nodes,
    using pegasus-transfer. When pulling in an image from a container hub
    repository, pegasus-transfer will export the container as a tar file in
    case of Docker, and as .img file in case of Singularity</para>

    <section id="shifter_containers_staging">
      <title>Shifter Containers</title>

      <para>Shifter containers are different from docker and singularity with
      respect to the fact that the containers cannot be exported to a
      container image file that can reside on a filesystem. Additionally, the
      container are expected to be available locally on the compute sites in
      the local Shifter registry. Because of this, Pegasus does not do any
      transfer of Shifter containers. You can specify a shifter container
      using the shifter url scheme. For example, below is a transformation
      catalog for a namd transformation that is executed in a shifter
      container. </para>

      <programlisting>cont namd_image{
     # can be either docker or singularity
     type "shifter"

     # image loaded in the local shifter repository at cori
     image "shifter:///papajim/namd_image:latest" 

     # optional site attribute to tell pegasus which site tar file
     # exists. useful for handling file URL's correctly
     image_site "cori"
}

tr namd2 {
    site cori {
        pfn "/opt/NAMD_2.12_Linux-x86_64-multicore/namd2"
        arch "x86_64"
        os "LINUX"
        type "INSTALLED"
        container "namd_image"
        profile globus "maxTime" "20"
        profile pegasus "exitcode.successmsg" "End of program"
    }
}</programlisting>
    </section>

    <section id="container-symlinking">
      <title>Symlinking and File Copy From Host OS</title>

      <para>Since, Pegasus by default only mounts the job directory determined
      by PegasusLite into the application container, symlinking of input data
      sets works only if in the container definition in the transformation
      catalog user defines the directories containing the input data to be
      mounted in the container using the <emphasis><emphasis
      role="bold">mount</emphasis></emphasis> key word. We recommend to keep
      the source and destination directories to be the same i.e. the host path
      is mounted in the same location in the container.</para>

      <para>The above is also true for the case, where you input datasets are
      on the shared filesystem on the compute site and you want a file copy to
      happen, when PegasusLite job starts the container.</para>

      <para>For example in the example below, we have input datasets
      accessible on /lizard on the compute nodes, and mounting them as
      read-only into the container at /lizard</para>

      <programlisting>cont centos-base{
     type "singularity"

     # URL to image in a docker hub or a url to an existing
     # singularity image file
     image "gsiftp://bamboo.isi.edu/lfs1/bamboo-tests/data/centos7.img"

     # optional site attribute to tell pegasus which site tar file
     # exists. useful for handling file URL's correctly
     image_site "local"

     # mount point in the container
     mount "/lizard:/lizard:ro"
  
     # specify env profile via env option do docker run
     profile env "JAVA_HOME" "/opt/java/1.6"	    
}</programlisting>

      <para>To enable symlinking for containers set the following
      properties</para>

      <programlisting># Tells Pegasus to try and create symlinks for input files
pegasus.transfer.links true

# Tells Pegasus to by the staging site ( creation of stage-in jobs) as 
# data is available directly on compute nodes
pegasus.transfer.bypass.input.staging true

</programlisting>

      <para>f you don't set pegasus.transfer.bypass.input.staging then you
      still can have symlinking if</para>

      <orderedlist>
        <listitem>
          <para>your staging site is same as your compute site</para>
        </listitem>

        <listitem>
          <para>the scratch directory specified in the site catalog is visible
          to the worker nodes</para>
        </listitem>

        <listitem>
          <para>you mount the scratch directory in the container definition,
          NOT the original source directory.</para>
        </listitem>
      </orderedlist>

      <para>Enabling symlinking of containers is useful, when running large
      workflows on a single cluster. Pegasus can pull the image from the
      container repository once, and place it on the shared filesystem where
      it can then be symlinked from, when the PegasusLite jobs start on the
      worker nodes of that cluster. In order to do this, you need to be
      running the nonsharedfs data configuration mode with the staging site
      set to be the same as the compute site.</para>
    </section>
  </section>

  <section id="container-example">
    <title>Container Example - Montage Workflow</title>

    <xi:include href="example_workflows.xml"
                xpointer="example_containers_montage"
                xmlns:xi="http://www.w3.org/2001/XInclude"/>
  </section>
</chapter>
