<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN"
"http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<chapter id="tutorial">
  <title>Tutorial</title>

  <section id="tutorial_introduction">
    <title>Introduction</title>

    <para>This tutorial will take you through the steps of running simple
    workflows using Pegasus. This tutorial is intended for new users who want
    to get a quick overview of Pegasus concepts and usage. The tutorial comes
    configured with a variety of examples. The instructions listed here refer
    mainly to the simple diamond workflow example. The tutorial starts with
    submission of an already generated example workflow with Pegasus and shows
    you how to use the workflow dashboard and the command line tools for
    monitoring, debugging, and generating statistics. The tutorial then covers
    the creation of the workflow using system provided API's and configuration
    of the catalogs. More information about the topics covered in this
    tutorial can be found in later chapters of this user's guide.</para>

    <para>All of the steps in this tutorial are performed on the command-line.
    The convention we will use for command-line input and output is to put
    things that you should type in bold, monospace font, and to put the output
    you should get in a normal weight, monospace font, like this:</para>

    <programlisting>[user@host dir]$ <emphasis role="bold">you type this</emphasis>
you get this</programlisting>

    <para>Where <literal>[user@host dir]$</literal> is the terminal prompt,
    the text you should type is “<literal>you type this</literal>”, and the
    output you should get is "<literal>you get this</literal>". The terminal
    prompt will be abbreviated as <literal>$</literal>. Because some of the
    outputs are long, we don’t always include everything. Where the output is
    truncated we will add an ellipsis '...' to indicate the omitted
    output.</para>

    <para><emphasis role="bold">If you are having trouble with this tutorial,
    or anything else related to Pegasus, you can contact the Pegasus Users
    mailing list at <email>pegasus-users@isi.edu</email> to get help. You can
    also contact us on our <ulink
    url="https://pegasus.isi.edu/support">support chatroom</ulink> on HipChat.
    </emphasis></para>
  </section>

  <section id="tutorial_started">
    <title>Getting Started</title>

    <para>Easiest way to start the tutorial is to connect to a hosted service
    using SSH as shown below.</para>

    <programlisting>$ <emphasis role="bold">ssh tutorial@pegasus-tutorial.isi.edu</emphasis>
tutorial@pegasus-tutorial.isi.edu's password: <emphasis role="bold">pegasus123</emphasis></programlisting>

    <para><note>
        <para>The workflow dashboard is not run the hosted tutorial service.
        To try out the workflow dashboard use the virtual machines provided
        below.</para>
      </note></para>

    <para><emphasis role="bold">OR</emphasis></para>

    <para>We have provided several virtual machines that contain all of the
    software required for this tutorial. Virtual machine images are provided
    for <link linkend="vm_virtualbox">VirtualBox</link> and <link
    linkend="vm_amazon">Amazon EC2</link>. Information about deploying the
    tutorial VM on these platforms is in <link linkend="tutorial_vm">the
    appendix</link>. If you want to use the tutorial VM, please go to the
    appendix for the platform you are using and follow the instructions for
    starting the VM found there before continuing with this tutorial.</para>

    <para><emphasis role="bold">Advanced Users:</emphasis> If you have
    installed Pegasus and Condor on your own machine, then you don't need to
    use the VM for the tutorial. If you installed Pegasus from one of the
    native packages (RPM, DEB, DMG), then you can find the tutorial files in
    <filename> /usr/share/doc/pegasus/tutorial</filename>. If you installed a
    binary tarball, or compiled Pegasus from source, then you can find the
    tutorial files in
    <filename>PEGASUS_HOME/share/doc/pegasus/tutorial</filename>. These files
    will need to be modified in several places to fix the paths to the users
    home directory (which is assumed to be <filename>/home/tutorial
    </filename>). It is assumed that Pegasus was installed from a native
    package, so the path to the Pegasus install is assumed to be
    <filename>/usr</filename>. Condor should be installed in the "Personal
    Condor" configuration. You will also need a passwordless ssh key to enable
    SCP file transfers to/from localhost. Getting everything set up correctly
    can be tricky, so we recommend getting started with one of the VMs if you
    are not familiar with Condor and UNIX.</para>

    <para>The remainder of this tutorial will assume that you have a terminal
    open to the directory where the tutorial files are installed. If you are
    using one of the tutorial VMs these files are located in the tutorial
    user's home directory <filename>/home/tutorial/examples</filename>.</para>
  </section>

  <section id="tutorial_scientific_workflows">
    <title>What are Scientific Workflows</title>

    <para>Scientific workflows allow users to easily express multi-step
    computational tasks, for example retrieve data from an instrument or a
    database, reformat the data, and run an analysis. A scientific workflow
    describes the dependencies between the tasks and in most cases the
    workflow is described as a directed acyclic graph (DAG), where the nodes
    are tasks and the edges denote the task dependencies. A defining property
    for a scientific workflow is that it manages data flow. The tasks in a
    scientific workflow can be everything from short serial tasks to very
    large parallel tasks (MPI for example) surrounded by a large number of
    small, serial tasks used for pre- and post-processing.</para>

    <para>Workflows can vary from simple to complex. Below are some examples.
    In the figures below, the task are designated by circles/ellipses while
    the files created by the tasks are indicated by rectangles. Arrows
    indicate task dependencies.</para>

    <para><emphasis role="bold">Process Workflow</emphasis></para>

    <para>It consists of a single task that runs the `ls` command and
    generates a listing of the files in the `/` directory.</para>

    <figure>
      <title>Single Task Workflow</title>

      <mediaobject>
        <imageobject>
          <imagedata align="center" contentdepth="50%"
                     fileref="images/tutorial-single-job-wf.jpg"/>
        </imageobject>
      </mediaobject>
    </figure>

    <para><emphasis role="bold">Pipeline of Tasks</emphasis></para>

    <para>The pipeline workflow consists of two tasks linked together in a
    pipeline. The first job runs the `curl` command to fetch the Pegasus home
    page and store it as an HTML file. The result is passed to the `wc`
    command, which counts the number of lines in the HTML file. <figure>
        <title>Pipeline of Tasks</title>

        <mediaobject>
          <imageobject>
            <imagedata align="center" contentdepth="70%"
                       fileref="images/tutorial-pipeline-tasks-wf.jpg"/>
          </imageobject>
        </mediaobject>
      </figure></para>

    <para><emphasis role="bold">Split Workflow</emphasis></para>

    <para>The split workflow downloads the Pegasus home page using the `curl`
    command, then uses the `split` command to divide it into 4 pieces. The
    result is passed to the `wc` command to count the number of lines in each
    piece.<figure>
        <title>Split Workflow</title>

        <mediaobject>
          <imageobject>
            <imagedata align="center" contentwidth="70%"
                       fileref="images/tutorial-split-wf.jpg"/>
          </imageobject>
        </mediaobject>
      </figure></para>

    <para><emphasis role="bold">Merge Workflow</emphasis></para>

    <para>The merge workflow runs the `ls` command on several */bin
    directories and passes the results to the `cat` command, which merges the
    files into a single listing.<figure>
        <title>Merge Workflow</title>

        <mediaobject>
          <imageobject>
            <imagedata align="center" contentwidth="70%"
                       fileref="images/tutorial-merge-wf.jpg"/>
          </imageobject>
        </mediaobject>
      </figure></para>

    <para>The above examples can be used as building blocks for much complex
    workflows. Some of these are showcased on the <ulink
    url="https://pegasus.isi.edu/applications">Pegasus Applications
    page</ulink>.</para>
  </section>

  <section id="tutorial_submitting_wf">
    <title>Submitting an Example Workflow</title>

    <para>The tutorial comes configured with a variety of examples. For the
    rest of the tutorial we will be using the diamond example. Other examples
    are also structured similarly.</para>

    <programlisting>$ <emphasis role="bold">cd /home/tutorial/examples</emphasis>
$ ls 
﻿diamond  merge  pipeline  process  split
</programlisting>

    <para>To start the tutorial, we will run an already generated diamond
    workflow through Pegasus. The diamond workflow looks like this</para>

    <figure>
      <title>Diamond Workflow</title>

      <mediaobject>
        <imageobject>
          <imagedata fileref="images/concepts-diamond.jpg"/>
        </imageobject>
      </mediaobject>
    </figure>

    <para>The input workflow description for Pegasus is called the DAX. In
    each of the example directories, there is a file with a .dax extension. We
    will cover on how to create the DAX programmatically later in the <link
    linkend="tutorial_wf_generation">tutorial</link>. Pegasus takes in the DAX
    and generates an executable HTCondor workflow that is run on an execution
    site.</para>

    <para>The <literal>pegasus-plan</literal> command is used to submit the
    workflow through Pegasus. This command takes quite a few arguments, so we
    created a <filename>plan_dax.sh</filename> wrapper script that has all of
    the arguments required for the diamond workflow:</para>

    <programlisting>$ <emphasis role="bold">more plan_dax.sh</emphasis>

#!/bin/bash

if [ $# -ne 1 ]; then
   echo "Usage: $0 DAXFILE"
   exit 1
fi

DAXFILE=$1

# This command tells Pegasus to plan the workflow contained in 
# "diamond.dax" using the config file "pegasus.conf". The planned
# workflow will be stored in a relative directory named "submit".
# The execution site is "PegasusVM" and the output site is "local".
# <emphasis role="bold">--input-dir</emphasis> tells Pegasus to pick up inputs for the workflow from that directory
# -<emphasis role="bold">-output-dir</emphasis> tells Pegasus to place the outputs in that directory
# <emphasis role="bold">--dir</emphasis>        tells Pegasus where to generate the HTCondor workflow
# <emphasis role="bold">--force</emphasis>      tells Pegasus not to prune anything from the workflow, and
# <emphasis role="bold">--nocleanup</emphasis>  tells Pegasus not to generate cleanup jobs.
pegasus-plan --conf pegasus.conf --dax $DAXFILE --dir submit \
             --input-dir ./input --output-dir ./outputs \
             --force --sites PegasusVM -o local --cleanup none --submit
</programlisting>

    <para>The pegasus-plan command takes in the input workflow ( DAX file
    specified by --dax option) and maps the abstract DAX to one or more
    execution sites, and submits the generated executable workflow for
    execution. </para>

    <para>To plan the diamond workflow invoke the
    <filename>plan_dax.sh</filename> script with the path to the DAX
    file:</para>

    <programlisting>$ <emphasis role="bold">./plan_dax.sh diamond.dax</emphasis>
﻿
2015.10.19 16:07:19.131 PDT:   Log of DAGMan debugging messages                 : diamond-0.dag.dagman.out 
2015.10.19 16:07:19.140 PDT:   Log of Condor library output                     : diamond-0.dag.lib.out 
2015.10.19 16:07:19.146 PDT:   Log of Condor library error messages             : diamond-0.dag.lib.err 
2015.10.19 16:07:19.151 PDT:   Log of the life of condor_dagman itself          : diamond-0.dag.dagman.log 
2015.10.19 16:07:19.157 PDT:    
2015.10.19 16:07:19.176 PDT:   ----------------------------------------------------------------------- 
2015.10.19 16:07:20.163 PDT:   Your database is compatible with Pegasus version: 4.5.3 
2015.10.19 16:07:20.457 PDT:   Submitting to condor diamond-0.dag.condor.sub 
2015.10.19 16:07:20.548 PDT:   Submitting job(s). 
2015.10.19 16:07:20.556 PDT:   1 job(s) submitted to cluster 14. 
2015.10.19 16:07:20.569 PDT:    
2015.10.19 16:07:20.607 PDT:   Your workflow has been started and is running in the base directory: 
2015.10.19 16:07:20.617 PDT:    
2015.10.19 16:07:20.655 PDT:     /home/tutorial/examples/diamond/submit/tutorial/pegasus/diamond/run0001 
2015.10.19 16:07:20.701 PDT:    
2015.10.19 16:07:20.744 PDT:   *** To monitor the workflow you can run *** 
2015.10.19 16:07:20.789 PDT:    
2015.10.19 16:07:20.812 PDT:     pegasus-status -l /home/tutorial/examples/diamond/submit/tutorial/pegasus/diamond/run0001 
2015.10.19 16:07:20.835 PDT:    
2015.10.19 16:07:20.845 PDT:   *** To remove your workflow run *** 
2015.10.19 16:07:20.864 PDT:    
2015.10.19 16:07:20.874 PDT:     pegasus-remove /home/tutorial/examples/diamond/submit/tutorial/pegasus/diamond/run0001 
2015.10.19 16:07:20.885 PDT:    
2015.10.19 16:07:21.745 PDT:   Time taken to execute is 4.427 seconds 
</programlisting>

    <note>
      <para>The line in the output that starts with
      <literal>pegasus-status</literal> , contains the command you can use on
      the command line to monitor the status of the workflow . The path it
      contains is the path to the submit directory where all of the files
      required to submit and monitor the workflow are stored.</para>
    </note>

    <para> </para>

    <para>This is what the diamond workflow looks like after Pegasus has
    finished planning the DAX:</para>

    <figure>
      <title>Diamond DAG</title>

      <mediaobject>
        <imageobject>
          <imagedata contentwidth="70%"
                     fileref="images/concepts-diamond-dag.png"/>
        </imageobject>
      </mediaobject>
    </figure>

    <para>For this workflow the only jobs Pegasus needs to add are a directory
    creation job, a stage-in job (for f.a), and a stage-out job (for f.d). No
    registration jobs are added because all the files in the DAX are marked
    register="false", and no cleanup jobs are added because we passed the
    <literal>--nocleanup</literal> argument to
    <literal>pegasus-plan</literal>.</para>
  </section>

  <section id="tutorial_wf_dashboard">
    <title>Workflow Dashboard for Monitoring and Debugging</title>

    <para><note>
        <para>If you are running this tutorial through the hosted service then
        skip this step. To try out the workflow dashboard use the virtual
        machines provided above.</para>
      </note></para>

    <para>The Pegasus Dashboard is a web interface for monitoring and
    debugging workflows and is shipped with each Pegasus release. We will now
    use the web dashboard to monitor the status of the workflow.</para>

    <para>By default, the dashboard server can only monitor workflows run by
    the current user i.e. the user who is running the pegasus-service.</para>

    <para>To access the workflow dashboard, in the VirtualBox VM you can
    launch Firefox by clicking the Firefox icon in the top menu of the
    desktop. The home page for the dashboard is accessible at
    https://localhost:5000 . If you are using EC2 you will need to replace
    'localhost' with the IP address of your EC2 instance.</para>

    <para>When the webpage loads up, it will ask you for a username and a
    password. Log in as user "<emphasis role="bold">tutorial</emphasis>" with
    password "<emphasis role="bold">pegasus</emphasis>".</para>

    <para>The Dashboard's home page lists all workflows, which have been run
    by the current-user. The home page shows the status of each of the
    workflow i.e. Running/Successful/Failed/Failing. The home page lists only
    the top level workflows (Pegasus supports hierarchical workflows i.e.
    workflows within a workflow). The rows in the table are color coded</para>

    <itemizedlist>
      <listitem>
        <para><emphasis role="bold">Green</emphasis>: indicates workflow
        finished successfully.</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">Red</emphasis>: indicates workflow
        finished with a failure.</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">Blue</emphasis>: indicates a workflow is
        currently running.</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">Gray</emphasis>: indicates a workflow that
        was archived.</para>
      </listitem>
    </itemizedlist>

    <figure>
      <title>Dashboard Home Page</title>

      <mediaobject>
        <imageobject>
          <imagedata fileref="images/dashboard_home.png" width="100%"/>
        </imageobject>
      </mediaobject>
    </figure>

    <para>To view details specific to a workflow, the user can click on
    corresponding workflow label. The workflow details page lists workflow
    specific information like workflow label, workflow status, location of the
    submit directory, etc. The details page also displays pie charts showing
    the distribution of jobs based on status.</para>

    <para>In addition, the details page displays a tab listing all
    sub-workflows and their statuses. Additional tabs exist which list
    information for all running, failed, successful, and failing jobs.</para>

    <para>The information displayed for a job depends on it's status. For
    example, the failed jobs tab displays the job name, exit code, links to
    available standard output, and standard error contents.</para>

    <figure>
      <title>Dashboard Workflow Page</title>

      <mediaobject>
        <imageobject>
          <imagedata fileref="images/dashboard_workflow_details.png"
                     width="100%"/>
        </imageobject>
      </mediaobject>
    </figure>

    <para>To view details specific to a job the user can click on the
    corresponding job's job label. The job details page lists information
    relevant to a specific job. For example, the page lists information like
    job name, exit code, run time, etc.</para>

    <para>The job instance section of the job details page lists all attempts
    made to run the job i.e. if a job failed in its first attempt due to
    transient errors, but ran successfully when retried, the job instance
    section shows two entries; one for each attempt to run the job.</para>

    <para>The job details page also shows tab's for failed, and successful
    task invocations (Pegasus allows users to group multiple smaller task's
    into a single job i.e. a job may consist of one or more tasks)</para>

    <figure>
      <title>Dashboard Job Description Page</title>

      <mediaobject>
        <imageobject>
          <imagedata fileref="images/dashboard_job_details.png" width="100%"/>
        </imageobject>
      </mediaobject>
    </figure>

    <para>The task invocation details page provides task specific information
    like task name, exit code, duration etc. Task details differ from job
    details, as they are more granular in nature.</para>

    <figure>
      <title>Dashboard Invocation Page</title>

      <mediaobject>
        <imageobject>
          <imagedata fileref="images/dashboard_invocation_details.png"
                     width="100%"/>
        </imageobject>
      </mediaobject>
    </figure>

    <para>The dashboard also has web pages for workflow statistics and
    workflow charts, which graphically renders information provided by the
    pegasus-statistics and pegasus-plots command respectively.</para>

    <para>The Statistics page shows the following statistics.</para>

    <orderedlist>
      <listitem>
        <para>Workflow level statistics</para>
      </listitem>

      <listitem>
        <para>Job breakdown statistics</para>
      </listitem>

      <listitem>
        <para>Job specific statistics</para>
      </listitem>
    </orderedlist>

    <figure>
      <title>Dashboard Statistics Page</title>

      <mediaobject>
        <imageobject>
          <imagedata fileref="images/dashboard_statistics.png" width="100%"/>
        </imageobject>
      </mediaobject>
    </figure>

    <para>The Charts page shows the following charts.</para>

    <orderedlist>
      <listitem>
        <para>Job Distribution by Count/Time</para>
      </listitem>

      <listitem>
        <para>Time Chart by Job/Invocation</para>
      </listitem>

      <listitem>
        <para>Workflow Execution Gantt Chart</para>
      </listitem>
    </orderedlist>

    <para>The chart below shows the invocation distribution by count or
    time.</para>

    <figure>
      <title>Dashboard Plots - Job Distribution</title>

      <mediaobject>
        <imageobject>
          <imagedata fileref="images/dashboard_plots_job_dist.png"
                     width="100%"/>
        </imageobject>
      </mediaobject>
    </figure>

    <para>The time chart shown below shows the number of jobs/invocations in
    the workflow and their total runtime</para>

    <figure>
      <title>Dashboard Plots - Time Chart</title>

      <mediaobject>
        <imageobject>
          <imagedata fileref="images/dashboard_plots_time_charts.png"
                     width="100%"/>
        </imageobject>
      </mediaobject>
    </figure>

    <para>The workflow gantt chart lays out the execution of the jobs in the
    workflow over time.</para>

    <figure>
      <title>Dashboard Plots - Workflow Gantt Chart</title>

      <mediaobject>
        <imageobject>
          <imagedata fileref="images/dashboard_plots_wf_gantt.png"
                     width="100%"/>
        </imageobject>
      </mediaobject>
    </figure>
  </section>

  <section id="tutorial_monitoring_cmd_tools">
    <title>Command line tools for Monitoring and Debugging</title>

    <para>Pegasus also comes with a series of command line tools that users
    can use to monitor and debug their workflows.</para>

    <itemizedlist>
      <listitem>
        <para>pegasus-status : monitor the status of the workflow</para>
      </listitem>

      <listitem>
        <para>pegasus-analyzer : debug a failed workflow</para>
      </listitem>

      <listitem>
        <para>pegasus-statistics : generate statistics from a workflow
        run.</para>
      </listitem>
    </itemizedlist>

    <section>
      <title>pegasus-status - monitoring the workflow</title>

      <para>After the workflow has been submitted you can monitor it using the
      <literal>pegasus-status</literal> command:</para>

      <programlisting>$ <emphasis role="bold">pegasus-status submit/tutorial/pegasus/diamond/run0001</emphasis>
STAT  IN_STATE  JOB
Run      01:48  diamond-0
Run      00:05   |-findrange_ID0000002
Run      00:05   \_findrange_ID0000003
Summary: 3 Condor jobs total (R:3)

UNREADY   READY     PRE  QUEUED    POST SUCCESS FAILURE %DONE
      2       0       0       3       0       3       0  37.5
Summary: 1 DAG total (Running:1)
</programlisting>

      <para>This command shows the workflow (diamond-0) and the running jobs
      (in the above output it shows the two findrange jobs). It also gives
      statistics on the number of jobs in each state and the percentage of the
      jobs in the workflow that have finished successfully.</para>

      <para>Use the <literal>watch</literal> command to continuously monitor
      the workflow:</para>

      <programlisting>$ <emphasis role="bold">watch pegasus-status submit/tutorial/pegasus/diamond/run0001</emphasis>
...</programlisting>

      <para>You should see all of the jobs in the workflow run one after the
      other. After a few minutes you will see:</para>

      <programlisting>(no matching jobs found in Condor Q)
UNREADY   READY     PRE  QUEUED    POST SUCCESS FAILURE %DONE
      0       0       0       0       0       8       0 100.0
Summary: 1 DAG total (Success:1)
</programlisting>

      <para>That means the workflow is finished successfully. You can type
      <literal>ctrl-c</literal> to terminate the <literal>watch</literal>
      command.</para>

      <para>If the workflow finished successfully you should see the output
      file <filename>f.d</filename> in the <filename>output</filename>
      directory. This file was created by the various transformations in the
      workflow and shows all of the executables that were invoked by the
      workflow:</para>

      <programlisting>$ <emphasis role="bold">more output/f.d</emphasis>
/home/tutorial/examples/diamond/bin/analyze:
/home/tutorial/examples/diamond/bin/findrange:
/home/tutorial/examples/diamond/bin/preprocess:
This is the input file of the diamond workflow
/home/tutorial/examples/diamond/bin/findrange:
/home/tutorial/examples/diamond/bin/preprocess:
This is the input file of the diamond workflow
</programlisting>

      <para>Remember that the example transformations in this workflow just
      print their name to all of their output files and then copy all of their
      input files to their output files.</para>
    </section>

    <section>
      <title>pegasus-analyzer - debug a failed workflow</title>

      <para>In the case that one or more jobs fails, then the output of the
      <literal>pegasus-status</literal> command above will have a non-zero
      value in the <literal>FAILURE</literal> column.</para>

      <para>You can debug the failure using the
      <literal>pegasus-analyzer</literal> command. This command will identify
      the jobs that failed and show their output. Because the workflow
      succeeded, <literal>pegasus-analyzer</literal> will only show some basic
      statistics about the number of successful jobs:</para>

      <programlisting>$ <emphasis role="bold">pegasus-analyzer submit/tutorial/pegasus/diamond/run0001</emphasis>
pegasus-analyzer: initializing...

****************************Summary***************************

 Total jobs         :      7 (100.00%)
 # jobs succeeded   :      7 (100.00%)
 # jobs failed      :      0 (0.00%)
 # jobs unsubmitted :      0 (0.00%)
</programlisting>

      <para>If the workflow had failed you would see something like
      this:</para>

      <programlisting>$ <emphasis role="bold">pegasus-analyzer submit/tutorial/pegasus/diamond/run0002</emphasis>
pegasus-analyzer: initializing...

**************************Summary*************************************

 Total jobs         :      7 (100.00%)
 # jobs succeeded   :      2 (28.57%)
 # jobs failed      :      1 (14.29%)
 # jobs unsubmitted :      4 (57.14%)

**********************Failed jobs' details****************************

====================preprocess_ID0000001==============================

 last state: POST_SCRIPT_FAILED
       site: PegasusVM
submit file: preprocess_ID0000001.sub
output file: preprocess_ID0000001.out.003
 error file: preprocess_ID0000001.err.003

-----------------------Task #1 - Summary-----------------------------

site        : PegasusVM
hostname    : ip-10-252-31-58.us-west-2.compute.internal
executable  : /home/tutorial/bin/preprocess
arguments   : -i f.a -o f.b1 -o f.b2
exitcode    : -128
working dir : -

-------------Task #1 - preprocess - ID0000001 - stderr---------------

FATAL: The main job specification is invalid or missing.
</programlisting>

      <para>In this example I removed the <filename>bin/preprocess</filename>
      executable and re-planned/re-submitted the workflow (that is why the
      command has run0002). The output of <literal>pegasus-analyzer</literal>
      indicates that the preprocess task failed with an error message that
      indicates that the executable could not be found.</para>
    </section>

    <section>
      <title>pegasus-statistics - collect statistics about a workflow
      run</title>

      <para>The <literal>pegasus-statistics</literal> command can be used to
      gather statistics about the runtime of the workflow and its jobs. The
      <literal>-s all</literal> argument tells the program to generate all
      statistics it knows how to calculate:</para>

      <programlisting>$ <emphasis role="bold">pegasus-statistics –s all submit/tutorial/pegasus/diamond/run0001</emphasis>

#
# Pegasus Workflow Management System - http://pegasus.isi.edu
#
# Workflow summary:
#   Summary of the workflow execution. It shows total
#   tasks/jobs/sub workflows run, how many succeeded/failed etc.
#   In case of hierarchical workflow the calculation shows the
#   statistics across all the sub workflows.It shows the following
#   statistics about tasks, jobs and sub workflows.
#     * Succeeded - total count of succeeded tasks/jobs/sub workflows.
#     * Failed - total count of failed tasks/jobs/sub workflows.
#     * Incomplete - total count of tasks/jobs/sub workflows that are
#       not in succeeded or failed state. This includes all the jobs
#       that are not submitted, submitted but not completed etc. This
#       is calculated as  difference between 'total' count and sum of
#       'succeeded' and 'failed' count.
#     * Total - total count of tasks/jobs/sub workflows.
#     * Retries - total retry count of tasks/jobs/sub workflows.
#     * Total+Retries - total count of tasks/jobs/sub workflows executed
#       during workflow run. This is the cumulative of retries,
#       succeeded and failed count.
# Workflow wall time:
#   The wall time from the start of the workflow execution to the end as
#   reported by the DAGMAN.In case of rescue dag the value is the
#   cumulative of all retries.
# Workflow cumulative job wall time:
#   The sum of the wall time of all jobs as reported by kickstart.
#   In case of job retries the value is the cumulative of all retries.
#   For workflows having sub workflow jobs (i.e SUBDAG and SUBDAX jobs),
#   the wall time value includes jobs from the sub workflows as well.
# Cumulative job wall time as seen from submit side:
#   The sum of the wall time of all jobs as reported by DAGMan.
#   This is similar to the regular cumulative job wall time, but includes
#   job management overhead and delays. In case of job retries the value
#   is the cumulative of all retries. For workflows having sub workflow
#   jobs (i.e SUBDAG and SUBDAX jobs), the wall time value includes jobs
#   from the sub workflows as well.
# Cumulative job wall time as seen from submit side:
#   The sum of the wall time of all jobs as reported by DAGMan.
#   This is similar to the regular cumulative job wall time, but includes
#   job management overhead and delays. In case of job retries the value
#   is the cumulative of all retries. For workflows having sub workflow
#   jobs (i.e SUBDAG and SUBDAX jobs), the wall time value includes jobs
#   from the sub workflows as well.
# Workflow cumulative job badput wall time:
#   The sum of the wall time of all failed jobs as reported by kickstart.
#   In case of job retries the value is the cumulative of all retries.
#   For workflows having sub workflow jobs (i.e SUBDAG and SUBDAX jobs),
#   the wall time value includes jobs from the sub workflows as well.
# Cumulative job badput wall time as seen from submit side:
#   The sum of the wall time of all failed jobs as reported by DAGMan.
#   This is similar to the regular cumulative job badput wall time, but includes
#   job management overhead and delays. In case of job retries the value
#   is the cumulative of all retries. For workflows having sub workflow
#   jobs (i.e SUBDAG and SUBDAX jobs), the wall time value includes jobs
#   from the sub workflows as well.

-----------------------------------------------------------------------
Type            Succeeded  Failed  Incomplete  Total     Retries  Total Run
Tasks           4          0       0           4     ||  0        4
Jobs            7          0       0           7     ||  0        7
Sub Workflows   0          0       0           0     ||  0        0
-----------------------------------------------------------------------

Workflow wall time                                       : 3 mins, 25 secs
Workflow cumulative job wall time                        : 2 mins, 0 secs
Cumulative job wall time as seen from submit side        : 2 mins, 0 secs
Workflow cumulative job badput wall time                 : 0
Cumulative job badput wall time as seen from submit side : 0

Summary: submit/tutorial/pegasus/diamond/run0001/statistics/summary.txt

</programlisting>

      <para>The output of <literal>pegasus-statistics</literal> contains many
      definitions to help users understand what all of the values reported
      mean. Among these are the total wall time of the workflow, which is the
      time from when the workflow was submitted until it finished, and the
      total cumulative job wall time, which is the sum of the runtimes of all
      the jobs.</para>

      <para>The <literal>pegasus-statistics</literal> command also writes out
      several reports in the <filename>statistics</filename> subdirectory of
      the workflow submit directory:</para>

      <programlisting>$ <emphasis role="bold">ls submit/tutorial/pegasus/diamond/run0001/statistics/</emphasis>
breakdown.csv  jobs.txt          summary.txt         time.txt
breakdown.txt  summary-time.csv  time-per-host.csv   workflow.csv
jobs.csv       summary.csv       time.csv            workflow.txt</programlisting>

      <para>The file <filename>breakdown.txt</filename>, for example, has min,
      max, and mean runtimes for each transformation:</para>

      <programlisting>$ <emphasis role="bold">more submit/tutorial/pegasus/diamond/run0001/statistics/breakdown.txt</emphasis>
# legends
# Transformation - name of the transformation.
# Count          - the number of times the invocations corresponding to
#                  the transformation was executed.
# Succeeded      - the count of the succeeded invocations corresponding
#                  to the transformation.
# Failed         - the count of the failed invocations corresponding to
#                  the transformation.
# Min(sec)       - the minimum invocation runtime value corresponding to
#                  the transformation.
# Max(sec)       - the maximum invocation runtime value corresponding to
#                  the transformation.
# Mean(sec)      - the mean of the invocation runtime corresponding to
#                  the transformation.
# Total(sec)     - the cumulative of invocation runtime corresponding to
#                  the transformation.

# a1f5ba03-a827-4d0a-8d59-9941cbfbd83d (diamond)
Transformation   Count  Succeeded  Failed  Min     Max     Mean     Total
analyze          1      1          0       30.008  30.008  30.008   30.008
dagman::post     7      7          0       5.0     6.0     5.143    36.0
findrange        2      2          0       30.009  30.014  30.011   60.023
pegasus::dirmanager 1   1          0       0.194   0.194   0.194    0.194
pegasus::transfer 2     2          0       0.248   0.411   0.33     0.659
preprocess       1      1          0       30.025  30.025  30.025   30.025

# All
Transformation   Count  Succeeded  Failed  Min     Max     Mean     Total
analyze          1      1          0       30.008  30.008  30.008   30.008
dagman::post     7      7          0       5.0     6.0     5.143    36.0
findrange        2      2          0       30.009  30.014  30.011   60.023
pegasus::dirmanager 1   1          0       0.194   0.194   0.194    0.194
pegasus::transfer 2     2          0       0.248   0.411   0.33     0.659
preprocess       1      1          0       30.025  30.025  30.025   30.025
</programlisting>

      <para>In this case, because the example transformation sleeps for 30
      seconds, the min, mean, and max runtimes for each of the analyze,
      findrange, and preprocess transformations are all close to 30.</para>
    </section>
  </section>

  <section>
    <title id="tutorial_wf_generation">Generating the Workflow</title>

    <para>The example that you ran earlier already had the workflow
    description (diamond.dax) generated. Pegasus reads workflow descriptions
    from DAX files. The term “DAX” is short for “Directed Acyclic Graph in
    XML”. DAX is an XML file format that has syntax for expressing jobs,
    arguments, files, and dependencies. We now will be creating the diamond
    workflow that we just ran using the Pegasus provided DAX API:</para>

    <figure>
      <title>Diamond Workflow</title>

      <mediaobject>
        <imageobject>
          <imagedata fileref="images/concepts-diamond.jpg"/>
        </imageobject>
      </mediaobject>
    </figure>

    <para>In this diagram, the ovals represent computational jobs, the
    dog-eared squares are files, and the arrows are dependencies.</para>

    <para>In order to create a DAX it is necessary to write code for a DAX
    generator. Pegasus comes with Perl, Java, and Python libraries for writing
    DAX generators. In this tutorial we will show how to use the Python
    library.</para>

    <para>The DAX generator for the diamond workflow is in the file
    <filename>generate_dax.py</filename>. Look at the file by typing:</para>

    <programlisting>$ <emphasis role="bold">more generate_dax.py</emphasis>
...</programlisting>

    <tip>
      <para>We will be using the <literal>more</literal> command to inspect
      several files in this tutorial. <literal>more</literal> is a pager
      application, meaning that it splits text files into pages and displays
      the pages one at a time. You can view the next page of a file by
      pressing the spacebar. Type 'h' to get help on using
      <literal>more</literal>. When you are done, you can type 'q' to close
      the file.</para>
    </tip>

    <para>The code has 5 sections:</para>

    <orderedlist>
      <listitem>
        <para>A few system libraries and the Pegasus.DAX3 library are
        imported. The search path is modified to include the directory with
        the Pegasus Python library.</para>
      </listitem>

      <listitem>
        <para>The name for the DAX output file is retrieved from the
        arguments.</para>
      </listitem>

      <listitem>
        <para>A new ADAG object is created. This is the main object to which
        jobs and dependencies are added.</para>
      </listitem>

      <listitem>
        <para>Jobs and files are added. The 4 jobs in the diagram above are
        added and the 6 files are referenced. Arguments are defined using
        strings and File objects. The input and output files are defined for
        each job. This is an important step, as it allows Pegasus to track the
        files, and stage the data if necessary. Workflow outputs are tagged
        with “transfer=true”.</para>
      </listitem>

      <listitem>
        <para>Dependencies are added. These are shown as arrows in the diagram
        above. They define the parent/child relationships between the jobs.
        When the workflow is executing, the order in which the jobs will be
        run is determined by the dependencies between them.</para>
      </listitem>
    </orderedlist>

    <para>Generate a DAX file named <filename>diamond.dax</filename> by
    typing:</para>

    <programlisting>$ <emphasis role="bold">./generate_dax.py diamond.dax</emphasis>
Creating ADAG...
Adding preprocess job...
Adding left Findrange job...
Adding right Findrange job...
Adding Analyze job...
Adding control flow dependencies...
Writing diamond.dax</programlisting>

    <para>The <filename>diamond.dax</filename> file should contain an XML
    representation of the diamond workflow. You can inspect it by
    typing:</para>

    <programlisting>$ <emphasis role="bold">more diamond.dax</emphasis>
...</programlisting>
  </section>

  <section id="tutorial_catalogs">
    <title>Information Catalogs</title>

    <para>The workflow description that you specify to Pegasus is portable,
    and usually does not contain any locations.There are three information
    catalogs that Pegasus uses when planning the workflow. These are the <link
    linkend="tut_site_catalog">Site Catalog</link>, <link
    linkend="tut_xform_catalog">Transformation Catalog</link>, and <link
    linkend="tut_replica_catalog">Replica Catalog</link>.</para>

    <section id="tut_site_catalog">
      <title>The Site Catalog</title>

      <para>The site catalog describes the sites where the workflow jobs are
      to be executed. Typically the sites in the site catalog describe remote
      clusters, such as PBS clusters or Condor pools. In this tutorial we
      assume that you have a Personal Condor pool running on localhost. If you
      are using one of the tutorial VMs this has already been setup for
      you.</para>

      <para>The site catalog is in <filename>sites.xml</filename>:</para>

      <programlisting>$ <emphasis role="bold">more sites.xml</emphasis>
...
﻿    ﻿&lt;!-- The local site contains information about the submit host --&gt;
    &lt;!-- The arch and os keywords are used to match binaries in the transformation catalog --&gt;
    &lt;site handle="local" arch="x86_64" os="LINUX"&gt;

        &lt;!-- These are the paths on the submit host were Pegasus stores data --&gt;
        &lt;!-- Scratch is where temporary files go --&gt;
        &lt;directory type="shared-scratch" path="/home/tutorial/run"&gt;
            &lt;file-server operation="all" url="file:///home/tutorial/run"/&gt;
        &lt;/directory&gt;
        &lt;!-- Storage is where pegasus stores output files --&gt;
        &lt;directory type="local-storage" path="/home/tutorial/outputs"&gt;
            &lt;file-server operation="all" url="file:///home/tutorial/outputs"/&gt;
        &lt;/directory&gt;

        &lt;!-- This profile tells Pegasus where to find the user's private key for SCP transfers --&gt;
        &lt;profile namespace="env" key="SSH_PRIVATE_KEY"&gt;/home/tutorial/.ssh/id_rsa&lt;/profile&gt;
    &lt;/site&gt;


...</programlisting>

      <para>There are two sites defined in the site catalog: “local” and
      “PegasusVM”. The “local” site is used by Pegasus to learn about the
      submit host where the workflow management system runs. The “PegasusVM”
      site is the personal Condor pool running on your (virtual) machine. In
      this case, the local site and the PegasusVM site refer to the same
      machine, but they are logically separate as far as Pegasus is
      concerned.</para>

      <para>The local site is configured with a “storage” file system that is
      mounted on the submit host (indicated by the file:// URL). This file
      system is where the output data from the workflow will be stored. When
      the workflow is planned we will tell Pegasus that the output site is
      “local”.</para>

      <para>The PegasusVM site is configured with a “scratch” file system
      accessible via SCP (indicated by the scp:// URL). This file system is
      where the working directory will be created. When we plan the workflow
      we will tell Pegasus that the execution site is “PegasusVM”.</para>

      <para>The local site also has an environment variable called
      SSH_PRIVATE_KEY that tells Pegasus where to find the private key to use
      for SCP transfers. If you are running this tutorial on your own machine
      you will need to set up a passwordless ssh key and add it to
      authorized_keys. If you are using the tutorial VM this has already been
      set up for you.</para>

      <para>Pegasus supports many different file transfer protocols. In this
      case the site catalog is set up so that input and output files are
      transferred to/from the PegasusVM site using SCP. Since both the local
      site and the PegasusVM site are actually the same machine, this
      configuration will just SCP files to/from localhost, which is just a
      complicated way to copy the files.</para>

      <para>Finally, the PegasusVM site is configured with two profiles that
      tell Pegasus that it is a plain Condor pool. Pegasus supports many ways
      of submitting tasks to a remote cluster. In this configuration it will
      submit vanilla Condor jobs.</para>
    </section>

    <section>
      <title id="tut_xform_catalog">The Transformation Catalog</title>

      <para>The transformation catalog describes all of the executables
      (called “transformations”) used by the workflow. This description
      includes the site(s) where they are located, the architecture and
      operating system they are compiled for, and any other information
      required to properly transfer them to the execution site and run
      them.</para>

      <para>For this tutorial, the transformation catalog is in the file
      <filename>tc.txt</filename>:</para>

      <programlisting>$ <emphasis role="bold">more tc.txt</emphasis>
...
﻿# This is the transformation catalog. It lists information about each of the
# executables that are used by the workflow.

tr preprocess {
    site PegasusVM {
        pfn "/home/tutorial/examples/diamond/bin/preprocess"
        arch "x86_64"
        os "linux"
        type "INSTALLED"
    }
}


...</programlisting>

      <para>The <filename>tc.txt</filename> file contains information about
      three transformations: preprocess, findrange, and analyze. These three
      transformations are referenced in the diamond DAX. The transformation
      catalog indicates that all three transformations are installed on the
      PegasusVM site, and are compiled for x86_64 Linux.</para>

      <para>The actual executable files are located in the
      <filename>bin</filename> directory. All three executables are actually
      symlinked to the same Python script. This script is just an example
      transformation that sleeps for 30 seconds, and then writes its own name
      and the contents of all its input files to all of its output
      files.</para>
    </section>

    <section>
      <title id="tut_replica_catalog">The Replica Catalog</title>

      <para>The example that you ran , was configured with the inputs already
      present on the submit host ( where pegasus is installed) in a directory.
      If you have inputs at external servers, then you can specify the URL's
      to the input files in the Replica Catalog. This catalog tells Pegasus
      where to find each of the input files for the workflow.</para>

      <para>All files in a Pegasus workflow are referred to in the DAX using
      their Logical File Name (LFN). These LFNs are mapped to Physical File
      Names (PFNs) when Pegasus plans the workflow. This level of indirection
      enables Pegasus to map abstract DAXes to different execution sites and
      plan out the required file transfers automatically.</para>

      <para>The Replica Catalog for the diamond workflow is in the
      <filename>rc.dat</filename> file:</para>

      <programlisting>$ <emphasis role="bold">more rc.dat</emphasis>
# This is the replica catalog. It lists information about each of the
# input files used by the workflow.

# The format is:
# LFN     PFN    site="SITE"

f.a    file:///home/tutorial/examples/diamond/input/f.a    site="local"</programlisting>

      <para>This replica catalog contains only one entry for the diamond
      workflow’s only input file. This entry has an LFN of “f.a” with a PFN of
      “file:///home/tutorial/input/f.a” and the file is stored on the local
      site, which implies that it will need to be transferred to the PegasusVM
      site when the workflow runs.</para>
    </section>
  </section>

  <section id="tutorial_configuration">
    <title>Configuring Pegasus</title>

    <para>In addition to the information catalogs, Pegasus takes a
    configuration file that specifies settings that control how it plans the
    workflow.</para>

    <para>For the diamond workflow, the Pegasus configuration file is
    relatively simple. It only contains settings to help Pegasus find the
    information catalogs. These settings are in the
    <filename>pegasus.conf</filename> file:</para>

    <programlisting>$ <emphasis role="bold">more pegasus.conf</emphasis>
# This tells Pegasus where to find the Site Catalog
pegasus.catalog.site=XML
pegasus.catalog.site.file=sites.xml

# This tells Pegasus where to find the Replica Catalog
pegasus.catalog.replica=File
pegasus.catalog.replica.file=rc.dat

# This tells Pegasus where to find the Transformation Catalog
pegasus.catalog.transformation=Text
pegasus.catalog.transformation.file=tc.txt</programlisting>
  </section>

  <section>
    <title id="tutorial_conclusion">Conclusion</title>

    <para>Congratulations! You have completed the tutorial.</para>

    <para>If you used Amazon EC2 for this tutorial make sure to terminate your
    VM. Refer to the <link linkend="tutorial_vm">appendix</link> for more
    information about how to do this.</para>

    <para>Refer to the other chapters in this guide for more information about
    creating, planning, and executing workflows with Pegasus.</para>

    <para>Please contact the Pegasus Users Mailing list at
    <email>pegasus-users@isi.edu</email> if you need help.</para>
  </section>
</chapter>
