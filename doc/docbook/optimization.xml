<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN"
"http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<chapter id="optimization">
  <title>Optimizing Workflows for Efficiency and Scalability</title>

  <para>By default, Pegasus generates workflows which targets the most common
  usecases and execution environments. For more specialized environments or
  workflows, the following sections can provide hints on how to optimize your
  workflow to scale better, and run more efficient. Below are some common
  issues and solutions.</para>

  <section id="short_jobs">
    <title>Optimizing Short Jobs / Scheduling Delays</title>

    <para><emphasis>Issue:</emphasis> Even though HTCondor is a high
    throughput system, there are overheads when scheduling short jobs. Common
    overheads include scheduling, data transfers, state notifications, and
    task book keeping. These overheads can be very noticeable for short jobs,
    but not noticeable at all for longer jobs as the ration between the
    computation and the overhead is higher.</para>

    <para><emphasis>Solution:</emphasis> If you have many short tasks to run,
    the solution to minimize the overheads is to use <link
    linkend="job_clustering">task clustering</link>. This instructs Pegasus to
    take a set of tasks, selected <link
    linkend="horizontal_clustering">horizontally</link>, by <link
    linkend="label_clustering">labels</link>, or by <link
    linkend="runtime_clustering">runtime</link>, and create jobs containing
    that whole set of tasks. The result is more efficient jobs, for wich the
    overheads are less noticeable.</para>
  </section>

  <section id="job_clustering">
    <title>Job Clustering</title>

    <para>A large number of workflows executed through the Pegasus Workflow
    Management System, are composed of several jobs that run for only a few
    seconds or so. The overhead of running any job on the grid is usually 60
    seconds or more. Hence, it makes sense to cluster small independent jobs
    into a larger job. This is done while mapping an abstract workflow to an
    executable workflow. Site specific or transformation specific criteria are
    taken into consideration while clustering smaller jobs into a larger job
    in the executable workflow. The user is allowed to control the granularity
    of this clustering on a per transformation per site basis.</para>

    <section>
      <title>Overview</title>

      <para>The abstract workflow is mapped onto the various sites by the Site
      Selector. This semi executable workflow is then passed to the clustering
      module. The clustering of the workflow can be either be</para>

      <itemizedlist>
        <listitem>
          <para>level based horizontal clustering - where you can denote how
          many jobs get clustered into a single clustered job per level, or
          how many clustered jobs should be created per level of the
          workflow</para>
        </listitem>

        <listitem>
          <para>level based runtime clustering - similar to horizontal
          clustering , but while creating the clusters per level take into
          account the job runtimes.</para>
        </listitem>

        <listitem>
          <para>label based (label clustering)</para>
        </listitem>
      </itemizedlist>

      <para>The clustering module clusters the jobs into larger/clustered
      jobs, that can then be executed on the remote sites. The execution can
      either be sequential on a single node or on multiple nodes using MPI. To
      specify which clustering technique to use the user has to pass the
      <emphasis role="bold">--cluster</emphasis> option to <emphasis
      role="bold">pegasus-plan</emphasis> .</para>

      <section>
        <title>Generating Clustered Executable Workflow</title>

        <para>The clustering of a workflow is activated by passing the
        <emphasis role="bold">--cluster|-C</emphasis> option to <emphasis
        role="bold">pegasus-plan</emphasis>. The clustering granularity of a
        particular logical transformation on a particular site is dependant
        upon the clustering techniques being used. The executable that is used
        for running the clustered job on a particular site is determined as
        explained in section 7.<programlisting>#Running pegasus-plan to generate clustered workflows

$ pegasus-plan --dax example.dax --dir ./dags -p siteX --output local
               --cluster [comma separated list of clustering techniques]  -verbose

Valid clustering techniques are horizontal and label.</programlisting></para>

        <para>The naming convention of submit files of the clustered jobs
        is<emphasis role="bold"> merge_NAME_IDX.sub</emphasis> . The NAME is
        derived from the logical transformation name. The IDX is an integer
        number between 1 and the total number of jobs in a cluster. Each of
        the submit files has a corresponding input file, following the naming
        convention <emphasis role="bold">merge_NAME_IDX.in </emphasis>. The
        input file contains the respective execution targets and the arguments
        for each of the jobs that make up the clustered job.</para>

        <section id="horizontal_clustering">
          <title>Horizontal Clustering</title>

          <para>In case of horizontal clustering, each job in the workflow is
          associated with a level. The levels of the workflow are determined
          by doing a modified Breadth First Traversal of the workflow starting
          from the root nodes. The level associated with a node, is the
          furthest distance of it from the root node instead of it being the
          shortest distance as in normal BFS. For each level the jobs are
          grouped by the site on which they have been scheduled by the Site
          Selector. Only jobs of same type (txnamespace, txname, txversion)
          can be clustered into a larger job. To use horizontal clustering the
          user needs to set the <emphasis role="bold">--cluster</emphasis>
          option of <emphasis role="bold">pegasus-plan to
          horizontal</emphasis> .</para>

          <section>
            <title>Controlling Clustering Granularity</title>

            <para>The number of jobs that have to be clustered into a single
            large job, is determined by the value of two parameters associated
            with the smaller jobs. Both these parameters are specified by the
            use of a PEGASUS namespace profile keys. The keys can be specified
            at any of the placeholders for the profiles (abstract
            transformation in the DAX, site in the site catalog,
            transformation in the transformation catalog). The normal
            overloading semantics apply i.e. profile in transformation catalog
            overrides the one in the site catalog and that in turn overrides
            the one in the DAX. The two parameters are described below.</para>

            <itemizedlist>
              <listitem>
                <para><emphasis role="bold">clusters.size
                factor</emphasis></para>

                <para>The clusters.size factor denotes how many jobs need to
                be merged into a single clustered job. It is specified via the
                use of a PEGASUS namespace profile key 'clusters.size'. for
                e.g. if at a particular level, say 4 jobs referring to logical
                transformation B have been scheduled to a siteX. The
                clusters.size factor associated with job B for siteX is say 3.
                This will result in 2 clustered jobs, one composed of 3 jobs
                and another of 2 jobs. The clusters.size factor can be
                specified in the transformation catalog as follows</para>

                <programlisting># multiple line text-based transformation catalog: 2014-09-30T16:05:01.731-07:00
tr B {
        site siteX {
                profile pegasus "clusters.size" "3" 
                pfn "/shared/PEGASUS/bin/jobB"
                arch "x86"
                os "LINUX"
                type "INSTALLED"
        }
}

tr C {
        site siteX {
                profile pegasus "clusters.size" "2" 
                pfn "/shared/PEGASUS/bin/jobC"
                arch "x86"
                os "LINUX"
                type "INSTALLED"
        }
}

</programlisting>

                <figure>
                  <title>Clustering by clusters.size</title>

                  <mediaobject>
                    <imageobject role="html">
                      <imagedata align="center" contentdepth="750px"
                                 fileref="images/advanced-clustering-1.png"/>
                    </imageobject>

                    <imageobject role="fo">
                      <imagedata align="center" contentdepth="5in"
                                 fileref="images/advanced-clustering-1.png"/>
                    </imageobject>
                  </mediaobject>
                </figure>
              </listitem>

              <listitem>
                <para><emphasis role="bold">clusters.num
                factor</emphasis></para>

                <para>The clusters.num factor denotes how many clustered jobs
                does the user want to see per level per site. It is specified
                via the use of a PEGASUS namespace profile key 'clusters.num'.
                for e.g. if at a particular level, say 4 jobs referring to
                logical transformation B have been scheduled to a siteX. The
                'clusters.num' factor associated with job B for siteX is say
                3. This will result in 3 clustered jobs, one composed of 2
                jobs and others of a single job each. The clusters.num factor
                in the transformation catalog can be specified as
                follows</para>

                <programlisting># multiple line text-based transformation catalog: 2014-09-30T16:06:23.397-07:00
tr B {
        site siteX {
                profile pegasus "clusters.num" "3" 
                pfn "/shared/PEGASUS/bin/jobB"
                arch "x86"
                os "LINUX"
                type "INSTALLED"
        }
}

tr C {
        site siteX {
                profile pegasus "clusters.num" "2" 
                pfn "/shared/PEGASUS/bin/jobC"
                arch "x86"
                os "LINUX"
                type "INSTALLED"
        }
}

</programlisting>

                <para>In the case, where both the factors are associated with
                the job, the clusters.num value supersedes the clusters.size
                value.</para>

                <programlisting># multiple line text-based transformation catalog: 2014-09-30T16:08:01.537-07:00
tr B {
        site siteX {
                profile pegasus "clusters.num" "3" 
                profile pegasus "clusters.size" "3" 
                pfn "/shared/PEGASUS/bin/jobB"
                arch "x86"
                os "LINUX"
                type "INSTALLED"
        }
}
</programlisting>

                <para>In the above case the jobs referring to logical
                transformation B scheduled on siteX will be clustered on the
                basis of 'clusters.num' value. Hence, if there are 4 jobs
                referring to logical transformation B scheduled to siteX, then
                3 clustered jobs will be created.</para>

                <figure>
                  <title>Clustering by clusters.num</title>

                  <mediaobject>
                    <imageobject role="html">
                      <imagedata align="center" contentdepth="750px"
                                 fileref="images/advanced-clustering-2.png"/>
                    </imageobject>

                    <imageobject role="fo">
                      <imagedata align="center" contentdepth="5in"
                                 fileref="images/advanced-clustering-2.png"/>
                    </imageobject>
                  </mediaobject>
                </figure>
              </listitem>
            </itemizedlist>
          </section>
        </section>

        <section id="runtime_clustering">
          <title>Runtime Clustering</title>

          <para>Workflows often consist of jobs of same type, but have varying
          run times. Two or more instances of the same job, with varying
          inputs can differ significantly in their runtimes. A simple way to
          think about this is running the same program on two distinct input
          sets, where one input is smaller (1 MB) as compared to the other
          which is 10 GB in size. In such a case the two jobs will having
          significantly differing run times. When such jobs are clustered
          using horizontal clustering, the benefits of job clustering may be
          lost if all smaller jobs get clustered together, while the larger
          jobs are clustered together. In such scenarios it would be
          beneficial to be able to cluster jobs together such that all
          clustered jobs have similar runtimes.</para>

          <para>In case of runtime clustering, jobs in the workflow are
          associated with a level. The levels of the workflow are determined
          in the same manner as in horizontal clustering. For each level the
          jobs are grouped by the site on which they have been scheduled by
          the Site Selector. Only jobs of same type (txnamespace, txname,
          txversion) can be clustered into a larger job. To use runtime
          clustering the user needs to set the <emphasis
          role="bold">--cluster</emphasis> option of <emphasis
          role="bold">pegasus-plan to horizontal</emphasis>, and set the
          Pegasus property <emphasis
          role="bold">pegasus.clusterer.preference</emphasis> to <emphasis
          role="bold">Runtime</emphasis>.</para>

          <para>Runtime clustering supports two modes of operation.</para>

          <orderedlist>
            <listitem>
              <para>Clusters jobs together such that the clustered job's
              runtime does not exceed a user specified maxruntime.</para>

              <para>Basic Algorithm of grouping jobs into clusters is as
              follows</para>

              <para><programlisting>// cluster.maxruntime - Is the maximum runtime for which the clustered job should run.
// j.runtime - Is the runtime of the job j.
1. Create a set of jobs of the same type (txnamespace, txname, txversion), and that run on the same site.
2. Sort the jobs in decreasing order of their runtime.
3. For each job j, repeat
  a. If j.runtime &gt; cluster.maxruntime then 
        ignore j.
  // Sum of runtime of jobs already in the bin + j.runtime &lt;= cluster.maxruntime
  b. If j can be added to any existing bin (clustered job) then 
        Add j to bin
     Else
        Add a new bin
        Add job j to newly added bin</programlisting>The runtime of a job, and
              the maximum runtime for which a clustered jobs should run is
              determined by the value of two parameters associated with the
              jobs.</para>

              <para><itemizedlist>
                  <listitem>
                    <para><emphasis role="bold">runtime</emphasis></para>

                    <para>expected runtime for a job</para>
                  </listitem>

                  <listitem>
                    <para><emphasis
                    role="bold">clusters.maxruntime</emphasis></para>

                    <para>maxruntime for the clustered job i.e. Group as many
                    jobs as possible into a cluster, as long as the clustered
                    jobs' runtime does not exceed clusters.maxruntime.</para>
                  </listitem>
                </itemizedlist></para>
            </listitem>

            <listitem>
              <para>Clusters all the into a fixed number of clusters
              (clusters.num), such that the runtimes of the clustered jobs are
              similar.</para>

              <para>Basic Algorithm of grouping jobs into clusters is as
              follows</para>

              <para><programlisting>// cluster.num - Is the number of clustered jobs to create.
// j.runtime - Is the runtime of the job j.
1. Create a set of jobs of the same type (txnamespace, txname, txversion), and that run on the same site.
2. Sort the jobs in decreasing order of their runtime.
3. Create a heap containing clusters.num number of clustered jobs.
4. For each job j, repeat
  a. Get cluster job cj, having the shortest runtime
  b. Add job j to clustered job cj </programlisting></para>

              <para>The runtime of a job, and the number of clustered jobs to
              create is determined by the value of two parameters associated
              with the jobs.</para>

              <itemizedlist>
                <listitem>
                  <para><emphasis role="bold">runtime</emphasis></para>

                  <para>expected runtime for a job</para>
                </listitem>

                <listitem>
                  <para><emphasis role="bold">clusters.num</emphasis></para>

                  <para>clusters.num factor denotes how many clustered jobs
                  does the user want to see per level per site</para>
                </listitem>
              </itemizedlist>
            </listitem>
          </orderedlist>

          <para><note>
              <para>Users should either specify clusters.maxruntime or
              clusters.num. If both of them are specified, then clusters.num
              profile will be ignored by the clustering engine.</para>
            </note></para>

          <para>All of these parameters are specified by the use of a PEGASUS
          namespace profile keys. The keys can be specified at any of the
          placeholders for the profiles (abstract transformation in the DAX,
          site in the site catalog, transformation in the transformation
          catalog). The normal overloading semantics apply i.e. profile in
          transformation catalog overrides the one in the site catalog and
          that in turn overrides the one in the DAX. The two parameters are
          described below.</para>

          <programlisting># multiple line text-based transformation catalog: 2014-09-30T16:09:40.610-07:00
#Cluster all jobs of type B at siteX, into 2 clusters such that the 2 clusters have similar runtimes
tr B {
        site siteX {
                profile pegasus "clusters.num" "2" 
                profile pegasus "runtime" "100" 
                pfn "/shared/PEGASUS/bin/jobB"
                arch "x86"
                os "LINUX"
                type "INSTALLED"
        }
}

#Cluster all jobs of type C at siteX, such that the duration of the clustered job does not exceed 300.
tr C {
        site siteX {
                profile pegasus "clusters.maxruntime" "300" 
                profile pegasus "runtime" "100" 
                pfn "/shared/PEGASUS/bin/jobC"
                arch "x86"
                os "LINUX"
                type "INSTALLED"
        }
}

</programlisting>

          <figure>
            <title>Clustering by runtime</title>

            <mediaobject>
              <imageobject role="html">
                <imagedata align="center" contentdepth="750px"
                           fileref="images/advanced-clustering-5.png"/>
              </imageobject>

              <imageobject role="fo">
                <imagedata align="center" contentdepth="5in"
                           fileref="images/advanced-clustering-5.png"/>
              </imageobject>
            </mediaobject>
          </figure>

          <para>In the above case the jobs referring to logical transformation
          B scheduled on siteX will be clustered such that all clustered jobs
          will run approximately for the same duration specified by the
          clusters.maxruntime property. In the above case we assume all jobs
          referring to transformation B run for 100 seconds. For jobs with
          significantly differing runtime, the runtime property will be
          associated with the jobs in the DAX.</para>

          <para>In addition to the above two profiles, we need to inform
          pegasus-plan to use runtime clustering. This is done by setting the
          following property .</para>

          <programlisting><emphasis role="bold"> pegasus.clusterer.preference          Runtime</emphasis> </programlisting>

          <para/>
        </section>

        <section id="label_clustering">
          <title>Label Clustering</title>

          <para>In label based clustering, the user labels the workflow. All
          jobs having the same label value are clustered into a single
          clustered job. This allows the user to create clusters or use a
          clustering technique that is specific to his workflows. If there is
          no label associated with the job, the job is not clustered and is
          executed as is<figure>
              <title>Label-based clustering</title>

              <mediaobject>
                <imageobject role="html">
                  <imagedata align="center" contentdepth="750px"
                             fileref="images/advanced-clustering-3.png"/>
                </imageobject>

                <imageobject role="fo">
                  <imagedata align="center" contentdepth="5in"
                             fileref="images/advanced-clustering-3.png"/>
                </imageobject>
              </mediaobject>
            </figure></para>

          <para>Since, the jobs in a cluster in this case are not independent,
          it is important the jobs are executed in the correct order. This is
          done by doing a topological sort on the jobs in each cluster. To use
          label based clustering the user needs to set the <emphasis
          role="bold">--cluster</emphasis> option of <emphasis
          role="bold">pegasus-plan</emphasis> to label.</para>

          <section>
            <title>Labelling the Workflow</title>

            <para>The labels for the jobs in the workflow are specified by
            associated <emphasis role="bold">pegasus</emphasis> profile keys
            with the jobs during the DAX generation process. The user can
            choose which profile key to use for labeling the workflow. By
            default, it is assumed that the user is using the PEGASUS profile
            key label to associate the labels. To use another key, in the
            <emphasis role="bold">pegasus</emphasis> namespace the user needs
            to set the following property</para>

            <itemizedlist>
              <listitem>
                <para>pegasus.clusterer.label.key</para>
              </listitem>
            </itemizedlist>

            <para>For example if the user sets <emphasis
            role="bold">pegasus.clusterer.label.key </emphasis>to <emphasis
            role="bold">user_label</emphasis> then the job description in the
            DAX looks as follows</para>

            <programlisting>&lt;adag &gt;
...
  &lt;job id="ID000004" namespace="app" name="analyze" version="1.0" level="1" &gt;
    &lt;argument&gt;-a bottom -T60  -i &lt;filename file="user.f.c1"/&gt;  -o &lt;filename file="user.f.d"/&gt;&lt;/argument&gt;
    &lt;profile namespace="pegasus" key="user_label"&gt;p1&lt;/profile&gt;
    &lt;uses file="user.f.c1" link="input" register="true" transfer="true"/&gt;
    &lt;uses file="user.f.c2" link="input" register="true" transfer="true"/&gt;
    &lt;uses file="user.f.d" link="output" register="true" transfer="true"/&gt;
  &lt;/job&gt;
...
&lt;/adag&gt;</programlisting>

            <itemizedlist>
              <listitem>
                <para>The above states that the <emphasis
                role="bold">pegasus</emphasis> profiles with key as <emphasis
                role="bold">user_label</emphasis> are to be used for
                designating clusters.</para>
              </listitem>

              <listitem>
                <para>Each job with the same value for <emphasis
                role="bold">pegasus</emphasis> profile key <emphasis
                role="bold">user_label </emphasis>appears in the same
                cluster.</para>
              </listitem>
            </itemizedlist>
          </section>
        </section>

        <section>
          <title>Recursive Clustering</title>

          <para>In some cases, a user may want to use a combination of
          clustering techniques. For e.g. a user may want some jobs in the
          workflow to be horizontally clustered and some to be label
          clustered. This can be achieved by specifying a comma separated list
          of clustering techniques to the<emphasis role="bold">
          --cluster</emphasis> option of <emphasis
          role="bold">pegasus-plan</emphasis>. In this case the clustering
          techniques are applied one after the other on the workflow in the
          order specified on the command line.</para>

          <para>For example</para>

          <programlisting>$ <emphasis>pegasus-plan --dax example.dax --dir ./dags --cluster label,horizontal -s siteX --output local --verbose</emphasis></programlisting>

          <figure>
            <title>Recursive clustering</title>

            <mediaobject>
              <imageobject role="html">
                <imagedata align="center" contentdepth="1000px"
                           fileref="images/advanced-clustering-4.png"/>
              </imageobject>

              <imageobject role="fo">
                <imagedata align="center" contentdepth="7in"
                           fileref="images/advanced-clustering-4.png"/>
              </imageobject>
            </mediaobject>
          </figure>
        </section>
      </section>

      <section>
        <title>Execution of the Clustered Job</title>

        <para>The execution of the clustered job on the remote site, involves
        the execution of the smaller constituent jobs either</para>

        <itemizedlist>
          <listitem>
            <para><emphasis role="bold">sequentially on a single node of the
            remote site</emphasis></para>

            <para>The clustered job is executed using <emphasis
            role="bold">pegasus-cluster</emphasis>, a wrapper tool written in
            C that is distributed as part of the PEGASUS. It takes in the jobs
            passed to it, and ends up executing them sequentially on a single
            node. To use pegasus-cluster for executing any clustered job on a
            siteX, there needs to be an entry in the transformation catalog
            for an executable with the logical name seqexec and namespace as
            pegasus.</para>

            <programlisting><emphasis role="bold">#site  transformation   pfn            type                 architecture    profiles</emphasis>

siteX    pegasus::seqexec     /usr/pegasus/bin/pegasus-cluster INSTALLED       INTEL32::LINUX NULL</programlisting>

            <para>If the entry is not specified, Pegasus will attempt create a
            default path on the basis of the environment profile PEGASUS_HOME
            specified in the site catalog for the remote site.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">On multiple nodes of the remote site
            using MPI based task management tool called Pegasus MPI Cluster
            (PMC)</emphasis></para>

            <para>The clustered job is executed using <emphasis
            role="bold">pegasus-mpi-cluster</emphasis>, a wrapper MPI program
            written in C that is distributed as part of the PEGASUS. A PMC job
            consists of a single master process (this process is rank 0 in MPI
            parlance) and several worker processes. These processes follow the
            standard master-worker architecture. The master process manages
            the workflow and assigns workflow tasks to workers for execution.
            The workers execute the tasks and return the results to the
            master. Communication between the master and the workers is
            accomplished using a simple text-based protocol implemented using
            MPI_Send and MPI_Recv. PMC relies on a shared filesystem on the
            remote site to manage the individual tasks stdout and stderr and
            stage it back to the submit host as part of it's own
            stdout/stderr.</para>

            <para>The input format for PMC is a DAG based format similar to
            Condor DAGMan's. PMC follows the dependencies specified in the DAG
            to release the jobs in the right order and executes parallel jobs
            via the workers when possible. The input file for PMC is
            automatically generated by the Pegasus Planner when generating the
            executable workflow. PMC allows for a finer grained control on how
            each task is executed. This can be enabled by associating the
            following pegasus profiles with the jobs in the DAX</para>

            <table>
              <title>Pegasus Profiles that can be associated with jobs in the
              DAX for PMC</title>

              <tgroup cols="2">
                <tbody>
                  <row>
                    <entry><emphasis role="bold">Key</emphasis></entry>

                    <entry><emphasis
                    role="bold">Description</emphasis></entry>
                  </row>

                  <row>
                    <entry>pmc_request_memory</entry>

                    <entry>This key is used to set the -m option for
                    pegasus-mpi-cluster. It specifies the amount of memory in
                    MB that a job requires. This profile is usually set in the
                    DAX for each job.</entry>
                  </row>

                  <row>
                    <entry>pmc_request_cpus</entry>

                    <entry>This key is used to set the -c option for
                    pegasus-mpi-cluster. It specifies the number of cpu's that
                    a job requires. This profile is usually set in the DAX for
                    each job.</entry>
                  </row>

                  <row>
                    <entry>pmc_priority</entry>

                    <entry>This key is used to set the -p option for
                    pegasus-mpi-cluster. It specifies the priority for a job .
                    This profile is usually set in the DAX for each job.
                    Negative values are allowed for priorities.</entry>
                  </row>

                  <row>
                    <entry>pmc_task_arguments</entry>

                    <entry>The key is used to pass any extra arguments to the
                    PMC task during the planning time. They are added to the
                    very end of the argument string constructed for the task
                    in the PMC file. Hence, allows for overriding of any
                    argument constructed by the planner for any particular
                    task in the PMC job.</entry>
                  </row>
                </tbody>
              </tgroup>
            </table>

            <para>Refer to the pegasus-mpi-cluster man page in the <link
            linkend="cli">command line tools chapter</link> to know more about
            PMC and how it schedules individual tasks.</para>

            <para>It is recommended to have a pegasus::mpiexec entry in the
            transformation catalog to specify the path to PMC on the remote
            and specify the relevant globus profiles such as xcount,
            host_xcount and maxwalltime to control size of the MPI job.</para>

            <programlisting># multiple line text-based transformation catalog: 2014-09-30T16:11:11.947-07:00
tr pegasus::mpiexec {
        site siteX {
                profile globus "host_xcount" "1" 
                profile globus "xcount" "32" 
                pfn "/usr/pegasus/bin/pegasus-mpi-cluster"
                arch "x86"
                os "LINUX"
                type "INSTALLED"
        }
}</programlisting>

            <para>the entry is not specified, Pegasus will attempt create a
            default path on the basis of the environment profile PEGASUS_HOME
            specified in the site catalog for the remote site.</para>

            <tip>
              <para>Users are encouraged to use label based clustering in
              conjunction with PMC</para>
            </tip>
          </listitem>
        </itemizedlist>

        <section>
          <title>Specification of Method of Execution for Clustered
          Jobs</title>

          <para>The method execution of the clustered job(whether to launch
          via mpiexec or seqexec) can be specified</para>

          <orderedlist>
            <listitem>
              <para><emphasis role="bold">globally in the properties
              file</emphasis></para>

              <para>The user can set a property in the properties file that
              results in all the clustered jobs of the workflow being executed
              by the same type of executable.</para>

              <programlisting><emphasis role="bold">#PEGASUS PROPERTIES FILE</emphasis>
pegasus.clusterer.job.aggregator seqexec|mpiexec</programlisting>

              <para>In the above example, all the clustered jobs on the remote
              sites are going to be launched via the property value, as long
              as the property value is not overridden in the site
              catalog.</para>
            </listitem>

            <listitem>
              <para><emphasis role="bold">associating profile key
              job.aggregator with the site in the site
              catalog</emphasis></para>

              <programlisting>&lt;site handle="siteX" gridlaunch = "/shared/PEGASUS/bin/kickstart"&gt;
    &lt;profile namespace="env" key="GLOBUS_LOCATION" &gt;/home/shared/globus&lt;/profile&gt;
    &lt;profile namespace="env" key="LD_LIBRARY_PATH"&gt;/home/shared/globus/lib&lt;/profile&gt;
    &lt;profile namespace="pegasus" key="job.aggregator" &gt;seqexec&lt;/profile&gt;
    &lt;lrc url="rls://siteX.edu" /&gt;
    &lt;gridftp  url="gsiftp://siteX.edu/" storage="/home/shared/work" major="2" minor="4" patch="0" /&gt;
    &lt;jobmanager universe="transfer" url="siteX.edu/jobmanager-fork" major="2" minor="4" patch="0" /&gt;
    &lt;jobmanager universe="vanilla" url="siteX.edu/jobmanager-condor" major="2" minor="4" patch="0" /&gt;
    &lt;workdirectory &gt;/home/shared/storage&lt;/workdirectory&gt;
  &lt;/site&gt;</programlisting>

              <para>In the above example, all the clustered jobs on a siteX
              are going to be executed via seqexec, as long as the value is
              not overridden in the transformation catalog.</para>
            </listitem>

            <listitem>
              <para><emphasis role="bold">associating profile key
              job.aggregator with the transformation that is being clustered,
              in the transformation catalog</emphasis></para>

              <programlisting># multiple line text-based transformation catalog: 2014-09-30T16:11:52.230-07:00
tr B {
        site siteX {
                profile pegasus "clusters.size" "3" 
                profile pegasus "job.aggregator" "mpiexec" 
                pfn "/shared/PEGASUS/bin/jobB"
                arch "x86"
                os "LINUX"
                type "INSTALLED"
        }
}
</programlisting>

              <para>In the above example, all the clustered jobs that consist
              of transformation B on siteX will be executed via
              mpiexec.</para>

              <note>
                <para><emphasis role="bold"> The clustering of jobs on a site
                only happens only if </emphasis><itemizedlist>
                    <listitem>
                      <para>there exists an entry in the transformation
                      catalog for the clustering executable that has been
                      determined by the above 3 rules</para>
                    </listitem>

                    <listitem>
                      <para>the number of jobs being clustered on the site are
                      more than 1</para>
                    </listitem>
                  </itemizedlist></para>
              </note>
            </listitem>
          </orderedlist>
        </section>
      </section>

      <section>
        <title>Outstanding Issues</title>

        <orderedlist>
          <listitem>
            <para><emphasis role="bold">Label Clustering</emphasis></para>

            <para>More rigorous checks are required to ensure that the
            labeling scheme applied by the user is valid.</para>
          </listitem>
        </orderedlist>
      </section>
    </section>
  </section>

  <section id="large_workflows">
    <title>How to Scale Large Workflows</title>

    <para><emphasis>Issue:</emphasis> When planning and running large
    workflows, there are some scalability issues to be aware of. During the
    planning stage, Pegasus traverses the graphs multiple times, and some of
    the graph transforms can be slow depending on if the graph is large in the
    number of tasks, the number of files, or the number of dependencies. Once
    planned, large workflows can also see scalability limits when interacting
    with the operating system. A common problem is the number of files in a
    single directory, such as thousands or millons input or output
    files.</para>

    <para><emphasis>Solution:</emphasis> The most common solution to these
    problems is to use <link linkend="hierarchial_workflows">hierarchical
    workflows</link>, which works really well if your workflow can be
    logically partitioned into smaller workflows. A hierarchical workflow
    still runs like a single workflow, with the difference being that some
    jobs in the workflow are actually sub-workflows.</para>

    <para>For workflows with a large number of files, you can control the
    number of files in a single directory by reorganizing the files into a
    deep directory structure.</para>
  </section>

  <section id="hierarchial_workflows">
    <title>Hierarchical Workflows</title>

    <section>
      <title>Introduction</title>

      <para>The Abstract Workflow in addition to containing compute jobs, can
      also contain jobs that refer to other workflows. This is useful for
      running large workflows or ensembles of workflows.</para>

      <para>Users can embed two types of workflow jobs in the DAX</para>

      <orderedlist>
        <listitem>
          <para>daxjob - refers to a sub workflow represented as a DAX. During
          the planning of a workflow, the DAX jobs are mapped to condor dagman
          jobs that have pegasus plan invocation on the dax ( referred to in
          the DAX job ) as the prescript.</para>

          <figure>
            <title>Planning of a DAX Job</title>

            <mediaobject>
              <imageobject>
                <imagedata contentdepth="100%"
                           fileref="./images/daxjob-mapping.png" scalefit="1"
                           width="100%"/>
              </imageobject>
            </mediaobject>
          </figure>
        </listitem>

        <listitem>
          <para>dagjob - refers to a sub workflow represented as a DAG. During
          the planning of a workflow, the DAG jobs are mapped to condor dagman
          and refer to the DAG file mentioned in the DAG job.</para>

          <figure>
            <title>Planning of a DAG Job</title>

            <mediaobject>
              <imageobject>
                <imagedata contentdepth="100%"
                           fileref="./images/dagjob-mapping.png" scalefit="1"
                           width="100%"/>
              </imageobject>
            </mediaobject>
          </figure>
        </listitem>
      </orderedlist>
    </section>

    <section>
      <title>Specifying a DAX Job in the DAX</title>

      <para>Specifying a DAXJob in a DAX is pretty similar to how normal
      compute jobs are specified. There are minor differences in terms of the
      xml element name ( dax vs job ) and the attributes specified. DAXJob XML
      specification is described in detail in the <link linkend="api">chapter
      on DAX API</link> . An example DAX Job in a DAX is shown below</para>

      <programlisting id="dax_job_example" language="">  &lt;dax id="ID000002" name="black.dax" node-label="bar" &gt;
    &lt;profile namespace="dagman" key="maxjobs"&gt;10&lt;/profile&gt;
    &lt;argument&gt;-Xmx1024 -Xms512 -Dpegasus.dir.storage=storagedir  -Dpegasus.dir.exec=execdir -o local -vvvvv --force -s dax_site &lt;/argument&gt;
  &lt;/dax&gt;</programlisting>

      <section>
        <title>DAX File Locations</title>

        <para>The name attribute in the dax element refers to the LFN (
        Logical File Name ) of the dax file. The location of the DAX file can
        be catalogued either in the</para>

        <para><orderedlist>
            <listitem>
              <para>Replica Catalog</para>
            </listitem>

            <listitem>
              <para>Replica Catalog Section in the <link
              linkend="dax_replica_catalog">DAX</link> .</para>

              <note>
                <para>Currently, only file url's on the local site ( submit
                host ) can be specified as DAX file locations.</para>
              </note>
            </listitem>
          </orderedlist></para>
      </section>

      <section>
        <title>Arguments for a DAX Job</title>

        <para>Users can specify specific arguments to the DAX Jobs. The
        arguments specified for the DAX Jobs are passed to the pegasus-plan
        invocation in the prescript for the corresponding condor dagman job in
        the executable workflow.</para>

        <para>The following options for pegasus-plan are inherited from the
        pegasus-plan invocation of the parent workflow. If an option is
        specified in the arguments section for the DAX Job then that overrides
        what is inherited.</para>

        <table>
          <title>Options inherited from parent workflow</title>

          <tgroup cols="2">
            <thead>
              <row>
                <entry>Option Name</entry>

                <entry>Description</entry>
              </row>
            </thead>

            <tbody>
              <row>
                <entry>--sites</entry>

                <entry>list of execution sites.</entry>
              </row>
            </tbody>
          </tgroup>
        </table>

        <para>It is highly recommended that users <emphasis role="bold">don't
        specify</emphasis> directory related options in the arguments section
        for the DAX Jobs. Pegasus assigns values to these options for the sub
        workflows automatically.</para>

        <orderedlist>
          <listitem>
            <para>--relative-dir</para>
          </listitem>

          <listitem>
            <para>--dir</para>
          </listitem>

          <listitem>
            <para>--relative-submit-dir</para>
          </listitem>
        </orderedlist>
      </section>

      <section>
        <title>Profiles for DAX Job</title>

        <para>Users can choose to specify dagman profiles with the DAX Job to
        control the behavior of the corresponding condor dagman instance in
        the executable workflow. In the example <link
        linkend="dax_job_example">above</link> maxjobs is set to 10 for the
        sub workflow.</para>
      </section>

      <section>
        <title>Execution of the PRE script and Condor DAGMan instance</title>

        <para>The pegasus plan that is invoked as part of the prescript to the
        condor dagman job is executed on the submit host. The log from the
        output of pegasus plan is redirected to a file ( ending with suffix
        pre.log ) in the submit directory of the workflow that contains the
        DAX Job. The path to pegasus-plan is automatically determined.</para>

        <para>The DAX Job maps to a Condor DAGMan job. The path to condor
        dagman binary is determined according to the following rules -</para>

        <orderedlist>
          <listitem>
            <para>entry in the transformation catalog for condor::dagman for
            site local, else</para>
          </listitem>

          <listitem>
            <para>pick up the value of CONDOR_HOME from the environment if
            specified and set path to condor dagman as
            $CONDOR_HOME/bin/condor_dagman , else</para>
          </listitem>

          <listitem>
            <para>pick up the value of CONDOR_LOCATION from the environment if
            specified and set path to condor dagman as
            $CONDOR_LOCATION/bin/condor_dagman , else</para>
          </listitem>

          <listitem>
            <para>pick up the path to condor dagman from what is defined in
            the user's PATH</para>
          </listitem>
        </orderedlist>

        <tip>
          <para>It is recommended that users specify dagman.maxpre in their
          properties file to control the maximum number of pegasus plan
          instances launched by each running dagman instance.</para>
        </tip>
      </section>
    </section>

    <section>
      <title>Specifying a DAG Job in the DAX</title>

      <para>Specifying a DAGJob in a DAX is pretty similar to how normal
      compute jobs are specified. There are minor differences in terms of the
      xml element name ( dag vs job ) and the attributes specified. For DAGJob
      XML details,see the <link linkend="api"> API Reference </link> chapter .
      An example DAG Job in a DAX is shown below</para>

      <programlisting id="dag_job_example">  &lt;dag id="ID000003" name="black.dag" node-label="foo" &gt;
    &lt;profile namespace="dagman" key="maxjobs"&gt;10&lt;/profile&gt;
    &lt;profile namespace="dagman" key="DIR"&gt;/dag-dir/test&lt;/profile&gt;
  &lt;/dag&gt;</programlisting>

      <section>
        <title>DAG File Locations</title>

        <para>The name attribute in the dag element refers to the LFN (
        Logical File Name ) of the dax file. The location of the DAX file can
        be catalogued either in the</para>

        <para><orderedlist>
            <listitem>
              <para>Replica Catalog</para>
            </listitem>

            <listitem>
              <para>Replica Catalog Section in the DAX.</para>

              <note>
                <para>Currently, only file url's on the local site ( submit
                host ) can be specified as DAG file locations.</para>
              </note>
            </listitem>
          </orderedlist></para>
      </section>

      <section>
        <title>Profiles for DAG Job</title>

        <para>Users can choose to specify dagman profiles with the DAX Job to
        control the behavior of the corresponding condor dagman instance in
        the executable workflow. In the example above, maxjobs is set to 10
        for the sub workflow.</para>

        <para>The dagman profile DIR allows users to specify the directory in
        which they want the condor dagman instance to execute. In the example
        <link linkend="dag_job_example">above</link> black.dag is set to be
        executed in directory /dag-dir/test . The /dag-dir/test should be
        created beforehand.</para>
      </section>
    </section>

    <section>
      <title>File Dependencies Across DAX Jobs</title>

      <para>In hierarchal workflows , if a sub workflow generates some output
      files required by another sub workflow then there should be an edge
      connecting the two dax jobs. Pegasus will ensure that the prescript for
      the child sub-workflow, has the path to the cache file generated during
      the planning of the parent sub workflow. The cache file in the submit
      directory for a workflow is a textual replica catalog that lists the
      locations of all the output files created in the remote workflow
      execution directory when the workflow executes.</para>

      <para>This automatic passing of the cache file to a child sub-workflow
      ensures that the datasets from the same workflow run are used. However,
      the passing the locations in a cache file also ensures that Pegasus will
      prefer them over all other locations in the Replica Catalog. If you need
      the Replica Selection to consider locations in the Replica Catalog also,
      then set the following property.</para>

      <programlisting><emphasis role="bold">pegasus.catalog.replica.cache.asrc  true</emphasis></programlisting>

      <para>The above is useful in the case, where you are staging out the
      output files to a storage site, and you want the child sub workflow to
      stage these files from the storage output site instead of the workflow
      execution directory where the files were originally created.</para>
    </section>

    <section>
      <title>Recursion in Hierarchal Workflows</title>

      <para>It is possible for a user to add a dax jobs to a dax that already
      contain dax jobs in them. Pegasus does not place a limit on how many
      levels of recursion a user can have in their workflows. From Pegasus
      perspective recursion in hierarchal workflows ends when a DAX with only
      compute jobs is encountered . However, the levels of recursion are
      limited by the system resources consumed by the DAGMan processes that
      are running (each level of nesting produces another DAGMan process)
      .</para>

      <para>The figure below illustrates an example with recursion 2 levels
      deep.</para>

      <figure>
        <title>Recursion in Hierarchal Workflows</title>

        <mediaobject>
          <imageobject role="html">
            <imagedata align="center" contentdepth="500px"
                       fileref="./images/recursion_in_hierarchal_workflows.png"/>
          </imageobject>

          <imageobject role="fo">
            <imagedata align="center" contentdepth="5in"
                       fileref="./images/recursion_in_hierarchal_workflows.png"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>The execution time-line of the various jobs in the above figure is
      illustrated below.</para>

      <figure>
        <title>Execution Time-line for Hierarchal Workflows</title>

        <mediaobject>
          <imageobject role="html">
            <imagedata align="center" contentdepth="500px"
                       fileref="./images/hierarchal_workflows_execution_timeline.png"/>
          </imageobject>

          <imageobject role="fo">
            <imagedata align="center" contentdepth="5in"
                       fileref="./images/hierarchal_workflows_execution_timeline.png"/>
          </imageobject>
        </mediaobject>
      </figure>
    </section>

    <section>
      <title>Example</title>

      <para>The Galactic Plane workflow is a Hierarchical workflow of many
      Montage workflows. For details, see <link
      linkend="example_workflows">Workflow of Workflows</link>.</para>
    </section>
  </section>

  <section id="data_transfers">
    <title>Optimizing Data Transfers</title>

    <para><emphasis>Issue:</emphasis> When it comes to data transfers, Pegasus
    ships with a default configuration which is trying to strike a balance
    between performance and aggressiveness. We obviously want data transfers
    to be as quick as possibly, but we also do not want our transfers to
    overwhelm data services and systems.</para>

    <para><emphasis>Solution:</emphasis> Starting 4.8.0 release, the default
    configuration of Pegasus now adds transfer jobs and cleanup jobs based on
    the number of jobs at a particular level of the workflow. For example, for
    every 10 compute jobs on a level of a workflow, one data transfer job(
    stage-in and stage-out) is created. The default configuration also sets
    how many threads such a pegasus-transfer job can spawn. Cleanup jobs are
    similarly constructed with an internal ratio of 5.</para>

    <para>Additionally, Pegasus makes use of DAGMan categories and associates
    the following default values with the transfer and cleanup
    jobs.
    <xi:include xpointer="table-default-job-categories"
    xmlns:xi="http://www.w3.org/2001/XInclude"/>
    </para>
    

    <para>Information on how to control manully the maxinum number of stagein
    and stageout jobs can be found in the <link
    linkend="data_movement_nodes">Data Movement Nodes</link> section.</para>

    <para>How to control the number of threads pegasus-transfer can use
    depends on if you want to control standard transfer jobs, or PegasusLite.
    For the former, see the <link
    linkend="transfer_props">pegasus.transfer.threads</link> property, and for
    the latter the <link
    linkend="transfer_props">pegasus.transfer.lite.threads</link>
    property.</para>
  </section>

  <section id="job_throttling">
    <title>Job Throttling</title>

    <para><emphasis>Issue:</emphasis> For large workflows you may want to
    control the number of jobs released by DAGMan in local condor queue, or
    number of remote jobs submitted.</para>

    <para><emphasis>Solution:</emphasis> HTCondor DAGMan has knobs that can be
    tuned at a per workflow level to control it's behavior. These knobs
    control how it interacts with the local HTCondor Schedd to which it
    submits jobs that are ready to run in a particular DAG. These knobs are
    exposed as<link linkend="dagman_profiles"> DAGMan profiles</link>
    (maxidle, maxjobs, maxpre and maxpost) that you can set in your properties
    files. <xi:include href="reference_configuration.xml"
    xpointer="dagman_throttling_profiles"
    xmlns:xi="http://www.w3.org/2001/XInclude"/></para>

    <para>Within a single workflow, you can also control the number of jobs
    submitted per type ( or category ) of jobs. To associate categories, you
    needs to associate dagman profile key named category with the jobs and
    specify the property dagman.[CATEGORY-NAME].* in the properties file. More
    information about HTCondor DAGMan categories can be found in the <ulink
    url="http://research.cs.wisc.edu/htcondor/manual/v8.3.5/2_10DAGMan_Applications.html#SECTION003108400000000000000">HTCondor
    Documentation</ulink>.</para>

    <para>By default, pegasus associates default category names to following
    types of auxillary jobs</para>

    <table id="table-default-job-categories">
      <title>Default Category names associated by Pegasus</title>

      <tgroup cols="3">
        <tbody>
          <row>
            <entry><emphasis role="bold">DAGMan Category
            Name</emphasis></entry>

            <entry><emphasis role="bold">Auxillary Job applied
            to.</emphasis></entry>

            <entry><emphasis role="bold">Default Value Assigned in generated
            DAG file</emphasis></entry>
          </row>

          <row>
            <entry><literallayout>stage-in </literallayout></entry>

            <entry>data stage-in jobs</entry>

            <entry>10</entry>
          </row>

          <row>
            <entry><literallayout>stage-out</literallayout></entry>

            <entry>data stage-out jobs</entry>

            <entry>10</entry>
          </row>

          <row>
            <entry><literallayout>stage-inter</literallayout></entry>

            <entry>inter site data transfer jobs</entry>

            <entry>-</entry>
          </row>

          <row>
            <entry><literallayout>cleanup</literallayout></entry>

            <entry>data cleanup jobs</entry>

            <entry>4</entry>
          </row>

          <row>
            <entry><literallayout>registration </literallayout></entry>

            <entry>registration jobs</entry>

            <entry>1 (for file based RC)</entry>
          </row>
        </tbody>
      </tgroup>
    </table>

    <para>Below is a sample properties file that illustrates how categories
    can be specified in the properties file</para>

    <programlisting># pegasus properties file snippet illustrating 
# how to specify dagman categories for different types of jobs

dagman.stage-in.maxjobs 4
dagman.stage-out.maxjobs 1
dagman.cleanup.maxjobs 2

</programlisting>

    <para>HTCondor also exposes useful configuration parameters that can be
    specified in it's configuration file (condor_config_val -conf will list
    the condor configuration files), to control job submission across
    workflows. Some of the useful parameters that you may want to tune
    are</para>

    <table id="condor_throttling_variables">
      <title>Useful HTCondor Job Throttling Configuration Parameters</title>

      <tgroup cols="2">
        <tbody>
          <row>
            <entry><emphasis role="bold">HTCondor Configuration
            Parameter</emphasis></entry>

            <entry><emphasis role="bold">Description</emphasis></entry>
          </row>

          <row>
            <entry><literallayout><emphasis role="bold"><emphasis role="bold">Parameter Name: </emphasis></emphasis>START_LOCAL_UNIVERSE<emphasis
                  role="bold"><emphasis role="bold">
Sample Value  : </emphasis></emphasis>TotalLocalJobsRunning &lt; 20<emphasis
                  role="bold">
</emphasis></literallayout></entry>

            <entry>Most of the pegauss added auxillary jobs ( createdir,
            cleanup, registration and data cleanup ) run in the local universe
            on the submit host. If you have a lot of workflows running,
            HTCondor may try to start too many local universe jobs, that may
            bring down your submit host. This global parameter is used to
            configure condor to not launch too many local universe
            jobs.</entry>
          </row>

          <row>
            <entry><literallayout><emphasis role="bold"><emphasis role="bold">Parameter Name: </emphasis></emphasis>GRIDMANAGER_MAX_JOBMANAGERS_PER_RESOURCE<emphasis
                  role="bold"><emphasis role="bold">
Sample Value  : </emphasis></emphasis>Integer<emphasis role="bold">
</emphasis></literallayout></entry>

            <entry>For grid jobs of type gt2, limits the number of
            globus-job-manager processes that the condor_gridmanager lets run
            at a time on the remote head node. Allowing too many
            globus-job-managers to run causes severe load on the head note,
            possibly making it non-functional. Usually the default value in
            htcondor ( as of version 8.3.5) is 10.<para>This parameter is
            useful when you are doing remote job submissions using HTCondor-G.
            </para></entry>
          </row>

          <row>
            <entry><literallayout><emphasis role="bold"><emphasis role="bold">Parameter Name: </emphasis></emphasis>GRIDMANAGER_MAX_SUBMITTED_JOBS_PER_RESOURCE<emphasis
                  role="bold"><emphasis role="bold">
Sample Value  : </emphasis></emphasis> Integer<emphasis role="bold">
</emphasis></literallayout></entry>

            <entry>An integer value that limits the number of jobs that a
            condor_gridmanager daemon will submit to a resource. A
            comma-separated list of pairs that follows this integer limit will
            specify limits for specific remote resources. Each pair is a host
            name and the job limit for that host. Consider the example
            <programlisting>GRIDMANAGER_MAX_SUBMITTED_JOBS_PER_RESOURCE = 
                         200, foo.edu, 50, bar.com, 100.</programlisting><para>
            In this example, all resources have a job limit of 200, except
            foo.edu, which has a limit of 50, and bar.com, which has a limit
            of 100. Limits specific to grid types can be set by appending the
            name of the grid type to the configuration variable name, as the
            example GRIDMANAGER_MAX_SUBMITTED_JOBS_PER_RESOURCE_CREAM = 300 In
            this example, the job limit for all CREAM resources is 300.
            Defaults to 1000 ( as of version 8.3.5).</para><para>This
            parameter is useful when you are doing remote job submissions
            using HTCondor-G.</para></entry>
          </row>
        </tbody>
      </tgroup>
    </table>

    <section id="job_throttling_across_workflows">
      <title>Job Throttling Across Workflows</title>

      <para><emphasis>Issue:</emphasis> DAGMan throttling knobs are per
      workflow, and don't work across workflows. Is there any way to control
      different types of jobs run at a time across workflows?</para>

      <para><emphasis>Solution:</emphasis> While not possible in all cases, it
      is possible to throttle different types of jobs across workflows if you
      configure the jobs to run in vanilla universe by leverage <ulink
      url="http://research.cs.wisc.edu/htcondor/manual/v8.2/3_12Setting_Up.html#SECTION0041215000000000000000">HTCondor
      concurrency limits</ulink>. Most of the Pegasus generated jobs ( data
      transfer jobs and auxillary jobs such as create dir, cleanup and
      registration) execute in local universe where concurrency limits don't
      work. To use this you need to do the following</para>

      <orderedlist>
        <listitem>
          <para>Get the local universe jobs to run locally in vanilla
          universe. You can do this by associating condor profiles universe
          and requirements in the site catalog for local site or individually
          in the transformation catalog for each pegasus executable. Here is
          an example local site catalog entry.</para>

          <programlisting> &lt;site handle="local" arch="x86_64" os="LINUX"&gt;
      &lt;directory type="shared-scratch" path="/shared-scratch/local"&gt;
         &lt;file-server operation="all" url="file:///shared-scratch/local"/&gt;
      &lt;/directory&gt;
      &lt;directory type="local-storage" path="/storage/local"&gt;
         &lt;file-server operation="all" url="file:///storage/local"/&gt;
      &lt;/directory&gt;

      &lt;!-- keys to make jobs scheduled to local site run on local site in vanilla universe --&gt;
      &lt;profile namespace="condor" key="universe"&gt;vanilla&lt;/profile&gt;
      &lt;profile namespace="condor" key="requirements"&gt;(Machine=="submit.example.com")&lt;/profile&gt;
   &lt;/site&gt;
</programlisting>

          <para>Replace the Machine value in requirements with the hostname of
          your submit host.</para>
        </listitem>

        <listitem>
          <para>Copy condor_config.pegasus file from share/pegasus/htcondor
          directory to your condor config.d directory.</para>
        </listitem>
      </orderedlist>

      <para>Starting Pegasus 4.5.1 release, the following values for
      concurrency limits can be associated with different types of jobs
      Pegasus creates. To enable the generation of concurrency limits with the
      jobs set the following property in your properties file.</para>

      <programlisting>pegasus.condor.concurrency.limits   true</programlisting>

      <table id="pegasus_concurrency_limits_mapping">
        <title>Pegasus Job Types To Condor Concurrency Limits</title>

        <tgroup cols="2">
          <tbody>
            <row>
              <entry><emphasis role="bold">Pegasus Job Type</emphasis></entry>

              <entry><emphasis role="bold">HTCondor Concurrency Limit
              Compatible with distributed condor_config.pegasus
              </emphasis></entry>
            </row>

            <row>
              <entry><literallayout>Data Stagein Job<emphasis role="bold">
</emphasis></literallayout></entry>

              <entry>pegasus_transfer.stagein</entry>
            </row>

            <row>
              <entry><literallayout>Data Stageout Job<emphasis role="bold">
</emphasis></literallayout></entry>

              <entry>pegasus_transfer.stageout</entry>
            </row>

            <row>
              <entry><literallayout>Inter Site Data Transfer Job<emphasis
                    role="bold">
</emphasis></literallayout></entry>

              <entry>pegasus_transfer.inter</entry>
            </row>

            <row>
              <entry><literallayout>Worker Pacakge Staging Job</literallayout></entry>

              <entry>pegasus_transfer.worker</entry>
            </row>

            <row>
              <entry><literallayout>Create Directory Job<emphasis role="bold">
</emphasis></literallayout></entry>

              <entry>pegasus_auxillary.createdir</entry>
            </row>

            <row>
              <entry><literallayout>Data Cleanup Job<emphasis role="bold">
</emphasis></literallayout></entry>

              <entry>pegasus_auxillary.cleanup</entry>
            </row>

            <row>
              <entry><literallayout>Replica Registration Job<emphasis
                    role="bold">
</emphasis></literallayout></entry>

              <entry>pegasus_auxillary.registration</entry>
            </row>

            <row>
              <entry><literallayout>Set XBit Job<emphasis role="bold">
</emphasis></literallayout></entry>

              <entry>pegasus_auxillary.chmod</entry>
            </row>

            <row>
              <entry><literallayout>User Compute Job<emphasis role="bold">
</emphasis></literallayout></entry>

              <entry>pegasus_compute</entry>
            </row>
          </tbody>
        </tgroup>
      </table>

      <note>
        <para>It is not recommended to set limit for compute jobs unless you
        know what you are doing.</para>
      </note>
    </section>
  </section>
</chapter>
