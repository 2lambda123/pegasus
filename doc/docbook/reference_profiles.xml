<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE section PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN"
"http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<section id="profiles">
  <title>Configuration</title>

  <para>Pegasus has configuration options to configure</para>

  <orderedlist>
    <listitem>
      <para>the behaviour of an individual job via <emphasis
      role="bold">profiles</emphasis></para>
    </listitem>

    <listitem>
      <para>the behavior of the whole system via <emphasis
      role="bold">properties</emphasis></para>
    </listitem>
  </orderedlist>

  <para>For job level configuration ( such as what environment a job is set
  with ), the Pegasus Workflow Mapper uses the concept of profiles. Profiles
  encapsulate configurations for various aspects of dealing with the Grid
  infrastructure. They provide an abstract yet uniform interface to specify
  configuration options for various layers from planner/mapper behavior to
  remote environment settings. At various stages during the mapping process,
  profiles may be added associated with the job. The system supports five
  diffferent namespaces, with each namespace refers to a different aspect of a
  job's runtime settings. A profile's representation in the executable
  workflow (e.g. the Condor submit files) depends on its namespace. Pegasus
  supports the following Namespaces for profiles:</para>

  <itemizedlist>
    <listitem>
      <para><emphasis role="bold">env</emphasis> permits remote environment
      variables to be set.</para>
    </listitem>

    <listitem>
      <para><emphasis role="bold">globus</emphasis> sets Globus RSL
      parameters.</para>
    </listitem>

    <listitem>
      <para><emphasis role="bold">condor</emphasis> sets Condor configuration
      parameters for the submit file.</para>
    </listitem>

    <listitem>
      <para><emphasis role="bold">dagman</emphasis> introduces Condor DAGMan
      configuration parameters.</para>
    </listitem>

    <listitem>
      <para><emphasis role="bold">pegasus</emphasis> configures the behaviour
      of various planner/mapper components.</para>
    </listitem>
  </itemizedlist>

  <para/>

  <para>Properties are primarily used to configure the behavior of the Pegasus
  WMS system at a global level. The properties file is actually a java
  properties file and follows the same conventions as that to specify the
  properties.</para>

  <para>This chapter describes various types of profiles and properties,
  levels of priorities for intersecting profiles, and how to specify profiles
  in different contexts.</para>

  <section>
    <title>Differences between Profiles and Properties</title>

    <para>The main difference between properties and profiles is that profiles
    eventually get associated at a per job level in the workflow. On the other
    hand, properties are a way of configuring and controlling the behavior of
    the whole system. While all profiles can be specified in the properties
    file, not all properties can be used as profiles. This section lists out
    the properties supported by Pegasus and if any can be used as a profile,
    it is clearly indicated.</para>
  </section>

  <section>
    <title>Profile Structure Heading</title>

    <para>All profiles are triples comprised of a namespace, a name or key,
    and a value. The namespace is a simple identifier. The key has only
    meaning within its namespace, and it's yet another identifier. There are
    no constraints on the contents of a value</para>

    <para>Profiles may be represented with different syntaxes in different
    context. However, each syntax will describe the underlying triple.</para>
  </section>

  <section>
    <title>Sources for Profiles</title>

    <para>Profiles may enter the job-processing stream at various stages.
    Depending on the requirements and scope a profile is to apply, profiles
    can be associated at</para>

    <itemizedlist>
      <listitem>
        <para>as user property settings.</para>
      </listitem>

      <listitem>
        <para>dax level</para>
      </listitem>

      <listitem>
        <para>in the site catalog</para>
      </listitem>

      <listitem>
        <para>in the transformation catalog</para>
      </listitem>
    </itemizedlist>

    <para>Unfortunately, a different syntax applies to each level and context.
    This section shows the different profile sources and syntaxes. However, at
    the foundation of each profile lies the triple of namespace, key and
    value.</para>

    <section>
      <title>User Profiles in Properties</title>

      <para>Users can specify all profiles in the properties files where the
      property name is <emphasis role="bold">[namespace].key</emphasis> and
      <emphasis role="bold">value</emphasis> of the property is the value of
      the profile.</para>

      <para>Namespace can be env|condor|globus|dagman|pegasus</para>

      <para>Any profile specified as a property applies to the whole workflow
      i.e (all jobs in the workflow) unless overridden at the DAX level , Site
      Catalog , Transformation Catalog Level.</para>

      <para>Some profiles that they can be set in the properties file are
      listed below</para>

      <programlisting>env.JAVA_HOME "/software/bin/java"

condor.periodic_release 5
condor.periodic_remove  my_own_expression
condor.stream_error true
condor.stream_output fa

globus.maxwalltime  1000
globus.maxtime      900
globus.maxcputime   10
globus.project      test_project
globus.queue        main_queue

dagman.post.arguments --test arguments
dagman.retry  4
dagman.post simple_exitcode
dagman.post.path.simple_exitcode  /bin/exitcode/exitcode.sh
dagman.post.scope all
dagman.maxpre  12
dagman.priority 13

dagman.bigjobs.maxjobs 1


pegasus.clusters.size 5

pegasus.stagein.clusters 3</programlisting>
    </section>

    <section>
      <title>Profiles in DAX</title>

      <para>The user can associate profiles with logical transformations in
      DAX. Environment settings required by a job's application, or a maximum
      estimate on the run-time are examples for profiles at this stage.</para>

      <programlisting>&lt;job id="ID000001" namespace="asdf" name="preprocess" version="1.0"
 level="3" dv-namespace="voeckler" dv-name="top" dv-version="1.0"&gt;
  &lt;argument&gt;-a top -T10  -i &lt;filename file="voeckler.f.a"/&gt;
 -o &lt;filename file="voeckler.f.b1"/&gt;
 &lt;filename file="voeckler.f.b2"/&gt;&lt;/argument&gt;
  <emphasis role="bold">&lt;profile namespace="pegasus" key="walltime"&gt;2&lt;/profile&gt;
  &lt;profile namespace="pegasus" key="diskspace"&gt;1&lt;/profile&gt;</emphasis>
  &amp;mldr;
&lt;/job&gt;
</programlisting>
    </section>

    <section>
      <title>Profiles in Site Catalog</title>

      <para>If it becomes necessary to limit the scope of a profile to a
      single site, these profiles should go into the site catalog. A profile
      in the site catalog applies to all jobs and all application run at the
      site. Commonly, site catalog profiles set environment settings like the
      LD_LIBRARY_PATH, or globus rsl parameters like queue and project
      names.</para>

      <para>Currently, there is no tool to manipulate the site catalog, e.g.
      by adding profiles. Modifying the site catalog requires that you load it
      into your editor.</para>

      <para>The XML version of the site catalog uses the following
      syntax:</para>

      <programlisting><emphasis role="bold">&lt;profile namespace=</emphasis>"<emphasis>namespace</emphasis>" <emphasis
          role="bold">key=</emphasis>"<emphasis>key</emphasis>"&gt;<emphasis>value</emphasis><emphasis
          role="bold">&lt;/profile&gt;</emphasis></programlisting>

      <para>The XML schema requires that profiles are the first children of a
      pool element. If the element ordering is wrong, the XML parser will
      produce errors and warnings:</para>

      <programlisting>&lt;pool handle="isi_condor" gridlaunch="/home/shared/pegasus/bin/kickstart"&gt;
  <emphasis role="bold">&lt;profile namespace="env"
   key="GLOBUS_LOCATION"&gt;/home/shared/globus/&lt;/profile&gt;
  &lt;profile namespace="env"
   key="LD_LIBRARY_PATH" &gt;/home/shared/globus/lib&lt;/profile&gt;</emphasis>
  &lt;lrc url="rls://sukhna.isi.edu" /&gt;
  &amp;mldr;
&lt;/pool&gt;
</programlisting>

      <para>The multi-line textual version of the site catalog uses the
      following syntax:</para>

      <programlisting><emphasis role="bold">profile</emphasis> <emphasis>namespace "key" "value"</emphasis></programlisting>

      <para>The order within the textual pool definition is not important.
      Profiles can appear anywhere:</para>

      <programlisting>pool isi_condor {
  gridlaunch "/home/shared/pegasus/bin/kickstart"
  <emphasis role="bold">profile env "GLOBUS_LOCATION" "/home/shared/globus"
  profile env "LD_LIBRARY_PATH" "/home/shared/globus/lib"</emphasis>
  &amp;mldr;
}
</programlisting>
    </section>

    <section>
      <title>Profiles in Transformation Catalog</title>

      <para>Some profiles require a narrower scope than the site catalog
      offers. Some profiles only apply to certain applications on certain
      sites, or change with each application and site. Transformation-specific
      and CPU-specific environment variables, or job clustering profiles are
      good candidates. Such profiles are best specified in the transformation
      catalog.</para>

      <para>Profiles associate with a physical transformation and site in the
      transformation catalog. The Database version of the transformation
      catalog also permits the convenience of connecting a transformation with
      a profile.</para>

      <para>The Pegasus tc-client tool is a convenient helper to associate
      profiles with transformation catalog entries. As benefit, the user does
      not have to worry about formats of profiles in the various
      transformation catalog instances.</para>

      <programlisting>tc-client -a -P -E -p /home/shared/executables/analyze -t INSTALLED -r isi_condor -e env::GLOBUS_LOCATION=&amp;rdquor;/home/shared/globus&amp;rdquor;</programlisting>

      <para>The above example adds an environment variable GLOBUS_LOCATION to
      the application /home/shared/executables/analyze on site isi_condor. The
      transformation catalog guide has more details on the usage of the
      tc-client.</para>
    </section>
  </section>

  <section>
    <title>Profiles Conflict Resolution</title>

    <para>Irrespective of where the profiles are specified, eventually the
    profiles are associated with jobs. Multiple sources may specify the same
    profile for the same job. For instance, DAX may specify an environment
    variable X. The site catalog may also specify an environment variable X
    for the chosen site. The transformation catalog may specify an environment
    variable X for the chosen site and application. When the job is
    concretized, these three conflicts need to be resolved.</para>

    <para>Pegasus defines a priority ordering of profiles. The higher priority
    takes precedence (overwrites) a profile of a lower priority.</para>

    <orderedlist>
      <listitem>
        <para>Transformation Catalog Profiles</para>
      </listitem>

      <listitem>
        <para>Site Catalog Profiles</para>
      </listitem>

      <listitem>
        <para>DAX Profiles</para>
      </listitem>

      <listitem>
        <para>Profiles in Properties</para>
      </listitem>
    </orderedlist>
  </section>

  <section>
    <title>Details of Profile Handling</title>

    <para>The previous sections omitted some of the finer details for the sake
    of clarity. To understand some of the constraints that Pegasus imposes, it
    is required to look at the way profiles affect jobs.</para>

    <section>
      <title>Details of env Profiles</title>

      <para>Profiles in the env namespace are translated to a
      semicolon-separated list of key-value pairs. The list becomes the
      argument for the Condor environment command in the job's submit
      file.</para>

      <programlisting>######################################################################
# Pegasus WMS  SUBMIT FILE GENERATOR
# DAG : black-diamond, Index = 0, Count = 1
# SUBMIT FILE NAME : findrange_ID000002.sub
######################################################################
globusrsl = (jobtype=single)
<emphasis role="bold">environment=GLOBUS_LOCATION=/shared/globus;LD_LIBRARY_PATH=/shared/globus/lib;</emphasis>
executable = /shared/software/linux/pegasus/default/bin/kickstart
globusscheduler = columbus.isi.edu/jobmanager-condor
remote_initialdir = /shared/CONDOR/workdir/isi_hourglass
universe = globus
&amp;mldr;
queue
######################################################################
# END OF SUBMIT FILE
</programlisting>

      <para>Condor-G, in turn, will translate the
      <emphasis>environment</emphasis> command for any remote job into Globus
      RSL environment settings, and append them to any existing RSL syntax it
      generates. To permit proper mixing, all <emphasis>environment</emphasis>
      setting should solely use the env profiles, and none of the Condor nor
      Globus environment settings.</para>

      <para>If <emphasis>kickstart</emphasis> starts a job, it may make use of
      environment variables in its executable and arguments setting.</para>
    </section>

    <section>
      <title>Details of globus Profiles</title>

      <para>Profiles in the <emphasis>globus</emphasis> Namespaces are
      translated into a list of paranthesis-enclosed equal-separated key-value
      pairs. The list becomes the value for the Condor
      <emphasis>globusrsl</emphasis> setting in the job's submit file:</para>

      <programlisting>######################################################################
# Pegasus WMS SUBMIT FILE GENERATOR
# DAG : black-diamond, Index = 0, Count = 1
# SUBMIT FILE NAME : findrange_ID000002.sub
######################################################################
<emphasis role="bold">globusrsl = (jobtype=single)(queue=fast)(project=nvo)</emphasis>
executable = /shared/software/linux/pegasus/default/bin/kickstart
globusscheduler = columbus.isi.edu/jobmanager-condor
remote_initialdir = /shared/CONDOR/workdir/isi_hourglass
universe = globus
&amp;mldr;
queue
######################################################################
# END OF SUBMIT FILE
</programlisting>

      <para>For this reason, Pegasus prohibits the use of the
      <emphasis>globusrsl</emphasis> key in the <emphasis>condor</emphasis>
      profile namespace.</para>
    </section>
  </section>

  <section>
    <title>The env Profile Namespace</title>

    <para>The <emphasis>env</emphasis> namespace allows users to specify
    environment variables of remote jobs. Globus transports the environment
    variables, and ensure that they are set before the job starts.</para>

    <para>The key used in conjunction with an <emphasis>env</emphasis> profile
    denotes the name of the environment variable. The value of the profile
    becomes the value of the remote environment variable.</para>

    <para>Grid jobs usually only set a minimum of environment variables by
    virtue of Globus. You cannot compare the environment variables visible
    from an interactive login with those visible to a grid job. Thus, it often
    becomes necessary to set environment variables like LD_LIBRARY_PATH for
    remote jobs.</para>

    <para>If you use any of the Pegasus worker package tools like transfer or
    the rc-client, it becomes necessary to set PEGASUS_HOME and
    GLOBUS_LOCATION even for jobs that run locally</para>

    <table>
      <title>Useful Environment Settings</title>

      <tgroup cols="4">
        <tbody>
          <row>
            <entry><emphasis role="bold">Property Key </emphasis></entry>

            <entry><emphasis role="bold">Corresponding Profile Key
            </emphasis></entry>

            <entry><emphasis role="bold">Where can it be
            specified</emphasis></entry>

            <entry><emphasis role="bold">Description</emphasis></entry>
          </row>

          <row>
            <entry>env.PEGASUS_HOME</entry>

            <entry>PEGASUS_HOME</entry>

            <entry>Transformation Catalog, Site Catalog, DAX, Properties
            File</entry>

            <entry>Used by auxillary jobs created by Pegasus both on remote
            site and local site. Should be set usually set in the Site Catalog
            for the sites</entry>
          </row>

          <row>
            <entry>env.GLOBUS_LOCATION</entry>

            <entry>GLOBUS_LOCATION</entry>

            <entry>Transformation Catalog, Site Catalog, DAX, Properties
            File</entry>

            <entry>Used by auxillary jobs created by Pegasus both on remote
            site and local site. Should be set usually set in the Site Catalog
            for the sites</entry>
          </row>

          <row>
            <entry>env.LD_LIBRARY_PATH</entry>

            <entry>LD_LIBRARY_PATH</entry>

            <entry>Transformation Catalog, Site Catalog, DAX, Properties
            File</entry>

            <entry>Point this to $GLOBUS_LOCATION/lib, except you cannot use
            the dollar variable. You must use the full path. Applies to both,
            local and remote jobs that use Globus components and should be
            usually set in the site catalog for the sites</entry>
          </row>
        </tbody>
      </tgroup>
    </table>

    <para>Even though Condor and Globus both permit environment variable
    settings through their profiles, all remote environment variables must be
    set through the means of <emphasis>env</emphasis> profiles.</para>
  </section>

  <section>
    <title>The Globus Profile Namespace</title>

    <para>The <emphasis>globus</emphasis> profile namespace encapsulates
    Globus resource specification language (RSL) instructions. The RSL
    configures settings and behavior of the remote scheduling system. Some
    systems require queue name to schedule jobs, a project name for accounting
    purposes, or a run-time estimate to schedule jobs. The Globus RSL
    addresses all these issues.</para>

    <para>A key in the <emphasis>globus</emphasis> namespace denotes the
    command name of an RLS instruction. The profile value becomes the RSL
    value. Even though Globus RSL is typically shown using parentheses around
    the instruction, the out pair of parentheses is not necessary in globus
    profile specifications</para>

    <para>Table 2 shows some commonly used RSL instructions. For an
    authoritative list of all possible RSL instructions refer to the Globus
    RSL specification.</para>

    <table>
      <title>Useful Globus RSL Instructions</title>

      <tgroup cols="4">
        <tbody>
          <row>
            <entry><emphasis role="bold">Property Key </emphasis></entry>

            <entry><emphasis role="bold">Corresponding Profile
            Key</emphasis></entry>

            <entry><emphasis role="bold">Where can it be
            specified</emphasis></entry>

            <entry><emphasis role="bold">Description</emphasis></entry>
          </row>

          <row>
            <entry>globus.count</entry>

            <entry>count</entry>

            <entry>Transformation Catalog, Site Catalog, DAX, Properties
            File</entry>

            <entry>the number of times an executable is started.</entry>
          </row>

          <row>
            <entry>globus.jobtype</entry>

            <entry>jobtype</entry>

            <entry>Transformation Catalog, Site Catalog, DAX, Properties
            File</entry>

            <entry>specifies how the job manager should start the remote job.
            While Pegasus defaults to single, use mpi when running MPI
            jobs.</entry>
          </row>

          <row>
            <entry>globus.maxcputime</entry>

            <entry>maxcputime</entry>

            <entry>Transformation Catalog, Site Catalog, DAX, Properties
            File</entry>

            <entry>the max CPU time in minutes for a single execution of a
            job.</entry>
          </row>

          <row>
            <entry>globus.maxmemory</entry>

            <entry>maxmemory</entry>

            <entry>Transformation Catalog, Site Catalog, DAX, Properties
            File</entry>

            <entry>the maximum memory in MB required for the job</entry>
          </row>

          <row>
            <entry>globus.maxtime</entry>

            <entry>maxtime</entry>

            <entry>Transformation Catalog, Site Catalog, DAX, Properties
            File</entry>

            <entry>the maximum time or walltime in minutes for a single
            execution of a job.</entry>
          </row>

          <row>
            <entry>globus.maxwalltime</entry>

            <entry>maxwalltime</entry>

            <entry>Transformation Catalog, Site Catalog, DAX, Properties
            File</entry>

            <entry>the maximum walltime in minutes for a single execution of a
            job.</entry>
          </row>

          <row>
            <entry>globus.minmemory</entry>

            <entry>minmemory</entry>

            <entry>Transformation Catalog, Site Catalog, DAX, Properties
            File</entry>

            <entry>the minumum amount of memory required for this job</entry>
          </row>

          <row>
            <entry>globus.project</entry>

            <entry>project</entry>

            <entry>Transformation Catalog, Site Catalog, DAX, Properties
            File</entry>

            <entry>associates an account with a job at the remote end.</entry>
          </row>

          <row>
            <entry>globus.queue</entry>

            <entry>queue</entry>

            <entry>Transformation Catalog, Site Catalog, DAX, Properties
            File</entry>

            <entry>the remote queue in which the job should be run. Used when
            remote scheduler is PBS that supports queues.</entry>
          </row>
        </tbody>
      </tgroup>
    </table>

    <para>Pegasus prevents the user from specifying certain RSL instructions
    as globus profiles, because they are either automatically generated or can
    be overridden through some different means. For instance, if you need to
    specify remote environment settings, do not use the environment key in the
    globus profiles. Use one or more env profiles instead.</para>

    <table>
      <title>RSL Instructions that are not permissible</title>

      <tgroup cols="2">
        <tbody>
          <row>
            <entry><emphasis role="bold">Key</emphasis></entry>

            <entry><emphasis role="bold">Reason for
            Prohibition</emphasis></entry>
          </row>

          <row>
            <entry>arguments</entry>

            <entry>you specify arguments in the arguments section for a job in
            the DAX</entry>
          </row>

          <row>
            <entry>directory</entry>

            <entry>the site catalog and properties determine which directory a
            job will run in.</entry>
          </row>

          <row>
            <entry>environment</entry>

            <entry>use multiple env profiles instead</entry>
          </row>

          <row>
            <entry>executable</entry>

            <entry>the physical executable to be used is specified in the
            transformation catalog and is also dependant on the gridstart
            module being used. If you are launching jobs via kickstart then
            the executable created is the path to kickstart and the
            application executable path appears in the arguments for
            kickstart</entry>
          </row>

          <row>
            <entry>stdin</entry>

            <entry>you specify in the DAX for the job</entry>
          </row>

          <row>
            <entry>stdout</entry>

            <entry>you specify in the DAX for the job</entry>
          </row>

          <row>
            <entry>stderr</entry>

            <entry>you specify in the DAX for the job</entry>
          </row>
        </tbody>
      </tgroup>
    </table>
  </section>

  <section>
    <title>The Condor Profile Namespace</title>

    <para>The Condor submit file controls every detail how and where a job is
    run. The <emphasis>condor</emphasis> profiles permit to add or overwrite
    instructions in the Condor submit file.</para>

    <para>The <emphasis>condor</emphasis> namespace directly sets commands in
    the Condor submit file for a job the profile applies to. Keys in the
    <emphasis>condor</emphasis> profile namespace denote the name of the
    Condor command. The profile value becomes the command's argument. All
    <emphasis>condor</emphasis> profiles are translated into key=value lines
    in the Condor submit file</para>

    <para>Some of the common condor commands that a user may need to specify
    are listed below. For an authoritative list refer to the online condor
    documentation. Note: Pegasus Workflow Planner/Mapper by default specify a
    lot of condor commands in the submit files depending upon the job, and
    where it is being run.</para>

    <table>
      <title>Useful Condor Commands</title>

      <tgroup cols="4">
        <tbody>
          <row>
            <entry><emphasis role="bold">Property Key </emphasis></entry>

            <entry><emphasis role="bold">Corresponding Profile
            Key</emphasis></entry>

            <entry><emphasis role="bold">Where can it be
            specified</emphasis></entry>

            <entry><emphasis role="bold">Description</emphasis></entry>
          </row>

          <row>
            <entry>condor.universe</entry>

            <entry>universe</entry>

            <entry>Transformation Catalog, Site Catalog, DAX, Properties
            File</entry>

            <entry>Pegasus defaults to either globus or scheduler universes.
            Set to standard for compute jobs that require standard universe.
            Set to vanilla to run natively in a condor pool, or to run on
            resources grabbed via condor glidein.</entry>
          </row>

          <row>
            <entry>condor.periodic_release</entry>

            <entry>periodic_release</entry>

            <entry>Transformation Catalog, Site Catalog, DAX, Properties
            File</entry>

            <entry>is the number of times job is released back to the queue if
            it goes to HOLD, e.g. due to Globus errors. Pegasus defaults to
            3.</entry>
          </row>

          <row>
            <entry>condor.periodic_remove</entry>

            <entry>periodic_remove</entry>

            <entry>Transformation Catalog, Site Catalog, DAX, Properties
            File</entry>

            <entry>is the number of times a job is allowed to get into HOLD
            state before being removed from the queue. Pegasus defaults to
            3.</entry>
          </row>

          <row>
            <entry>condor.filesystemdomain</entry>

            <entry>filesystemdomain</entry>

            <entry>Transformation Catalog, Site Catalog, DAX, Properties
            File</entry>

            <entry>Useful for Condor glide-ins to pin a job to a remote
            site.</entry>
          </row>

          <row>
            <entry>condor.stream_error</entry>

            <entry>stream_error</entry>

            <entry>Transformation Catalog, Site Catalog, DAX, Properties
            File</entry>

            <entry>boolean to turn on the streaming of the stderr of the
            remote job back to submit host.</entry>
          </row>

          <row>
            <entry>condor.stream_output</entry>

            <entry>stream_output</entry>

            <entry>Transformation Catalog, Site Catalog, DAX, Properties
            File</entry>

            <entry>boolean to turn on the streaming of the stdout of the
            remote job back to submit host.</entry>
          </row>

          <row>
            <entry>condor.priority</entry>

            <entry>priority</entry>

            <entry>Transformation Catalog, Site Catalog, DAX, Properties
            File</entry>

            <entry>integer value to assign the priority of a job. Higher value
            means higher priority. The priorities are only applied for vanilla
            / standard/ local universe jobs. Determines the order in which a
            users own jobs are executed.</entry>
          </row>

          <row>
            <entry>condor.request_cpus</entry>

            <entry>request_cpus</entry>

            <entry>Transformation Catalog, Site Catalog, DAX, Properties
            File</entry>

            <entry>New in Condor 7.8.0 . Number of CPU's a job
            requires.</entry>
          </row>

          <row>
            <entry>condor.request_memory</entry>

            <entry>request_memory</entry>

            <entry>Transformation Catalog, Site Catalog, DAX, Properties
            File</entry>

            <entry>New in Condor 7.8.0 . Amount of memory a job
            requires.</entry>
          </row>

          <row>
            <entry>condor.request_disk</entry>

            <entry>request_disk</entry>

            <entry>Transformation Catalog, Site Catalog, DAX, Properties
            File</entry>

            <entry>New in Condor 7.8.0 . Amount of disk a job
            requires.</entry>
          </row>
        </tbody>
      </tgroup>
    </table>

    <para>Other useful condor keys, that advanced users may find useful and
    can be set by profiles are</para>

    <orderedlist>
      <listitem>
        <para>should_transfer_files</para>
      </listitem>

      <listitem>
        <para>transfer_output</para>
      </listitem>

      <listitem>
        <para>transfer_error</para>
      </listitem>

      <listitem>
        <para>whentotransferoutput</para>
      </listitem>

      <listitem>
        <para>requirements</para>
      </listitem>

      <listitem>
        <para>rank</para>
      </listitem>
    </orderedlist>

    <para>Pegasus prevents the user from specifying certain Condor commands in
    condor profiles, because they are automatically generated or can be
    overridden through some different means. Table 5 shows prohibited Condor
    commands.</para>

    <table>
      <title>Table 5: Condor commands prohibited in condor profiles</title>

      <tgroup cols="2">
        <tbody>
          <row>
            <entry><emphasis role="bold">Key</emphasis></entry>

            <entry><emphasis role="bold">Reason for
            Prohibition</emphasis></entry>
          </row>

          <row>
            <entry>arguments</entry>

            <entry>you specify arguments in the arguments section for a job in
            the DAX</entry>
          </row>

          <row>
            <entry>environment</entry>

            <entry>use multiple env profiles instead</entry>
          </row>

          <row>
            <entry>executable</entry>

            <entry>the physical executable to be used is specified in the
            transformation catalog and is also dependant on the gridstart
            module being used. If you are launching jobs via kickstart then
            the executable created is the path to kickstart and the
            application executable path appears in the arguments for
            kickstart</entry>
          </row>
        </tbody>
      </tgroup>
    </table>
  </section>

  <section>
    <title>The Dagman Profile Namespace</title>

    <para>DAGMan is Condor's workflow manager. While planners generate most of
    DAGMan's configuration, it is possible to tweak certain job-related
    characteristics using dagman profiles. A dagman profile can be used to
    specify a DAGMan pre- or post-script.</para>

    <para>Pre- and post-scripts execute on the submit machine. Both inherit
    the environment settings from the submit host when pegasus-submit-dag or
    pegasus-run is invoked.</para>

    <para>By default, kickstart launches all jobs except standard universe and
    MPI jobs. Kickstart tracks the execution of the job, and returns usage
    statistics for the job. A DAGMan post-script starts the Pegasus
    application exitcode to determine, if the job succeeded. DAGMan receives
    the success indication as exit status from exitcode.</para>

    <para>If you need to run your own post-script, you have to take over the
    job success parsing. The planner is set up to pass the file name of the
    remote job's stdout, usually the output from kickstart, as sole argument
    to the post-script.</para>

    <para>Table 6 shows the keys in the dagman profile domain that are
    understood by Pegasus and can be associated at a per job basis.</para>

    <para><table>
        <title>Useful dagman Commands that can be associated at a per job
        basis</title>

        <tgroup cols="4">
          <tbody>
            <row>
              <entry><emphasis role="bold">Property Key </emphasis></entry>

              <entry><emphasis role="bold">Corresponding Profile Key
              </emphasis></entry>

              <entry><emphasis role="bold">Where can it be
              specified</emphasis></entry>

              <entry><emphasis role="bold">Description</emphasis></entry>
            </row>

            <row>
              <entry>dagman.pre</entry>

              <entry>PRE</entry>

              <entry>Transformation Catalog, Site Catalog, DAX, Properties
              File</entry>

              <entry>is the path to the pre-script. DAGMan executes the
              pre-script before it runs the job.</entry>
            </row>

            <row>
              <entry>dagman.pre.arguments</entry>

              <entry>PRE.ARGUMENTS</entry>

              <entry>Transformation Catalog, Site Catalog, DAX, Properties
              File</entry>

              <entry>are command-line arguments for the pre-script, if
              any.</entry>
            </row>

            <row>
              <entry>dagman.post</entry>

              <entry>POST</entry>

              <entry>Transformation Catalog, Site Catalog, DAX, Properties
              File</entry>

              <entry>is the postscript type/mode that a user wants to
              associate with a job. <orderedlist>
                  <listitem>
                    <para><emphasis role="bold">pegasus-exitcode</emphasis> -
                    pegasus will by default associate this postscript with all
                    jobs launched via kickstart, as long the POST.SCOPE value
                    is not set to NONE.</para>
                  </listitem>

                  <listitem>
                    <para><emphasis role="bold">none</emphasis> -means that no
                    postscript is generated for the jobs. This is useful for
                    MPI jobs that are not launched via kickstart
                    currently.</para>
                  </listitem>

                  <listitem>
                    <para><emphasis role="bold">any legal
                    identifier</emphasis> - Any other identifier of the form
                    ([_A-Za-z][_A-Za-z0-9]*), than one of the 2 reserved
                    keywords above, signifies a user postscript. This allows
                    the user to specify their own postscript for the jobs in
                    the workflow. The path to the postscript can be specified
                    by the dagman profile <emphasis
                    role="bold">POST.PATH.[value</emphasis>] where [value] is
                    this legal identifier specified. The user postscript is
                    passed the name of the .out file of the job as the last
                    argument on the command line.</para>

                    <para>For e.g. if the following dagman profiles were
                    associated with a job X</para>

                    <orderedlist>
                      <listitem>
                        <para>POST with value user_script
                        /bin/user_postscript</para>
                      </listitem>

                      <listitem>
                        <para>POST.PATH.user_script with value
                        /path/to/user/script</para>
                      </listitem>

                      <listitem>
                        <para>POST.ARGUMENTS with value -verbose</para>
                      </listitem>
                    </orderedlist>

                    <para>then the following postscript will be associated
                    with the job X in the .dag file</para>

                    <para>/path/to/user/script -verbose X.out where X.out
                    contains the stdout of the job X</para>
                  </listitem>
                </orderedlist></entry>
            </row>

            <row>
              <entry>dagman.post.path.[value of dagman.post]</entry>

              <entry>POST.PATH.* ( where * is replaced by the value of the
              POST Profile )</entry>

              <entry>Transformation Catalog, Site Catalog, DAX, Properties
              File</entry>

              <entry>the path to the post script on the submit host.</entry>
            </row>

            <row>
              <entry>dagman.post.arguments</entry>

              <entry>POST.ARGUMENTS</entry>

              <entry>Transformation Catalog, Site Catalog, DAX, Properties
              File</entry>

              <entry>are the command line arguments for the post script, if
              any.</entry>
            </row>

            <row>
              <entry>dagman.retry</entry>

              <entry>RETRY</entry>

              <entry>Transformation Catalog, Site Catalog, DAX, Properties
              File</entry>

              <entry>is the number of times DAGMan retries the full job cycle
              from pre-script through post-script, if failure was
              detected.</entry>
            </row>

            <row>
              <entry>dagman.category</entry>

              <entry>CATEGORY</entry>

              <entry>Transformation Catalog, Site Catalog, DAX, Properties
              File</entry>

              <entry>the DAGMan category the job belongs to.</entry>
            </row>

            <row>
              <entry>dagman.priority</entry>

              <entry>PRIORITY</entry>

              <entry>Transformation Catalog, Site Catalog, DAX, Properties
              File</entry>

              <entry>the priority to apply to a job. DAGMan uses this to
              select what jobs to release when MAXJOBS is enforced for the
              DAG.</entry>
            </row>
          </tbody>
        </tgroup>
      </table></para>

    <para/>

    <para>Table 7 shows the keys in the dagman profile domain that are
    understood by Pegasus and can be used to apply to the whole workflow.
    These are used to control DAGMan's behavior at the workflow level, and are
    recommended to be specified in the properties file.</para>

    <table>
      <title>Useful dagman Commands that can be specified in the properties
      file.</title>

      <tgroup cols="4">
        <tbody>
          <row>
            <entry><emphasis role="bold">Property Key </emphasis></entry>

            <entry><emphasis role="bold">Corresponding Profile
            Key</emphasis></entry>

            <entry><emphasis role="bold">Where can it be
            specified</emphasis></entry>

            <entry><emphasis role="bold">Description</emphasis></entry>
          </row>

          <row>
            <entry>dagman.maxpre</entry>

            <entry>MAXPRE</entry>

            <entry>Properties File</entry>

            <entry>sets the maximum number of PRE scripts within the DAG that
            may be running at one time</entry>
          </row>

          <row>
            <entry>dagman.maxpost</entry>

            <entry>MAXPOST</entry>

            <entry>Properties File</entry>

            <entry>sets the maximum number of PRE scripts within the DAG that
            may be running at one time</entry>
          </row>

          <row>
            <entry>dagman.maxjobs</entry>

            <entry>MAXJOBS</entry>

            <entry>Properties File</entry>

            <entry>sets the maximum number of jobs within the DAG that will be
            submitted to Condor at one time.</entry>
          </row>

          <row>
            <entry>dagman.maxidle</entry>

            <entry>MAXIDLE</entry>

            <entry>Properties File</entry>

            <entry>sets the maximum number of idle jobs within the DAG that
            will be submitted to Condor at one time.</entry>
          </row>

          <row>
            <entry>dagman.[CATEGORY-NAME].maxjobs</entry>

            <entry>[CATEGORY-NAME].MAXJOBS</entry>

            <entry>Properties File</entry>

            <entry>is the value of maxjobs for a particular category. Users
            can associate different categories to the jobs at a per job basis.
            However, the value of a dagman knob for a category can only be
            specified at a per workflow basis in the properties.</entry>
          </row>

          <row>
            <entry>dagman.post.scope</entry>

            <entry>POST.SCOPE</entry>

            <entry>Properties File</entry>

            <entry>scope for the postscripts. <orderedlist>
                <listitem>
                  <para>If set to <emphasis role="bold">all</emphasis> , means
                  each job in the workflow will have a postscript associated
                  with it.</para>
                </listitem>

                <listitem>
                  <para>If set to <emphasis role="bold">none</emphasis> ,
                  means no job has postscript associated with it. None mode
                  should be used if you are running vanilla / standard/ local
                  universe jobs, as in those cases Condor traps the remote
                  exitcode correctly. None scope is not recommended for grid
                  universe jobs.</para>
                </listitem>

                <listitem>
                  <para>If set to <emphasis role="bold">essential</emphasis>,
                  means only essential jobs have post scripts associated with
                  them. At present the only non essential job is the replica
                  registration job.</para>
                </listitem>
              </orderedlist></entry>
          </row>
        </tbody>
      </tgroup>
    </table>
  </section>

  <section>
    <title>The Pegasus Profile Namespace</title>

    <para>The <emphasis>pegasus</emphasis> profiles allow users to configure
    extra options to the Pegasus Workflow Planner that can be applied
    selectively to a job or a group of jobs. Site selectors may use a sub-set
    of <emphasis>pegasus</emphasis> profiles for their decision-making.</para>

    <para>Table 8 shows some of the useful configuration option Pegasus
    understands.</para>

    <table>
      <title>Useful pegasus Profiles.</title>

      <tgroup cols="4">
        <tbody>
          <row>
            <entry><emphasis role="bold">Property Key </emphasis></entry>

            <entry><emphasis role="bold">Corresponding Profile
            Key</emphasis></entry>

            <entry><emphasis role="bold">Where can it be
            specified</emphasis></entry>

            <entry><emphasis role="bold">Description</emphasis></entry>
          </row>

          <row>
            <entry>pegasus.clusters.num</entry>

            <entry>clusters.num</entry>

            <entry>Transformation Catalog, Site Catalog, DAX, Properties
            File</entry>

            <entry>Please refer to the <link
            linkend="horizontal_clustering">Pegasus Clustering Guide</link>
            for detailed description. This option determines the total number
            of clusters per level. Jobs are evenly spread across
            clusters.</entry>
          </row>

          <row>
            <entry>pegasus.clusters.size</entry>

            <entry>clusters.size</entry>

            <entry>Transformation Catalog, Site Catalog, DAX, Properties
            File</entry>

            <entry>Please refer to the <link
            linkend="horizontal_clustering">Pegasus Clustering Guide</link>
            for detailed description. This profile determines the number of
            jobs in each cluster. The number of clusters depends on the total
            number of jobs on the level.</entry>
          </row>

          <row>
            <entry>pegasus.cores</entry>

            <entry>cores</entry>

            <entry>Transformation Catalog, Site Catalog, DAX, Properties
            File</entry>

            <entry>The number of cores, associated with the job. This is
            solely used for accounting purposes in the database while
            generating statistics. It corresponds to the multiplier_factor in
            the job_instance table described <link
            linkend="stampede-schema">here</link>.</entry>
          </row>

          <row>
            <entry>pegasus.runtime</entry>

            <entry>runtime</entry>

            <entry>Transformation Catalog, Site Catalog, DAX, Properties
            File</entry>

            <entry>Please refer to the <link
            linkend="runtime_clustering">Pegasus Clustering Guide</link> for
            detailed description. This profile specifies the expected runtime
            of a job.</entry>
          </row>

          <row>
            <entry>pegasus.clusters.maxruntime</entry>

            <entry>clusters.maxruntime</entry>

            <entry>Transformation Catalog, Site Catalog, DAX, Properties
            File</entry>

            <entry>Please refer to the <link
            linkend="runtime_clustering">Pegasus Clustering Guide</link> for
            detailed description. This profile specifies the maximum runtime
            of a job.</entry>
          </row>

          <row>
            <entry>pegasus.job.aggregator</entry>

            <entry>job.aggregator</entry>

            <entry>Transformation Catalog, Site Catalog, DAX, Properties
            File</entry>

            <entry>Indicates the clustering executable that is used to run the
            clustered job on the remote site.</entry>
          </row>

          <row>
            <entry>pegasus.gridstart</entry>

            <entry>gridstart</entry>

            <entry>Transformation Catalog, Site Catalog, DAX, Properties
            File</entry>

            <entry>Determines the executable for launching a job. Possible
            values are <emphasis role="bold"><emphasis>Kickstart |
            NoGridStart</emphasis></emphasis> at the moment.</entry>
          </row>

          <row>
            <entry>pegasus.gridstart.path</entry>

            <entry>gridstart.path</entry>

            <entry>Transformation Catalog, Site Catalog, DAX, Properties
            File</entry>

            <entry>Sets the path to the gridstart . This profile is best set
            in the Site Catalog.</entry>
          </row>

          <row>
            <entry>pegasus.gridstart.arguments</entry>

            <entry>gridstart.arguments</entry>

            <entry>Transformation Catalog, Site Catalog, DAX, Properties
            File</entry>

            <entry>Sets the arguments with which GridStart is used to launch a
            job on the remote site.</entry>
          </row>

          <row>
            <entry>pegasus.stagein.clusters</entry>

            <entry>stagein.clusters</entry>

            <entry>Transformation Catalog, Site Catalog, DAX, Properties
            File</entry>

            <entry>This key determines the maximum number of
            <emphasis>stage-in</emphasis> jobs that are can executed locally
            or remotely per compute site per workflow. This is used to
            configure the <emphasis>Bundle</emphasis> Transfer Refiner, which
            is the Default Refiner used in Pegasus. This profile is best set
            in the Site Catalog or in the Properties file</entry>
          </row>

          <row>
            <entry>pegasus.stagein.local.clusters</entry>

            <entry>stagein.local.clusters</entry>

            <entry>Transformation Catalog, Site Catalog, DAX, Properties
            File</entry>

            <entry>This key provides finer grained control in determining the
            number of stage-in jobs that are executed locally and are
            responsible for staging data to a particular remote site. This
            profile is best set in the Site Catalog or in the Properties
            file</entry>
          </row>

          <row>
            <entry>pegasus.stagein.remote.clusters</entry>

            <entry>stagein.remote.clusters</entry>

            <entry>Transformation Catalog, Site Catalog, DAX, Properties
            File</entry>

            <entry>This key provides finer grained control in determining the
            number of stage-in jobs that are executed remotely on the remote
            site and are responsible for staging data to it. This profile is
            best set in the Site Catalog or in the Properties file</entry>
          </row>

          <row>
            <entry>pegasus.stageout.clusters</entry>

            <entry>stageout.clusters</entry>

            <entry>Transformation Catalog, Site Catalog, DAX, Properties
            File</entry>

            <entry>This key determines the maximum number of
            <emphasis>stage-out</emphasis> jobs that are can executed locally
            or remotely per compute site per workflow. This is used to
            configure the <emphasis>Bundle</emphasis> Transfer Refiner, ,
            which is the Default Refiner used in Pegasus.</entry>
          </row>

          <row>
            <entry>pegasus.stageout.local.clusters</entry>

            <entry>stageout.local.clusters</entry>

            <entry>Transformation Catalog, Site Catalog, DAX, Properties
            File</entry>

            <entry>This key provides finer grained control in determining the
            number of stage-out jobs that are executed locally and are
            responsible for staging data from a particular remote site. This
            profile is best set in the Site Catalog or in the Properties
            file</entry>
          </row>

          <row>
            <entry>pegasus.stageout.remote.clusters</entry>

            <entry>stageout.remote.clusters</entry>

            <entry>Transformation Catalog, Site Catalog, DAX, Properties
            File</entry>

            <entry>This key provides finer grained control in determining the
            number of stage-out jobs that are executed remotely on the remote
            site and are responsible for staging data from it. This profile is
            best set in the Site Catalog or in the Properties file</entry>
          </row>

          <row>
            <entry>pegasus.group</entry>

            <entry>group</entry>

            <entry>Transformation Catalog, Site Catalog, DAX, Properties
            File</entry>

            <entry>Tags a job with an arbitrary group identifier. The group
            site selector makes use of the tag.</entry>
          </row>

          <row>
            <entry>pegasus.change.dir</entry>

            <entry>change.dir</entry>

            <entry>Transformation Catalog, Site Catalog, DAX, Properties
            File</entry>

            <entry>If true, tells <emphasis>kickstart</emphasis> to change
            into the remote working directory. Kickstart itself is executed in
            whichever directory the remote scheduling system chose for the
            job.</entry>
          </row>

          <row>
            <entry>pegasus.create.dir</entry>

            <entry>create.dir</entry>

            <entry>Transformation Catalog, Site Catalog, DAX, Properties
            File</entry>

            <entry>If true, tells <emphasis>kickstart</emphasis> to create the
            the remote working directory before changing into the remote
            working directory. Kickstart itself is executed in whichever
            directory the remote scheduling system chose for the job.</entry>
          </row>

          <row>
            <entry>pegasus.transfer.proxy</entry>

            <entry>transfer.proxy</entry>

            <entry>Transformation Catalog, Site Catalog, DAX, Properties
            File</entry>

            <entry>If true, tells Pegasus to explicitly transfer the proxy for
            transfer jobs to the remote site. This is useful, when you want to
            use a full proxy at the remote end, instead of the limited proxy
            that is transferred by CondorG.</entry>
          </row>

          <row>
            <entry>pegasus.transfer.arguments</entry>

            <entry>transfer.arguments</entry>

            <entry>Transformation Catalog, Site Catalog, DAX, Properties
            File</entry>

            <entry>Allows the user to specify the arguments with which the
            transfer executable is invoked. However certain options are always
            generated for the transfer executable. The profile applies to the
            separate data transfer nodes added by Pegasus to the executable
            workflow.</entry>
          </row>

          <row>
            <entry>pegasus.transfer.threads</entry>

            <entry>transfer.threads</entry>

            <entry>Transformation Catalog, Site Catalog, DAX, Properties
            File</entry>

            <entry>the number of threads launched by pegasus-transfer when
            doing the transfers. The profile applies to the separate data
            transfer nodes added by Pegasus to the executable
            workflow.</entry>
          </row>

          <row>
            <entry>pegasus.transfer.lite.arguments</entry>

            <entry>transfer.lite.arguments</entry>

            <entry>Transformation Catalog, Site Catalog, DAX, Properties
            File</entry>

            <entry>Similar to transfer.arguments, but applies to the transfers
            performed in the PegasusLite jobs in the nonsharedfs data
            configuration.</entry>
          </row>

          <row>
            <entry>pegasus.transfer.lite.threads</entry>

            <entry>transfer.lite.threads</entry>

            <entry>Transformation Catalog, Site Catalog, DAX, Properties
            File</entry>

            <entry>Similar to threads, but applies to the transfers performed
            in the PegasusLite jobs in the nonsharedfs data
            configuration.</entry>
          </row>

          <row>
            <entry>pegasus.style</entry>

            <entry>style</entry>

            <entry>Transformation Catalog, Site Catalog, DAX, Properties
            File</entry>

            <entry>Sets the condor submit file style. If set to globus, submit
            file generated refers to CondorG job submissions. If set to
            condor, submit file generated refers to direct Condor submission
            to the local Condor pool. It applies for glidein, where nodes from
            remote grid sites are glided into the local condor pool. The
            default style that is applied is globus.</entry>
          </row>

          <row>
            <entry>pegasus.pmc_request_memory</entry>

            <entry>pmc_request_memory</entry>

            <entry>Transformation Catalog, Site Catalog, DAX, Properties
            File</entry>

            <entry>This key is used to set the -m option for
            pegasus-mpi-cluster. It specifies the amount of memory in MB that
            a job requires. This profile is usually set in the DAX for each
            job.</entry>
          </row>

          <row>
            <entry>pegasus.pmc_request_cpus</entry>

            <entry>pmc_request_cpus</entry>

            <entry>Transformation Catalog, Site Catalog, DAX, Properties
            File</entry>

            <entry>This key is used to set the -c option for
            pegasus-mpi-cluster. It specifies the number of cpu's that a job
            requires. This profile is usually set in the DAX for each
            job.</entry>
          </row>

          <row>
            <entry>pegasus.pmc_priority</entry>

            <entry>pmc_priority</entry>

            <entry>Transformation Catalog, Site Catalog, DAX, Properties
            File</entry>

            <entry>This key is used to set the -p option for
            pegasus-mpi-cluster. It specifies the priority for a job . This
            profile is usually set in the DAX for each job. Negative values
            are allowed for priorities.</entry>
          </row>

          <row>
            <entry>pegasus.pmc_task_arguments</entry>

            <entry>pmc_task_arguments</entry>

            <entry>Transformation Catalog, Site Catalog, DAX, Properties
            File</entry>

            <entry>The key is used to pass any extra arguments to the PMC task
            during the planning time. They are added to the very end of the
            argument string constructed for the task in the PMC file. Hence,
            allows for overriding of any argument constructed by the planner
            for any particular task in the PMC job.</entry>
          </row>

          <row>
            <entry>pegasus.exitcode.failuremsg</entry>

            <entry>exitcode.failuremsg</entry>

            <entry>Transformation Catalog, Site Catalog, DAX, Properties
            File</entry>

            <entry>The message string that pegasus-exitcode searches for in
            the stdout and stderr of the job to flag failures.</entry>
          </row>

          <row>
            <entry>pegasus.exitcode.successmsg</entry>

            <entry>exitcode.successmsg</entry>

            <entry>Transformation Catalog, Site Catalog, DAX, Properties
            File</entry>

            <entry>The message string that pegasus-exitcode searches for in
            the stdout and stderr of the job to determine whether a job logged
            it's success message or not. Note this value is used to check for
            whether a job failed or not i.e if this profile is specified, and
            pegasus-exitcode DOES NOT find the string in the job stdout or
            stderr, the job is flagged as failed. The complete rules for
            determining failure are described in the man page for
            pegasus-exitcode.</entry>
          </row>

          <row>
            <entry>pegasus.checkpoint_time</entry>

            <entry>checkpoint_time</entry>

            <entry>Transformation Catalog, Site Catalog, DAX, Properties
            File</entry>

            <entry>the expected time in seconds for a job after which it
            should be sent a TERM signal to generate a job checkpoint
            file</entry>
          </row>

          <row>
            <entry>pegasus.maxwalltime</entry>

            <entry>maxwalltime</entry>

            <entry>Transformation Catalog, Site Catalog, DAX, Properties
            File</entry>

            <entry>the maximum walltime in minutes for a single execution of a
            job.</entry>
          </row>
        </tbody>
      </tgroup>
    </table>
  </section>

  <section>
    <title>Properties Introduction</title>

    <para>Properties are primarily used to configure the behavior of the
    Pegasus Workflow Planner at a global level. The properties file is
    actually a java properties file and follows the same conventions as that
    to specify the properties.</para>

    <para>Please note that the values rely on proper capitalization, unless
    explicitly noted otherwise.</para>

    <para>Some properties rely with their default on the value of other
    properties. As a notation, the curly braces refer to the value of the
    named property. For instance, ${pegasus.home} means that the value depends
    on the value of the pegasus.home property plus any noted additions. You
    can use this notation to refer to other properties, though the extent of
    the subsitutions are limited. Usually, you want to refer to a set of the
    standard system properties. Nesting is not allowed. Substitutions will
    only be done once.</para>

    <para>There is a priority to the order of reading and evaluating
    properties. Usually one does not need to worry about the priorities.
    However, it is good to know the details of when which property applies,
    and how one property is able to overwrite another. The following is a
    mutually exclusive list ( highest priority first ) of property file
    locations.</para>

    <para><orderedlist>
        <listitem>
           --conf option to the tools. Almost all of the clients that use properties have a --conf option to specify the property file to pick up. 
        </listitem>

        <listitem>
           submit-dir/pegasus.xxxxxxx.properties file. All tools that work on the submit directory ( i.e after pegasus has planned a workflow) pick up the pegasus.xxxxx.properties file from the submit directory. The location for the pegasus.xxxxxxx.propertiesis picked up from the braindump file. 
        </listitem>

        <listitem>
           The properties defined in the user property file 

          <emphasis>${user.home}/.pegasusrc</emphasis>

           have lowest priority. 
        </listitem>
      </orderedlist></para>

    <para>Commandline properties have the highest priority. These override any
    property loaded from a property file. Each commandline property is
    introduced by a -D argument. Note that these arguments are parsed by the
    shell wrapper, and thus the -D arguments must be the first arguments to
    any command. Commandline properties are useful for debugging
    purposes.</para>

    <para>From Pegasus 3.1 release onwards, support has been dropped for the
    following properties that were used to signify the location of the
    properties file</para>

    <para><itemizedlist>
        <listitem>
           pegasus.properties 
        </listitem>

        <listitem>
           pegasus.user.properties 
        </listitem>
      </itemizedlist></para>

    <para>The following example provides a sensible set of properties to be
    set by the user property file. These properties use mostly non-default
    settings. It is an example only, and will not work for you:</para>

    <para><screen>
pegasus.catalog.replica              File
pegasus.catalog.replica.file         ${pegasus.home}/etc/sample.rc.data
pegasus.catalog.transformation       Text
pegasus.catalog.transformation.file  ${pegasus.home}/etc/sample.tc.text
pegasus.catalog.site.file            ${pegasus.home}/etc/sample.sites.xml
</screen></para>

    <para>If you are in doubt which properties are actually visible, pegasus
    during the planning of the workflow dumps all properties after reading and
    prioritizing in the submit directory in a file with the suffix
    properties.</para>
  </section>

  <section>
    <title>Local Directories Properties</title>

    <para>This section describes the GNU directory structure conventions. GNU
    distinguishes between architecture independent and thus sharable
    directories, and directories with data specific to a platform, and thus
    often local. It also distinguishes between frequently modified data and
    rarely changing data. These two axis form a space of four distinct
    directories.</para>

    <para><table>
        <title>Local Directories Related Properties</title>

        <tgroup cols="2">
          <tbody>
            <row>
              <entry><emphasis role="bold">Key Attributes </emphasis></entry>

              <entry><emphasis role="bold">Description</emphasis></entry>
            </row>

            <row>
              <entry><literallayout><emphasis role="bold"><emphasis
                      role="bold">Property Key: </emphasis></emphasis>pegasus.home.datadir<emphasis
                    role="bold"><emphasis role="bold">
Profile  Key: </emphasis></emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 2.0
<emphasis role="bold">Type        : </emphasis>file path
<emphasis role="bold">Default     :</emphasis> ${pegasus.home}/share</literallayout></entry>

              <entry>The datadir directory contains broadly visible and
              possibly exported configuration files that rarely change. This
              directory is currently unused.</entry>
            </row>

            <row>
              <entry><literallayout><emphasis role="bold"><emphasis
                      role="bold">Property Key: </emphasis></emphasis>pegasus.home.sysconfdir<emphasis
                    role="bold"><emphasis role="bold">
Profile  Key: </emphasis></emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 2.0
<emphasis role="bold">Type        : </emphasis>file path
<emphasis role="bold">Default     :</emphasis> ${pegasus.home}/etc</literallayout></entry>

              <entry>The system configuration directory contains configuration
              files that are specific to the machine or installation, and that
              rarely change. This is the directory where the XML schema
              definition copies are stored, and where the base pool
              configuration file is stored.</entry>
            </row>

            <row>
              <entry><literallayout><emphasis role="bold"><emphasis
                      role="bold">Property Key: </emphasis></emphasis>pegasus.home.sharedstatedir<emphasis
                    role="bold"><emphasis role="bold">
Profile  Key: </emphasis></emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 2.0
<emphasis role="bold">Type        : </emphasis>file path
<emphasis role="bold">Default     :</emphasis> ${pegasus.home}/com</literallayout></entry>

              <entry>Frequently changing files that are broadly visible are
              stored in the shared state directory. This is currently
              unused.</entry>
            </row>

            <row>
              <entry><literallayout><emphasis role="bold"><emphasis
                      role="bold">Property Key: </emphasis></emphasis>pegasus.home.localstatedir<emphasis
                    role="bold"><emphasis role="bold">
Profile  Key: </emphasis></emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 2.0
<emphasis role="bold">Type        : </emphasis>file path
<emphasis role="bold">Default     :</emphasis> ${pegasus.home}/var</literallayout></entry>

              <entry>Frequently changing files that are specific to a machine
              and/or installation are stored in the local state directory.
              This is currently unused</entry>
            </row>

            <row>
              <entry><literallayout><emphasis role="bold"><emphasis
                      role="bold">Property Key: </emphasis></emphasis>pegasus.dir.submit.logs<emphasis
                    role="bold"><emphasis role="bold">
Profile  Key: </emphasis></emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 2.0
<emphasis role="bold">Type        : </emphasis>file path
<emphasis role="bold">Default     :</emphasis> (no default)</literallayout></entry>

              <entry><para>This property can be used to specify the directory
              where the condor logs for the workflow should go to. By default,
              starting 4.2.1 release, Pegasus will setup the log to be in the
              workflow submit directory. This can create problems, in case
              users submit directories are on NSF.</para><para>This is done to
              ensure that the logs are created in a local directory even
              though the submit directory maybe on NFS</para></entry>
            </row>
          </tbody>
        </tgroup>
      </table></para>
  </section>

  <section>
    <title>Site Directories Properties</title>

    <para>The site directory properties modify the behavior of remotely run
    jobs. In rare occasions, it may also pertain to locally run compute
    jobs.</para>

    <table>
      <title>Site Directories Related Properties</title>

      <tgroup cols="2">
        <tbody>
          <row>
            <entry><emphasis role="bold">Key Attributes </emphasis></entry>

            <entry><emphasis role="bold">Description</emphasis></entry>
          </row>

          <row>
            <entry><literallayout><emphasis role="bold"><emphasis role="bold">Property Key: </emphasis></emphasis>pegasus.dir.useTimestamp<emphasis
                  role="bold"><emphasis role="bold">
Profile  Key: </emphasis></emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 2.1
<emphasis role="bold">Type        : </emphasis>Boolean
<emphasis role="bold">Default     :</emphasis> false</literallayout></entry>

            <entry>While creating the submit directory, Pegasus employs a run
            numbering scheme. Users can use this Boolean property to use a
            timestamp based numbering scheme instead of the runxxxx
            scheme.</entry>
          </row>

          <row>
            <entry><literallayout><emphasis role="bold"><emphasis role="bold">Property Key: </emphasis></emphasis>pegasus.dir.exec<emphasis
                  role="bold"><emphasis role="bold">
Profile  Key: </emphasis></emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 2.0
<emphasis role="bold">Type        : </emphasis>file path
<emphasis role="bold">Default     :</emphasis> (no default)</literallayout></entry>

            <entry>This property modifies the remote location work directory
            in which all your jobs will run. If the path is relative then it
            is appended to the work directory (associated with the site), as
            specified in the site catalog. If the path is absolute then it
            overrides the work directory specified in the site
            catalog.</entry>
          </row>

          <row>
            <entry><literallayout><emphasis role="bold"><emphasis role="bold">Property Key: </emphasis></emphasis>pegasus.dir.storage.mapper<emphasis
                  role="bold"><emphasis role="bold">
Profile  Key: </emphasis></emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 4.3
<emphasis role="bold">Type        : </emphasis>Enumeration
<emphasis role="bold">Values      :</emphasis> Flat|Fixed|Hashed|Replica
<emphasis role="bold">Default     :</emphasis> Flat</literallayout></entry>

            <entry>This property modifies determines how the output files are
            mapped on the output site storage location. <para/>In order to
            preserve backward compatibility, setting the boolean property
            pegasus.dir.storage.deep results in the Hashed output mapper to be
            loaded, if no output mapper property is specified. <variablelist>
                <varlistentry>
                  <term>Flat</term>

                  <listitem>
                     By default, Pegasus will place the output files in the storage directory specified in the site catalog for the output site. 
                  </listitem>
                </varlistentry>

                <varlistentry>
                  <term>Fixed</term>

                  <listitem>
                     Using this mapper, users can specify an externally accesible url to the storage directory in their properties file. The following property needs to be set. 

                    <screen>
pegasus.dir.storage.mapper.fixed.url  an externally accessible URL to the
storage directory on the output site
e.g. gsiftp://outputs.isi.edu/shared/outputs
</screen>

                     Note: For hierarchal workflows, the above property needs to be set separately for each dax job, if you want the sub workflow outputs to goto a different directory. 
                  </listitem>
                </varlistentry>

                <varlistentry>
                  <term>Hashed</term>

                  <listitem>
                     This mapper results in the creation of a deep directory structure on the output site, while populating the results. The base directory on the remote end is determined from the site catalog. Depending on the number of files being staged to the remote site a Hashed File Structure is created that ensures that only 256 files reside in one directory. To create this directory structure on the storage site, Pegasus relies on the directory creation feature of the Grid FTP server, which appeared in globus 4.0.x 
                  </listitem>
                </varlistentry>

                <varlistentry>
                  <term>Replica</term>

                  <listitem>
                     This mapper determines the path for an output file on the output site by querying an output replica catalog. The output site is one that is passed on the command line. The output replica catalog can be configured by specifiing the properties with the prefix pegasus.dir.storage.replica. By default, a Regex File based backend is assumed unless overridden. For example 

                    <screen>
pegasus.dir.storage.mapper.replica       Regex|File
pegasus.dir.storage.mapper.replica.file  the RC file at the backend to use if using a file based RC
</screen>

                     
                  </listitem>
                </varlistentry>
              </variablelist></entry>
          </row>

          <row>
            <entry><literallayout><emphasis role="bold"><emphasis role="bold">Property Key: </emphasis></emphasis>pegasus.dir.storage.deep<emphasis
                  role="bold"><emphasis role="bold">
Profile  Key: </emphasis></emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 2.1
<emphasis role="bold">Type        : </emphasis>Boolean
<emphasis role="bold">Default     :</emphasis> false</literallayout></entry>

            <entry><para>This Boolean property results in the creation of a
            deep directory structure on the output site, while populating the
            results. The base directory on the remote end is determined from
            the site catalog.</para><para>To this base directory, the relative
            submit directory structure ( $user/$vogroup/$label/runxxxx ) is
            appended.</para><para>$storage = $base +
            $relative_submit_directory</para><para>This is the base directory
            that is passed to the storage mapper.</para><para>Note: To
            preserve backward compatibilty, setting this property results in
            the Hashed mapper to be loaded unless pegasus.dir.storage.mapper
            is explicitly specified. Before 4.3, this property resulted in
            HashedDirectory structure.</para></entry>
          </row>

          <row>
            <entry><literallayout><emphasis role="bold"><emphasis role="bold">Property Key: </emphasis></emphasis>pegasus.dir.create.strategy<emphasis
                  role="bold"><emphasis role="bold">
Profile  Key: </emphasis></emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 2.2
<emphasis role="bold">Type        : </emphasis>Enumeration
<emphasis role="bold">Values      :</emphasis> HourGlass|Tentacles|Minimal<emphasis
                  role="bold">
Default     :</emphasis> Minimal</literallayout></entry>

            <entry><para>If the <screen>--randomdir</screen> option is given
            to the Planner at runtime, the Pegasus planner adds nodes that
            create the random directories at the remote pool sites, before any
            jobs are actually run. The two modes determine the placement of
            these nodes and their dependencies to the rest of the
            graph.</para><para><variablelist>
                <varlistentry>
                  <term>HourGlass</term>

                  <listitem>
                     It adds a make directory node at the top level of the graph, and all these concat to a single dummy job before branching out to the root nodes of the original/ concrete dag so far. So we introduce a classic X shape at the top of the graph. Hence the name HourGlass. 
                  </listitem>
                </varlistentry>

                <varlistentry>
                  <term>Tentacles</term>

                  <listitem>
                     This option places the jobs creating directories at the top of the graph. However instead of constricting it to an hour glass shape, this mode links the top node to all the relevant nodes for which the create dir job is necessary. It looks as if the node spreads its tentacleas all around. This puts more load on the DAGMan because of the added dependencies but removes the restriction of the plan progressing only when all the create directory jobs have progressed on the remote pools, as is the case in the HourGlass model. 
                  </listitem>
                </varlistentry>

                <varlistentry>
                  <term>Minimal</term>

                  <listitem>
                     The strategy involves in walking the graph in a BFS order, and updating a bit set associated with each job based on the BitSet of the parent jobs. The BitSet indicates whether an edge exists from the create dir job to an ancestor of the node. For a node, the bit set is the union of all the parents BitSets. The BFS traversal ensures that the bitsets are of a node are only updated once the parents have been processed. 
                  </listitem>
                </varlistentry>
              </variablelist></para></entry>
          </row>
        </tbody>
      </tgroup>
    </table>

    <para/>
  </section>

  <section>
    <title>Schema File Location Properties</title>

    <para>This section defines the location of XML schema files that are used
    to parse the various XML document instances in the PEGASUS. The schema
    backups in the installed file-system permit PEGASUS operations without
    being online.</para>

    <para><table>
        <title>Schema File Location Properties</title>

        <tgroup cols="2">
          <tbody>
            <row>
              <entry><emphasis role="bold">Key Attributes </emphasis></entry>

              <entry><emphasis role="bold">Description</emphasis></entry>
            </row>

            <row>
              <entry><literallayout><emphasis role="bold"><emphasis
                      role="bold">Property Key: </emphasis></emphasis>pegasus.schema.dax<emphasis
                    role="bold"><emphasis role="bold">
Profile  Key: </emphasis></emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 2.0
<emphasis role="bold">Type        : </emphasis>file path
<emphasis role="bold">Default     :</emphasis> ${pegasus.home.sysconfdir}/dax-3.4.xsd</literallayout></entry>

              <entry>This file is a copy of the XML schema that describes
              abstract DAG files that are the result of the abstract planning
              process, and input into any concrete planning. Providing a copy
              of the schema enables the parser to use the local copy instead
              of reaching out to the Internet, and obtaining the latest
              version from the Pegasus website dynamically.</entry>
            </row>

            <row>
              <entry><literallayout><emphasis role="bold"><emphasis
                      role="bold">Property Key: </emphasis></emphasis>pegasus.schema.sc<emphasis
                    role="bold"><emphasis role="bold">
Profile  Key: </emphasis></emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 2.0
<emphasis role="bold">Type        : </emphasis>file path
<emphasis role="bold">Default     :</emphasis> ${pegasus.home.sysconfdir}/sc-4.0.xsd</literallayout></entry>

              <entry>This file is a copy of the XML schema that describes the
              xml description of the site catalog. Providing a copy of the
              schema enables the parser to use the local copy instead of
              reaching out to the internet, and obtaining the latest version
              from the GriPhyN website dynamically.</entry>
            </row>

            <row>
              <entry><literallayout><emphasis role="bold"><emphasis
                      role="bold">Property Key: </emphasis></emphasis>pegasus.schema.ivr<emphasis
                    role="bold"><emphasis role="bold">
Profile  Key: </emphasis></emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 2.0
<emphasis role="bold">Type        : </emphasis>file path
<emphasis role="bold">Default     :</emphasis> ${pegasus.home.sysconfdir}/iv-2.0.xsd</literallayout></entry>

              <entry>This file is a copy of the XML schema that describes
              invocation record files that are the result of the a grid launch
              in a remote or local site. Providing a copy of the schema
              enables the parser to use the local copy instead of reaching out
              to the Internet, and obtaining the latest version from the
              Pegasus website dynamically.</entry>
            </row>
          </tbody>
        </tgroup>
      </table></para>
  </section>

  <section>
    <title>Database Drivers For All Relational Catalogs</title>

    <para/>

    <table>
      <title>Database Driver Properties</title>

      <tgroup cols="2">
        <tbody>
          <row>
            <entry><emphasis role="bold">Property Key </emphasis></entry>

            <entry><emphasis role="bold">Description</emphasis></entry>
          </row>

          <row>
            <entry><literallayout><emphasis role="bold"><emphasis role="bold">Property Key: </emphasis></emphasis>pegasus.catalog.*.db.driver<emphasis
                  role="bold"><emphasis role="bold">
Profile  Key: </emphasis></emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 2.0
<emphasis role="bold">Type        : </emphasis>Enumeration
<emphasis role="bold">Values      : </emphasis>MySQL|PostGres|SQLite<emphasis
                  role="bold">
Default     :</emphasis> (no default)</literallayout></entry>

            <entry><para>The database driver class is dynamically loaded, as
            required by the schema. Currently, only MySQL 5.x, PostGreSQL 7.3
            and SQlite are supported. Their respective JDBC3 driver is
            provided as part and parcel of the PEGASUS.</para><para>The * in
            the property name can be replaced by a catalog name to apply the
            property only for that catalog. Valid catalog names
            are</para><para><screen>replica
</screen></para></entry>
          </row>

          <row>
            <entry><literallayout><emphasis role="bold"><emphasis role="bold">Property Key: </emphasis></emphasis>pegasus.catalog.*.db.url<emphasis
                  role="bold"><emphasis role="bold">
Profile  Key: </emphasis></emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 2.0
<emphasis role="bold">Type        : </emphasis>Database URL
<emphasis role="bold">Default     :</emphasis> (no default)</literallayout></entry>

            <entry>Each database has its own string to contact the database on
            a given host, port, and database. Although most driver URLs allow
            to pass arbitrary arguments, please use the
            pegasus.catalog.[catalog-name].db.* keys or pegasus.catalog.*.db.*
            to preload these arguments. <para/>THE URL IS A MANDATORY PROPERTY
            FOR ANY DBMS BACKEND.</entry>
          </row>

          <row>
            <entry><literallayout><emphasis role="bold"><emphasis role="bold">Property Key: </emphasis></emphasis>pegasus.catalog.*.db.user<emphasis
                  role="bold"><emphasis role="bold">
Profile  Key: </emphasis></emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 2.0
<emphasis role="bold">Type        : </emphasis>String
<emphasis role="bold">Default     :</emphasis> </literallayout></entry>

            <entry><para>In order to access a database, you must provide the
            name of your account on the DBMS. This property is
            database-independent. THIS IS A MANDATORY PROPERTY FOR MANY DBMS
            BACKENDS.</para><para>The * in the property name can be replaced
            by a catalog name to apply the property only for that catalog.
            Valid catalog names are</para><para><screen>replica</screen></para></entry>
          </row>

          <row>
            <entry><literallayout><emphasis role="bold"><emphasis role="bold">Property Key: </emphasis></emphasis>pegasus.catalog.*.db.password<emphasis
                  role="bold"><emphasis role="bold">
Profile  Key: </emphasis></emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 2.0
<emphasis role="bold">Type        : </emphasis>String
<emphasis role="bold">Default     :</emphasis> (no default)</literallayout></entry>

            <entry><para>In order to access a database, you must provide an
            optional password of your account on the DBMS. This property is
            database-independent. THIS IS A MANDATORY PROPERTY, IF YOUR DBMS
            BACKEND ACCOUNT REQUIRES A PASSWORD.</para><para>The * in the
            property name can be replaced by a catalog name to apply the
            property only for that catalog. Valid catalog names are<screen>replica</screen></para></entry>
          </row>

          <row>
            <entry><literallayout><emphasis role="bold"><emphasis role="bold">Property Key: </emphasis></emphasis>pegasus.catalog.*.db.*<emphasis
                  role="bold"><emphasis role="bold">
Profile  Key: </emphasis></emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 2.0
<emphasis role="bold">Type        : </emphasis>String
<emphasis role="bold">Default     :</emphasis> (no default)</literallayout></entry>

            <entry><para/><para>Each database has a multitude of options to
            control in fine detail the further behaviour. You may want to
            check the JDBC3 documentation of the JDBC driver for your database
            for details. The keys will be passed as part of the connect
            properties by stripping the "pegasus.catalog.[catalog-name].db."
            prefix from them. The catalog-name can be replaced by the
            following values provenance for Provenance Catalog (PTC), replica
            for Replica Catalog (RC)</para><para>Postgres 7.3 parses the
            following properties: <screen>
pegasus.catalog.*.db.user
pegasus.catalog.*.db.password
pegasus.catalog.*.db.PGHOST
pegasus.catalog.*.db.PGPORT
pegasus.catalog.*.db.charSet
pegasus.catalog.*.db.compatible
</screen></para><para>MySQL 5.0 parses the following
            properties:</para><para><screen>
pegasus.catalog.*.db.user
pegasus.catalog.*.db.password
pegasus.catalog.*.db.databaseName
pegasus.catalog.*.db.serverName
pegasus.catalog.*.db.portNumber
pegasus.catalog.*.db.socketFactory
pegasus.catalog.*.db.strictUpdates
pegasus.catalog.*.db.ignoreNonTxTables
pegasus.catalog.*.db.secondsBeforeRetryMaster
pegasus.catalog.*.db.queriesBeforeRetryMaster
pegasus.catalog.*.db.allowLoadLocalInfile
pegasus.catalog.*.db.continueBatchOnError
pegasus.catalog.*.db.pedantic
pegasus.catalog.*.db.useStreamLengthsInPrepStmts
pegasus.catalog.*.db.useTimezone
pegasus.catalog.*.db.relaxAutoCommit
pegasus.catalog.*.db.paranoid
pegasus.catalog.*.db.autoReconnect
pegasus.catalog.*.db.capitalizeTypeNames
pegasus.catalog.*.db.ultraDevHack
pegasus.catalog.*.db.strictFloatingPoint
pegasus.catalog.*.db.useSSL
pegasus.catalog.*.db.useCompression
pegasus.catalog.*.db.socketTimeout
pegasus.catalog.*.db.maxReconnects
pegasus.catalog.*.db.initialTimeout
pegasus.catalog.*.db.maxRows
pegasus.catalog.*.db.useHostsInPrivileges
pegasus.catalog.*.db.interactiveClient
pegasus.catalog.*.db.useUnicode
pegasus.catalog.*.db.characterEncoding
</screen></para><para>MS SQL Server 2000 support the following properties
            (keys are case-insensitive, e.g. both "user" and "User" are
            valid):</para><para><screen>
pegasus.catalog.*.db.User
pegasus.catalog.*.db.Password
pegasus.catalog.*.db.DatabaseName
pegasus.catalog.*.db.ServerName
pegasus.catalog.*.db.HostProcess
pegasus.catalog.*.db.NetAddress
pegasus.catalog.*.db.PortNumber
pegasus.catalog.*.db.ProgramName
pegasus.catalog.*.db.SendStringParametersAsUnicode
pegasus.catalog.*.db.SelectMethod
</screen></para><para>The * in the property name can be replaced by a catalog
            name to apply the property only for that catalog. Valid catalog
            names are</para><para><screen>replica</screen></para></entry>
          </row>
        </tbody>
      </tgroup>
    </table>
  </section>

  <section>
    <title>Catalog Related Properties</title>

    <para/>

    <table>
      <title>Replica Catalog Properties</title>

      <tgroup cols="2">
        <tbody>
          <row>
            <entry><emphasis role="bold">Key Attributes</emphasis></entry>

            <entry><emphasis role="bold">Description</emphasis></entry>
          </row>

          <row>
            <entry><literallayout><emphasis role="bold">Property Key: </emphasis>pegasus.catalog.replica<emphasis
                  role="bold">
Profile  Key: </emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 2.0
<emphasis role="bold">Default     :</emphasis> File
</literallayout></entry>

            <entry><para>Pegasus queries a Replica Catalog to discover the
            physical filenames (PFN) for input files specified in the DAX.
            Pegasus can interface with various types of Replica Catalogs. This
            property specifies which type of Replica Catalog to use during the
            planning process.</para><para><variablelist>
                <varlistentry>
                  <term>JDBCRC</term>

                  <listitem>
                     In this mode, Pegasus queries a SQL based replica catalog that is accessed via JDBC. The sql schema's for this catalog can be found at $PEGASUS_HOME/sql directory. To use JDBCRC, the user additionally needs to set the following properties 

                    <orderedlist>
                      <listitem>pegasus.catalog.replica.db.driver =
                      mysql</listitem>

                      <listitem>pegasus.catalog.replica.db.url = jdbc url to
                      database e.g
                      jdbc:mysql://database-host.isi.edu/database-name</listitem>

                      <listitem>pegasus.catalog.replica.db.user =
                      database-user</listitem>

                      <listitem>pegasus.catalog.replica.db.password =
                      database-password</listitem>
                    </orderedlist>

                     
                  </listitem>
                </varlistentry>

                <varlistentry>
                  <term>File</term>

                  <listitem>
                    <para>In this mode, Pegasus queries a file based replica
                    catalog. It is neither transactionally safe, nor advised
                    to use for production purposes in any way. Multiple
                    concurrent instances <emphasis>will clobber</emphasis>
                    each other!. The site attribute should be specified
                    whenever possible. The attribute key for the site
                    attribute is "pool".</para>

                    <para>The LFN may or may not be quoted. If it contains
                    linear whitespace, quotes, backslash or an equality sign,
                    it must be quoted and escaped. Ditto for the PFN. The
                    attribute key-value pairs are separated by an equality
                    sign without any whitespaces. The value may be in quoted.
                    The LFN sentiments about quoting apply.</para>

                    <para><screen>
LFN PFN
LFN PFN a=b [..]
LFN PFN a="b" [..]
"LFN w/LWS" "PFN w/LWS" [..]
</screen></para>

                    <para>To use File, the user additionally needs to specify
                    pegasus.catalog.replica.file property to specify the path
                    to the file based RC.</para>
                  </listitem>
                </varlistentry>

                <varlistentry>
                  <term>Regex</term>

                  <listitem>
                    <para>In this mode, Pegasus queries a file based replica
                    catalog. It is neither transactionally safe, nor advised
                    to use for production purposes in any way. Multiple
                    concurrent access to the File will end up clobbering the
                    contents of the file. The site attribute should be
                    specified whenever possible. The attribute key for the
                    site attribute is "pool".</para>

                    <para>The LFN may or may not be quoted. If it contains
                    linear whitespace, quotes, backslash or an equality sign,
                    it must be quoted and escaped. Ditto for the PFN. The
                    attribute key-value pairs are separated by an equality
                    sign without any whitespaces. The value may be in quoted.
                    The LFN sentiments about quoting apply.</para>

                    <para>In addition users can specifiy regular expression
                    based LFN's. A regular expression based entry should be
                    qualified with an attribute named 'regex'. The attribute
                    regex when set to true identifies the catalog entry as a
                    regular expression based entry. Regular expressions should
                    follow Java regular expression syntax.</para>

                    <para>For example, consider a replica catalog as shown
                    below.</para>

                    <para>Entry 1 refers to an entry which does not use a
                    resular expressions. This entry would only match a file
                    named 'f.a', and nothing else. Entry 2 referes to an entry
                    which uses a regular expression. In this entry f.a referes
                    to files having name as f[any-character]a i.e. faa, f.a,
                    f0a, etc.</para>

                    <para><screen>
f.a file:///Vol/input/f.a pool="local"
f.a file:///Vol/input/f.a pool="local" regex="true"
</screen></para>

                    <para>Regular expression based entries also support
                    substitutions. For example, consider the regular
                    expression based entry shown below.</para>

                    <para>Entry 3 will match files with name alpha.csv,
                    alpha.txt, alpha.xml. In addition, values matched in the
                    expression can be used to generate a PFN.</para>

                    <para>For the entry below if the file being looked up is
                    alpha.csv, the PFN for the file would be generated as
                    file:///Volumes/data/input/csv/alpha.csv. Similary if the
                    file being lookedup was alpha.csv, the PFN for the file
                    would be generated as
                    file:///Volumes/data/input/xml/alpha.xml i.e. The section
                    [0], [1] will be replaced. Section [0] refers to the
                    entire string i.e. alpha.csv. Section [1] refers to a
                    partial match in the input i.e. csv, or txt, or xml. Users
                    can utilize as many sections as they wish.</para>

                    <para><screen>
alpha\.(csv|txt|xml) file:///Vol/input/[1]/[0] pool="local" regex="true"
</screen></para>

                    <para>To use File, the user additionally needs to specify
                    pegasus.catalog.replica.file property to specify the path
                    to the file based RC.</para>
                  </listitem>
                </varlistentry>

                <varlistentry>
                  <term>Directory</term>

                  <listitem>
                    <para>In this mode, Pegasus does a directory listing on an
                    input directory to create the LFN to PFN mappings. The
                    directory listing is performed recursively, resulting in
                    deep LFN mappings. For example, if an input directory
                    $input is specified with the following structure <screen>
$input
$input/f.1
$input/f.2
$input/D1
$input/D1/f.3
</screen> Pegasus will create the mappings the following LFN PFN mappings
                    internally <screen>
f.1 file://$input/f.1  pool="local"
f.2 file://$input/f.2  pool="local"
D1/f.3 file://$input/D2/f.3 pool="local"
</screen></para>

                    <para>If you don't want the deep lfn's to be created then,
                    you can set pegasus.catalog.replica.directory.flat.lfn to
                    true In that case, for the previous example, Pegasus will
                    create the following LFN PFN mappings internally. <screen>
f.1 file://$input/f.1  pool="local"
f.2 file://$input/f.2  pool="local"
f.3 file://$input/D2/f.3 pool="local"
</screen></para>

                    <para>pegasus-plan has --input-dir option that can be used
                    to specify an input directory.</para>

                    <para>Users can optionally specify additional properties
                    to configure the behvavior of this implementation.</para>

                    <para>pegasus.catalog.replica.directory.site to specify a
                    site attribute other than local to associate with the
                    mappings.</para>

                    <para>pegasus.catalog.replica.directory.url.prefix to
                    associate a URL prefix for the PFN's constructed. If not
                    specified, the URL defaults to file://</para>
                  </listitem>
                </varlistentry>

                <varlistentry>
                  <term>MRC</term>

                  <listitem>
                    <para>In this mode, Pegasus queries multiple replica
                    catalogs to discover the file locations on the grid. To
                    use it set</para>

                    <para><screen>
pegasus.catalog.replica MRC
</screen></para>

                    <para>Each associated replica catalog can be configured
                    via properties as follows.</para>

                    <para>The user associates a variable name referred to as
                    [value] for each of the catalogs, where [value] is any
                    legal identifier (concretely [A-Za-z][_A-Za-z0-9]*) For
                    each associated replica catalogs the user specifies the
                    following properties.</para>

                    <para><screen>
pegasus.catalog.replica.mrc.[value]       specifies the type of \
                                          replica catalog.
pegasus.catalog.replica.mrc.[value].key   specifies a property name\
                                          key for a particular catalog
</screen></para>

                    <para><screen>
pegasus.catalog.replica.mrc.directory1 LRC
pegasus.catalog.replica.mrc.directory1.url /input/dir1
pegasus.catalog.replica.mrc.directory2 LRC
pegasus.catalog.replica.mrc.directory2.url /input/dir2
</screen></para>

                    <para>In the above example, directory1, directory2 are any
                    valid identifier names and url is the property key that
                    needed to be specified.</para>
                  </listitem>
                </varlistentry>
              </variablelist></para></entry>
          </row>

          <row>
            <entry><literallayout><emphasis role="bold"><emphasis role="bold">Property Key: </emphasis></emphasis>pegasus.catalog.replica.url<emphasis
                  role="bold"><emphasis role="bold">
Profile  Key: </emphasis></emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 2.0
<emphasis role="bold">Default     :</emphasis> (no default)</literallayout></entry>

            <entry>When using the modern RLS replica catalog, the URI to the
            Replica catalog must be provided to Pegasus to enable it to look
            up filenames. There is no default.</entry>
          </row>

          <row>
            <entry><literallayout><emphasis role="bold"><emphasis role="bold"><emphasis
                      role="bold"><emphasis role="bold"><emphasis role="bold">Property Key:</emphasis></emphasis></emphasis></emphasis><emphasis
                    role="bold"><emphasis role="bold"><emphasis role="bold"> </emphasis></emphasis></emphasis></emphasis>pegasus.catalog.replica.chunk.size<emphasis
                  role="bold"><emphasis role="bold"><emphasis role="bold">
Profile  Key: </emphasis></emphasis></emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 2.0
<emphasis role="bold">Default     :</emphasis> 1000
</literallayout></entry>

            <entry><para>The pegasus-rc-client takes in an input file
            containing the mappings upon which to work. This property
            determines, the number of lines that are read in at a time, and
            worked upon at together. This allows the various operations like
            insert, delete happen in bulk if the underlying replica
            implementation supports it.</para></entry>
          </row>

          <row>
            <entry><literallayout><emphasis role="bold"><emphasis role="bold"><emphasis
                      role="bold"><emphasis role="bold"><emphasis role="bold"><emphasis
                            role="bold"><emphasis role="bold"><emphasis
                                role="bold">Property Key:</emphasis></emphasis></emphasis></emphasis></emphasis></emphasis> </emphasis></emphasis>pegasus.catalog.replica.cache.asrc<emphasis
                  role="bold"><emphasis role="bold"><emphasis role="bold"><emphasis
                        role="bold">
Profile Key :</emphasis></emphasis></emphasis></emphasis><emphasis role="bold"><emphasis
                    role="bold"><emphasis role="bold"> </emphasis></emphasis></emphasis>N/A<emphasis
                  role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 2.0
<emphasis role="bold">Default     :</emphasis> false
</literallayout></entry>

            <entry><para>This Boolean property determines whether to treat the
            cache file specified as a supplemental replica catalog or not.
            User can specify on the command line to pegasus-plan a comma
            separated list of cache files using the --cache option. By
            default, the LFN-&gt;PFN mappings contained in the cache file are
            treated as cache, i.e if an entry is found in a cache file the
            replica catalog is not queried. This results in only the entry
            specified in the cache file to be available for replica
            selection.</para>Setting this property to true, results in the
            cache files to be treated as supplemental replica catalogs. This
            results in the mappings found in the replica catalog (as specified
            by pegasus.catalog.replica) to be merged with the ones found in
            the cache files. Thus, mappings for a particular LFN found in both
            the cache and the replica catalog are available for replica
            selection.</entry>
          </row>
        </tbody>
      </tgroup>
    </table>

    <table>
      <title>Site Catalog Properties</title>

      <tgroup cols="2">
        <tbody>
          <row>
            <entry><emphasis role="bold">Key Attributes</emphasis></entry>

            <entry><emphasis role="bold">Description</emphasis></entry>
          </row>

          <row>
            <entry><literallayout><emphasis role="bold"><emphasis role="bold"><emphasis
                      role="bold"><emphasis role="bold"><emphasis role="bold"><emphasis
                            role="bold"><emphasis role="bold"><emphasis
                                role="bold"><emphasis role="bold">Property Key: </emphasis></emphasis></emphasis></emphasis></emphasis></emphasis></emphasis></emphasis></emphasis>pegasus.catalog.site<emphasis
                  role="bold">
Profile  Key: </emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 2.0
<emphasis role="bold">Default     :</emphasis> XML
</literallayout></entry>

            <entry>Pegasus supports two different types of site catalogs in
            XML format conforming to sc-3.0.xsd and sc-4.0.xsd. Pegasus is
            able to auto-detect what schema a user site catalog refers to.
            Hence, this property may no longer be set.</entry>
          </row>

          <row>
            <entry><literallayout><emphasis role="bold"><emphasis role="bold">Property Key:</emphasis> </emphasis>pegasus.catalog.site.file<emphasis
                  role="bold"><emphasis role="bold">
Profile Key : </emphasis></emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 2.0
<emphasis role="bold">Default     :</emphasis> ${pegasus.home.sysconfdir}/sites.xml</literallayout></entry>

            <entry>The path to the site catalog file, that describes the
            various sites and their layouts to Pegasus.</entry>
          </row>
        </tbody>
      </tgroup>
    </table>

    <table>
      <title>Transformation Catalog Properties</title>

      <tgroup cols="2">
        <tbody>
          <row>
            <entry><emphasis role="bold">Key Attributes</emphasis></entry>

            <entry><emphasis role="bold">Description</emphasis></entry>
          </row>

          <row>
            <entry><literallayout><emphasis role="bold">Property Key: </emphasis>pegasus.catalog.transformation<emphasis
                  role="bold">
Profile  Key: </emphasis>N/A<emphasis role="bold">
Scope       : </emphasis>Properties
<emphasis role="bold">Since       :</emphasis> 2.0
<emphasis role="bold">Default     :</emphasis> Text
</literallayout></entry>

            <entry><para>The only recommended and supported version of
            Transformation Catalog for Pegasus is Text. For the old File based
            formats, users should use pegasus-tc-converter to convert File
            format to Text Format.</para><para><variablelist>
                <varlistentry>
                  <term>Text</term>

                  <listitem>
                    <para>In this mode, a multiline file based format is
                    understood. The file is read and cached in memory. Any
                    modifications, as adding or deleting, causes an update of
                    the memory and hence to the file underneath. All queries
                    are done against the memory representation.</para>

                    <para>The file sample.tc.text in the etc directory
                    contains an example</para>

                    <para>Here is a sample textual format for transfomation
                    catalog containing one transformation on two sites</para>

                    <para><screen>
tr example::keg:1.0 {
#specify profiles that apply for all the sites for the transformation
#in each site entry the profile can be overriden
profile env "APP_HOME" "/tmp/karan"
profile env "JAVA_HOME" "/bin/app"
site isi {
profile env "me" "with"
profile condor "more" "test"
profile env "JAVA_HOME" "/bin/java.1.6"
pfn "/path/to/keg"
arch  "x86"
os    "linux"
osrelease "fc"
osversion "4"
type "INSTALLED"
site wind {
profile env "me" "with"
profile condor "more" "test"
pfn "/path/to/keg"
arch  "x86"
os    "linux"
osrelease "fc"
osversion "4"
type "STAGEABLE"
</screen></para>
                  </listitem>
                </varlistentry>
              </variablelist></para></entry>
          </row>

          <row>
            <entry><literallayout><emphasis role="bold"><emphasis role="bold"><emphasis
                      role="bold">Property Key: </emphasis></emphasis></emphasis>pegasus.catalog.transformation<emphasis
                  role="bold"><emphasis role="bold">
Profile Key : </emphasis></emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 2.0
<emphasis role="bold">Default     :</emphasis> ${pegasus.home.sysconfdir}/tc.text </literallayout></entry>

            <entry>The path to the transformation catalog file, that describes
            the locations of the executables.</entry>
          </row>
        </tbody>
      </tgroup>
    </table>
  </section>

  <section>
    <title>Replica Selection Properties</title>

    <para><table>
        <title>Replica Selection Properties</title>

        <tgroup cols="2">
          <tbody>
            <row>
              <entry><emphasis role="bold">Key Attributes</emphasis></entry>

              <entry><emphasis role="bold">Description</emphasis></entry>
            </row>

            <row>
              <entry><literallayout><emphasis role="bold">Property Key: </emphasis>pegasus.selector.replica<emphasis
                    role="bold">
Profile  Key: </emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 2.0
<emphasis role="bold">Type        : </emphasis>String
<emphasis role="bold">Default     :</emphasis> Default
<emphasis role="bold">See Also    :</emphasis> pegasus.selector.replica.*.ignore.stagein.sites<emphasis
                    role="bold">
See Also    :</emphasis> pegasus.selector.replica.*.prefer.stagein.sites
</literallayout></entry>

              <entry><para>Each job in the DAX maybe associated with input
              LFN's denoting the files that are required for the job to run.
              To determine the physical replica (PFN) for a LFN, Pegasus
              queries the replica catalog to get all the PFN's (replicas)
              associated with a LFN. Pegasus then calls out to a replica
              selector to select a replica amongst the various replicas
              returned. This property determines the replica selector to use
              for selecting the replicas.</para><para><variablelist>
                  <varlistentry>
                    <term>Default</term>

                    <listitem>
                       If a PFN that is a file URL (starting with file:///) and has a pool attribute matching to the site handle of the site where the compute is to be run is found, then that is returned. Else,a random PFN is selected amongst all the PFN's that have a pool attribute matching to the site handle of the site where a compute job is to be run. Else, a random pfn is selected amongst all the PFN's. 
                    </listitem>
                  </varlistentry>

                  <varlistentry>
                    <term>Restricted</term>

                    <listitem>
                      <para>This replica selector, allows the user to specify
                      good sites and bad sites for staging in data to a
                      particular compute site. A good site for a compute site
                      X, is a preferred site from which replicas should be
                      staged to site X. If there are more than one good sites
                      having a particular replica, then a random site is
                      selected amongst these preferred sites.</para>

                      <para>A bad site for a compute site X, is a site from
                      which replica's should not be staged. The reason of not
                      accessing replica from a bad site can vary from the link
                      being down, to the user not having permissions on that
                      site's data.</para>

                      <para>The good | bad sites are specified by the
                      properties</para>

                      <para><screen>
pegasus.replica.*.prefer.stagein.sites
pegasus.replica.*.ignore.stagein.sites
</screen></para>

                      <para>where the * in the property name denotes the name
                      of the compute site. A * in the property key is taken to
                      mean all sites.</para>

                      <para>The pegasus.replica.*.prefer.stagein.sites
                      property takes precedence over
                      pegasus.replica.*.ignore.stagein.sites property i.e. if
                      for a site X, a site Y is specified both in the ignored
                      and the preferred set, then site Y is taken to mean as
                      only a preferred site for a site X.</para>
                    </listitem>
                  </varlistentry>

                  <varlistentry>
                    <term>Regex</term>

                    <listitem>
                      <para>This replica selector allows the user allows the
                      user to specific regex expressions that can be used to
                      rank various PFN's returned from the Replica Catalog for
                      a particular LFN. This replica selector selects the
                      highest ranked PFN i.e the replica with the lowest rank
                      value.</para>

                      <para>The regular expressions are assigned different
                      rank, that determine the order in which the expressions
                      are employed. The rank values for the regex can
                      expressed in user properties using the property.</para>

                      <para><screen>
pegasus.selector.replica.regex.rank.[value]   regex-expression
</screen></para>

                      <para>The value is an integer value that denotes the
                      rank of an expression with a rank value of 1 being the
                      highest rank.</para>

                      <para>Please note that before applying any regular
                      expressions on the PFN's, the file URL's that dont match
                      the preferred site are explicitly filtered out.</para>
                    </listitem>
                  </varlistentry>

                  <varlistentry>
                    <term>Local</term>

                    <listitem>
                       This replica selector prefers replicas from the local host and that start with a file: URL scheme. It is useful, when users want to stagin files to a remote site from your submit host using the Condor file transfer mechanism. 
                    </listitem>
                  </varlistentry>
                </variablelist></para></entry>
            </row>

            <row>
              <entry><literallayout><emphasis role="bold"><emphasis
                      role="bold">Property Key: </emphasis></emphasis>pegasus.selector.replica.*.ignore.stagein.sites<emphasis
                    role="bold"><emphasis role="bold">
Profile  Key: </emphasis></emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 2.0
<emphasis role="bold">Default     :</emphasis> (no default)<emphasis
                    role="bold">
See Also    :</emphasis> pegasus.selector.replica<emphasis role="bold">
See Also    :</emphasis> pegasus.selector.replica.*.prefer.stagein.sites</literallayout></entry>

              <entry><para>A comma separated list of storage sites from which
              to never stage in data to a compute site. The property can apply
              to all or a single compute site, depending on how the * in the
              property name is expanded.</para><para>The * in the property
              name means all compute sites unless replaced by a site
              name.</para><para>For e.g setting
              pegasus.selector.replica.*.ignore.stagein.sites to usc means
              that ignore all replicas from site usc for staging in to any
              compute site. Setting pegasus.replica.isi.ignore.stagein.sites
              to usc means that ignore all replicas from site usc for staging
              in data to site isi.</para></entry>
            </row>

            <row>
              <entry><literallayout><emphasis role="bold"><emphasis
                      role="bold">Property Key: </emphasis></emphasis>pegasus.selector.replica.*.prefer.stagein.sites<emphasis
                    role="bold"><emphasis role="bold">
Profile  Key: </emphasis></emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 2.0
<emphasis role="bold">Default     :</emphasis> (no default)<emphasis
                    role="bold">
See Also    :</emphasis> pegasus.selector.replica<emphasis role="bold">
See Also    :</emphasis> pegasus.selector.replica.*.ignore.stagein.sites</literallayout></entry>

              <entry><para>A comma separated list of preferred storage sites
              from which to stage in data to a compute site. The property can
              apply to all or a single compute site, depending on how the * in
              the property name is expanded.</para><para>The * in the property
              name means all compute sites unless replaced by a site
              name.</para><para>For e.g setting
              pegasus.selector.replica.*.prefer.stagein.sites to usc means
              that prefer all replicas from site usc for staging in to any
              compute site. Setting pegasus.replica.isi.prefer.stagein.sites
              to usc means that prefer all replicas from site usc for staging
              in data to site isi.</para></entry>
            </row>

            <row>
              <entry><literallayout><emphasis role="bold"><emphasis
                      role="bold">Property Key: </emphasis></emphasis>pegasus.selector.replica.regex.rank.[value]<emphasis
                    role="bold"><emphasis role="bold">
Profile  Key: </emphasis></emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 2.3.0
<emphasis role="bold">Default     :</emphasis> (no default)<emphasis
                    role="bold">
See Also    :</emphasis> pegasus.selector.replica</literallayout></entry>

              <entry><para>Specifies the regex expressions to be applied on
              the PFNs returned for a particular LFN. Refer to <screen>
http://java.sun.com/javase/6/docs/api/java/util/regex/Pattern.html
</screen> on information of how to construct a regex
              expression.</para><para>The [value] in the property key is to be
              replaced by an int value that designates the rank value for the
              regex expression to be applied in the Regex replica
              selector.</para><para>The example below indicates preference for
              file URL's over URL's referring to gridftp server at
              example.isi.edu</para><para><screen>
pegasus.selector.replica.regex.rank.1 file://.*
pegasus.selector.replica.regex.rank.2 gsiftp://example\.isi\.edu.*
</screen></para></entry>
            </row>
          </tbody>
        </tgroup>
      </table></para>
  </section>

  <section>
    <title>Site Selection Properties</title>

    <para><table>
        <title>Site Selection Properties</title>

        <tgroup cols="2">
          <tbody>
            <row>
              <entry><emphasis role="bold">Key Attributes</emphasis></entry>

              <entry><emphasis role="bold">Description</emphasis></entry>
            </row>

            <row>
              <entry><literallayout><emphasis role="bold">Property Key: </emphasis>pegasus.selector.site<emphasis
                    role="bold">
Profile  Key: </emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 2.0
<emphasis role="bold">Type        : </emphasis>String
<emphasis role="bold">Default     :</emphasis> Random
<emphasis role="bold">See Also    :</emphasis> pegasus.selector.site.path<emphasis
                    role="bold">
See Also    :</emphasis> pegasus.selector.site.timeout
<emphasis role="bold">See Also    :</emphasis> pegasus.selector.site.keep.tmp<emphasis
                    role="bold">
See Also    : </emphasis>pegasus.selector.site.env.*</literallayout></entry>

              <entry><para>The site selection in Pegasus can be on basis of
              any of the following strategies.</para><para><variablelist>
                  <varlistentry>
                    <term>Random</term>

                    <listitem>
                       In this mode, the jobs will be randomly distributed among the sites that can execute them. 
                    </listitem>
                  </varlistentry>

                  <varlistentry>
                    <term>RoundRobin</term>

                    <listitem>
                       In this mode. the jobs will be assigned in a round robin manner amongst the sites that can execute them. Since each site cannot execute everytype of job, the round robin scheduling is done per level on a sorted list. The sorting is on the basis of the number of jobs a particular site has been assigned in that level so far. If a job cannot be run on the first site in the queue (due to no matching entry in the transformation catalog for the transformation referred to by the job), it goes to the next one and so on. This implementation defaults to classic round robin in the case where all the jobs in the workflow can run on all the sites. 
                    </listitem>
                  </varlistentry>

                  <varlistentry>
                    <term>NonJavaCallout</term>

                    <listitem>
                      <para>In this mode, Pegasus will callout to an external
                      site selector.In this mode a temporary file is prepared
                      containing the job information that is passed to the
                      site selector as an argument while invoking it. The path
                      to the site selector is specified by setting the
                      property pegasus.site.selector.path. The environment
                      variables that need to be set to run the site selector
                      can be specified using the properties with a
                      pegasus.site.selector.env. prefix. The temporary file
                      contains information about the job that needs to be
                      scheduled. It contains key value pairs with each key
                      value pair being on a new line and separated by a
                      =.</para>

                      <para>The following pairs are currently generated for
                      the site selector temporary file that is generated in
                      the NonJavaCallout.</para>

                      <para><informaltable frame="none">
                          <tgroup align="left" cols="2" colsep="1" rowsep="1">
                            <tbody>
                              <row>
                                <entry>version</entry>

                                <entry>is the version of the site selector
                                api,currently 2.0.</entry>
                              </row>

                              <row>
                                <entry>transformation</entry>

                                <entry>is the fully-qualified definition identifier
                                for the transformation (TR)
                                namespace::name:version.</entry>
                              </row>

                              <row>
                                <entry>derivation</entry>

                                <entry>is teh fully qualified definition identifier
                                for the derivation (DV),
                                namespace::name:version.</entry>
                              </row>

                              <row>
                                <entry>job.level</entry>

                                <entry>is the job's depth in the tree of the
                                workflow DAG.</entry>
                              </row>

                              <row>
                                <entry>job.id</entry>

                                <entry>is the job's ID, as used in the DAX
                                file.</entry>
                              </row>

                              <row>
                                <entry>resource.id</entry>

                                <entry>is a pool handle, followed by whitespace,
                                followed by a gridftp server. Typically, each
                                gridftp server is enumerated once, so you may have
                                multiple occurances of the same site. There can be
                                multiple occurances of this key.</entry>
                              </row>

                              <row>
                                <entry>input.lfn</entry>

                                <entry>is an input LFN, optionally followed by a
                                whitespace and file size. There can be multiple
                                occurances of this key,one for each input LFN
                                required by the job.</entry>
                              </row>

                              <row>
                                <entry>wf.name</entry>

                                <entry>label of the dax, as found in the DAX's root
                                element. wf.index is the DAX index, that is
                                incremented for each partition in case of deferred
                                planning.</entry>
                              </row>

                              <row>
                                <entry>wf.time</entry>

                                <entry>is the mtime of the workflow.</entry>
                              </row>

                              <row>
                                <entry>wf.manager</entry>

                                <entry>is the name of the workflow manager being
                                used .e.g condor</entry>
                              </row>

                              <row>
                                <entry>vo.name</entry>

                                <entry>is the name of the virtual organization that
                                is running this workflow. It is currently set to
                                NONE</entry>
                              </row>

                              <row>
                                <entry>vo.group</entry>

                                <entry>unused at present and is set to NONE.</entry>
                              </row>

                              <row>
                                <entry/>
                              </row>
                            </tbody>
                          </tgroup>
                        </informaltable></para>
                    </listitem>
                  </varlistentry>

                  <varlistentry>
                    <term>Group</term>

                    <listitem>
                       In this mode, a group of jobs will be assigned to the same site that can execute them. The use of the PEGASUS profile key group in the dax, associates a job with a particular group. The jobs that do not have the profile key associated with them, will be put in the default group. The jobs in the default group are handed over to the "Random" Site Selector for scheduling. 
                    </listitem>
                  </varlistentry>

                  <varlistentry>
                    <term>Heft</term>

                    <listitem>
                      <para>In this mode, a version of the HEFT processor
                      scheduling algorithm is used to schedule jobs in the
                      workflow to multiple grid sites. The implementation
                      assumes default data communication costs when jobs are
                      not scheduled on to the same site. Later on this may be
                      made more configurable.</para>

                      <para>The runtime for the jobs is specified in the
                      transformation catalog by associating the pegasus
                      profile key runtime with the entries.</para>

                      <para>The number of processors in a site is picked up
                      from the attribute idle-nodes associated with the
                      vanilla jobmanager of the site in the site
                      catalog.</para>
                    </listitem>
                  </varlistentry>
                </variablelist></para></entry>
            </row>

            <row>
              <entry><literallayout><emphasis role="bold"><emphasis
                      role="bold">Property Key: </emphasis></emphasis>pegasus.selector.site.path<emphasis
                    role="bold"><emphasis role="bold">
Profile  Key: </emphasis></emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 2.0
<emphasis role="bold">Default     :</emphasis> (no default)</literallayout></entry>

              <entry>If one calls out to an external site selector using the
              NonJavaCallout mode, this refers to the path where the site
              selector is installed. In case other strategies are used it does
              not need to be set.</entry>
            </row>

            <row>
              <entry><literallayout><emphasis role="bold"><emphasis
                      role="bold">Property Key: </emphasis></emphasis>pegasus.selector.site.env.*<emphasis
                    role="bold"><emphasis role="bold">
Profile  Key: </emphasis></emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 2.0
<emphasis role="bold">Default     :</emphasis> (no default)</literallayout></entry>

              <entry><para>The environment variables that need to be set while
              callout to the site selector. These are the variables that the
              user would set if running the site selector on the command line.
              The name of the environment variable is got by stripping the
              keys of the prefix "pegasus.site.selector.env." prefix from
              them. The value of the environment variable is the value of the
              property.</para><para>e.g
              pegasus.site.selector.path.LD_LIBRARY_PATH /globus/lib would
              lead to the site selector being called with the LD_LIBRARY_PATH
              set to /globus/lib.</para></entry>
            </row>

            <row>
              <entry><literallayout><emphasis role="bold"><emphasis
                      role="bold">Property Key: </emphasis></emphasis>pegasus.selector.site.timeout<emphasis
                    role="bold"><emphasis role="bold">
Profile  Key: </emphasis></emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 2.3.0
<emphasis role="bold">Default     :</emphasis> 60<emphasis role="bold">
See Also    :</emphasis> pegasus.selector.site</literallayout></entry>

              <entry>It sets the number of seconds Pegasus waits to hear back
              from an external site selector using the NonJavaCallout
              interface before timing out.</entry>
            </row>

            <row>
              <entry><literallayout><emphasis role="bold"><emphasis
                      role="bold">Property Key: </emphasis></emphasis>pegasus.selector.site.keep.tmp<emphasis
                    role="bold"><emphasis role="bold">
Profile  Key: </emphasis></emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 2.3.0
<emphasis role="bold">Values</emphasis>      : onerror|always|never
<emphasis role="bold">Default     :</emphasis> onerror<emphasis role="bold">
See Also    :</emphasis> pegasus.selector.site</literallayout></entry>

              <entry><para>It determines whether Pegasus deletes the temporary
              input files that are generated in the temp directory or not.
              These temporary input files are passed as input to the external
              site selectors.</para><para>A temporary input file is created
              for each that needs to be scheduled.</para></entry>
            </row>
          </tbody>
        </tgroup>
      </table></para>
  </section>

  <section>
    <title>Data Staging Configuration Properties</title>

    <para><table>
        <title>Data Configuration Properties</title>

        <tgroup cols="2">
          <tbody>
            <row>
              <entry><emphasis role="bold">Key Attributes</emphasis></entry>

              <entry><emphasis role="bold">Description</emphasis></entry>
            </row>

            <row>
              <entry><literallayout><emphasis role="bold"><emphasis
                      role="bold">Property Key: </emphasis></emphasis>pegasus.data.configuration<emphasis
                    role="bold"><emphasis role="bold">
Profile  Key: </emphasis></emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 4.0.0
<emphasis role="bold">Values</emphasis>      : sharedfs|nonsharedfs|condorio
<emphasis role="bold">Default     :</emphasis> sharedfs<emphasis role="bold">
See Also    :</emphasis> pegasus.transfer.bypass.input.staging</literallayout></entry>

              <entry><para>This property sets up Pegasus to run in different
              environments.</para><para><variablelist>
                  <varlistentry>
                    <term>sharedfs</term>

                    <listitem>
                       If this is set, Pegasus will be setup to execute jobs on the shared filesystem on the execution site. This assumes, that the head node of a cluster and the worker nodes share a filesystem. The staging site in this case is the same as the execution site. Pegasus adds a create dir job to the executable workflow that creates a workflow specific directory on the shared filesystem . The data transfer jobs in the executable workflow ( stage_in_ , stage_inter_ , stage_out_ ) transfer the data to this directory.The compute jobs in the executable workflow are launched in the directory on the shared filesystem. Internally, if this is set the following properties are set. 

                      <screen>
pegasus.execute.*.filesystem.local   false
</screen>

                       
                    </listitem>
                  </varlistentry>

                  <varlistentry>
                    <term>condorio</term>

                    <listitem>
                       If this is set, Pegasus will be setup to run jobs in a pure condor pool, with the nodes not sharing a filesystem. Data is staged to the compute nodes from the submit host using Condor File IO. The planner is automatically setup to use the submit host ( site local ) as the staging site. All the auxillary jobs added by the planner to the executable workflow ( create dir, data stagein and stage-out, cleanup ) jobs refer to the workflow specific directory on the local site. The data transfer jobs in the executable workflow ( stage_in_ , stage_inter_ , stage_out_ ) transfer the data to this directory. When the compute jobs start, the input data for each job is shipped from the workflow specific directory on the submit host to compute/worker node using Condor file IO. The output data for each job is similarly shipped back to the submit host from the compute/worker node. This setup is particularly helpful when running workflows in the cloud environment where setting up a shared filesystem across the VM's may be tricky. On loading this property, internally the following properies are set 

                      <screen>
pegasus.transfer.lite.*.impl          Condor
pegasus.execute.*.filesystem.local   true
pegasus.gridstart 		   PegasusLite
pegasus.transfer.worker.package      true
</screen>

                       
                    </listitem>
                  </varlistentry>

                  <varlistentry>
                    <term>nonsharedfs</term>

                    <listitem>
                       If this is set, Pegasus will be setup to execute jobs on an execution site without relying on a shared filesystem between the head node and the worker nodes. You can specify staging site ( using --staging-site option to pegasus-plan) to indicate the site to use as a central storage location for a workflow. The staging site is independant of the execution sites on which a workflow executes. All the auxillary jobs added by the planner to the executable workflow ( create dir, data stagein and stage-out, cleanup ) jobs refer to the workflow specific directory on the staging site. The data transfer jobs in the executable workflow ( stage_in_ , stage_inter_ , stage_out_ ) transfer the data to this directory. When the compute jobs start, the input data for each job is shipped from the workflow specific directory on the submit host to compute/worker node using pegasus-transfer. The output data for each job is similarly shipped back to the submit host from the compute/worker node. The protocols supported are at this time SRM, GridFTP, iRods, S3. This setup is particularly helpful when running workflows on OSG where most of the execution sites don't have enough data storage. Only a few sites have large amounts of data storage exposed that can be used to place data during a workflow run. This setup is also helpful when running workflows in the cloud environment where setting up a shared filesystem across the VM's may be tricky. On loading this property, internally the following properies are set 

                      <screen>
pegasus.execute.*.filesystem.local   true
pegasus.gridstart 		   PegasusLite
pegasus.transfer.worker.package      true
</screen>

                       
                    </listitem>
                  </varlistentry>
                </variablelist></para></entry>
            </row>

            <row>
              <entry><literallayout><emphasis role="bold"><emphasis
                      role="bold">Property Key: </emphasis></emphasis>pegasus.transfer.bypass.input.staging<emphasis
                    role="bold"><emphasis role="bold">
Profile  Key: </emphasis></emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 4.3.0
<emphasis role="bold">Type        : </emphasis>Boolean
<emphasis role="bold">Default     :</emphasis> false<emphasis role="bold">
See Also    :</emphasis> pegasus.data.configuration</literallayout></entry>

              <entry><para>When executiing in a non shared filesystem setup
              i.e data configuration set to nonsharedfs or condorio, Pegasus
              always stages the input files through the staging site i.e the
              stage-in job stages in data from the input site to the staging
              site. The PegasusLite jobs that start up on the worker nodes,
              then pull the input data from the staging site for each
              job.</para><para>This property can be used to setup the
              PegasusLite jobs to pull input data directly from the input site
              without going through the staging server. This is based on the
              assumption that the worker nodes can access the input site. If
              users set this to true, they should be aware that the access to
              the input site is no longer throttled ( as in case of stage in
              jobs). If large number of compute jobs start at the same time in
              a workflow, the input server will see a connection from each
              job.</para></entry>
            </row>
          </tbody>
        </tgroup>
      </table></para>
  </section>

  <section>
    <title>Transfer Configuration Properties</title>

    <para><table>
        <title>Transfer Configuration Properties</title>

        <tgroup cols="2">
          <tbody>
            <row>
              <entry><emphasis role="bold">Key Attributes</emphasis></entry>

              <entry><emphasis role="bold">Description</emphasis></entry>
            </row>

            <row>
              <entry><literallayout><emphasis role="bold"><emphasis
                      role="bold">Property Key: </emphasis></emphasis>pegasus.transfer.*.impl<emphasis
                    role="bold"><emphasis role="bold">
Profile  Key: </emphasis></emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 2.0.0
<emphasis role="bold">Values</emphasis>      : Transfer|GUC
<emphasis role="bold">Default     :</emphasis> Transfer<emphasis role="bold">
See Also    :</emphasis> pegasus.transfer.refiner</literallayout></entry>

              <entrytbl cols="1">
                <tbody>
                  <row>
                    <entry><para>Each compute job usually has data products
                    that are required to be staged in to the execution site,
                    materialized data products staged out to a final resting
                    place, or staged to another job running at a different
                    site. This property determines the underlying grid
                    transfer tool that is used to manage the
                    transfers.</para><para>The * in the property name can be
                    replaced to achieve finer grained control to dictate what
                    type of transfer jobs need to be managed with which grid
                    transfer tool.</para><para>Usually,the arguments with
                    which the client is invoked can be specified by <screen>
- the property pegasus.transfer.arguments
- associating the PEGASUS profile key transfer.arguments
</screen></para><para>The table below illustrates all the possible variations
                    of the property.</para><para><informaltable frame="none">
                        <tgroup align="left" cols="2" colsep="1" rowsep="1">
                          <tbody>
                            <row>
                              <entry>Property Name</entry>

                              <entry>Applies to</entry>
                            </row>

                            <row>
                              <entry>pegasus.transfer.stagein.impl</entry>

                              <entry>the stage in transfer jobs</entry>
                            </row>

                            <row>
                              <entry>pegasus.transfer.stageout.impl</entry>

                              <entry>the stage out transfer jobs</entry>
                            </row>

                            <row>
                              <entry>pegasus.transfer.inter.impl</entry>

                              <entry>the inter pool transfer jobs</entry>
                            </row>

                            <row>
                              <entry>pegasus.transfer.setup.impl</entry>

                              <entry>the setup transfer job</entry>
                            </row>

                            <row>
                              <entry>pegasus.transfer.*.impl</entry>

                              <entry>apply to types of transfer jobs</entry>
                            </row>

                            <row>
                              <entry/>
                            </row>
                          </tbody>
                        </tgroup>
                      </informaltable></para><para>Note: Since version 2.2.0
                    the worker package is staged automatically during staging
                    of executables to the remote site. This is achieved by
                    adding a setup transfer job to the workflow. The setup
                    transfer job by default uses GUC to stage the data. The
                    implementation to use can be configured by setting the
                    property <screen>pegasus.transfer.setup.impl </screen>property.
                    However, if you have pegasus.transfer.*.impl set in your
                    properties file, then you need to set
                    pegasus.transfer.setup.impl to GUC</para><para>The various
                    grid transfer tools that can be used to manage data
                    transfers are explained below</para><para><variablelist>
                        <varlistentry>
                          <term>Transfer</term>

                          <listitem>
                            <para>This results in pegasus-transfer to be used
                            for transferring of files. It is a python based
                            wrapper around various transfer clients like
                            globus-url-copy, lcg-copy, wget, cp, ln .
                            pegasus-transfer looks at source and destination url
                            and figures out automatically which underlying
                            client to use. pegasus-transfer is distributed with
                            the PEGASUS and can be found at
                            $PEGASUS_HOME/bin/pegasus-transfer.</para>

                            <para>For remote sites, Pegasus constructs the
                            default path to pegasus-transfer on the basis of
                            PEGASUS_HOME env profile specified in the site
                            catalog. To specify a different path to the
                            pegasus-transfer client , users can add an entry
                            into the transformation catalog with fully qualified
                            logical name as pegasus::pegasus-transfer</para>
                          </listitem>
                        </varlistentry>

                        <varlistentry>
                          <term>GUC</term>

                          <listitem>
                             This refers to the new guc client that does multiple file transfers per invocation. The globus-url-copy client distributed with Globus 4.x is compatible with this mode. 
                          </listitem>
                        </varlistentry>
                      </variablelist></para></entry>
                  </row>
                </tbody>
              </entrytbl>
            </row>

            <row>
              <entry><literallayout><emphasis role="bold"><emphasis
                      role="bold">Property Key: </emphasis></emphasis>pegasus.transfer.lite.*.impl<emphasis
                    role="bold"><emphasis role="bold">
Profile  Key: </emphasis></emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 4.0
<emphasis role="bold">Type        : </emphasis>String
<emphasis role="bold">Values</emphasis>      : Transfer|Condor<emphasis
                    role="bold">
Default     :</emphasis> Transfer<emphasis role="bold">
See Also    :</emphasis> pegasus.data.configuration
<emphasis role="bold">See Also    :</emphasis> pegasus.execute.*.filesystem.local</literallayout></entry>

              <entrytbl cols="1">
                <tbody>
                  <row>
                    <entry><para>This property specifies the transfer tool to
                    be used for staging of input and output data between the
                    staging site and worker node filesystems in
                    PegasusLite.</para><para>Currently, the * in the property
                    name CANNOT be replaced to achieve finer grained control
                    to dictate what type of transfers need to be managed with
                    which grid transfer tool.</para><para>The various grid
                    transfer tools that can be used to manage PegasusLite data
                    transfers are explained below</para><para><variablelist>
                        <varlistentry>
                          <term>Transfer</term>

                          <listitem>
                            <para>This results in pegasus-transfer to be used
                            for transferring of files. It is a python based
                            wrapper around various transfer clients like
                            globus-url-copy, lcg-copy, wget, cp, ln .
                            pegasus-transfer looks at source and destination url
                            and figures out automatically which underlying
                            client to use. pegasus-transfer is distributed with
                            the PEGASUS and can be found at
                            $PEGASUS_HOME/bin/pegasus-transfer.</para>

                            <para>For remote sites, Pegasus constructs the
                            default path to pegasus-transfer on the basis of
                            PEGASUS_HOME env profile specified in the site
                            catalog. To specify a different path to the
                            pegasus-transfer client , users can add an entry
                            into the transformation catalog with fully qualified
                            logical name as pegasus::pegasus-transfer</para>
                          </listitem>
                        </varlistentry>

                        <varlistentry>
                          <term>Condor</term>

                          <listitem>
                            <para>This results in Condor file transfer mechanism
                            to be used to transfer the input data files from the
                            submit host directly to the worker node directories.
                            This is used when running in pure Condor mode or in
                            a Condor pool that does not have a shared filesystem
                            between the nodes.</para>

                            <para>When setting the PegasusLite transfers to
                            Condor make sure that the following properties are
                            also set <screen>
pegasus.gridstart		        PegasusLite
pegasus.execute.*.filesystem.local  true
</screen> Alternatively, you can set <screen>
pegasus.data.configuration           condorio
</screen> in lieu of the above 3 properties.</para>

                            <para>Also make sure that pegasus.gridstart is not
                            set.</para>

                            <para>Please refer to the section on "Condor Pool
                            Without a Shared Filesystem" in the chapter on
                            Planning and Submitting.</para>
                          </listitem>
                        </varlistentry>
                      </variablelist></para></entry>
                  </row>
                </tbody>
              </entrytbl>
            </row>

            <row>
              <entry><literallayout><emphasis role="bold"><emphasis
                      role="bold">Property Key: </emphasis></emphasis>pegasus.transfer.arguments<emphasis
                    role="bold"><emphasis role="bold">
Profile  Key: </emphasis></emphasis>transfer.arguments<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 2.0.0
<emphasis role="bold">Type        : </emphasis>String<emphasis role="bold">
Default     :</emphasis> (no default)<emphasis role="bold">
See Also    :</emphasis> pegasus.transfer.lite.arguments</literallayout></entry>

              <entry><para>This determines the extra arguments with which the
              transfer implementation is invoked. The transfer executable that
              is invoked is dependant upon the transfer mode that has been
              selected. The property can be overloaded by associated the
              pegasus profile key transfer.arguments either with the site in
              the site catalog or the corresponding transfer executable in the
              transformation catalog.</para></entry>
            </row>

            <row>
              <entry><literallayout><emphasis role="bold"><emphasis
                      role="bold">Property Key: </emphasis></emphasis>pegasus.transfer.threads<emphasis
                    role="bold"><emphasis role="bold">
Profile  Key: </emphasis></emphasis>transfer.threads<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 4.4.0
<emphasis role="bold">Type        : </emphasis>Integer<emphasis role="bold">
Default     :</emphasis> 2<emphasis role="bold">
See Also    :</emphasis> pegasus.transfer.lite.threads</literallayout></entry>

              <entry><para>This property set the number of threads
              pegasus-transfer uses to transfer the files. This property to
              applies to the separate data transfer nodes that are added by
              Pegasus to the executable workflow. The property can be
              overloaded by associated the pegasus profile key
              transfer.threads either with the site in the site catalog or the
              corresponding transfer executable in the transformation
              catalog.</para></entry>
            </row>

            <row>
              <entry><literallayout><emphasis role="bold"><emphasis
                      role="bold">Property Key: </emphasis></emphasis>pegasus.transfer.lite.arguments<emphasis
                    role="bold"><emphasis role="bold">
Profile  Key: </emphasis></emphasis>transfer.lite.arguments<emphasis
                    role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 4.4.0
<emphasis role="bold">Type        : </emphasis>String<emphasis role="bold">
Default     :</emphasis> (no default)<emphasis role="bold">
See Also    :</emphasis> pegasus.transfer.arguments</literallayout></entry>

              <entry>This determines the extra arguments with which the
              PegasusLite transfer implementation is invoked. The transfer
              executable that is invoked is dependant upon the PegasusLite
              transfer implementation that has been selected.</entry>
            </row>

            <row>
              <entry><literallayout><emphasis role="bold"><emphasis
                      role="bold">Property Key: </emphasis></emphasis>pegasus.transfer.lite.threads<emphasis
                    role="bold"><emphasis role="bold">
Profile  Key: </emphasis></emphasis>transfer.lite.threads<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 4.4.0
<emphasis role="bold">Type        : </emphasis>Integer<emphasis role="bold">
Default     :</emphasis> 1<emphasis role="bold">
See Also    :</emphasis> pegasus.transfer.threads</literallayout></entry>

              <entry>This property set the number of threads pegasus-transfer
              uses to transfer the files. This property applies to the
              pegasus-transfer invocations in the PegasusLite jobs in the
              nonsharedfs data configuration. The property can be overloaded
              by associated the pegasus profile key transfer.lite.threads
              either with the site in the site catalog or the DAX for the
              associate compute jobs.</entry>
            </row>

            <row>
              <entry><literallayout><emphasis role="bold"><emphasis
                      role="bold">Property Key: </emphasis></emphasis>pegasus.transfer.worker.package<emphasis
                    role="bold"><emphasis role="bold">
Profile  Key: </emphasis></emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 2.0.0
<emphasis role="bold">Type        : </emphasis>Boolean<emphasis role="bold">
Default     :</emphasis> false<emphasis role="bold">
See Also    :</emphasis> pegasus.data.configuration</literallayout></entry>

              <entry><para>By default, Pegasus relies on the worker package to
              be installed in a directory accessible to the worker nodes on
              the remote sites . Pegasus uses the value of PEGASUS_HOME
              environment profile in the site catalog for the remote sites, to
              then construct paths to pegasus auxillary executables like
              kickstart, pegasus-transfer, seqexec etc.</para><para>If the
              Pegasus worker package is not installed on the remote sites
              users can set this property to true to get Pegasus to deploy
              worker package on the nodes.</para><para>In the case of sharedfs
              setup, the worker package is deployed on the shared scratch
              directory for the workflow , that is accessible to all the
              compute nodes of the remote sites.</para><para>When running in
              nonsharefs environments, the worker package is first brought to
              the submit directory and then transferred to the worker node
              filesystem using Condor file IO.</para></entry>
            </row>

            <row>
              <entry><literallayout><emphasis role="bold"><emphasis
                      role="bold">Property Key: </emphasis></emphasis>pegasus.transfer.links<emphasis
                    role="bold"><emphasis role="bold">
Profile  Key: </emphasis></emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 2.0.0
<emphasis role="bold">Type        : </emphasis>Boolean<emphasis role="bold">
Default     :</emphasis> false<emphasis role="bold">
</emphasis></literallayout></entry>

              <entry>If this is set, and the transfer implementation is set to
              Transfer i.e. using the transfer executable distributed with the
              PEGASUS. On setting this property, if Pegasus while fetching
              data from the Replica Catalog sees a pool attribute associated
              with the PFN that matches the execution pool on which the data
              has to be transferred to, Pegasus instead of the URL returned by
              the Replica Catalog replaces it with a file based URL. This is
              based on the assumption that the if the pools match the
              filesystems are visible to the remote execution directory where
              input data resides. On seeing both the source and destination
              urls as file based URLs the transfer executable spawns a job
              that creates a symbolic link by calling ln -s on the remote
              site.</entry>
            </row>

            <row>
              <entry><literallayout><emphasis role="bold"><emphasis
                      role="bold">Property Key: </emphasis></emphasis>pegasus.transfer.*.remote.sites<emphasis
                    role="bold"><emphasis role="bold">
Profile  Key: </emphasis></emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 2.0.0
<emphasis role="bold">Type        : </emphasis>comma separated list of sites<emphasis
                    role="bold">
Default     :</emphasis> (no default)<emphasis role="bold">
</emphasis></literallayout></entry>

              <entry><para>By default Pegasus looks at the source and
              destination URL's for to determine whether the associated
              transfer job runs on the submit host or the head node of a
              remote site, with preference set to run a transfer job to run on
              submit host.</para><para>Pegasus will run transfer jobs on the
              remote sites</para><para><screen>
-  if the file server for the compute site is a file server i.e url prefix file://
-  symlink jobs need to be added that require the symlink transfer jobs to
be run remotely.
</screen></para><para>This property can be used to change the default
              behaviour of Pegasus and force pegasus to run different types of
              transfer jobs for the sites specified on the remote
              site.</para><para>The table below illustrates all the possible
              variations of the property.</para><para><informaltable
                  frame="none">
                  <tgroup align="left" cols="2" colsep="1" rowsep="1">
                    <tbody>
                      <row>
                        <entry>Property Name</entry>

                        <entry>Applies to</entry>
                      </row>

                      <row>
                        <entry>pegasus.transfer.stagein.remote.sites</entry>

                        <entry>the stage in transfer jobs</entry>
                      </row>

                      <row>
                        <entry>pegasus.transfer.stageout.remote.sites</entry>

                        <entry>the stage out transfer jobs</entry>
                      </row>

                      <row>
                        <entry>pegasus.transfer.inter.remote.sites</entry>

                        <entry>the inter pool transfer jobs</entry>
                      </row>

                      <row>
                        <entry>pegasus.transfer.*.remote.sites</entry>

                        <entry>apply to types of transfer jobs</entry>
                      </row>

                      <row>
                        <entry/>
                      </row>
                    </tbody>
                  </tgroup>
                </informaltable></para><para>In addition * can be specified as
              a property value, to designate that it applies to all
              sites.</para></entry>
            </row>

            <row>
              <entry><literallayout><emphasis role="bold"><emphasis
                      role="bold">Property Key: </emphasis></emphasis>pegasus.transfer.staging.delimiter<emphasis
                    role="bold"><emphasis role="bold">
Profile  Key: </emphasis></emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 2.0.0
<emphasis role="bold">Type        : </emphasis>String<emphasis role="bold">
Default     :</emphasis> :<emphasis role="bold">
</emphasis></literallayout></entry>

              <entry>Pegasus supports executable staging as part of the
              workflow. Currently staging of statically linked executables is
              supported only. An executable is normally staged to the work
              directory for the workflow/partition on the remote site. The
              basename of the staged executable is derived from the
              namespace,name and version of the transformation in the
              transformation catalog. This property sets the delimiter that is
              used for the construction of the name of the staged
              executable.</entry>
            </row>

            <row>
              <entry><literallayout><emphasis role="bold"><emphasis
                      role="bold">Property Key: </emphasis></emphasis>pegasus.transfer.disable.chmod.sites<emphasis
                    role="bold"><emphasis role="bold">
Profile  Key: </emphasis></emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 2.0.0
<emphasis role="bold">Type        : </emphasis>comma separated list of sites<emphasis
                    role="bold">
Default     :</emphasis> (no default)<emphasis role="bold">
</emphasis></literallayout></entry>

              <entry><para>During staging of executables to remote sites,
              chmod jobs are added to the workflow. These jobs run on the
              remote sites and do a chmod on the staged executable. For some
              sites, this maynot be required. The permissions might be
              preserved, or there maybe an automatic mechanism that does
              it.</para><para>This property allows you to specify the list of
              sites, where you do not want the chmod jobs to be executed. For
              those sites, the chmod jobs are replaced by NoOP jobs. The NoOP
              jobs are executed by Condor, and instead will immediately have a
              terminate event written to the job log file and removed from the
              queue.</para></entry>
            </row>

            <row>
              <entry><literallayout><emphasis role="bold"><emphasis
                      role="bold">Property Key: </emphasis></emphasis>pegasus.transfer.setup.source.base.url<emphasis
                    role="bold"><emphasis role="bold">
Profile  Key: </emphasis></emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 2.0.0
<emphasis role="bold">Type        : </emphasis>URL
<emphasis role="bold">Default     :</emphasis> (no default)<emphasis
                    role="bold">
</emphasis></literallayout></entry>

              <entry>This property specifies the base URL to the directory
              containing the Pegasus worker package builds. During Staging of
              Executable, the Pegasus Worker Package is also staged to the
              remote site. The worker packages are by default pulled from the
              http server at pegasus.isi.edu. This property can be used to
              override the location from where the worker package are staged.
              This maybe required if the remote computes sites don't allows
              files transfers from a http server.</entry>
            </row>
          </tbody>
        </tgroup>
      </table></para>
  </section>

  <section>
    <title>Monitoring Properties</title>

    <table>
      <title>Monitoring Properties</title>

      <tgroup cols="2">
        <tbody>
          <row>
            <entry><emphasis role="bold">Key Attributes</emphasis></entry>

            <entry><emphasis role="bold">Description</emphasis></entry>
          </row>

          <row>
            <entry><literallayout><emphasis role="bold"><emphasis role="bold">Property Key: </emphasis></emphasis>pegasus.monitord.events<emphasis
                  role="bold"><emphasis role="bold">
Profile  Key: </emphasis></emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 3.0.2
<emphasis role="bold">Type  </emphasis>      : String
<emphasis role="bold">Default     :</emphasis> true<emphasis role="bold">
See Also    :</emphasis> pegasus.monitord.output</literallayout></entry>

            <entry>This property determines whether pegasus-monitord generates
            log events. If log events are disabled using this property, no bp
            file, or database will be created, even if the
            pegasus.monitord.output property is specified.</entry>
          </row>

          <row>
            <entry><literallayout><emphasis role="bold"><emphasis role="bold">Property Key: </emphasis></emphasis>pegasus.monitord.output<emphasis
                  role="bold"><emphasis role="bold">
Profile  Key: </emphasis></emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 3.0.2
<emphasis role="bold">Type  </emphasis>      : String
<emphasis role="bold">Default     :</emphasis> SQlite database in submit
              directory.
<emphasis role="bold">See Also    :</emphasis> pegasus.monitord.events</literallayout></entry>

            <entry><para>This property specifies the destination for generated
            log events in pegasus-monitord. By default, events are stored in a
            sqlite database in the workflow directory, which will be created
            with the workflow's name, and a ".stampede.db" extension. Users
            can specify an alternative database by using a SQLAlchemy
            connection string. Details are available at: <screen>
http://www.sqlalchemy.org/docs/05/reference/dialects/index.html
</screen> It is important to note that users will need to have the appropriate
            db interface library installed. Which is to say, SQLAlchemy is a
            wrapper around the mysql interface library (for instance), it does
            not provide a MySQL driver itself. The Pegasus distribution
            includes both SQLAlchemy and the SQLite Python driver. As a final
            note, it is important to mention that unlike when using SQLite
            databases, using SQLAlchemy with other database servers, e.g.
            MySQL or Postgres , the target database needs to exist. Users can
            also specify a file name using this property in order to create a
            file with the log events.</para><para>Example values for the
            SQLAlchemy connection string for various end points are listed
            below</para><para><informaltable frame="none">
                <tgroup align="left" cols="2" colsep="1" rowsep="1">
                  <tbody>
                    <row>
                      <entry>SQL Alchemy End Point</entry>

                      <entry>Example Value</entry>
                    </row>

                    <row>
                      <entry>Netlogger BP File</entry>

                      <entry>file:///submit/dir/myworkflow.bp</entry>
                    </row>

                    <row>
                      <entry>SQL Lite Database</entry>

                      <entry>sqlite:///submit/dir/myworkflow.db</entry>
                    </row>

                    <row>
                      <entry>MySQL Database</entry>

                      <entry>mysql://user:password@host:port/databasename</entry>
                    </row>

                    <row>
                      <entry/>
                    </row>
                  </tbody>
                </tgroup>
              </informaltable></para></entry>
          </row>

          <row>
            <entry><literallayout><emphasis role="bold"><emphasis role="bold">Property Key: </emphasis></emphasis>pegasus.dashboard.output<emphasis
                  role="bold"><emphasis role="bold">
Profile  Key: </emphasis></emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 4.2
<emphasis role="bold">Type  </emphasis>      : String
<emphasis role="bold">Default     :</emphasis> sqlite database in $HOME/.pegasus/workflow.db<emphasis
                  role="bold">
See Also    :</emphasis> pegasus.monitord.output</literallayout></entry>

            <entry><para>This property specifies the destination for the
            workflow dashboard database. By default, the workflow dashboard
            datbase defaults to a sqlite database named workflow.db in the
            $HOME/.pegasus directory. This is database is shared for all
            workflows run as a particular user. Users can specify an
            alternative database by using a SQLAlchemy connection string.
            Details are available at: <screen>
http://www.sqlalchemy.org/docs/05/reference/dialects/index.html
</screen> It is important to note that users will need to have the appropriate
            db interface library installed. Which is to say, SQLAlchemy is a
            wrapper around the mysql interface library (for instance), it does
            not provide a MySQL driver itself. The Pegasus distribution
            includes both SQLAlchemy and the SQLite Python driver. As a final
            note, it is important to mention that unlike when using SQLite
            databases, using SQLAlchemy with other database servers, e.g.
            MySQL or Postgres , the target database needs to exist. Users can
            also specify a file name using this property in order to create a
            file with the log events.</para><para>Example values for the
            SQLAlchemy connection string for various end points are listed
            below</para><para><informaltable frame="none">
                <tgroup align="left" cols="2" colsep="1" rowsep="1">
                  <tbody>
                    <row>
                      <entry>SQL Alchemy End Point</entry>

                      <entry>Example Value</entry>
                    </row>

                    <row>
                      <entry>SQL Lite Database</entry>

                      <entry>sqlite:///shared/myworkflow.db</entry>
                    </row>

                    <row>
                      <entry>MySQL Database</entry>

                      <entry>mysql://user:password@host:port/databasename</entry>
                    </row>

                    <row>
                      <entry/>
                    </row>
                  </tbody>
                </tgroup>
              </informaltable></para></entry>
          </row>

          <row>
            <entry><literallayout><emphasis role="bold"><emphasis role="bold">Property Key: </emphasis></emphasis>pegasus.monitord.notifications<emphasis
                  role="bold"><emphasis role="bold">
Profile  Key: </emphasis></emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 3.1.0
<emphasis role="bold">Type        : </emphasis>Boolean
<emphasis role="bold">Default     :</emphasis> true<emphasis role="bold">
See Also    :</emphasis> pegasus.monitord.notifications.max<emphasis
                  role="bold">
See Also    :</emphasis> pegasus.monitord.notifications.timeout</literallayout></entry>

            <entry>This property determines how many notification scripts
            pegasus-monitord will call concurrently. Upon reaching this limit,
            pegasus-monitord will wait for one notification script to finish
            before issuing another one. This is a way to keep the number of
            processes under control at the submit host. Setting this property
            to 0 will disable notifications completely.</entry>
          </row>

          <row>
            <entry><literallayout><emphasis role="bold"><emphasis role="bold">Property Key: </emphasis></emphasis>pegasus.monitord.notifications.max<emphasis
                  role="bold"><emphasis role="bold">
Profile  Key: </emphasis></emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 3.1.0
<emphasis role="bold">Type        : </emphasis>Integer
<emphasis role="bold">Default     :</emphasis> 10<emphasis role="bold">
See Also    :</emphasis> pegasus.monitord.notifications <emphasis role="bold">
See Also    :</emphasis> pegasus.monitord.notifications.timeout</literallayout></entry>

            <entry>This property determines whether pegasus-monitord processes
            notifications. When notifications are enabled, pegasus-monitord
            will parse the .notify file generated by pegasus-plan and will
            invoke notification scripts whenever conditions matches one of the
            notifications.</entry>
          </row>

          <row>
            <entry><literallayout><emphasis role="bold"><emphasis role="bold">Property Key: </emphasis></emphasis>pegasus.monitord.notifications.timeout<emphasis
                  role="bold"><emphasis role="bold">
Profile  Key: </emphasis></emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 3.1.0
<emphasis role="bold">Type        : </emphasis>Integer
<emphasis role="bold">Default     :</emphasis> true<emphasis role="bold">
See Also    :</emphasis> pegasus.monitord.notifications.<emphasis role="bold">
See Also    :</emphasis> pegasus.monitord.notifications.max</literallayout></entry>

            <entry>This property determines how long will pegasus-monitord let
            notification scripts run before terminating them. When this
            property is set to 0 (default), pegasus-monitord will not
            terminate any notification scripts, letting them run indefinitely.
            If some notification scripts missbehave, this has the potential
            problem of starving pegasus-monitord's notification slots (see the
            pegasus.monitord.notifications.max property), and block further
            notifications. In addition, users should be aware that
            pegasus-monitord will not exit until all notification scripts are
            finished.</entry>
          </row>

          <row>
            <entry><literallayout><emphasis role="bold"><emphasis role="bold">Property Key: </emphasis></emphasis>pegasus.monitord.stdout.disable.parsing<emphasis
                  role="bold"><emphasis role="bold">
Profile  Key: </emphasis></emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 3.1.1
<emphasis role="bold">Type        : </emphasis>Boolean
<emphasis role="bold">Default     :</emphasis> false</literallayout></entry>

            <entry>By default, pegasus-monitord parses the stdout/stderr
            section of the kickstart to populate the applications captured
            stdout and stderr in the job instance table for the stampede
            schema. For large workflows, this may slow down monitord
            especially if the application is generating a lot of output to
            it's stdout and stderr. This property, can be used to turn of the
            database population.</entry>
          </row>
        </tbody>
      </tgroup>
    </table>

    <para/>
  </section>

  <section>
    <title>Job Clustering Properties</title>

    <table>
      <title>Job Clustering Properties</title>

      <tgroup cols="2">
        <tbody>
          <row>
            <entry><emphasis role="bold">Key Attributes</emphasis></entry>

            <entry><emphasis role="bold">Description</emphasis></entry>
          </row>

          <row>
            <entry><literallayout><emphasis role="bold"><emphasis role="bold">Property Key: </emphasis></emphasis>pegasus.clusterer.job.aggregator<emphasis
                  role="bold"><emphasis role="bold">
Profile  Key: </emphasis></emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 2.0
<emphasis role="bold">Type  </emphasis>      : String
<emphasis role="bold">Values</emphasis>      : seqexec|mpiexec
<emphasis role="bold">Default     :</emphasis> seqexec</literallayout></entry>

            <entry><para>A large number of workflows executed through the
            Virtual Data System, are composed of several jobs that run for
            only a few seconds or so. The overhead of running any job on the
            grid is usually 60 seconds or more. Hence, it makes sense to
            collapse small independent jobs into a larger job. This property
            determines, the executable that will be used for running the
            larger job on the remote site.</para><para><variablelist>
                <varlistentry>
                  <term>seqexec</term>

                  <listitem>
                     In this mode, the executable used to run the merged job is "pegasus-cluster" that runs each of the smaller jobs sequentially on the same node. The executable "pegasus-cluster" is a PEGASUS tool distributed in the PEGASUS worker package, and can be usually found at {pegasus.home}/bin/seqexec. 
                  </listitem>
                </varlistentry>

                <varlistentry>
                  <term>mpiexec</term>

                  <listitem>
                     In this mode, the executable used to run the merged job is "pegasus-mpi-cluster" (PMC) that runs the smaller jobs via mpi on n nodes where n is the nodecount associated with the merged job. The executable "pegasus-mpi-cluster" is a PEGASUS tool distributed in the PEGASUS distribution and is built only if mpi compiler is available. 
                  </listitem>
                </varlistentry>
              </variablelist></para></entry>
          </row>

          <row>
            <entry><literallayout><emphasis role="bold"><emphasis role="bold">Property Key: </emphasis></emphasis>pegasus.clusterer.job.aggregator.seqexec.log<emphasis
                  role="bold"><emphasis role="bold">
Profile  Key: </emphasis></emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 2.3
<emphasis role="bold">Type        : </emphasis>Boolean
<emphasis role="bold">Default     :</emphasis> false<emphasis role="bold">
See Also    :</emphasis> pegasus.clusterer.job.aggregator<emphasis role="bold">
See Also    :</emphasis> pegasus.clusterer.job.aggregator.seqexec.log.global</literallayout></entry>

            <entry><para>The tool pegasus-cluster logs the progress of the
            jobs that are being run by it in a progress file on the remote
            cluster where it is executed.</para><para>This property sets the
            Boolean flag, that indicates whether to turn on the logging or
            not.</para></entry>
          </row>

          <row>
            <entry><literallayout><emphasis role="bold"><emphasis role="bold">Property Key: </emphasis></emphasis>pegasus.clusterer.job.aggregator.seqexec.log<emphasis
                  role="bold"><emphasis role="bold">
Profile  Key: </emphasis></emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 2.3
<emphasis role="bold">Type        : </emphasis>Boolean
<emphasis role="bold">Default     :</emphasis> false<emphasis role="bold">
See Also    :</emphasis> pegasus.clusterer.job.aggregator<emphasis role="bold">
See Also    :</emphasis> pegasus.clusterer.job.aggregator.seqexec.log.global</literallayout></entry>

            <entry><para>The tool pegasus-cluster logs the progress of the
            jobs that are being run by it in a progress file on the remote
            cluster where it is executed. The progress log is useful for you
            to track the progress of your computations and remote grid
            debugging. The progress log file can be shared by multiple
            pegasus-cluster jobs that are running on a particular cluster as
            part of the same workflow. Or it can be per job.</para><para>This
            property sets the Boolean flag, that indicates whether to have a
            single global log for all the pegasus-cluster jobs on a particular
            cluster or progress log per job.</para></entry>
          </row>

          <row>
            <entry><literallayout><emphasis role="bold"><emphasis role="bold">Property Key: </emphasis></emphasis>pegasus.clusterer.job.aggregator.seqexec.firstjobfail<emphasis
                  role="bold"><emphasis role="bold">
Profile  Key: </emphasis></emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 2.2
<emphasis role="bold">Type        : </emphasis>Boolean
<emphasis role="bold">Default     :</emphasis> true<emphasis role="bold">
See Also    :</emphasis> pegasus.clusterer.job.aggregator</literallayout></entry>

            <entry><para>By default "pegasus-cluster" does not stop execution
            even if one of the clustered jobs it is executing fails. This is
            because "pegasus-cluster" tries to get as much work done as
            possible.</para><para>This property sets the Boolean flag, that
            indicates whether to make "pegasus-cluster" stop on the first job
            failure it detects.</para></entry>
          </row>

          <row>
            <entry><literallayout><emphasis role="bold"><emphasis role="bold">Property Key: </emphasis></emphasis>pegasus.clusterer.label.key<emphasis
                  role="bold"><emphasis role="bold">
Profile  Key: </emphasis></emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 2.0
<emphasis role="bold">Type  </emphasis>      : String
<emphasis role="bold">Default     :</emphasis> label
</literallayout></entry>

            <entrytbl cols="1">
              <tbody>
                <row>
                  <entry><para>While clustering jobs in the workflow into
                  larger jobs, you can optionally label your graph to control
                  which jobs are clustered and to which clustered job they
                  belong. This done using a label based clustering scheme and
                  is done by associating a profile/label key in the PEGASUS
                  namespace with the jobs in the DAX. Each job that has the
                  same value/label value for this profile key, is put in the
                  same clustered job.</para><para>This property allows you to
                  specify the PEGASUS profile key that you want to use for
                  label based clustering.</para></entry>
                </row>
              </tbody>
            </entrytbl>
          </row>
        </tbody>
      </tgroup>
    </table>

    <para/>
  </section>

  <section>
    <title>Logging Properties</title>

    <table>
      <title>Logging Properties</title>

      <tgroup cols="2">
        <tbody>
          <row>
            <entry><emphasis role="bold">Key Attributes</emphasis></entry>

            <entry><emphasis role="bold">Description</emphasis></entry>
          </row>

          <row>
            <entry><literallayout><emphasis role="bold"><emphasis role="bold">Property Key: </emphasis></emphasis>pegasus.log.manager<emphasis
                  role="bold"><emphasis role="bold">
Profile  Key: </emphasis></emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 2.2.0
<emphasis role="bold">Type  </emphasis>      : String
<emphasis role="bold">Values</emphasis>      : Default|Log4J
<emphasis role="bold">Default     :</emphasis> Default<emphasis role="bold">
See Also    :</emphasis>pegasus.log.manager.formatter</literallayout></entry>

            <entry><para>This property sets the logging implementation to use
            for logging.</para><para><variablelist>
                <varlistentry>
                  <term>Default</term>

                  <listitem>
                     This implementation refers to the legacy Pegasus logger, that logs directly to stdout and stderr. It however, does have the concept of levels similar to log4j or syslog. 
                  </listitem>
                </varlistentry>

                <varlistentry>
                  <term>Log4j</term>

                  <listitem>
                     This implementation, uses Log4j to log messages. The log4j properties can be specified in a properties file, the location of which is specified by the property 

                    <screen>
pegasus.log.manager.log4j.conf
</screen>

                     
                  </listitem>
                </varlistentry>
              </variablelist></para></entry>
          </row>

          <row>
            <entry><literallayout><emphasis role="bold"><emphasis role="bold">Property Key: </emphasis></emphasis>pegasus.log.manager.formatter<emphasis
                  role="bold"><emphasis role="bold">
Profile  Key: </emphasis></emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 2.2.0
<emphasis role="bold">Type  </emphasis>      : String
<emphasis role="bold">Values</emphasis>      : Simple|Netlogger
<emphasis role="bold">Default     :</emphasis> Simple<emphasis role="bold">
See Also    :</emphasis>pegasus.log.manager</literallayout></entry>

            <entrytbl cols="1">
              <tbody>
                <row>
                  <entry><para>This property sets the formatter to use for
                  formatting the log messages while
                  logging.</para><para><variablelist>
                      <varlistentry>
                        <term>Simple</term>

                        <listitem>
                           This formats the messages in a simple format. The messages are logged as is with minimal formatting. Below are sample log messages in this format while ranking a dax according to performance. 

                          <screen>
event.pegasus.ranking dax.id se18-gda.dax  - STARTED
event.pegasus.parsing.dax dax.id se18-gda-nested.dax  - STARTED
event.pegasus.parsing.dax dax.id se18-gda-nested.dax  - FINISHED
job.id jobGDA
job.id jobGDA query.name getpredicted performace time 10.00
event.pegasus.ranking dax.id se18-gda.dax  - FINISHED
</screen>

                           
                        </listitem>
                      </varlistentry>

                      <varlistentry>
                        <term>Netlogger</term>

                        <listitem>
                          <para>This formats the messages in the Netlogger
                          format , that is based on key value pairs. The
                          netlogger format is useful for loading the logs into
                          a database to do some meaningful analysis. Below are
                          sample log messages in this format while ranking a
                          dax according to performance. <screen>
ts=2008-09-06T12:26:20.100502Z event=event.pegasus.ranking.start \
msgid=6bc49c1f-112e-4cdb-af54-3e0afb5d593c \
eventId=event.pegasus.ranking_8d7c0a3c-9271-4c9c-a0f2-1fb57c6394d5 \
dax.id=se18-gda.dax prog=Pegasus
ts=2008-09-06T12:26:20.100750Z event=event.pegasus.parsing.dax.start \
msgid=fed3ebdf-68e6-4711-8224-a16bb1ad2969 \
eventId=event.pegasus.parsing.dax_887134a8-39cb-40f1-b11c-b49def0c5232\
dax.id=se18-gda-nested.dax prog=Pegasus
ts=2008-09-06T12:26:20.100894Z event=event.pegasus.parsing.dax.end \
msgid=a81e92ba-27df-451f-bb2b-b60d232ed1ad \
eventId=event.pegasus.parsing.dax_887134a8-39cb-40f1-b11c-b49def0c5232
ts=2008-09-06T12:26:20.100395Z event=event.pegasus.ranking \
msgid=4dcecb68-74fe-4fd5-aa9e-ea1cee88727d \
eventId=event.pegasus.ranking_8d7c0a3c-9271-4c9c-a0f2-1fb57c6394d5 \
job.id="jobGDA"
ts=2008-09-06T12:26:20.100395Z event=event.pegasus.ranking \
msgid=4dcecb68-74fe-4fd5-aa9e-ea1cee88727d \
eventId=event.pegasus.ranking_8d7c0a3c-9271-4c9c-a0f2-1fb57c6394d5 \
job.id="jobGDA" query.name="getpredicted performace" time="10.00"
ts=2008-09-06T12:26:20.102003Z event=event.pegasus.ranking.end \
msgid=31f50f39-efe2-47fc-9f4c-07121280cd64 \
eventId=event.pegasus.ranking_8d7c0a3c-9271-4c9c-a0f2-1fb57c6394d5
</screen></para>
                        </listitem>
                      </varlistentry>
                    </variablelist></para></entry>
                </row>
              </tbody>
            </entrytbl>
          </row>

          <row>
            <entry><literallayout><emphasis role="bold"><emphasis role="bold">Property Key: </emphasis></emphasis>pegasus.log.*<emphasis
                  role="bold"><emphasis role="bold">
Profile  Key: </emphasis></emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 2.0
<emphasis role="bold">Type        : </emphasis>file path
<emphasis role="bold">Default     :</emphasis> no default</literallayout></entry>

            <entry>This property sets the path to the file where all the
            logging for Pegasus can be redirected to. Both stdout and stderr
            are logged to the file specified.</entry>
          </row>

          <row>
            <entry><literallayout><emphasis role="bold"><emphasis role="bold">Property Key: </emphasis></emphasis>pegasus.log.memory.usage<emphasis
                  role="bold"><emphasis role="bold">
Profile  Key: </emphasis></emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 4.3.4
<emphasis role="bold">Type        : </emphasis>Boolean
<emphasis role="bold">Default     :</emphasis> false</literallayout></entry>

            <entry>This property if set to true, will result in the planner
            writing out JVM heap memory statistics at the end of the planning
            process at the INFO level. This is useful, if users want to fine
            tune their java memory settings by setting JAVA_HEAPMAX and
            JAVA_HEAPMIN for large workflows.</entry>
          </row>

          <row>
            <entry><literallayout><emphasis role="bold"><emphasis role="bold">Property Key: </emphasis></emphasis>pegasus.metrics.app<emphasis
                  role="bold"><emphasis role="bold">
Profile  Key: </emphasis></emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 4.3.0
<emphasis role="bold">Type        : </emphasis>String
<emphasis role="bold">Default     :</emphasis> (no default)</literallayout></entry>

            <entry><para>This property namespace allows users to pass
            application level metrics to the metrics server. The value of this
            property is the name of the application.</para><para>Additional
            application specific attributes can be passed by using the prefix
            pegasus.metrics.app <screen>
pegasus.metrics.app.[arribute-name]       attribute-value
</screen></para><para>Note: the attribute cannot be named name. This attribute
            is automatically assigned the value from
            pegasus.metrics.app</para></entry>
          </row>
        </tbody>
      </tgroup>
    </table>

    <para/>
  </section>

  <section>
    <title>Cleanup Properties</title>

    <table>
      <title>Cleanup Properties</title>

      <tgroup cols="2">
        <tbody>
          <row>
            <entry><emphasis role="bold">Key Attributes</emphasis></entry>

            <entry><emphasis role="bold">Description</emphasis></entry>
          </row>

          <row>
            <entry><literallayout><emphasis role="bold"><emphasis role="bold">Property Key: </emphasis></emphasis>pegasus.file.cleanup.strategy<emphasis
                  role="bold"><emphasis role="bold">
Profile  Key: </emphasis></emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 2.2-
<emphasis role="bold">Type  </emphasis>      : String
<emphasis role="bold">Default     :</emphasis> InPlace</literallayout></entry>

            <entry><para>This property is used to select the strategy of how
            the the cleanup nodes are added to the executable
            workflow.</para><para><variablelist>
                <varlistentry>
                  <term>InPlace</term>

                  <listitem>
                     This is the only mode available . 
                  </listitem>
                </varlistentry>
              </variablelist></para></entry>
          </row>

          <row>
            <entry><literallayout><emphasis role="bold"><emphasis role="bold">Property Key: </emphasis></emphasis>pegasus.file.cleanup.impl<emphasis
                  role="bold"><emphasis role="bold">
Profile  Key: </emphasis></emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 2.2
<emphasis role="bold">Type        : </emphasis>String
<emphasis role="bold">Default     :</emphasis> Cleanup</literallayout></entry>

            <entry><para>This property is used to select the executable that
            is used to create the working directory on the compute
            sites.</para><para><variablelist>
                <varlistentry>
                  <term>Cleanup</term>

                  <listitem>
                     The default executable that is used to delete files is the "pegasus-cleanup" executable shipped with Pegasus. It is found at $PEGASUS_HOME/bin/pegasus-cleanup in the pegasus distribution. An entry for transformation pegasus::dirmanager needs to exist in the Transformation Catalog or the PEGASUS_HOME environment variable should be specified in the site catalog for the sites for this mode to work. 
                  </listitem>
                </varlistentry>

                <varlistentry>
                  <term>RM</term>

                  <listitem>
                     This mode results in the rm executable to be used to delete files from remote directories. The rm executable is standard on *nix systems and is usually found at /bin/rm location. 
                  </listitem>
                </varlistentry>
              </variablelist></para></entry>
          </row>

          <row>
            <entry><literallayout><emphasis role="bold"><emphasis role="bold">Property Key: </emphasis></emphasis>pegasus.file.cleanup.clusters.num<emphasis
                  role="bold"><emphasis role="bold">
Profile  Key: </emphasis></emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 4.2.0
<emphasis role="bold">Type        : </emphasis>Integer
<emphasis role="bold">Default     :</emphasis> 2</literallayout></entry>

            <entry>In case of the InPlace strategy for adding the cleanup
            nodes to the workflow, this property specifies the maximum number
            of cleanup jobs that are added to the executable workflow on each
            level.</entry>
          </row>

          <row>
            <entry><literallayout><emphasis role="bold"><emphasis role="bold">Property Key: </emphasis></emphasis>pegasus.file.cleanup.clusters.size<emphasis
                  role="bold"><emphasis role="bold">
Profile  Key: </emphasis></emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 4.2.0
<emphasis role="bold">Type        : </emphasis>Integer
<emphasis role="bold">Default     :</emphasis> 2</literallayout></entry>

            <entry>In case of the InPlace strategy this property sets the
            number of cleanup jobs that get clustered into a bigger cleanup
            job. This parameter is only used if
            pegasus.file.cleanup.clusters.num is not set.</entry>
          </row>

          <row>
            <entry><literallayout><emphasis role="bold"><emphasis role="bold">Property Key: </emphasis></emphasis>pegasus.file.cleanup.scope<emphasis
                  role="bold"><emphasis role="bold">
Profile  Key: </emphasis></emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 2.3.0
<emphasis role="bold">Type        : </emphasis>Enumeration
<emphasis role="bold">Value       : </emphasis>fullahead|deferred
<emphasis role="bold">Default     :</emphasis> fullahead</literallayout></entry>

            <entry><para>By default in case of deferred planning InPlace file
            cleanup is turned OFF. This is because the cleanup algorithm does
            not work across partitions. This property can be used to turn on
            the cleanup in case of deferred planning. <variablelist>
                <varlistentry>
                  <term>fullahead</term>

                  <listitem>
                     This is the default scope. The pegasus cleanup algorithm does not work across partitions in deferred planning. Hence the cleanup is always turned OFF , when deferred planning occurs and cleanup scope is set to full ahead. 
                  </listitem>
                </varlistentry>

                <varlistentry>
                  <term>deferred</term>

                  <listitem>
                     If the scope is set to deferred, then Pegasus will not disable file cleanup in case of deferred planning. This is useful for scenarios where the partitions themselves are independant ( i.e. dont share files ). Even if the scope is set to deferred, users can turn off cleanup by specifying --nocleanup option to pegasus-plan. 
                  </listitem>
                </varlistentry>
              </variablelist></para></entry>
          </row>
        </tbody>
      </tgroup>
    </table>

    <para/>
  </section>

  <section>
    <title>Miscellaneous Properties</title>

    <table>
      <title>Miscellaneous Properties</title>

      <tgroup cols="2">
        <tbody>
          <row>
            <entry><emphasis role="bold">Key Attributes</emphasis></entry>

            <entry><emphasis role="bold">Description</emphasis></entry>
          </row>

          <row>
            <entry><literallayout><emphasis role="bold"><emphasis role="bold">Property Key: </emphasis></emphasis>pegasus.code.generator<emphasis
                  role="bold"><emphasis role="bold">
Profile  Key: </emphasis></emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 3.0
<emphasis role="bold">Type  </emphasis>      : String
<emphasis role="bold">Values</emphasis>      : Condor|Shell|PMC
<emphasis role="bold">Default     :</emphasis> Condor<emphasis role="bold">
See Also    :</emphasis> pegasus.log.manager.formatter</literallayout></entry>

            <entry><para>This property is used to load the appropriate Code
            Generator to use for writing out the executable
            workflow.</para><para><variablelist>
                <varlistentry>
                  <term>Condor</term>

                  <listitem>
                     This is the default code generator for Pegasus . This generator generates the executable workflow as a Condor DAG file and associated job submit files. The Condor DAG file is passed as input to Condor DAGMan for job execution. 
                  </listitem>
                </varlistentry>

                <varlistentry>
                  <term>Shell</term>

                  <listitem>
                     This Code Generator generates the executable workflow as a shell script that can be executed on the submit host. While using this code generator, all the jobs should be mapped to site local i.e specify --sites local to pegasus-plan. 
                  </listitem>
                </varlistentry>

                <varlistentry>
                  <term>PMC</term>

                  <listitem>
                     This Code Generator generates the executable workflow as a PMC task workflow. This is useful to run on platforms where it not feasible to run Condor such as the new XSEDE machines such as Blue Waters. In this mode, Pegasus will generate the executable workflow as a PMC task workflow and a sample PBS submit script that submits this workflow. 
                  </listitem>
                </varlistentry>
              </variablelist></para></entry>
          </row>

          <row>
            <entry><literallayout><emphasis role="bold"><emphasis role="bold">Property Key: </emphasis></emphasis>pegasus.register<emphasis
                  role="bold"><emphasis role="bold">
Profile  Key: </emphasis></emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 4.1.-
<emphasis role="bold">Type  </emphasis>      : Boolean
<emphasis role="bold">Default     :</emphasis> true</literallayout></entry>

            <entry><para>Pegasus creates registration jobs to register the
            output files in the replica catalog. An output file is registered
            only if</para><para>1) a user has configured a replica catalog in
            the properties 2) the register flags for the output files in the
            DAX are set to true</para><para>This property can be used to turn
            off the creation of the registration jobs even though the files
            maybe marked to be registered in the replica
            catalog.</para></entry>
          </row>

          <row>
            <entry><literallayout><emphasis role="bold"><emphasis role="bold">Property Key: </emphasis></emphasis>pegasus.data.reuse.scope<emphasis
                  role="bold"><emphasis role="bold">
Profile  Key: </emphasis></emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 4.5.0
<emphasis role="bold">Type        : </emphasis>Enumeration
<emphasis role="bold">Value       : </emphasis>none|partial|full
<emphasis role="bold">Default     :</emphasis> full</literallayout></entry>

            <entry><para>This property is used to control the behavior of the
            data reuse algorithm in Pegasus</para><para><variablelist>
                <varlistentry>
                  <term>none</term>

                  <listitem>
                     This is same as disabling data reuse. It is equivalent to passing the --force option to pegasus-plan on the command line. 
                  </listitem>
                </varlistentry>

                <varlistentry>
                  <term>partial</term>

                  <listitem>
                     In this case, only certain jobs ( those that have pegasus profile key enable_for_data_reuse set to true ) are checked for presence of output files in the replica catalog. This gives users control over what jobs are deleted as part of the data reuse algorithm. 
                  </listitem>
                </varlistentry>

                <varlistentry>
                  <term>full</term>

                  <listitem>
                     This is the default behavior, where all the jobs output files are looked up in the replica catalog. 
                  </listitem>
                </varlistentry>
              </variablelist></para></entry>
          </row>

          <row>
            <entry><literallayout><emphasis role="bold"><emphasis role="bold">Property Key: </emphasis></emphasis>pegasus.catalog.transformation.mapper<emphasis
                  role="bold"><emphasis role="bold">
Profile  Key: </emphasis></emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 2.0
<emphasis role="bold">Type        : </emphasis>Enumeration
<emphasis role="bold">Value       : </emphasis>All|Installed|Staged|Submit
<emphasis role="bold">Default     :</emphasis> All</literallayout></entry>

            <entry><para>Pegasus supports transfer of statically linked
            executables as part of the executable workflow. At present, there
            is only support for staging of executables referred to by the
            compute jobs specified in the DAX file. Pegasus determines the
            source locations of the binaries from the transformation catalog,
            where it searches for entries of type STATIC_BINARY for a
            particular architecture type. The PFN for these entries should
            refer to a globus-url-copy valid and accessible remote URL. For
            transfer of executables, Pegasus constructs a soft state map that
            resides on top of the transformation catalog, that helps in
            determining the locations from where an executable can be staged
            to the remote site.</para><para>This property determines, how that
            map is created. <variablelist>
                <varlistentry>
                  <term>All</term>

                  <listitem>
                     In this mode, all sources with entries of type STATIC_BINARY for a particular transformation are considered valid sources for the transfer of executables. This the most general mode, and results in the constructing the map as a result of the cartesian product of the matches. 
                  </listitem>
                </varlistentry>

                <varlistentry>
                  <term>Installed</term>

                  <listitem>
                     In this mode, only entries that are of type INSTALLED are used while constructing the soft state map. This results in Pegasus never doing any transfer of executables as part of the workflow. It always prefers the installed executables at the remote sites. 
                  </listitem>
                </varlistentry>

                <varlistentry>
                  <term>Staged</term>

                  <listitem>
                     In this mode, only entries that are of type STATIC_BINARY are used while constructing the soft state map. This results in the concrete workflow referring only to the staged executables, irrespective of the fact that the executables are already installed at the remote end. 
                  </listitem>
                </varlistentry>

                <varlistentry>
                  <term>Submit</term>

                  <listitem>
                     In this mode, only entries that are of type STATIC_BINARY and reside at the submit host (pool local), are used while constructing the soft state map. This is especially helpful, when the user wants to use the latest compute code for his computations on the grid and that relies on his submit host. 
                  </listitem>
                </varlistentry>
              </variablelist></para></entry>
          </row>

          <row>
            <entry><literallayout><emphasis role="bold"><emphasis role="bold">Property Key: </emphasis></emphasis>pegasus.selector.transformation<emphasis
                  role="bold"><emphasis role="bold">
Profile  Key: </emphasis></emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 2.0
<emphasis role="bold">Type        : </emphasis>Enumeration
<emphasis role="bold">Value       : </emphasis>Random|Installed|Staged|Submit
<emphasis role="bold">Default     :</emphasis> Random</literallayout></entry>

            <entry><para>In case of transfer of executables, Pegasus could
            have various transformations to select from when it schedules to
            run a particular compute job at a remote site. For e.g it can have
            the choice of staging an executable from a particular remote pool,
            from the local (submit host) only, use the one that is installed
            on the remote site only.</para><para>This property determines, how
            a transformation amongst the various candidate transformations is
            selected, and is applied after the property pegasus.tc has been
            applied. For e.g specifying pegasus.tc as Staged and then
            pegasus.transformation.selector as INSTALLED does not work, as by
            the time this property is applied, the soft state map only has
            entries of type STAGED.</para><para><variablelist>
                <varlistentry>
                  <term>Random</term>

                  <listitem>
                     In this mode, a random matching candidate transformation is selected to be staged to the remote execution pool. 
                  </listitem>
                </varlistentry>

                <varlistentry>
                  <term>Installed</term>

                  <listitem>
                     In this mode, only entries that are of type INSTALLED are selected. This means that the concrete workflow only refers to the transformations already pre installed on the remote pools. 
                  </listitem>
                </varlistentry>

                <varlistentry>
                  <term>Staged</term>

                  <listitem>
                     In this mode, only entries that are of type STATIC_BINARY are selected, ignoring the ones that are installed at the remote site. 
                  </listitem>
                </varlistentry>

                <varlistentry>
                  <term>Submit</term>

                  <listitem>
                     In this mode, only entries that are of type STATIC_BINARY and reside at the submit host (pool local), are selected as sources for staging the executables to the remote execution pools
                  </listitem>
                </varlistentry>
              </variablelist></para></entry>
          </row>

          <row>
            <entry><literallayout><emphasis role="bold"><emphasis role="bold">Property Key: </emphasis></emphasis>pegasus.execute.*.filesystem.local<emphasis
                  role="bold"><emphasis role="bold">
Profile  Key: </emphasis></emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 2.1.0
<emphasis role="bold">Type        : </emphasis>Boolean
<emphasis role="bold">Default     :</emphasis> false<emphasis role="bold">
See Also    :</emphasis> pegasus.data.configuration</literallayout></entry>

            <entry><para>Normally, Pegasus transfers the data to and from a
            directory on the shared filesystem on the head node of a compute
            site. The directory needs to be visible to both the head node and
            the worker nodes for the compute jobs to execute
            correctly.</para><para>By setting this property to true, you can
            get Pegasus to execute jobs on the worker node filesystem. In this
            case, when the jobs are launched on the worker nodes, the jobs
            grab the input data from the workflow specific execution directory
            on the compute site and push the output data to the same directory
            after completion. The transfer of data to and from the worker node
            directory is referred to as PegasusLite Data
            Transfers.</para><para><emphasis role="bold">Note</emphasis>:
            Users don't need to set this property. This is an internal
            property for use of developers only</para></entry>
          </row>

          <row>
            <entry><literallayout><emphasis role="bold"><emphasis role="bold">Property Key: </emphasis></emphasis>pegasus.parser.dax.preserver.linebreaks<emphasis
                  role="bold"><emphasis role="bold">
Profile  Key: </emphasis></emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 2.2.0
<emphasis role="bold">Type        : </emphasis>Boolean
<emphasis role="bold">Default     :</emphasis> false</literallayout></entry>

            <entry>The DAX Parser normally does not preserve line breaks while
            parsing the CDATA section that appears in the arguments section of
            the job element in the DAX. On setting this to true, the DAX
            Parser preserves any line line breaks that appear in the CDATA
            section.</entry>
          </row>

          <row>
            <entry><literallayout><emphasis role="bold"><emphasis role="bold">Property Key: </emphasis></emphasis>pegasus.parser.dax.data.dependencies<emphasis
                  role="bold"><emphasis role="bold">
Profile  Key: </emphasis></emphasis>N/A<emphasis role="bold">
Scope       :</emphasis> Properties
<emphasis role="bold">Since       :</emphasis> 4.4.0
<emphasis role="bold">Type        : </emphasis>Boolean
<emphasis role="bold">Default     :</emphasis> true</literallayout></entry>

            <entry>If this property is set to true, then the planner will
            automatically add edges between jobs in the DAX on the basis of
            exisitng data dependencies between jobs. For example, if a JobA
            generates an output file that is listed as input for JobB, then
            the planner will automatically add an edge between JobA and
            JobB.</entry>
          </row>
        </tbody>
      </tgroup>
    </table>
  </section>
</section>
