<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE section PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN"
"http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<section id="transfer">
  <title>Data Transfers</title>

  <para>As part of the Workflow Mapping Process, Pegasus does data management
  for the executable workflow . It queries a Replica Catalog to discover the
  locations of the input datasets and adds data movement and registration
  nodes in the workflow to</para>

  <orderedlist>
    <listitem>
      <para>stage-in input data to the compute sites ( where the jobs in the
      workflow are executed )</para>
    </listitem>

    <listitem>
      <para>stage-out output data generated by the workflow to the final
      storage site.</para>
    </listitem>

    <listitem>
      <para>stage-in intermediate data between compute sites if
      required.</para>
    </listitem>

    <listitem>
      <para>data registration nodes to catalog the locations of the output
      data on the final storage site into the replica catalog.</para>
    </listitem>
  </orderedlist>

  <para>The separate data movement jobs that are added to the executable
  workflow are responsible for staging data to a workflow specific directory
  accessible to the staging server on a staging site associated with the
  compute sites. Currently, the staging site for a compute site is the compute
  site itself. In the default case, the staging server is usually on the
  headnode of the compute site and has access to the shared filesystem between
  the worker nodes and the head node. Pegasus adds a directory creation job in
  the executable workflow that creates the workflow specific directory on the
  staging server.</para>

  <figure>
    <title>Default Transfer Case : Input Data To Workflow Specific Directory
    on Shared File System</title>

    <mediaobject>
      <imageobject>
        <imagedata fileref="images/default-transfer-sharedfs.png" />
      </imageobject>
    </mediaobject>
  </figure>

  <para>In addition to data, Pegasus does transfer user executables to the
  compute sites if the executables are not installed on the remote sites
  before hand. This chapter gives an overview of how transfers of data and
  executables is managed in Pegasus.</para>

  <section>
    <title>Local versus Remote Transfers</title>

    <para>As far as possible, Pegasus will ensure that the transfer jobs added
    to the executable workflow are executed on the submit host. By default,
    Pegasus will schedule a transfer to be executed on the remote compute site
    only if there is no way to execute it on the submit host. For e.g if the
    file server specified for the compute site is a file server, then Pegasus
    will schedule all the stage in data movement jobs on the compute site to
    stage-in the input data for the workflow. Another case would be if a user
    has symlinking turned on. In that case, the transfer jobs that symlink
    against the input data on the compute site, will be executed remotely ( on
    the compute site ).</para>

    <para>Users can specify the property <emphasis
    role="bold">pegasus.transfer.*.remote.sites</emphasis> to change the
    default behaviour of Pegasus and force pegasus to run different types of
    transfer jobs for the sites specified on the remote site. The value of the
    property is a comma separated list of compute sites for which you want the
    transfer jobs to run remotely.</para>

    <para>The table below illustrates all the possible variations of the
    property.</para>

    <table>
      <title>Property Variations for pegasus.transfer.*.remote.sites</title>

      <tgroup cols="2">
        <thead>
          <row>
            <entry>Property Name</entry>

            <entry>Applies to</entry>
          </row>
        </thead>

        <tbody>
          <row>
            <entry>pegasus.transfer.stagein.remote.sites</entry>

            <entry>the stage in transfer jobs</entry>
          </row>

          <row>
            <entry>pegasus.transfer.stageout.remote.sites</entry>

            <entry>the stage out transfer jobs</entry>
          </row>

          <row>
            <entry>pegasus.transfer.inter.remote.sites</entry>

            <entry>the inter site transfer jobs</entry>
          </row>

          <row>
            <entry>pegasus.transfer.*.remote.sites</entry>

            <entry>all types of transfer jobs</entry>
          </row>
        </tbody>
      </tgroup>
    </table>

    <para>The prefix for the transfer job name indicates whether the transfer
    job is to be executed locallly ( on the submit host ) or remotely ( on the
    compute site ). For example stage_in_local_ in a transfer job name
    stage_in_local_isi_viz_0 indicates that the transfer job is a stage in
    transfer job that is executed locally and is used to transfer input data
    to compute site isi_viz. The prefix naming scheme for the transfer jobs is
    <emphasis
    role="bold">[stage_in|stage_out|inter]_[local|remote]_</emphasis> .</para>
  </section>

  <section>
    <title>Symlinking Against Input Data</title>

    <para>If input data for a job already exists on a compute site, then it is
    possible for Pegasus to symlink against that data. In this case, the
    remote stage in transfer jobs that Pegasus adds to the executable workflow
    will symlink instead of doing a copy of the data.</para>

    <para>Pegasus determines whether a file is on the same site as the compute
    site, by inspecting the pool attribute associated with the URL in the
    Replica Catalog. If the pool attribute of an input file location matches
    the compute site where the job is scheduled, then that particular input
    file is a candidate for symlinking.</para>

    <para>For Pegasus to symlink against existing input data on a compute
    site, following must be true</para>

    <orderedlist>
      <listitem>
        <para>Property <emphasis role="bold">pegasus.transfer.links</emphasis>
        is set to <emphasis role="bold">true</emphasis></para>
      </listitem>

      <listitem>
        <para>The input file location in the Replica Catalog has the pool
        attribute matching the compute site.</para>
      </listitem>
    </orderedlist>

    <tip>
      <para>To confirm if a particular input file is symlinked instead of
      being copied, look for the destination URL for that file in
      stage_in_remote*.in file. The destination URL will start with symlink://
      .</para>
    </tip>

    <para>In the symlinking case, Pegasus strips out URL prefix from a URL and
    replaces it with a file URL.</para>

    <para>For example if a user has the following URL catalogued in the
    Replica Catalog for an input file f.input</para>

    <programlisting>f.input   gsiftp://server.isi.edu/shared/storage/input/data/f.input pool="isi"</programlisting>

    <para>and the compute job that requires this file executes on a compute
    site named isi , then if symlinking is turned on the data stage in job
    (stage_in_remote_viz_0 ) will have the following source and destination
    specified for the file</para>

    <programlisting>#viz viz
file:///shared/storage/input/data/f.input  symlink://shared-scratch/workflow-exec-dir/f.input
</programlisting>
  </section>

  <section>
    <title>Addition of Separate Data Movement Nodes to Executable
    Workflow</title>

    <para>Pegasus relies on a Transfer Refiner that comes up with the strategy
    on how many data movement nodes are added to the executable workflow. All
    the compute jobs scheduled to a site share the same workflow specific
    directory. The transfer refiners ensure that only one copy of the input
    data is transferred to the workflow execution directory. This is to
    prevent data clobbering . Data clobbering can occur when compute jobs of a
    workflow share some input files, and have different stage in transfer jobs
    associated with them that are staging the shared files to the same
    destination workflow execution directory.</para>

    <para>The default Transfer Refiner used in Pegasus is the Bundle Refiner
    that allows the user to specify how many local|remote stagein|stageout
    jobs are created per execution site.</para>

    <para>The behavior of the refiner is controlled by specifying certain
    pegasus profiles</para>

    <orderedlist>
      <listitem>
        <para>either with the execution sites in the site catalog</para>
      </listitem>

      <listitem>
        <para>OR globally in the properties file</para>
      </listitem>
    </orderedlist>

    <table>
      <title>Pegasus Profile Keys For the Bundle Transfer Refiner</title>

      <tgroup cols="2">
        <thead>
          <row>
            <entry>Profile Key</entry>

            <entry>Description</entry>
          </row>
        </thead>

        <tbody>
          <row>
            <entry>stagein.clusters</entry>

            <entry>This key determines the maximum number of stage-in jobs
            that are can executed locally or remotely per compute site per
            workflow.</entry>
          </row>

          <row>
            <entry>stagein.local.clusters</entry>

            <entry>This key provides finer grained control in determining the
            number of stage-in jobs that are executed locally and are
            responsible for staging data to a particular remote site.</entry>
          </row>

          <row>
            <entry>stagein.remote.clusters</entry>

            <entry>This key provides finer grained control in determining the
            number of stage-in jobs that are executed remotely on the remote
            site and are responsible for staging data to it.</entry>
          </row>

          <row>
            <entry>stageout.clusters</entry>

            <entry>This key determines the maximum number of stage-out jobs
            that are can executed locally or remotely per compute site per
            workflow.</entry>
          </row>

          <row>
            <entry>stageout.local.clusters</entry>

            <entry>This key provides finer grained control in determining the
            number of stage-out jobs that are executed locally and are
            responsible for staging data from a particular remote
            site.</entry>
          </row>

          <row>
            <entry>stageout.remote.clusters</entry>

            <entry>This key provides finer grained control in determining the
            number of stage-out jobs that are executed remotely on the remote
            site and are responsible for staging data from it.</entry>
          </row>
        </tbody>
      </tgroup>
    </table>

    <figure>
      <title>Default Transfer Case : Input Data To Workflow Specific Directory
      on Shared File System</title>

      <mediaobject>
        <imageobject>
          <imagedata fileref="images/bundle-transfer-refiner.png" lang="" />
        </imageobject>
      </mediaobject>
    </figure>
  </section>

  <section>
    <title>Executable Used for Transfer Jobs</title>

    <para>Pegasus refers to a python script called <emphasis
    role="bold">pegasus-transfer</emphasis> as the executable in the transfer
    jobs to transfer the data. pegasus-transfer is a python based wrapper
    around various transfer clients . pegasus-transfer looks at source and
    destination url and figures out automatically which underlying client to
    use. pegasus-transfer is distributed with the PEGASUS and can be found at
    $PEGASUS_HOME/bin/pegasus-transfer.</para>

    <para>Currently, pegasus-transfer interfaces with the following transfer
    clients</para>

    <table>
      <title>Transfer Clients interfaced to by pegasus-transfer</title>

      <tgroup cols="2">
        <thead>
          <row>
            <entry>Transfer Client</entry>

            <entry>Used For</entry>
          </row>
        </thead>

        <tbody>
          <row>
            <entry>globus-url-copy</entry>

            <entry>staging files to and from a gridftp server.</entry>
          </row>

          <row>
            <entry>lcg-copy</entry>

            <entry>staging files to and from a SRM server.</entry>
          </row>

          <row>
            <entry>wget</entry>

            <entry>staging files from a HTTP server.</entry>
          </row>

          <row>
            <entry>cp</entry>

            <entry>copying files from a POSIX filesystem .</entry>
          </row>

          <row>
            <entry>ln</entry>

            <entry>symlinking against input files.</entry>
          </row>
        </tbody>
      </tgroup>
    </table>

    <para>For remote sites, Pegasus constructs the default path to
    pegasus-transfer on the basis of PEGASUS_HOME env profile specified in the
    site catalog. To specify a different path to the pegasus-transfer client ,
    users can add an entry into the transformation catalog with fully
    qualified logical name as <emphasis
    role="bold">pegasus::pegasus-transfer</emphasis></para>
  </section>

  <section>
    <title>Staging of Executables</title>

    <para>Users can get Pegasus to stage the user executables ( executables
    that the jobs in the DAX refer to ) as part of the transfer jobs to the
    workflow specific execution directory on the compute site. The URL
    locations of the executables need to be specified in the transformation
    catalog as the PFN and the type of executable needs to be set to <emphasis
    role="bold">STAGEABLE</emphasis> .</para>

    <para>The location of a transformation can be specified either in</para>

    <itemizedlist>
      <listitem>
        <para>DAX in the executables section. More details <link
        linkend="dax_transformation_catalog">here</link> .</para>
      </listitem>

      <listitem>
        <para>Transformation Catalog. More details <link
        linkend="transformation">here</link> .</para>
      </listitem>
    </itemizedlist>

    <para>A particular transformation catalog entry of type STAGEABLE is
    compatible with a compute site only if all the System Information
    attributes associated with the entry match with the System Information
    attributes for the compute site in the Site Catalog. The following
    attributes make up the System Information attributes</para>

    <orderedlist>
      <listitem>
        <para>arch</para>
      </listitem>

      <listitem>
        <para>os</para>
      </listitem>

      <listitem>
        <para>osrelease</para>
      </listitem>

      <listitem>
        <para>osversion</para>
      </listitem>
    </orderedlist>

    <section>
      <title>Transformation Mappers</title>

      <para>Pegasus has a notion of transformation mappers that determines
      what type of executables are picked up when a job is executed on a
      remote compute site. For transfer of executables, Pegasus constructs a
      soft state map that resides on top of the transformation catalog, that
      helps in determining the locations from where an executable can be
      staged to the remote site.</para>

      <para>Users can specify the following property to pick up a specific
      transformation mapper</para>

      <programlisting><emphasis role="bold">pegasus.catalog.transformation.mapper</emphasis> </programlisting>

      <para>Currently, the following transformation mappers are
      supported.</para>

      <table>
        <title>Transformation Mappers Supported in Pegasus</title>

        <tgroup cols="2">
          <thead>
            <row>
              <entry>Transformation Mapper</entry>

              <entry>Description</entry>
            </row>
          </thead>

          <tbody>
            <row>
              <entry>Installed</entry>

              <entry>This mapper only relies on transformation catalog entries
              that are of type INSTALLED to construct the soft state map. This
              results in Pegasus never doing any transfer of executables as
              part of the workflow. It always prefers the installed
              executables at the remote sites</entry>
            </row>

            <row>
              <entry>Staged</entry>

              <entry>This mapper only relies on matching transformation
              catalog entries that are of type STAGEABLE to construct the soft
              state map. This results in the executable workflow referring
              only to the staged executables, irrespective of the fact that
              the executables are already installed at the remote end</entry>
            </row>

            <row>
              <entry>All</entry>

              <entry>This mapper relies on all matching transformation catalog
              entries of type STAGEABLE or INSTALLED for a particular
              transformation as valid sources for the transfer of executables.
              This the most general mode, and results in the constructing the
              map as a result of the cartesian product of the matches.</entry>
            </row>

            <row>
              <entry>Submit</entry>

              <entry>This mapper only on matching transformation catalog
              entries that are of type STAGEABLE and reside at the submit host
              (pool local), are used while constructing the soft state map.
              This is especially helpful, when the user wants to use the
              latest compute code for his computations on the grid and that
              relies on his submit host.</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>
  </section>

  <section>
    <title>Staging of Pegasus Worker Package</title>

    <para>Pegasus can optionally stage the pegasus worker package as part of
    the executable workflow to remote workflow specific execution directory.
    The pegasus worker package contains the pegasus auxillary executables that
    are required on the remote site. If the worker package is not staged as
    part of the executable workflow, then Pegasus relies on the installed
    version of the worker package on the remote site. To determine the
    location of the installed version of the worker package on a remote site,
    Pegasus looks for an environment profile PEGASUS_HOME for the site in the
    Site Catalog.</para>

    <para>Users can set the following property to true to turn on worker
    package staging</para>

    <programlisting><emphasis role="bold">pegasus.transfer.worker.package          true</emphasis> </programlisting>

    <para>By default, when worker package staging is turned on pegasus pulls
    the compatible worker package from the Pegasus Website. To specify a
    different worker package location, users can specify the transformation
    <emphasis role="bold">pegasus::worker</emphasis> in the transformation
    catalog with</para>

    <itemizedlist>
      <listitem>
        <para>type set to STAGEABLE</para>
      </listitem>

      <listitem>
        <para>System Information attributes of the transformation catalog
        entry match the System Information attributes of the compute
        site.</para>
      </listitem>

      <listitem>
        <para>the PFN specified should be a remote URL that can be pulled to
        the compute site.</para>
      </listitem>
    </itemizedlist>
  </section>

  <section>
    <title>Second Level Staging</title>

    <para>By default, Pegasus executes the jobs in the workflow specific
    directory created on the shared filesystem of a compute site. However, if
    a user wants Pegasus can execute the jobs on the worker nodes filesystem.
    When the jobs are executed on the worker node, they pull the input data
    for the job from the workflow specific directory on the staging server (
    usually the shared filesystem on the compute site ) to a directory on the
    worker node filesystem, and after the job has completed stages out the
    output files from the worker node to the workflow specific execution
    directory.</para>

    <para>The separate data stagein and stageout jobs are still added to the
    workflow. They are responsible for getting the input data to the workflow
    specific directory on the staging server ( usually the shared filesystem
    on the compute site ) , and pushing out the output data to final storage
    site from that directory.</para>

    <figure>
      <title>Second Level Staging : Getting Data to and from a directory on
      the worker nodes</title>

      <mediaobject>
        <imageobject>
          <imagedata fileref="./images/sls-transfer-worker.png" />
        </imageobject>
      </mediaobject>
    </figure>

    <para>This mode is especially useful for running in the cloud environments
    where you don't want to setup a shared filesystem between the worker
    nodes. Running in that mode is explained in detail <link
    linkend="amazon_aws">here.</link></para>

    <para>To turn on second level staging for the workflows users should set
    the following properties</para>

    <programlisting><emphasis role="bold">pegasus.execute.*.filesystem.local = true   </emphasis>    # Turn on second-level staging (SLS)
<emphasis role="bold">pegasus.transfer.sls.s3.stage.sls.file = false</emphasis>  # Do not transfer .sls files via transfer jobs
<emphasis role="bold">pegasus.gridstart = SeqExec </emphasis>                    # Use SeqExec to launch the jobs</programlisting>
  </section>
</section>
