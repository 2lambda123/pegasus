<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE section PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN"
"http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<section id="transfer">
  <title>Data Transfers</title>

  <para>As part of the Workflow Mapping Process, Pegasus does data management
  for the executable workflow . It queries a Replica Catalog to discover the
  locations of the input datasets and adds data movement and registration
  nodes in the workflow to</para>

  <orderedlist>
    <listitem>
      <para>stage-in input data to the staging sites ( a site associated with
      the compute job to be used for staging. In the shared filesystem setup,
      staging site is the same as the execution sites where the jobs in the
      workflow are executed )</para>
    </listitem>

    <listitem>
      <para>stage-out output data generated by the workflow to the final
      storage site.</para>
    </listitem>

    <listitem>
      <para>stage-in intermediate data between compute sites if
      required.</para>
    </listitem>

    <listitem>
      <para>data registration nodes to catalog the locations of the output
      data on the final storage site into the replica catalog.</para>
    </listitem>
  </orderedlist>

  <para>The separate data movement jobs that are added to the executable
  workflow are responsible for staging data to a workflow specific directory
  accessible to the staging server on a staging site associated with the
  compute sites. Depending on the data staging configuration, the staging site
  for a compute site is the compute site itself. In the default case, the
  staging server is usually on the headnode of the compute site and has access
  to the shared filesystem between the worker nodes and the head node. Pegasus
  adds a directory creation job in the executable workflow that creates the
  workflow specific directory on the staging server.</para>

  <para>In addition to data, Pegasus does transfer user executables to the
  compute sites if the executables are not installed on the remote sites
  before hand. This chapter gives an overview of how transfers of data and
  executables is managed in Pegasus.</para>

  <section id="ref_data_staging_configuration">
    <title>Data Staging Configuration</title>

    <para>Pegasus can be broadly setup to run workflows in the following
    configurations</para>

    <itemizedlist>
      <listitem>
        <para><emphasis role="bold">Shared File System</emphasis></para>

        <para>This setup applies to where the head node and the worker nodes
        of a cluster share a filesystem. Compute jobs in the workflow run in a
        directory on the shared filesystem.</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">NonShared FileSystem</emphasis></para>

        <para>This setup applies to where the head node and the worker nodes
        of a cluster don't share a filesystem. Compute jobs in the workflow
        run in a local directory on the worker node</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">Condor Pool Without a shared
        filesystem</emphasis></para>

        <para>This setup applies to a condor pool where the worker nodes
        making up a condor pool don't share a filesystem. All data IO is
        achieved using Condor File IO. This is a special case of the non
        shared filesystem setup, where instead of using pegasus-transfer to
        transfer input and output data, Condor File IO is used.</para>
      </listitem>
    </itemizedlist>

    <para>For the purposes of data configuration various sites, and
    directories are defined below.</para>

    <orderedlist>
      <listitem>
        <para><emphasis role="bold">Submit Host</emphasis></para>

        <para>The host from where the workflows are submitted . This is where
        Pegasus and Condor DAGMan are installed. This is referred to as the
        <emphasis role="bold">"local"</emphasis> site in the site catalog
        .</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">Compute Site</emphasis></para>

        <para>The site where the jobs mentioned in the DAX are executed. There
        needs to be an entry in the Site Catalog for every compute site. The
        compute site is passed to pegasus-plan using <emphasis
        role="bold">--sites</emphasis> option</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">Staging Site</emphasis></para>

        <para>A site to which the separate transfer jobs in the executable
        workflow ( jobs with stage_in , stage_out and stage_inter prefixes
        that Pegasus adds using the transfer refiners) stage the input data to
        and the output data from to transfer to the final output site.
        Currently, the staging site is always the compute site where the jobs
        execute.</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">Output Site</emphasis></para>

        <para>The output site is the final storage site where the users want
        the output data from jobs to go to. The output site is passed to
        pegasus-plan using the <emphasis role="bold">--output</emphasis>
        option. The stageout jobs in the workflow stage the data from the
        staging site to the final storage site.</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">Input Site</emphasis></para>

        <para>The site where the input data is stored. The locations of the
        input data are catalogued in the Replica Catalog, and the pool
        attribute of the locations gives us the site handle for the input
        site.</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">Workflow Execution
        Directory</emphasis></para>

        <para>This is the directory created by the create dir jobs in the
        executable workflow on the Staging Site. This is a directory per
        workflow per staging site. Currently, the Staging site is always the
        Compute Site.</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">Worker Node Directory</emphasis></para>

        <para>This is the directory created on the worker nodes per job
        usually by the job wrapper that launches the job.</para>
      </listitem>
    </orderedlist>

    <section>
      <title>Shared File System</title>

      <para>By default Pegasus is setup to run workflows in the shared file
      system setup, where the worker nodes and the head node of a cluster
      share a filesystem.</para>

      <figure>
        <title>Shared File System Setup</title>

        <mediaobject>
          <imageobject>
            <imagedata align="center" contentdepth="100%"
                       fileref="images/data-configuration-sharedfs.png"
                       scalefit="1" width="100%"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>The data flow is as follows in this case</para>

      <orderedlist>
        <listitem>
          <para>Stagein Job executes ( either on Submit Host or Head Node ) to
          stage in input data from Input Sites ( 1---n) to a workflow specific
          execution directory on the shared filesystem.</para>
        </listitem>

        <listitem>
          <para>Compute Job starts on a worker node in the workflow execution
          directory. Accesses the input data using Posix IO</para>
        </listitem>

        <listitem>
          <para>Compute Job executes on the worker node and writes out output
          data to workflow execution directory using Posix IO</para>
        </listitem>

        <listitem>
          <para>Stageout Job executes ( either on Submit Host or Head Node )
          to stage out output data from the workflow specific execution
          directory to a directory on the final output site.</para>
        </listitem>
      </orderedlist>

      <tip>
        <para>Set <emphasis role="bold">pegasus.data.configuration</emphasis>
        to <emphasis role="bold">sharedfs</emphasis> to run in this
        configuration.</para>
      </tip>
    </section>

    <section>
      <title>Non Shared Filesystem</title>

      <para>In this setup , Pegasus runs workflows on local file-systems of
      worker nodes with the the worker nodes not sharing a filesystem. The
      data transfers happen between the worker node and a staging / data
      coordination site. The staging site server can be a file server on the
      head node of a cluster or can be on a separate machine.</para>

      <para><emphasis role="bold">Setup</emphasis><itemizedlist>
          <listitem>
            <para>compute and staging site are the different</para>
          </listitem>

          <listitem>
            <para>head node and worker nodes of compute site don't share a
            filesystem</para>
          </listitem>

          <listitem>
            <para>Input Data is staged from remote sites.</para>
          </listitem>

          <listitem>
            <para>Remote Output Site i.e site other than compute site. Can be
            submit host.</para>
          </listitem>
        </itemizedlist></para>

      <figure>
        <title>Non Shared Filesystem Setup</title>

        <mediaobject>
          <imageobject>
            <imagedata align="center" contentdepth="100%"
                       fileref="images/data-configuration-nonsharedfs.png"
                       id="Figure2" scalefit="1" width="100%"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>The data flow is as follows in this case</para>

      <orderedlist>
        <listitem>
          <para>Stagein Job executes ( either on Submit Host or on staging
          site ) to stage in input data from Input Sites ( 1---n) to a
          workflow specific execution directory on the staging site.</para>
        </listitem>

        <listitem>
          <para>Compute Job starts on a worker node in a local execution
          directory. Accesses the input data using pegasus transfer to
          transfer the data from the staging site to a local directory on the
          worker node</para>
        </listitem>

        <listitem>
          <para>The compute job executes in the worker node, and executes on
          the worker node.</para>
        </listitem>

        <listitem>
          <para>The compute Job writes out output data to the local directory
          on the worker node using Posix IO</para>
        </listitem>

        <listitem>
          <para>Output Data is pushed out to the staging site from the worker
          node using pegasus-transfer.</para>
        </listitem>

        <listitem>
          <para>Stageout Job executes ( either on Submit Host or staging site
          ) to stage out output data from the workflow specific execution
          directory to a directory on the final output site.</para>
        </listitem>
      </orderedlist>

      <para>In this case, the compute jobs are wrapped as <link
      linkend="pegasuslite">PegasusLite</link> instances.</para>

      <para>This mode is especially useful for running in the cloud
      environments where you don't want to setup a shared filesystem between
      the worker nodes. Running in that mode is explained in detail <link
      linkend="amazon_aws">here.</link></para>

      <tip>
        <para>Set p<emphasis role="bold">egasus.data.configuration</emphasis>
        to <emphasis role="bold">nonsharedfs</emphasis> to run in this
        configuration. The staging site can be specified using the <emphasis
        role="bold">--staging-site</emphasis> option to pegasus-plan.</para>
      </tip>
    </section>

    <section>
      <title>Condor Pool Without a Shared Filesystem</title>

      <para>This setup applies to a condor pool where the worker nodes making
      up a condor pool don't share a filesystem. All data IO is achieved using
      Condor File IO. This is a special case of the non shared filesystem
      setup, where instead of using pegasus-transfer to transfer input and
      output data, Condor File IO is used.</para>

      <para><emphasis role="bold">Setup</emphasis><itemizedlist>
          <listitem>
            <para>Submit Host and staging site are same</para>
          </listitem>

          <listitem>
            <para>head node and worker nodes of compute site don't share a
            filesystem</para>
          </listitem>

          <listitem>
            <para>Input Data is staged from remote sites.</para>
          </listitem>

          <listitem>
            <para>Remote Output Site i.e site other than compute site. Can be
            submit host.</para>
          </listitem>
        </itemizedlist></para>

      <figure>
        <title>Condor Pool Without a Shared Filesystem</title>

        <mediaobject>
          <imageobject>
            <imagedata align="center" contentdepth="100%"
                       fileref="images/data-configuration-condorio.png"
                       id="Figure13" scalefit="1" width="100%"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>The data flow is as follows in this case</para>

      <orderedlist>
        <listitem>
          <para>Stagein Job executeson the submit host to stage in input data
          from Input Sites ( 1---n) to a workflow specific execution directory
          on the submit host</para>
        </listitem>

        <listitem>
          <para>Compute Job starts on a worker node in a local execution
          directory. Before the compute job starts, Condor transfers the input
          data for the job from the workflow execution directory on thesubmit
          host to the local execution directory on the worker node.</para>
        </listitem>

        <listitem>
          <para>The compute job executes in the worker node, and executes on
          the worker node.</para>
        </listitem>

        <listitem>
          <para>The compute Job writes out output data to the local directory
          on the worker node using Posix IO</para>
        </listitem>

        <listitem>
          <para>When the compute job finishes, Condor transfers the output
          data for the job from the local execution directory on the worker
          node to the workflow execution directory on the submit host.</para>
        </listitem>

        <listitem>
          <para>Stageout Job executes ( either on Submit Host or staging site
          ) to stage out output data from the workflow specific execution
          directory to a directory on the final output site.</para>
        </listitem>
      </orderedlist>

      <para>In this case, the compute jobs are wrapped as <link
      linkend="pegasuslite">PegasusLite</link> instances.</para>

      <para>This mode is especially useful for running in the cloud
      environments where you don't want to setup a shared filesystem between
      the worker nodes. Running in that mode is explained in detail <link
      linkend="amazon_aws">here.</link></para>

      <tip>
        <para>Set p<emphasis role="bold">egasus.data.configuration</emphasis>
        to <emphasis role="bold">condorio</emphasis> to run in this
        configuration. In this mode, the staging site is automatically set to
        site <emphasis role="bold">local</emphasis></para>
      </tip>
    </section>
  </section>

  <section id="local_vs_remote_transfers">
    <title>Local versus Remote Transfers</title>

    <para>As far as possible, Pegasus will ensure that the transfer jobs added
    to the executable workflow are executed on the submit host. By default,
    Pegasus will schedule a transfer to be executed on the remote staging site
    only if there is no way to execute it on the submit host. For e.g if the
    file server specified for the staging site/compute site is a file server,
    then Pegasus will schedule all the stage in data movement jobs on the
    compute site to stage-in the input data for the workflow. Another case
    would be if a user has symlinking turned on. In that case, the transfer
    jobs that symlink against the input data on the compute site, will be
    executed remotely ( on the compute site ).</para>

    <para>Users can specify the property <emphasis
    role="bold">pegasus.transfer.*.remote.sites</emphasis> to change the
    default behaviour of Pegasus and force pegasus to run different types of
    transfer jobs for the sites specified on the remote site. The value of the
    property is a comma separated list of compute sites for which you want the
    transfer jobs to run remotely.</para>

    <para>The table below illustrates all the possible variations of the
    property.</para>

    <table>
      <title>Property Variations for pegasus.transfer.*.remote.sites</title>

      <tgroup cols="2">
        <thead>
          <row>
            <entry>Property Name</entry>

            <entry>Applies to</entry>
          </row>
        </thead>

        <tbody>
          <row>
            <entry>pegasus.transfer.stagein.remote.sites</entry>

            <entry>the stage in transfer jobs</entry>
          </row>

          <row>
            <entry>pegasus.transfer.stageout.remote.sites</entry>

            <entry>the stage out transfer jobs</entry>
          </row>

          <row>
            <entry>pegasus.transfer.inter.remote.sites</entry>

            <entry>the inter site transfer jobs</entry>
          </row>

          <row>
            <entry>pegasus.transfer.*.remote.sites</entry>

            <entry>all types of transfer jobs</entry>
          </row>
        </tbody>
      </tgroup>
    </table>

    <para>The prefix for the transfer job name indicates whether the transfer
    job is to be executed locallly ( on the submit host ) or remotely ( on the
    compute site ). For example stage_in_local_ in a transfer job name
    stage_in_local_isi_viz_0 indicates that the transfer job is a stage in
    transfer job that is executed locally and is used to transfer input data
    to compute site isi_viz. The prefix naming scheme for the transfer jobs is
    <emphasis
    role="bold">[stage_in|stage_out|inter]_[local|remote]_</emphasis> .</para>
  </section>

  <section>
    <title>Symlinking Against Input Data</title>

    <para>If input data for a job already exists on a compute site, then it is
    possible for Pegasus to symlink against that data. In this case, the
    remote stage in transfer jobs that Pegasus adds to the executable workflow
    will symlink instead of doing a copy of the data.</para>

    <para>Pegasus determines whether a file is on the same site as the compute
    site, by inspecting the pool attribute associated with the URL in the
    Replica Catalog. If the pool attribute of an input file location matches
    the compute site where the job is scheduled, then that particular input
    file is a candidate for symlinking.</para>

    <para>For Pegasus to symlink against existing input data on a compute
    site, following must be true</para>

    <orderedlist>
      <listitem>
        <para>Property <emphasis role="bold">pegasus.transfer.links</emphasis>
        is set to <emphasis role="bold">true</emphasis></para>
      </listitem>

      <listitem>
        <para>The input file location in the Replica Catalog has the pool
        attribute matching the compute site.</para>
      </listitem>
    </orderedlist>

    <tip>
      <para>To confirm if a particular input file is symlinked instead of
      being copied, look for the destination URL for that file in
      stage_in_remote*.in file. The destination URL will start with symlink://
      .</para>
    </tip>

    <para>In the symlinking case, Pegasus strips out URL prefix from a URL and
    replaces it with a file URL.</para>

    <para>For example if a user has the following URL catalogued in the
    Replica Catalog for an input file f.input</para>

    <programlisting>f.input   gsiftp://server.isi.edu/shared/storage/input/data/f.input pool="isi"</programlisting>

    <para>and the compute job that requires this file executes on a compute
    site named isi , then if symlinking is turned on the data stage in job
    (stage_in_remote_viz_0 ) will have the following source and destination
    specified for the file</para>

    <programlisting>#viz viz
file:///shared/storage/input/data/f.input  symlink://shared-scratch/workflow-exec-dir/f.input
</programlisting>
  </section>

  <section>
    <title>Addition of Separate Data Movement Nodes to Executable
    Workflow</title>

    <para>Pegasus relies on a Transfer Refiner that comes up with the strategy
    on how many data movement nodes are added to the executable workflow. All
    the compute jobs scheduled to a site share the same workflow specific
    directory. The transfer refiners ensure that only one copy of the input
    data is transferred to the workflow execution directory. This is to
    prevent data clobbering . Data clobbering can occur when compute jobs of a
    workflow share some input files, and have different stage in transfer jobs
    associated with them that are staging the shared files to the same
    destination workflow execution directory.</para>

    <para>The default Transfer Refiner used in Pegasus is the Bundle Refiner
    that allows the user to specify how many local|remote stagein|stageout
    jobs are created per execution site.</para>

    <para>The behavior of the refiner is controlled by specifying certain
    pegasus profiles</para>

    <orderedlist>
      <listitem>
        <para>either with the execution sites in the site catalog</para>
      </listitem>

      <listitem>
        <para>OR globally in the properties file</para>
      </listitem>
    </orderedlist>

    <table>
      <title>Pegasus Profile Keys For the Cluster Transfer Refiner</title>

      <tgroup cols="2">
        <thead>
          <row>
            <entry>Profile Key</entry>

            <entry>Description</entry>
          </row>
        </thead>

        <tbody>
          <row>
            <entry>stagein.clusters</entry>

            <entry>This key determines the maximum number of stage-in jobs
            that are can executed locally or remotely per compute site per
            workflow.</entry>
          </row>

          <row>
            <entry>stagein.local.clusters</entry>

            <entry>This key provides finer grained control in determining the
            number of stage-in jobs that are executed locally and are
            responsible for staging data to a particular remote site.</entry>
          </row>

          <row>
            <entry>stagein.remote.clusters</entry>

            <entry>This key provides finer grained control in determining the
            number of stage-in jobs that are executed remotely on the remote
            site and are responsible for staging data to it.</entry>
          </row>

          <row>
            <entry>stageout.clusters</entry>

            <entry>This key determines the maximum number of stage-out jobs
            that are can executed locally or remotely per compute site per
            workflow.</entry>
          </row>

          <row>
            <entry>stageout.local.clusters</entry>

            <entry>This key provides finer grained control in determining the
            number of stage-out jobs that are executed locally and are
            responsible for staging data from a particular remote
            site.</entry>
          </row>

          <row>
            <entry>stageout.remote.clusters</entry>

            <entry>This key provides finer grained control in determining the
            number of stage-out jobs that are executed remotely on the remote
            site and are responsible for staging data from it.</entry>
          </row>
        </tbody>
      </tgroup>
    </table>

    <figure>
      <title>Default Transfer Case : Input Data To Workflow Specific Directory
      on Shared File System</title>

      <mediaobject>
        <imageobject>
          <imagedata contentdepth="100%"
                     fileref="images/cluster-transfer-refiner.png"
                     scalefit="1" width="100%"/>
        </imageobject>
      </mediaobject>
    </figure>
  </section>

  <section>
    <title>Executable Used for Transfer Jobs</title>

    <para>Pegasus refers to a python script called <emphasis
    role="bold">pegasus-transfer</emphasis> as the executable in the transfer
    jobs to transfer the data. pegasus-transfer is a python based wrapper
    around various transfer clients . pegasus-transfer looks at source and
    destination url and figures out automatically which underlying client to
    use. pegasus-transfer is distributed with the PEGASUS and can be found at
    $PEGASUS_HOME/bin/pegasus-transfer.</para>

    <para>Currently, pegasus-transfer interfaces with the following transfer
    clients</para>

    <table>
      <title>Transfer Clients interfaced to by pegasus-transfer</title>

      <tgroup cols="2">
        <thead>
          <row>
            <entry>Transfer Client</entry>

            <entry>Used For</entry>
          </row>
        </thead>

        <tbody>
          <row>
            <entry>globus-url-copy</entry>

            <entry>staging files to and from a gridftp server.</entry>
          </row>

          <row>
            <entry>lcg-copy</entry>

            <entry>staging files to and from a SRM server.</entry>
          </row>

          <row>
            <entry>wget</entry>

            <entry>staging files from a HTTP server.</entry>
          </row>

          <row>
            <entry>cp</entry>

            <entry>copying files from a POSIX filesystem .</entry>
          </row>

          <row>
            <entry>ln</entry>

            <entry>symlinking against input files.</entry>
          </row>

          <row>
            <entry>pegasus-s3/s3cmd</entry>

            <entry>staging files to and from s3 bucket in the amazon
            cloud</entry>
          </row>

          <row>
            <entry>scp</entry>

            <entry>staging files using scp</entry>
          </row>

          <row>
            <entry>iget</entry>

            <entry>staging files to and from a irods server.</entry>
          </row>
        </tbody>
      </tgroup>
    </table>

    <para>For remote sites, Pegasus constructs the default path to
    pegasus-transfer on the basis of PEGASUS_HOME env profile specified in the
    site catalog. To specify a different path to the pegasus-transfer client ,
    users can add an entry into the transformation catalog with fully
    qualified logical name as <emphasis
    role="bold">pegasus::pegasus-transfer</emphasis></para>
  </section>

  <section>
    <title>Executables used for Directory Creation and Cleanup Jobs</title>

    <para>Starting 4.0, Pegasus has changed the way how the scratch
    directories are created on the staging site. The planner now prefers to
    schedule the directory creation and cleanup jobs locally. The jobs refer
    to python based tools, that call out to protocol specific clients to
    determine what client is picked up. For protocols, where specific remote
    cleanup and directory creation clients don't exist ( for example gridftp
    ), the python tools rely on the corresponding transfer tool to create a
    directory by initiating a transfer of an empty file. The python clients
    used to create directories and remove files are called</para>

    <itemizedlist>
      <listitem>
        <para>pegasus-create-dir</para>
      </listitem>

      <listitem>
        <para>pegasus-cleanup</para>
      </listitem>
    </itemizedlist>

    <para>Both these clients inspect the URL's to to determine what underlying
    client to pick up.</para>

    <table>
      <title>Clients interfaced to by pegasus-create-dir</title>

      <tgroup cols="2">
        <thead>
          <row>
            <entry>Client</entry>

            <entry>Used For</entry>
          </row>
        </thead>

        <tbody>
          <row>
            <entry>globus-url-copy</entry>

            <entry>to create directories against a gridftp/ftp server</entry>
          </row>

          <row>
            <entry>srm-mkdir</entry>

            <entry>to create directories against a SRM server.</entry>
          </row>

          <row>
            <entry>mkdir</entry>

            <entry>to create a directory on the local filesystem</entry>
          </row>

          <row>
            <entry>pegasus-s3</entry>

            <entry>to create a s3 bucket in the amazon cloud</entry>
          </row>

          <row>
            <entry>scp</entry>

            <entry>staging files using scp</entry>
          </row>

          <row>
            <entry>imkdir</entry>

            <entry>to create a directory against an IRODS server</entry>
          </row>
        </tbody>
      </tgroup>
    </table>

    <table>
      <title>Clients interfaced to by pegasus-cleanup</title>

      <tgroup cols="2">
        <thead>
          <row>
            <entry>Client</entry>

            <entry>Used For</entry>
          </row>
        </thead>

        <tbody>
          <row>
            <entry>globus-url-copy</entry>

            <entry>to remove a file against a gridftp/ftp server. In this case
            a zero byte file is created</entry>
          </row>

          <row>
            <entry>srm-rm</entry>

            <entry>to remove files against a SRM server.</entry>
          </row>

          <row>
            <entry>rm</entry>

            <entry>to remove a file on the local filesystem</entry>
          </row>

          <row>
            <entry>pegasus-s3</entry>

            <entry>to remove a file from the s3 bucket.</entry>
          </row>

          <row>
            <entry>scp</entry>

            <entry>to remove a file against a scp server. In this case a zero
            byte file is created.</entry>
          </row>

          <row>
            <entry>irm</entry>

            <entry>to remove a file against an IRODS server</entry>
          </row>
        </tbody>
      </tgroup>
    </table>

    <para>The only case, where the create dir and cleanup jobs are scheduled
    to run remotely is when for the staging site, a file server is
    specified.</para>
  </section>

  <section id="cred_staging">
    <title>Credentials Staging</title>

    <para>Pegasus tries to do data staging from localhost by default, but some
    data scenarios makes some <link linkend="local_vs_remote_transfers">remote
    jobs do data staging</link>. An example of such a case is when running in
    <link linkend="ref_data_staging_configuration">nonsharedfs</link> mode.
    Depending on the transfer protocols used, the job may have to carry
    credentials to enable these datat transfers. To specify where which
    credential to use and where Pegasus can find it, use environment variable
    profiles in your site catalog. The supported credential types are X.509
    grid proxies, Amazon AWS S3 keys, iRods password and SSH keys.</para>

    <section id="x509_cred">
      <title>X.509 Grid Proxies</title>

      <para>If the grid proxy is required by transfer jobs, and the proxy is
      in the standard location, Pegasus will pick the proxy up automatically.
      For non-standard proxy locations, you can use the
      <varname>X509_USER_PROXY</varname> environment variable. Site catalog
      example:</para>

      <programlisting>&lt;profile namespace="env" key="X509_USER_PROXY" &gt;/some/location/x509up&lt;/profile&gt;</programlisting>
    </section>

    <section id="s3_cred">
      <title>Amazon AWS S3</title>

      <para>If a workflow is using s3 URLs, Pegasus has to be told where to
      find the .s3cfg file. This format of the file is described in the <link
      linkend="cli-pegasus-s3">pegaus-s3 command line client's man
      page</link>. For the file to be picked up by the workflow, set the
      <varname>S3CFG</varname> environment profile to the location of the
      file. Site catalog example:</para>

      <programlisting>&lt;profile namespace="env" key="S3CFG" &gt;/home/user/.s3cfg&lt;/profile&gt;</programlisting>
    </section>

    <section id="irods_cred">
      <title>iRods Password</title>

      <para>If a workflow is using irods URLs, Pegasus has to be given an
      irodsEnv file. It is a standard file, with the addtion of an password
      attribute. Example:</para>

      <programlisting># iRODS personal configuration file.
#
# iRODS server host name:
irodsHost 'iren.renci.org'
# iRODS server port number:
irodsPort 1259

# Default storage resource name:
irodsDefResource 'renResc'
# Home directory in iRODS:
irodsHome '/tip-renci/home/mats'
# Current directory in iRODS:
irodsCwd '/tip-renci/home/mats'
# Account name:
irodsUserName 'mats'
# Zone:
irodsZone 'tip-renci' 

# this is used with Pegasus
irodsPassword 'somesecretpassword'</programlisting>

      <para>The location of the file can be given to the workflow using the
      <varname>irodsEnvFile</varname> environment profile. Site catalog
      example:</para>

      <programlisting>&lt;profile namespace="env" key="irodsEnvFile" &gt;/home/user/.irods/.irodsEnv&lt;/profile&gt;</programlisting>
    </section>

    <section id="ssh_cred">
      <title>SSH Keys</title>

      <para>New in Pegasus 4.0 is the support for data staging with scp using
      ssh public/private key authentication. In this mode, Pegasus transports
      a private key with the jobs. The storage machines will have to have the
      public part of the key listed in ~/.ssh/authorized_keys.</para>

      <warning>
        <para>SSH keys should be handled in a secure manner. In order to keep
        your personal ssh keys secure, It is recommended that a special set of
        keys are created for use with the workflow. Note that Pegasus will not
        pick up ssh keys automatically. The user will have to specify which
        key to use with <varname>SSH_PRIVATE_KEY</varname>.</para>
      </warning>

      <para>The location of the ssh private key can be specified with the
      <varname>SSH_PRIVATE_KEY</varname> environment profile. Site catalog
      example:</para>

      <programlisting>&lt;profile namespace="env" key="SSH_PRIVATE_KEY" &gt;/home/user/wf/wfsshkey&lt;/profile&gt;</programlisting>
    </section>
  </section>

  <section>
    <title>Staging of Executables</title>

    <para>Users can get Pegasus to stage the user executables ( executables
    that the jobs in the DAX refer to ) as part of the transfer jobs to the
    workflow specific execution directory on the compute site. The URL
    locations of the executables need to be specified in the transformation
    catalog as the PFN and the type of executable needs to be set to <emphasis
    role="bold">STAGEABLE</emphasis> .</para>

    <para>The location of a transformation can be specified either in</para>

    <itemizedlist>
      <listitem>
        <para>DAX in the executables section. More details <link
        linkend="dax_transformation_catalog">here</link> .</para>
      </listitem>

      <listitem>
        <para>Transformation Catalog. More details <link
        linkend="transformation">here</link> .</para>
      </listitem>
    </itemizedlist>

    <para>A particular transformation catalog entry of type STAGEABLE is
    compatible with a compute site only if all the System Information
    attributes associated with the entry match with the System Information
    attributes for the compute site in the Site Catalog. The following
    attributes make up the System Information attributes</para>

    <orderedlist>
      <listitem>
        <para>arch</para>
      </listitem>

      <listitem>
        <para>os</para>
      </listitem>

      <listitem>
        <para>osrelease</para>
      </listitem>

      <listitem>
        <para>osversion</para>
      </listitem>
    </orderedlist>

    <section>
      <title>Transformation Mappers</title>

      <para>Pegasus has a notion of transformation mappers that determines
      what type of executables are picked up when a job is executed on a
      remote compute site. For transfer of executables, Pegasus constructs a
      soft state map that resides on top of the transformation catalog, that
      helps in determining the locations from where an executable can be
      staged to the remote site.</para>

      <para>Users can specify the following property to pick up a specific
      transformation mapper</para>

      <programlisting><emphasis role="bold">pegasus.catalog.transformation.mapper</emphasis> </programlisting>

      <para>Currently, the following transformation mappers are
      supported.</para>

      <table>
        <title>Transformation Mappers Supported in Pegasus</title>

        <tgroup cols="2">
          <thead>
            <row>
              <entry>Transformation Mapper</entry>

              <entry>Description</entry>
            </row>
          </thead>

          <tbody>
            <row>
              <entry>Installed</entry>

              <entry>This mapper only relies on transformation catalog entries
              that are of type INSTALLED to construct the soft state map. This
              results in Pegasus never doing any transfer of executables as
              part of the workflow. It always prefers the installed
              executables at the remote sites</entry>
            </row>

            <row>
              <entry>Staged</entry>

              <entry>This mapper only relies on matching transformation
              catalog entries that are of type STAGEABLE to construct the soft
              state map. This results in the executable workflow referring
              only to the staged executables, irrespective of the fact that
              the executables are already installed at the remote end</entry>
            </row>

            <row>
              <entry>All</entry>

              <entry>This mapper relies on all matching transformation catalog
              entries of type STAGEABLE or INSTALLED for a particular
              transformation as valid sources for the transfer of executables.
              This the most general mode, and results in the constructing the
              map as a result of the cartesian product of the matches.</entry>
            </row>

            <row>
              <entry>Submit</entry>

              <entry>This mapper only on matching transformation catalog
              entries that are of type STAGEABLE and reside at the submit host
              (pool local), are used while constructing the soft state map.
              This is especially helpful, when the user wants to use the
              latest compute code for his computations on the grid and that
              relies on his submit host.</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>
  </section>

  <section>
    <title>Staging of Pegasus Worker Package</title>

    <para>Pegasus can optionally stage the pegasus worker package as part of
    the executable workflow to remote workflow specific execution directory.
    The pegasus worker package contains the pegasus auxillary executables that
    are required on the remote site. If the worker package is not staged as
    part of the executable workflow, then Pegasus relies on the installed
    version of the worker package on the remote site. To determine the
    location of the installed version of the worker package on a remote site,
    Pegasus looks for an environment profile PEGASUS_HOME for the site in the
    Site Catalog.</para>

    <para>Users can set the following property to true to turn on worker
    package staging</para>

    <programlisting><emphasis role="bold">pegasus.transfer.worker.package          true</emphasis> </programlisting>

    <para>By default, when worker package staging is turned on pegasus pulls
    the compatible worker package from the Pegasus Website. To specify a
    different worker package location, users can specify the transformation
    <emphasis role="bold">pegasus::worker</emphasis> in the transformation
    catalog with</para>

    <itemizedlist>
      <listitem>
        <para>type set to STAGEABLE</para>
      </listitem>

      <listitem>
        <para>System Information attributes of the transformation catalog
        entry match the System Information attributes of the compute
        site.</para>
      </listitem>

      <listitem>
        <para>the PFN specified should be a remote URL that can be pulled to
        the compute site.</para>
      </listitem>
    </itemizedlist>

    <section>
      <title>Worker Package Staging in Non Shared Filesystem setup</title>

      <para>Worker package staging is automatically set to true , when
      workflows are setup to run in a non shared filesystem setup i.e.
      <emphasis role="bold">pegasus.data.configuration</emphasis> is set to
      <emphasis role="bold">nonsharedfs</emphasis> or <emphasis
      role="bold">condorio</emphasis> . In these configurations, a
      stage_worker job is created that brings in the worker package to the
      submit directory of the workflow. For each job, the worker package is
      then transferred with the job using Condor File Transfers ( <emphasis
      role="bold">transfer_input_files</emphasis> ) . This transfer always
      happens unless, PEGASUS_HOME is specified in the site catalog for the
      site on which the job is scheduled to run.</para>

      <para>Users can explicitly set the following property to false, to turn
      off worker package staging by the Planner. This is applicable , when
      running in the cloud and virtual machines / worker nodes already have
      the pegasus worker tools installed.</para>

      <programlisting><emphasis role="bold">pegasus.transfer.worker.package          false</emphasis> </programlisting>
    </section>
  </section>
</section>
