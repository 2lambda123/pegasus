<?xml version="1.0"?>
<!DOCTYPE book PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN"
                      "http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<chapter id="advanced_concepts_configs">
   <title>Advanced Concepts and Configurations</title>
   <section>
       <title>Pegasus Profiles</title>



  <section>
    <title>Introduction</title>

    <para>The Pegasus Workflow Mapper uses the concept of profiles to
    encapsulate configurations for various aspects of dealing with the Grid
    infrastructure. Profiles provide an abstract yet uniform interface to
    specify configuration options for various layers from planner/mapper
    behavior to remote environment settings. At various stages during the
    mapping process, profiles may be added to the concretization of a
    job.</para>

    <para>This document describes various types of profiles, levels of
    priorities for intersecting profiles, and how to specify profiles in
    different contexts.</para>
  </section>

  <section>
    <title>Profile Structure</title>

    <para>All profiles are triples comprised of a namespace, a name or key,
    and a value. The namespace is a simple identifier. The key has only
    meaning within its namespace, and it’s yet another identifier. There are
    no constraints on the contents of a value</para>

    <para>Profiles may be represented with different syntaxes in different
    context. However, each syntax will describe the underlying triple.</para>
  </section>

  <section>
    <title>Profile Namespaces</title>

    <para>Each namespace refers to a different aspect of a job’s runtime
    settings. A profile’s representation in the concrete plan (e.g. the Condor
    submit files) depends its namespace. Pegasus supports the following
    namespaces for profiles:</para>

    <itemizedlist>
      <listitem>
        <para><emphasis role="bold">env</emphasis> permits remote environment
        variables to be set.</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">globus</emphasis> sets Globus RSL
        parameters.</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">condor</emphasis> sets Condor
        configuration parameters for the submit file.</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">dagman</emphasis> introduces Condor DAGMan
        configuration parameters.</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">pegasus</emphasis> configures the
        behaviour of various planner/mapper components.</para>
      </listitem>
    </itemizedlist>

    <section>
      <title>The env Profile Namespace</title>

      <para>The <emphasis>env</emphasis> namespace allows users to specify
      environment variables of remote jobs. Globus transports the environment
      variables, and ensure that they are set before the job starts.</para>

      <para>The key used in conjunction with an <emphasis>env</emphasis>
      profile denotes the name of the environment variable. The value of the
      profile becomes the value of the remote environment variable.</para>

      <para>Grid jobs usually only set a minimum of environment variables by
      virtue of Globus. You cannot compare the environment variables visible
      from an interactive login with those visible to a grid job. Thus, it
      often becomes necessary to set environment variables like
      LD_LIBRARY_PATH for remote jobs.</para>

      <para>If you use any of the Pegasus worker package tools like transfer
      or the rc-client, it becomes necessary to set PEGASUS_HOME and
      GLOBUS_LOCATION even for jobs that run locally</para>

      <table>
        <title>Table 1: Useful Environment Settings</title>

        <tgroup cols="2">
          <tbody>
            <row>
              <entry><emphasis role="bold">Environment
              Variable</emphasis></entry>

              <entry><emphasis role="bold">Description</emphasis></entry>
            </row>

            <row>
              <entry>PEGASUS_HOME</entry>

              <entry>Used by auxillary jobs created by Pegasus both on remote
              site and local site. Should be set usually set in the Site
              Catalog for the sites</entry>
            </row>

            <row>
              <entry>GLOBUS_LOCATION</entry>

              <entry>Used by auxillary jobs created by Pegasus both on remote
              site and local site. Should be set usually set in the Site
              Catalog for the sites</entry>
            </row>

            <row>
              <entry>LD_LIBRARY_PATH</entry>

              <entry>Point this to $GLOBUS_LOCATION/lib, except you cannot use
              the dollar variable. You must use the full path. Applies to
              both, local and remote jobs that use Globus components and
              should be usually set in the site catalog for the sites</entry>
            </row>

            <row>
              <entry>CLASSPATH</entry>

              <entry>Use with replica registration jobs on the local site. The
              CLASSPATH is now automatically set for the jobs by setting the
              value to the CLASSPATH set in the shell that was used to invoke
              pegasus-plan.</entry>
            </row>
          </tbody>
        </tgroup>
      </table>

      <para>Even though Condor and Globus both permit environment variable
      settings through their profiles, all remote environment variables must
      be set through the means of <emphasis>env</emphasis> profiles.</para>
    </section>

    <section>
      <title>The globus Profile Namespace</title>

      <para>The <emphasis>globus</emphasis> profile namespace encapsulates
      Globus resource specification language (RSL) instructions. The RSL
      configures settings and behavior of the remote scheduling system. Some
      systems require queue name to schedule jobs, a project name for
      accounting purposes, or a run-time estimate to schedule jobs. The Globus
      RSL addresses all these issues.</para>

      <para>A key in the <emphasis>globus</emphasis> namespace denotes the
      command name of an RLS instruction. The profile value becomes the RSL
      value. Even though Globus RSL is typically shown using parentheses
      around the instruction, the out pair of parentheses is not necessary in
      globus profile specifications</para>

      <para>Table 2 shows some commonly used RSL instructions. For an
      authoritative list of all possible RSL instructions refer to the Globus
      RSL specification.</para>

      <table>
        <title>Table 2: Useful Globus RSL Instructions</title>

        <tgroup cols="2">
          <tbody>
            <row>
              <entry><emphasis role="bold">Key</emphasis></entry>

              <entry><emphasis role="bold">Description</emphasis></entry>
            </row>

            <row>
              <entry>count</entry>

              <entry>the number of times an executable is started.</entry>
            </row>

            <row>
              <entry>jobtype</entry>

              <entry>specifies how the job manager should start the remote
              job. While Pegasus defaults to single, use mpi when running MPI
              jobs.</entry>
            </row>

            <row>
              <entry>maxcputime</entry>

              <entry>the max cpu time for a single execution of a job.</entry>
            </row>

            <row>
              <entry>maxmemory</entry>

              <entry>the maximum memory in MB required for the job</entry>
            </row>

            <row>
              <entry>maxtime</entry>

              <entry>the maximum time or walltime for a single execution of a
              job.</entry>
            </row>

            <row>
              <entry>maxwalltime</entry>

              <entry>the maximum walltime for a single execution of a
              job.</entry>
            </row>

            <row>
              <entry>minmemory</entry>

              <entry>the minumum amount of memory required for this
              job</entry>
            </row>

            <row>
              <entry>project</entry>

              <entry>associates an account with a job at the remote
              end.</entry>
            </row>

            <row>
              <entry>queue</entry>

              <entry>the remote queue in which the job should be run. Used
              when remote scheduler is PBS that supports queues.</entry>
            </row>
          </tbody>
        </tgroup>
      </table>

      <para>Pegasus prevents the user from specifying certain RSL instructions
      as globus profiles, because they are either automatically generated or
      can be overridden through some different means. For instance, if you
      need to specify remote environment settings, do not use the environment
      key in the globus profiles. Use one or more env profiles instead.</para>

      <table>
        <title>Table 3: RSL Instructions that are not permissible</title>

        <tgroup cols="2">
          <tbody>
            <row>
              <entry><emphasis role="bold">Key</emphasis></entry>

              <entry><emphasis role="bold">Reason for
              Prohibition</emphasis></entry>
            </row>

            <row>
              <entry>arguments</entry>

              <entry>you specify arguments in the arguments section for a job
              in the DAX</entry>
            </row>

            <row>
              <entry>directory</entry>

              <entry>the site catalog and properties determine which directory
              a job will run in.</entry>
            </row>

            <row>
              <entry>environment</entry>

              <entry>use multiple env profiles instead</entry>
            </row>

            <row>
              <entry>executable</entry>

              <entry>the physical executable to be used is specified in the
              transformation catalog and is also dependant on the gridstart
              module being used. If you are launching jobs via kickstart then
              the executable created is the path to kickstart and the
              application executable path appears in the arguments for
              kickstart</entry>
            </row>

            <row>
              <entry>stdin</entry>

              <entry>you specify in the DAX for the job</entry>
            </row>

            <row>
              <entry>stdout</entry>

              <entry>you specify in the DAX for the job</entry>
            </row>

            <row>
              <entry>stderr</entry>

              <entry>you specify in the DAX for the job</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>

    <section>
      <title>The condor Profile Namespace</title>

      <para>The Condor submit file controls every detail how and where a job
      is run. The <emphasis>condor</emphasis> profiles permit to add or
      overwrite instructions in the Condor submit file.</para>

      <para>The <emphasis>condor</emphasis> namespace directly sets commands
      in the Condor submit file for a job the profile applies to. Keys in the
      <emphasis>condor</emphasis> profile namespace denote the name of the
      Condor command. The profile value becomes the command's argument. All
      <emphasis>condor</emphasis> profiles are translated into key=value lines
      in the Condor submit file</para>

      <para>Some of the common condor commands that a user may need to specify
      are listed below. For an authoritative list refer to the online condor
      documentation. Note: Pegasus Workflow Planner/Mapper by default specify
      a lot of condor commands in the submit files depending upon the job, and
      where it is being run.</para>

      <table>
        <title>Table 4: Useful Condor Commands</title>

        <tgroup cols="2">
          <tbody>
            <row>
              <entry><emphasis role="bold">Key</emphasis></entry>

              <entry><emphasis role="bold">Description</emphasis></entry>
            </row>

            <row>
              <entry>universe</entry>

              <entry>Pegasus defaults to either globus or scheduler universes.
              Set to standard for compute jobs that require standard universe.
              Set to vanilla to run natively in a condor pool, or to run on
              resources grabbed via condor glidein.</entry>
            </row>

            <row>
              <entry>periodic_release</entry>

              <entry>is the number of times job is released back to the queue
              if it goes to HOLD, e.g. due to Globus errors. Pegasus defaults
              to 3. Use the pegasus.condor.release property to assign a
              different value</entry>
            </row>

            <row>
              <entry>periodic_remove</entry>

              <entry>is the number of times a job is allowed to get into HOLD
              state before being removed from the queue. Pegasus defaults to
              3. Use the pegasus.condor.remove property to assign a different
              value.</entry>
            </row>

            <row>
              <entry>filesystemdomain</entry>

              <entry>Useful for Condor glide-ins to pin a job to a remote
              site.</entry>
            </row>
          </tbody>
        </tgroup>
      </table>

      <para>Pegasus prevents the user from specifying certain Condor commands
      in condor profiles, because they are automatically generated or can be
      overridden through some different means. Table 5 shows prohibited Condor
      commands.</para>

      <table>
        <title>Table 5: Condor commands prohibited in condor profiles</title>

        <tgroup cols="2">
          <tbody>
            <row>
              <entry><emphasis role="bold">Key</emphasis></entry>

              <entry><emphasis role="bold">Reason for
              Prohibition</emphasis></entry>
            </row>

            <row>
              <entry>arguments</entry>

              <entry>you specify arguments in the arguments section for a job
              in the DAX</entry>
            </row>

            <row>
              <entry>environment</entry>

              <entry>use multiple env profiles instead</entry>
            </row>

            <row>
              <entry>executable</entry>

              <entry>the physical executable to be used is specified in the
              transformation catalog and is also dependant on the gridstart
              module being used. If you are launching jobs via kickstart then
              the executable created is the path to kickstart and the
              application executable path appears in the arguments for
              kickstart</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>

    <section>
      <title>The dagman Profile Namespace</title>

      <para>DAGMan is Condor's workflow manager. While planners generate most
      of DAGMan's configuration, it is possible to tweak certain job-related
      characteristics using dagman profiles. A dagman profile can be used to
      specify a DAGMan pre- or post-script. </para>

      <para>Pre- and post-scripts execute on the submit machine. Both inherit
      the environment settings from the submit host when pegasus-submit-dag or
      pegasus-run is invoked. </para>

      <para>By default, kickstart launches all jobs except standard universe
      and MPI jobs. Kickstart tracks the execution of the job, and returns
      usage statistics for the job. A DAGMan post-script starts the Pegasus
      application exitcode to determine, if the job succeeded. DAGMan receives
      the success indication as exit status from exitcode. </para>

      <para>If you need to run your own post-script, you have to take over the
      job success parsing. The planner is set up to pass the file name of the
      remote job's stdout, usually the output from kickstart, as sole argument
      to the post-script.</para>

      <para>Table 6 shows the keys in the dagman profile domain that are
      understood by Pegasus.</para>

      <para><table>
          <title>Table 6: Useful dagman Commands</title>

          <tgroup cols="2">
            <tbody>
              <row>
                <entry><emphasis role="bold">Key</emphasis></entry>

                <entry><emphasis role="bold">Description</emphasis></entry>
              </row>

              <row>
                <entry>PRE</entry>

                <entry>is the path to the pre-script. DAGMan executes the
                pre-script before it runs the job.</entry>
              </row>

              <row>
                <entry>PRE_ARGS</entry>

                <entry>are command-line arguments for the pre-script, if any.
                </entry>
              </row>

              <row>
                <entry>POST</entry>

                <entry>are command-line arguments for the post-script, if
                any.</entry>
              </row>

              <row>
                <entry>RETRY</entry>

                <entry>is the number of times DAGMan retries the full job
                cycle from pre-script through post-script, if failure was
                detected.</entry>
              </row>
            </tbody>
          </tgroup>
        </table></para>
    </section>

    <section>
      <title>The pegasus Profile Namespace</title>

      <para>The <emphasis>pegasus</emphasis> profiles allow users to configure
      extra options to the planners that can be applied selectively to a job
      or a group of jobs. Site selectors may use a sub-set of
      <emphasis>pegasus</emphasis> profiles for their decision-making.</para>

      <para>Table 7 shows some of the useful configuration option Pegasus
      understands.</para>

      <table>
        <title>Table 7: Useful pegasus Profiles.</title>

        <tgroup cols="2">
          <tbody>
            <row>
              <entry><emphasis role="bold">Key</emphasis></entry>

              <entry><emphasis role="bold">Description</emphasis></entry>
            </row>

            <row>
              <entry>workdir</entry>

              <entry>Sets the remote initial dir for a Condor-G job. Overrides
              the work directory algorithm that uses the site catalog and
              properties.</entry>
            </row>

            <row>
              <entry>bundle</entry>

              <entry>Please refer to the Pegasus Clustering Guide for detailed
              description. This option determines the total number of clusters
              per level. Jobs are evenly spread across clusters.</entry>
            </row>

            <row>
              <entry>collapse</entry>

              <entry>Please refer to the Pegasus Clustering Guide for detailed
              description. This profile determines the number of jobs in each
              cluster. The number of clusters depends on the total number of
              jobs on the level.</entry>
            </row>

            <row>
              <entry>collapser</entry>

              <entry>Indicates the clustering executable that is used to run
              the clustered job on the remote site.</entry>
            </row>

            <row>
              <entry>gridstart</entry>

              <entry>Determines the executable for launching a job. Possible
              values are <emphasis>Kickstart |
              NoGridStart|DCLauncher</emphasis> at the moment. </entry>
            </row>

            <row>
              <entry>bundle.stagein</entry>

              <entry>This option determines the maximum number of
              <emphasis>stage-in</emphasis> jobs for a remote site per
              workflow. This is used to configure the
              <emphasis>Bundle</emphasis> Transfer Refiner.</entry>
            </row>

            <row>
              <entry>bundle.stageout</entry>

              <entry>This option determines the maximum number of
              <emphasis>stage-out</emphasis> jobs for a remote site per level
              of a workflow. This is used to configure the
              <emphasis>Bundle</emphasis> Transfer Refiner.</entry>
            </row>

            <row>
              <entry>cluster.stagein</entry>

              <entry>This option determines the maximum number of
              <emphasis>stage-in</emphasis> jobs for a remote site per level
              of a workflow. This is used to configure the
              <emphasis>Cluster</emphasis> Transfer Refiner.</entry>
            </row>

            <row>
              <entry>cluster.stageout</entry>

              <entry>This option determines the maximum number of
              <emphasis>stage-out</emphasis> jobs for a remote site per level
              of a workflow. This is used to configure the
              <emphasis>Cluster</emphasis> Transfer Refiner.</entry>
            </row>

            <row>
              <entry>group</entry>

              <entry>Tags a job with an arbitrary group identifier. The group
              site selector makes use of the tag.</entry>
            </row>

            <row>
              <entry>change.dir</entry>

              <entry>If true, tells <emphasis>kickstart</emphasis> to change
              into the remote working directory. Kickstart itself is executed
              in whichever directory the remote scheduling system chose for
              the job.</entry>
            </row>

            <row>
              <entry>create.dir</entry>

              <entry>If true, tells <emphasis>kickstart</emphasis> to create
              the the remote working directory before changing into the remote
              working directory. Kickstart itself is executed in whichever
              directory the remote scheduling system chose for the
              job.</entry>
            </row>

            <row>
              <entry>transfer.proxy</entry>

              <entry>If true, tells Pegasus to explicitly transfer the proxy
              for transfer jobs to the remote site. This is useful, when you
              want to use a full proxy at the remote end, instead of the
              limited proxy that is transferred by CondorG. </entry>
            </row>

            <row>
              <entry>transfer.arguments</entry>

              <entry>Allows the user to specify the arguments with which the
              transfer executable is invoked. However certain options are
              always generated for the transfer executable(base-uri
              se-mount-point).</entry>
            </row>

            <row>
              <entry>style</entry>

              <entry>Sets the condor submit file style. If set to globus,
              submit file generated refers to CondorG job submissions. If set
              to condor, submit file generated refers to direct Condor
              submission to the local Condor pool. It applies for glidein,
              where nodes from remote grid sites are glided into the local
              condor pool. The default style that is applied is
              globus.</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>
  </section>

  <section>
    <title>Sources for Profiles</title>

    <para>Profiles may enter the job-processing stream at various stages.
    Depending on the requirements and scope a profile is to apply, profiles
    can be associated at </para>

    <itemizedlist>
      <listitem>
        <para>dax level</para>
      </listitem>

      <listitem>
        <para>in the site catalog</para>
      </listitem>

      <listitem>
        <para>in the transformation catalog</para>
      </listitem>

      <listitem>
        <para>as user property settings.</para>
      </listitem>
    </itemizedlist>

    <para>Unfortunately, a different syntax applies to each level and context.
    This section shows the different profile sources and syntaxes. However, at
    the foundation of each profile lies the triple of namespace, key and
    value. </para>

    <section>
      <title>Profiles in DAX</title>

      <para>The user can associate profiles with logical transformations in
      DAX. Environment settings required by a job's application, or a maximum
      estimate on the run-time are examples for profiles at this stage.
      </para>

      <programlisting>&lt;job id="ID000001" namespace="asdf" name="preprocess" version="1.0"
 level="3" dv-namespace="voeckler" dv-name="top" dv-version="1.0"&gt;
  &lt;argument&gt;-a top -T10  -i &lt;filename file="voeckler.f.a"/&gt; 
 -o &lt;filename file="voeckler.f.b1"/&gt;
 &lt;filename file="voeckler.f.b2"/&gt;&lt;/argument&gt;
  <emphasis role="bold">&lt;profile namespace="pegasus" key="walltime"&gt;2&lt;/profile&gt;
  &lt;profile namespace="pegasus" key="diskspace"&gt;1&lt;/profile&gt;</emphasis>
  …
&lt;/job&gt;
</programlisting>
    </section>

    <section>
      <title>Profiles in Site Catalog</title>

      <para>If it becomes necessary to limit the scope of a profile to a
      single site, these profiles should go into the site catalog. A profile
      in the site catalog applies to all jobs and all application run at the
      site. Commonly, site catalog profiles set environment settings like the
      LD_LIBRARY_PATH, or globus rsl parameters like queue and project
      names.</para>

      <para>Currently, there is no tool to manipulate the site catalog, e.g.
      by adding profiles. Modifying the site catalog requires that you load it
      into your editor. </para>

      <para>The XML version of the site catalog uses the following
      syntax:</para>

      <programlisting><emphasis role="bold">&lt;profile namespace=</emphasis>"<emphasis>namespace</emphasis>" <emphasis
          role="bold">key=</emphasis>"<emphasis>key</emphasis>"&gt;<emphasis>value</emphasis><emphasis
          role="bold">&lt;/profile&gt;</emphasis></programlisting>

      <para>The XML schema requires that profiles are the first children of a
      pool element. If the element ordering is wrong, the XML parser will
      produce errors and warnings:</para>

      <programlisting>&lt;pool handle="isi_condor" gridlaunch="/home/shared/pegasus/bin/kickstart"&gt;
  <emphasis role="bold">&lt;profile namespace="env"
   key="GLOBUS_LOCATION"&gt;/home/shared/globus/&lt;/profile&gt;
  &lt;profile namespace="env"
   key="LD_LIBRARY_PATH" &gt;/home/shared/globus/lib&lt;/profile&gt;</emphasis>
  &lt;lrc url="rls://sukhna.isi.edu" /&gt;
  …
&lt;/pool&gt;
</programlisting>

      <para>The multi-line textual version of the site catalog uses the
      following syntax:</para>

      <programlisting><emphasis role="bold">profile</emphasis> <emphasis>namespace "key" "value"</emphasis></programlisting>

      <para>The order within the textual pool definition is not important.
      Profiles can appear anywhere:</para>

      <programlisting>pool isi_condor {
  gridlaunch "/home/shared/pegasus/bin/kickstart"
  <emphasis role="bold">profile env "GLOBUS_LOCATION" "/home/shared/globus"
  profile env "LD_LIBRARY_PATH" "/home/shared/globus/lib"</emphasis>
  …
}
</programlisting>
    </section>

    <section>
      <title>Profiles in Transformation Catalog</title>

      <para>Some profiles require a narrower scope than the site catalog
      offers. Some profiles only apply to certain applications on certain
      sites, or change with each application and site. Transformation-specific
      and CPU-specific environment variables, or job clustering profiles are
      good candidates. Such profiles are best specified in the transformation
      catalog. </para>

      <para>Profiles associate with a physical transformation and site in the
      transformation catalog. The Database version of the transformation
      catalog also permits the convenience of connecting a transformation with
      a profile.</para>

      <para>The Pegasus tc-client tool is a convenient helper to associate
      profiles with transformation catalog entries. As benefit, the user does
      not have to worry about formats of profiles in the various
      transformation catalog instances. </para>

      <programlisting>tc-client -a -P -E -p /home/shared/executables/analyze -t INSTALLED -r isi_condor -e env::GLOBUS_LOCATION=”/home/shared/globus”</programlisting>

      <para>The above example adds an environment variable GLOBUS_LOCATION to
      the application /home/shared/executables/analyze on site isi_condor. The
      transformation catalog guide has more details on the usage of the
      tc-client. </para>
    </section>

    <section>
      <title>User Profiles in Properties</title>

      <para>They have yet to be implemented, but are expected very soon.
      Please note that property-specified profiles fall into three categories
      of scope</para>

      <itemizedlist>
        <listitem>
          <para>general applicability to every job,</para>
        </listitem>

        <listitem>
          <para>applicability only to jobs at one site, and</para>
        </listitem>

        <listitem>
          <para>applicability only to certain transformations at one
          site.</para>
        </listitem>
      </itemizedlist>

      <para>This chapter will be expanded as the feature becomes
      available.</para>
    </section>
  </section>

  <section>
    <title>Profiles Conflict Resolution</title>

    <para>Irrespective of where the profiles are specified, eventually the
    profiles are associated with jobs. Multiple sources may specify the same
    profile for the same job. For instance, DAX may specify an environment
    variable X. The site catalog may also specify an environment variable X
    for the chosen site. The transformation catalog may specify an environment
    variable X for the chosen site and application. When the job is
    concretized, these three conflicts need to be resolved. </para>

    <para>Pegasus defines a priority ordering of profiles. The higher priority
    takes precedence (overwrites) a profile of a lower priority. </para>

    <orderedlist>
      <listitem>
        <para>User Local Profiles</para>
      </listitem>

      <listitem>
        <para>Transformation Catalog Profiles</para>
      </listitem>

      <listitem>
        <para>Site Catalog Profiles</para>
      </listitem>

      <listitem>
        <para>DAX Profiles</para>
      </listitem>
    </orderedlist>

    <para>The support for level 1 does not exist, but is forthcoming.
    Currently, transformation catalog profiles have the highest
    priority.</para>
  </section>

  <section>
    <title>Details of Profile Handling</title>

    <para>The previous sections omitted some of the finer details for the sake
    of clarity. To understand some of the constraints that Pegasus imposes, it
    is required to look at the way profiles affect jobs.</para>

    <section>
      <title>Details of env Profiles</title>

      <para>Profiles in the env namespace are translated to a
      semicolon-separated list of key-value pairs. The list becomes the
      argument for the Condor environment command in the job's submit file.
      </para>

      <programlisting>######################################################################
# Pegasus WMS  SUBMIT FILE GENERATOR
# DAG : black-diamond, Index = 0, Count = 1
# SUBMIT FILE NAME : findrange_ID000002.sub
######################################################################
globusrsl = (jobtype=single)
<emphasis role="bold">environment=GLOBUS_LOCATION=/shared/globus;LD_LIBRARY_PATH=/shared/globus/lib;</emphasis>
executable = /shared/software/linux/pegasus/default/bin/kickstart
globusscheduler = columbus.isi.edu/jobmanager-condor
remote_initialdir = /shared/CONDOR/workdir/isi_hourglass
universe = globus
…
queue
######################################################################
# END OF SUBMIT FILE
</programlisting>

      <para>Condor-G, in turn, will translate the
      <emphasis>environment</emphasis> command for any remote job into Globus
      RSL environment settings, and append them to any existing RSL syntax it
      generates. To permit proper mixing, all <emphasis>environment</emphasis>
      setting should solely use the env profiles, and none of the Condor nor
      Globus environment settings. </para>

      <para>If <emphasis>kickstart</emphasis> starts a job, it may make use of
      environment variables in its executable and arguments setting. </para>
    </section>

    <section>
      <title>Details of globus Profiles</title>

      <para>Profiles in the <emphasis>globus</emphasis> namespaces are
      translated into a list of paranthesis-enclosed equal-separated key-value
      pairs. The list becomes the value for the Condor
      <emphasis>globusrsl</emphasis> setting in the job's submit file: </para>

      <programlisting>######################################################################
# Pegasus WMS SUBMIT FILE GENERATOR
# DAG : black-diamond, Index = 0, Count = 1
# SUBMIT FILE NAME : findrange_ID000002.sub
######################################################################
<emphasis role="bold">globusrsl = (jobtype=single)(queue=fast)(project=nvo)</emphasis>
executable = /shared/software/linux/pegasus/default/bin/kickstart
globusscheduler = columbus.isi.edu/jobmanager-condor
remote_initialdir = /shared/CONDOR/workdir/isi_hourglass
universe = globus
…
queue
######################################################################
# END OF SUBMIT FILE
</programlisting>

      <para>For this reason, Pegasus prohibits the use of the
      <emphasis>globusrsl</emphasis> key in the <emphasis>condor</emphasis>
      profile namespace. </para>
    </section>
</section>

   </section>
   <section>
       <title>Replica Selection</title>



  <section>
    <title>Introduction</title>

    <para>Each job in the DAX maybe associated with input LFN’s denoting the
    files that are required for the job to run. To determine the physical
    replica (PFN) for a LFN, Pegasus queries the Replica catalog to get all
    the PFN’s (replicas) associated with a LFN. The Replica Catalog may return
    multiple PFN's for each of the LFN's queried. Hence, Pegasus needs to
    select a single PFN amongst the various PFN's returned for each LFN. This
    process is known as replica selection in Pegasus. Users can specify the
    replica selector to use in the properties file.</para>

    <para>This document describes the various Replica Selection Strategies in
    Pegasus.</para>
  </section>

  <section>
    <title>Configuration</title>

    <para>The user properties determine what replica selector Pegasus Workflow
    Mapper uses. The property <emphasis
    role="bold">pegasus.selector.replica</emphasis> is used to specify the
    replica selection strategy. Currently supported Replica Selection
    strategies are </para>

    <orderedlist>
      <listitem>
        <para>Default</para>
      </listitem>

      <listitem>
        <para>Restricted</para>
      </listitem>

      <listitem>
        <para>Regex</para>
      </listitem>
    </orderedlist>

    <para>The values are case sensitive. For example the following property
    setting will throw a Factory Exception . </para>

    <programlisting>pegasus.selector.replica  default</programlisting>

    <para>The correct way to specify is </para>

    <programlisting>pegasus.selector.replica  Default</programlisting>
  </section>

  <section>
    <title>Supported Replica Selectors </title>

    <para>The various Replica Selectors supported in Pegasus Workflow Mapper
    are explained below</para>

    <section>
      <title>Default</title>

      <para>This is the default replica selector used in the Pegasus Workflow
      Mapper. If the property pegasus.selector.replica is not defined in
      properties, then Pegasus uses this selector.</para>

      <para>This selector looks at each PFN returned for a LFN and checks to
      see if </para>

      <orderedlist>
        <listitem>
          <para>the PFN is a file URL (starting with file:///) </para>
        </listitem>

        <listitem>
          <para>the PFN has a pool attribute matching to the site handle of
          the site where the compute job that requires the input file is to be
          run.</para>
        </listitem>
      </orderedlist>

      <para>If a PFN matching the conditions above exists then that is
      returned by the selector . </para>

      <para><emphasis role="bold">Else,</emphasis> a random PFN is selected
      amongst all the PFN’s that have a pool attribute matching to the site
      handle of the site where a compute job is to be run.</para>

      <para><emphasis role="bold">Else,</emphasis> a random pfn is selected
      amongst all the PFN’s</para>

      <para>To use this replica selector set the following
      property<programlisting>pegasus.selector.replica                  Default</programlisting></para>
    </section>

    <section>
      <title>Restricted</title>

      <para>This replica selector, allows the user to specify good sites and
      bad sites for staging in data to a particular compute site. A good site
      for a compute site X, is a preferred site from which replicas should be
      staged to site X. If there are more than one good sites having a
      particular replica, then a random site is selected amongst these
      preferred sites.</para>

      <para>A bad site for a compute site X, is a site from which replica’s
      should not be staged. The reason of not accessing replica from a bad
      site can vary from the link being down, to the user not having
      permissions on that site’s data.</para>

      <para>The good | bad sites are specified by the following
      properties</para>

      <programlisting>pegasus.replica.*.prefer.stagein.sites
pegasus.replica.*.ignore.stagein.sites</programlisting>

      <para>where the * in the property name denotes the name of the compute
      site. A * in the property key is taken to mean all sites. The value to
      these properties is a comma separated list of sites. </para>

      <para>For example the following settings </para>

      <programlisting>pegasus.selector.replica.*.prefer.stagein.sites            usc
pegasus.replica.uwm.prefer.stagein.sites                   isi,cit 
</programlisting>

      <para>means that prefer all replicas from site usc for staging in to any
      compute site. However, for uwm use a tighter constraint and prefer only
      replicas from site isi or cit. The pool attribute associated with the
      PFN's tells the replica selector to what site a replica/PFN is
      associated with.</para>

      <para>The pegasus.replica.*.prefer.stagein.sites property takes
      precedence over pegasus.replica.*.ignore.stagein.sites property i.e. if
      for a site X, a site Y is specified both in the ignored and the
      preferred set, then site Y is taken to mean as only a preferred site for
      a site X.</para>

      <para>To use this replica selector set the following property</para>

      <programlisting>pegasus.selector.replica                  Restricted</programlisting>
    </section>

    <section>
      <title>Regex</title>

      <para>This replica selector allows the user allows the user to specific
      regex expressions that can be used to rank various PFN’s returned from
      the Replica Catalog for a particular LFN. This replica selector selects
      the highest ranked PFN i.e the replica with the lowest rank
      value.</para>

      <para>The regular expressions are assigned different rank, that
      determine the order in which the expressions are employed. The rank
      values for the regex can expressed in user properties using the
      property.</para>

      <programlisting>pegasus.selector.replica.regex.rank.<emphasis
          role="bold">[value]</emphasis>                  regex-expression</programlisting>

      <para>The <emphasis role="bold">[value]</emphasis> in the above property
      is an integer value that denotes the rank of an expression with a rank
      value of 1 being the highest rank.</para>

      <para>For example, a user can specify the following regex expressions
      that will ask Pegasus to prefer file URL's over gsiftp url's from
      example.isi.edu</para>

      <programlisting>pegasus.selector.replica.regex.rank.1                       file://.*
pegasus.selector.replica.regex.rank.2                       gsiftp://example\.isi\.edu.*</programlisting>

      <para>User can specify as many regex expressions as they want. </para>

      <para>Since Pegasus is in Java , the regex expression support is what
      Java supports. It is pretty close to what is supported by Perl. More
      details can be found at
      http://java.sun.com/j2se/1.5.0/docs/api/java/util/regex/Pattern.html</para>

      <para>Before applying any regular expressions on the PFN’s for a
      particular LFN that has to be staged to a site X, the file URL’s that
      don't match the site X are explicitly filtered out.</para>
    </section>
  </section>


   </section>
   <section>
       <title>Job Clustering</title>
 <section>
    <title>Motivation</title>

    <para>A large number of workflows executed through the Pegasus Workflow
    Management System, are composed of several jobs that run for only a few
    seconds or so. The overhead of running any job on the grid is usually 60
    seconds or more. Hence, it makes sense to cluster small independent jobs
    into a larger job. This is done while mapping an abstract workflow to a
    concrete workflow. Site specific or transformation specific criteria are
    taken into consideration while clustering smaller jobs into a larger job
    in the concrete workflow. The user is allowed to control the granularity
    of this clustering on a per transformation per site basis.</para>
  </section>

  <section>
    <title>Overview</title>

    <para>The abstract workflow is mapped onto the various sites by the Site
    Selector. This semi executable workflow is then passed to the clustering
    module. The clustering of the workflow can be either be</para>

    <itemizedlist>
      <listitem>
        <para>level based (horizontal clustering )</para>
      </listitem>

      <listitem>
        <para>label based (label clustering)</para>
      </listitem>
    </itemizedlist>

    <para>The clustering module clusters the jobs into larger/clustered jobs,
    that can then be executed on the remote sites. The execution can either be
    sequential on a single node or on multiple nodes using MPI. To specify
    which clustering technique to use the user has to pass the <emphasis
    role="bold">--cluster</emphasis> option to <emphasis
    role="bold">pegasus-plan</emphasis> .</para>
  </section>
  <section>
    <title>Generating Clustered Concrete DAG</title>

    <para>The clustering of a workflow is activated by passing the <emphasis
    role="bold">--cluster|-C</emphasis> option to <emphasis
    role="bold">pegasus-plan</emphasis>. The clustering granularity of a
    particular logical transformation on a particular site is dependant upon
    the clustering techniques being used. The executable that is used for
    running the clustered job on a particular site is determined as explained
    in section 7.<programlisting>#Running pegasus-plan to generate clustered workflows

$ <emphasis>pegasus-plan –-dax example.dax --dir ./dags –p siteX –-output local 
               --cluster [ comma separated list of clustering techniques]  –verbose
</emphasis>
Valid clustering techniques are horizontal and label.</programlisting></para>

    <para>The naming convention of submit files of the clustered jobs
    is<emphasis role="bold"> merge_NAME_IDX.sub</emphasis> . The NAME is
    derived from the logical transformation name. The IDX is an integer number
    between 1 and the total number of jobs in a cluster. Each of the submit
    files has a corresponding input file, following the naming convention
    <emphasis role="bold">merge_NAME_IDX.in </emphasis>. The input file
    contains the respective execution targets and the arguments for each of
    the jobs that make up the clustered job.</para>
  </section>

  <section>
    <title>Horizontal Clustering</title>

    <para>In case of horizontal clustering, each job in the workflow is
    associated with a level. The levels of the workflow are determined by
    doing a modified Breadth First Traversal of the workflow starting from the
    root nodes. The level associated with a node, is the furthest distance of
    it from the root node instead of it being the shortest distance as in
    normal BFS. For each level the jobs are grouped by the site on which they
    have been scheduled by the Site Selector. Only jobs of same type
    (txnamespace, txname, txversion) can be clustered into a larger job. To
    use horizontal clustering the user needs to set the <emphasis
    role="bold">--cluster</emphasis> option of <emphasis
    role="bold">pegasus-plan to horizontal</emphasis> .</para>

    <section>
      <title>Controlling Clustering Granularity</title>

      <para>The number of jobs that have to be clustered into a single large
      job, is determined by the value of two parameters associated with the
      smaller jobs. Both these parameters are specified by the use of a
      PEGASUS namespace profile keys. The keys can be specified at any of the
      placeholders for the profiles (abstract transformation in the DAX, site
      in the site catalog, transformation in the transformation catalog). The
      normal overloading semantics apply i.e. profile in transformation
      catalog overrides the one in the site catalog and that in turn overrides
      the one in the DAX. The two parameters are described below.</para>

      <itemizedlist>
        <listitem>
          <para><emphasis role="bold">collapse factor</emphasis></para>

          <para>The collapse factor denotes how many jobs need to be merged
          into a single clustered job. It is specified via the use of a
          PEGASUS namespace profile key “collapse”. for e.g. if at a
          particular level, say 4 jobs referring to logical transformation B
          have been scheduled to a siteX. The collapse factor associated with
          job B for siteX is say 3. This will result in 2 clustered jobs, one
          composed of 3 jobs and another of 2 jobs. The collapse factor can be
          specified in the transformation catalog as follows</para>

          <programlisting><emphasis role="bold">#site   transformation   pfn            type               architecture  profiles
</emphasis>
siteX    B     /shared/PEGASUS/bin/jobB INSTALLED       INTEL32::LINUX  PEGASUS::collapse=3
siteX    C     /shared/PEGASUS/bin/jobC INSTALLED       INTEL32::LINUX  PEGASUS::collapse=2
</programlisting>

          <figure>
            <title></title>

            <mediaobject>
              <imageobject>
                  <imagedata fileref="images/advanced-clustering-1.png" />
              </imageobject>
            </mediaobject>
          </figure>
        </listitem>

        <listitem>
          <para><emphasis role="bold">bundle factor</emphasis></para>

          <para>The bundle factor denotes how many clustered jobs does the
          user want to see per level per site. It is specified via the use of
          a PEGASUS namespace profile key “bundle”. for e.g. if at a
          particular level, say 4 jobs referring to logical transformation B
          have been scheduled to a siteX. The “bundle” factor associated with
          job B for siteX is say 3. This will result in 3 clustered jobs, one
          composed of 2 jobs and others of a single job each. The bundle
          factor in the transformation catalog can be specified as
          follows</para>

          <programlisting><emphasis role="bold">#site  transformation      pfn           type            architecture    profiles
</emphasis>
siteX    B     /shared/PEGASUS/bin/jobB INSTALLED       INTEL32::LINUX  PEGASUS::bundle=3
siteX    C     /shared/PEGASUS/bin/jobC INSTALLED       INTEL32::LINUX  PEGASUS::bundle=2
</programlisting>

          <para>In the case, where both the factors are associated with the
          job, the bundle value supersedes the collapse value.</para>

          <programlisting><emphasis role="bold">#site  transformation   pfn             type             architecture   profiles
</emphasis>
siteX    B     /shared/PEGASUS/bin/jobB INSTALLED       INTEL32::LINUX PEGASUS::collapse=3,bundle=3
</programlisting>

          <para>In the above case the jobs referring to logical transformation
          B scheduled on siteX will be clustered on the basis of “bundle”
          value. Hence, if there are 4 jobs referring to logical
          transformation B scheduled to siteX, then 3 clustered jobs will be
          created.</para>

          <figure>
            <title></title>

            <mediaobject>
              <imageobject>
                  <imagedata fileref="images/advanced-clustering-2.png" />
              </imageobject>
            </mediaobject>
          </figure>
        </listitem>
      </itemizedlist>
    </section>
  </section>
  <section>
    <title>Label Clustering</title>

    <para>In label based clustering, the user labels the workflow. All jobs
    having the same label value are clustered into a single clustered job.
    This allows the user to create clusters or use a clustering technique that
    is specific to his workflows. If there is no label associated with the
    job, the job is not clustered and is executed as is<figure>
        <title></title>

        <mediaobject>
          <imageobject>
              <imagedata fileref="images/advanced-clustering-3.png" />
          </imageobject>
        </mediaobject>
      </figure></para>

    <para>Since, the jobs in a cluster in this case are not independent, it is
    important the jobs are executed in the correct order. This is done by
    doing a topological sort on the jobs in each cluster. To use label based
    clustering the user needs to set the <emphasis
    role="bold">--cluster</emphasis> option of <emphasis
    role="bold">pegasus-plan</emphasis> to label.</para>

    <section>
      <title>Labelling the Workflow</title>

      <para>The labels for the jobs in the workflow are specified by
      associated <emphasis role="bold">pegasus</emphasis> profile keys with
      the jobs during the DAX generation process. The user can choose which
      profile key to use for labeling the workflow. By default, it is assumed
      that the user is using the PEGASUS profile key label to associate the
      labels. To use another key, in the <emphasis
      role="bold">pegasus</emphasis> namespace the user needs to set the
      following property</para>

      <itemizedlist>
        <listitem>
          <para>pegasus.clusterer.label.key</para>
        </listitem>
      </itemizedlist>

      <para>For example if the user sets <emphasis
      role="bold">pegasus.clusterer.label.key </emphasis>to <emphasis
      role="bold">user_label</emphasis> then the job description in the DAX
      looks as follows</para>

      <programlisting>&lt;adag &gt;
…
  &lt;job id="ID000004" namespace="app" name="analyze" version="1.0" level="1" &gt;
    &lt;argument&gt;-a bottom -T60  -i &lt;filename file="user.f.c1"/&gt;  -o &lt;filename file="user.f.d"/&gt;&lt;/argument&gt;
    &lt;profile namespace=“pegasus” key=“user_label”&gt;p1&lt;/profile&gt;
    &lt;uses file="user.f.c1" link="input" dontRegister="false" dontTransfer="false"/&gt;
    &lt;uses file="user.f.c2" link="input" dontRegister="false" dontTransfer="false"/&gt;
    &lt;uses file="user.f.d" link="output" dontRegister="false" dontTransfer="false"/&gt;
  &lt;/job&gt;
…
&lt;/adag&gt;</programlisting>

      <itemizedlist>
        <listitem>
          <para>The above states that the <emphasis
          role="bold">pegasus</emphasis> profiles with key as <emphasis
          role="bold">user_label</emphasis> are to be used for designating
          clusters.</para>
        </listitem>

        <listitem>
          <para>Each job with the same value for <emphasis
          role="bold">pegasus</emphasis> profile key <emphasis
          role="bold">user_label </emphasis>appears in the same
          cluster.</para>
        </listitem>
      </itemizedlist>
    </section>
  </section>

  <section>
    <title>Recursive Clustering</title>

    <para>In some cases, a user may want to use a combination of clustering
    techniques. For e.g. a user may want some jobs in the workflow to be
    horizontally clustered and some to be label clustered. This can be
    achieved by specifying a comma separated list of clustering techniques to
    the<emphasis role="bold"> –-cluster</emphasis> option of <emphasis
    role="bold">pegasus-plan</emphasis>. In this case the clustering
    techniques are applied one after the other on the workflow in the order
    specified on the command line.</para>

    <para>For example</para>

    <programlisting>$ <emphasis>pegasus-plan –-dax example.dax --dir ./dags --cluster label,horizontal –s siteX –-output local --verbose</emphasis></programlisting>

    <figure>
      <title></title>

      <mediaobject>
        <imageobject>
            <imagedata fileref="images/advanced-clustering-4.png" />
        </imageobject>
      </mediaobject>
    </figure>
  </section>

  <section>
    <title>Execution of the Clustered Job</title>

    <para>The execution of the clustered job on the remote site, involves the
    execution of the smaller constituent jobs either</para>

    <itemizedlist>
      <listitem>
        <para><emphasis role="bold">sequentially on a single node of the
        remote site</emphasis></para>

        <para>The clustered job is executed using <emphasis
        role="bold">seqexec</emphasis>, a wrapper tool written in C that is
        distributed as part of the PEGASUS. It takes in the jobs passed to it,
        and ends up executing them sequentially on a single node. To use
        “<emphasis role="bold">seqexec</emphasis>” for executing any clustered
        job on a siteX, there needs to be an entry in the transformation
        catalog for an executable with the logical name seqexec and namespace
        as pegasus.</para>

        <programlisting><emphasis role="bold">#site  transformation   pfn            type                 architecture    profiles</emphasis>

siteX    pegasus::seqexec     /shared/PEGASUS/bin/seqexec INSTALLED       INTEL32::LINUX NULL</programlisting>

        <para>By default, the entry for seqexec on a site is automatically
        picked up if $PEGASUS_HOME or $VDS_HOME is specified in the site
        catalog for that site.</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">On multiple nodes of the remote site using
        MPI</emphasis></para>

        <para>The clustered job is executed using mpiexec, a wrapper mpi
        program written in C that is distributed as part of the PEGASUS. It is
        only distributed as source not as binary. The wrapper ends up being
        run on every mpi node, with the first one being the master and the
        rest of the ones as workers. The number of instances of mpiexec that
        are invoked is equal to the value of the globus rsl key nodecount. The
        master distributes the smaller constituent jobs to the workers.</para>

        <para>For e.g. If there were 10 jobs in the merged job and nodecount
        was 5, then one node acts as master, and the 10 jobs are distributed
        amongst the 4 slaves on demand. The master hands off a job to the
        slave node as and when it gets free. So initially all the 4 nodes are
        given a single job each, and then as and when they get done are handed
        more jobs till all the 10 jobs have been executed.</para>

        <para>To use “mpiexec” for executing the clustered job on a siteX,
        there needs to be an entry in the transformation catalog for an
        executable with the logical name mpiexec and namespace as
        pegasus.</para>

        <programlisting><emphasis role="bold">#site  transformation   pfn            type                 architecture    profiles</emphasis>

siteX    pegasus::seqexec     /shared/PEGASUS/bin/mpiexec INSTALLED       INTEL32::LINUX NULL</programlisting>

        <para>Another added advantage of using mpiexec, is that regular non
        mpi code can be run via MPI.</para>

        <para>Both the clustered job and the smaller constituent jobs are
        invoked via kickstart, unless the clustered job is being run via mpi
        (mpiexec). Kickstart is unable to launch mpi jobs. If kickstart is not
        installed on a particular site i.e. the gridlaunch attribute for site
        is not specified in the site catalog, the jobs are invoked
        directly.</para>
      </listitem>
    </itemizedlist>

    <section>
      <title>Specification of Method of Execution for Clustered Jobs</title>

      <para>The method execution of the clustered job(whether to launch via
      mpiexec or seqexec) can be specified</para>

      <orderedlist>
        <listitem>
          <para><emphasis role="bold">globally in the properties
          file</emphasis></para>

          <para>The user can set a property in the properties file that
          results in all the clustered jobs of the workflow being executed by
          the same type of executable.</para>

          <programlisting><emphasis role="bold">#PEGASUS PROPERTIES FILE</emphasis>
pegasus.clusterer.job.aggregator seqexec|mpiexec</programlisting>

          <para>In the above example, all the clustered jobs on the remote
          sites are going to be launched via the property value, as long as
          the property value is not overridden in the site catalog.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">associating profile key “collapser” with
          the site in the site catalog</emphasis></para>

          <programlisting>&lt;site handle="siteX" gridlaunch = "/shared/PEGASUS/bin/kickstart"&gt;
    &lt;profile namespace="env" key="GLOBUS_LOCATION" &gt;/home/shared/globus&lt;/profile&gt;
    &lt;profile namespace="env" key="LD_LIBRARY_PATH"&gt;/home/shared/globus/lib&lt;/profile&gt;
    &lt;profile namespace="pegasus" key="collapser" &gt;seqexec&lt;/profile&gt;
    &lt;lrc url="rls://siteX.edu" /&gt;
    &lt;gridftp  url="gsiftp://siteX.edu/" storage="/home/shared/work" major="2" minor="4" patch="0" /&gt;
    &lt;jobmanager universe="transfer" url="siteX.edu/jobmanager-fork" major="2" minor="4" patch="0" /&gt;
    &lt;jobmanager universe="vanilla" url="siteX.edu/jobmanager-condor" major="2" minor="4" patch="0" /&gt;
    &lt;workdirectory &gt;/home/shared/storage&lt;/workdirectory&gt;
  &lt;/site&gt;</programlisting>

          <para>In the above example, all the clustered jobs on a siteX are
          going to be executed via seqexec, as long as the value is not
          overridden in the transformation catalog.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">associating profile key “collapser” with
          the transformation that is being clustered, in the transformation
          catalog</emphasis></para>

          <programlisting><emphasis role="bold">#site  transformation   pfn            type                architecture profiles
</emphasis>
siteX    B     /shared/PEGASUS/bin/jobB INSTALLED       INTEL32::LINUX pegasus::collapse=3,collapser=mpiexec</programlisting>

          <para>In the above example, all the clustered jobs that consist of
          transformation B on siteX will be executed via mpiexec.</para>

          <para><emphasis role="bold">Note: The clustering of jobs on a site
          only happens only if </emphasis></para>

          <itemizedlist>
            <listitem>
              <para>there exists an entry in the transformation catalog for
              the clustering executable that has been determined by the above
              3 rules</para>
            </listitem>

            <listitem>
              <para>the number of jobs being clustered on the site are more
              than 1</para>
            </listitem>
          </itemizedlist>
        </listitem>
      </orderedlist>
    </section>
  </section>
  <section>
    <title>Outstanding Issues</title>

    <orderedlist>
      <listitem>
        <para><emphasis role="bold">Label Clustering</emphasis></para>

        <para>More rigourous checks are required to ensure that the labeling
        scheme applied by the user is valid.</para>
      </listitem>
    </orderedlist>

  </section>


   </section>
   <section>
       <title>Transfer Refiners</title>
       <para>...</para>
   </section>
   <section>
       <title>Hierarchical Workflows /Workflows of Workflows</title>
       <para>...</para>
   </section>
</chapter>

