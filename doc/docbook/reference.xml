<?xml version="1.0" encoding="US-ASCII"?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN"
"http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<chapter id="reference">
  <title>Reference Manual</title>

  <section id="Properties">
    <title>Properties</title>

    <para />

    <para>Users set default property values of system components to establish
    system configuration. <note> Values rely on proper capitalization, unless
    explicitly noted otherwise. </note></para>

    <para>Some properties rely on the value of other properties for their
    default value. Curly braces denote the value of the named property. For
    instance, ${pegasus.home} means that the value depends on the value of the
    pegasus.home property plus any noted additions. You can use this notation
    to refer to other properties, though the extent of the subsitutions are
    limited. Usually, you want to refer to a set of the standard system
    properties.</para>

    <para>When setting properties, be aware that nesting is not allowed, and
    substitutions shall only be done once.</para>

    <para>There is a priority to the order of reading and evaluating
    properties. Usually there is no need to worry about the priorities,
    however, it is good to know the details of when which property applies,
    and how one property is able to overwrite another.</para>

    <para>
      <orderedlist>
        <listitem>Property definitions in the system property file, usually
        found in <emphasis
        role="bold">${pegasus.home.sysconfdir}/properties</emphasis>, have the
        lowest priority. These properties are expected to be set up by the
        submit host's administrator.</listitem>

        <listitem>The properties defined in the user property file <emphasis
        role="bold">${user.home}/.pegasusrc</emphasis> have higher priority
        than system property definitions and overwrite system's properties
        settings. An example set of sensible property values to set on a
        production system is shown below.</listitem>

        <listitem>Commandline properties have the highest priority. Each
        commandline property is introduced by a -D argument.</listitem>
      </orderedlist>
    </para>

    <note>These arguments are parsed by the shell wrapper, and thus the -D
    arguments must be the first arguments to any command. Commandline
    properties are useful for debugging purposes.</note>

    <para>The following example provides a sensible set of properties to be
    set by the user property file. These properties use mostly non-default
    settings. <emphasis role="bold">It is an example only, and will not work
    for you</emphasis> :</para>

    <para>
      <screen>
pegasus.catalog.replica               File
pegasus.catalog.replica.file          ${pegasus.home}/etc/sample.rc.data
pegasus.catalog.transformation        Text
pegasus.catalog.transformation.file   ${pegasus.home}/etc/sample.tc.text
pegasus.catalog.site                  XML3
pegasus.catalog.site.file             ${pegasus.home}/etc/sample.sites.xml3
</screen>
    </para>

    <para>Pegasus dumps all properties into a file with the suffix properties
    in the Submit Directory during the planning process.</para>

    <section id="Propertiespegasus.home">
      <title>Pegasus.home</title>

      <para>
        <informaltable frame="none">
          <tgroup align="left" cols="2" colsep="1" rowsep="1">
            <tbody>
              <row>
                <entry>Systems:</entry>

                <entry>all</entry>
              </row>

              <row>
                <entry>Type:</entry>

                <entry>directory location string</entry>
              </row>

              <row>
                <entry>Default:</entry>

                <entry>"$PEGASUS_HOME"</entry>
              </row>
            </tbody>
          </tgroup>
        </informaltable>
      </para>

      <para />

      <para>The property pegasus.home cannot be set in the property file. This
      property is automatically set up by the pegasus clients internally by
      determining the installation directory of pegasus. Knowledge about this
      property is important for developers who want to invoke PEGASUS JAVA
      classes without the shell wrappers.</para>
    </section>

    <!-- end section -->

    <para>Use the following sections to guide you in setting component
    parameters for <emphasis role="bold">Property Files and
    Locations</emphasis>, and <emphasis>Catalog Properties</emphasis></para>

    <section id="PropertiesPropertyFilesAndLocations">
      <title>Property Files And Locations</title>

      <para />

      <para>This section describes the property file locations.</para>

      <section id="Propertiespegasus.properties">
        <title>pegasus.properties</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>Systems:</entry>

                  <entry>all</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>file location string</entry>
                </row>

                <row>
                  <entry>Default:</entry>

                  <entry>${pegasus.home.sysconfdir}/properties</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para />

        <para>The system-wide properties file is in its default location,
        normally in $PEGASUS_HOME/etc as file named
        <emphasis>properties</emphasis>.</para>
      </section>

      <!-- end section -->

      <section id="Propertiespegasus.user.properties">
        <title>pegasus.user.properties</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>Systems:</entry>

                  <entry>all</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>file location string</entry>
                </row>

                <row>
                  <entry>Default:</entry>

                  <entry>${user.home}/.pegasusrc</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para />

        <para>Each user can overwrite the system-wide properties with their
        own definitions. The user properties rely on the system's notion of
        the user home directory, as reflected in the JRE system properties. In
        the user's home directory, a file <emphasis>.pegasusrc</emphasis>
        contains user property definitions.</para>

        <note>${user.home} is a system property provided by the Java run-time
        environment (JRE).</note>

        <para>Older versions of PEGASUS supported a dot-chimerarc file. Both
        files are currently supported. However, in the presence of both files,
        precedence is granted to the dot-pegasusrc file.</para>
      </section>

      <!-- end section -->
    </section>

    <!-- end section -->

    <section id="PropertiesLocalDirectories">
      <title>Local Directories</title>

      <para />

      <para>This section describes the GNU directory structure conventions.
      GNU distinguishes between architecture independent and thus sharable
      directories, and directories with data specific to a platform, and thus
      often local. It also distinguishes between frequently modified data and
      rarely changing data. These two axis form a space of four distinct
      directories.</para>

      <section id="Propertiespegasus.home.datadir">
        <title>pegasus.home.datadir</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>Systems:</entry>

                  <entry>all</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>directory location string</entry>
                </row>

                <row>
                  <entry>Default:</entry>

                  <entry>${pegasus.home}/share</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para />

        <para>The datadir directory contains broadly visible and possibly
        exported configuration files that rarely change. This directory is
        currently unused.</para>
      </section>

      <!-- end section -->

      <section id="Propertiespegasus.home.sysconfdir">
        <title>pegasus.home.sysconfdir</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>Systems:</entry>

                  <entry>all</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>directory location string</entry>
                </row>

                <row>
                  <entry>Default:</entry>

                  <entry>${pegasus.home}/etc</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para />

        <para>The system configuration directory contains configuration files
        that are specific to the machine or installation, and that rarely
        change. This is the directory where the XML schema definition copies
        are stored, and where the base pool configuration file is
        stored.</para>
      </section>

      <!-- end section -->

      <section id="Propertiespegasus.home.sharedstatedir">
        <title>pegasus.home.sharedstatedir</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>Systems:</entry>

                  <entry>all</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>directory location string</entry>
                </row>

                <row>
                  <entry>Default:</entry>

                  <entry>${pegasus.home}/com</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para />

        <para>Frequently changing files that are broadly visible are stored in
        the shared state directory. This is currently unused.</para>
      </section>

      <!-- end section -->

      <section id="Propertiespegasus.home.localstatedir">
        <title>pegasus.home.localstatedir</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>Systems:</entry>

                  <entry>all</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>directory location string</entry>
                </row>

                <row>
                  <entry>Default:</entry>

                  <entry>${pegasus.home}/var</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para />

        <para>Frequently changing files that are specific to a machine and/or
        installation are stored in the local state directory. This directory
        is being used for the textual transformation catalog, and the
        file-based replica catalog.</para>
      </section>

      <!-- end section -->

      <section id="Propertiespegasus.dir.submit.logs">
        <title>pegasus.dir.submit.logs</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>System:</entry>

                  <entry>Pegasus</entry>
                </row>

                <row>
                  <entry>Since:</entry>

                  <entry>2.4</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>directory location string</entry>
                </row>

                <row>
                  <entry>Default:</entry>

                  <entry>false</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para />

        <para>By default, Pegasus points the condor logs for the workflow to
        /tmp directory. This is done to ensure that the logs are created in a
        local directory even though the submit directory may be on NFS. In the
        submit directory the symbolic link to the appropriate log file in the
        /tmp exists.</para>

        <note>
          <para>Since <emphasis role="bold">/tmp</emphasis> is automatically
          purged in most cases, users may want to preserve their condor logs
          in a directory on the local filesystem other than /tmp</para>
        </note>

        <para />
      </section>

      <!-- end section -->
    </section>

    <!-- end section -->

    <section id="PropertiesSiteDirectories">
      <title>Site Directories</title>

      <para>The site directory properties modify the behavior of remotely run
      jobs. In rare occasions, it may also pertain to locally run compute
      jobs.</para>

      <section id="Propertiespegasus.dir.useTimestamp">
        <title>pegasus.dir.useTimestamp</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>System:</entry>

                  <entry>Pegasus</entry>
                </row>

                <row>
                  <entry>Since:</entry>

                  <entry>2.1</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>Boolean</entry>
                </row>

                <row>
                  <entry>Default:</entry>

                  <entry>false</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para />

        <para>While creating the submit directory, Pegasus employs a run
        numbering scheme. Users can use this property to use a timestamp based
        numbering scheme instead of the runxxxx scheme.</para>
      </section>

      <!-- end section -->

      <section id="Propertiespegasus.dir.exec">
        <title>pegasus.dir.exec</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>System:</entry>

                  <entry>Pegasus</entry>
                </row>

                <row>
                  <entry>Since:</entry>

                  <entry>2.0</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>remote directory location string</entry>
                </row>

                <row>
                  <entry>Default:</entry>

                  <entry>(no default)</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para />

        <para>This property modifies the remote location work directory in
        which all your jobs will run. If the path is relative then it is
        appended to the work directory (associated with the site), as
        specified in the site catalog. If the path is absolute then it
        overrides the work directory specified in the site catalog.</para>
      </section>

      <!-- end section -->

      <section id="Propertiespegasus.dir.storage">
        <title>pegasus.dir.storage</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>System:</entry>

                  <entry>Pegasus</entry>
                </row>

                <row>
                  <entry>Since:</entry>

                  <entry>2.0</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>remote directory location string</entry>
                </row>

                <row>
                  <entry>Default:</entry>

                  <entry>(no default)</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para />

        <para>This property modifies the remote storage location on various
        pools. If the path is relative then it is appended to the storage
        mount point specified in the pool.config file. If the path is absolute
        then it overrides the storage mount point specified in the pool config
        file.</para>
      </section>

      <!-- end section -->

      <section id="Propertiespegasus.dir.storage.deep">
        <title>pegasus.dir.storage.deep</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>System:</entry>

                  <entry>Pegasus</entry>
                </row>

                <row>
                  <entry>Since:</entry>

                  <entry>2.1</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>Boolean</entry>
                </row>

                <row>
                  <entry>Default:</entry>

                  <entry>false</entry>
                </row>

                <row>
                  <entry>See Also:</entry>

                  <entry>pegasus.dir.storage</entry>
                </row>

                <row>
                  <entry>See Also:</entry>

                  <entry>pegasus.dir.useTimestamp</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para />

        <para>This property results in the creation of a deep directory
        structure on the output site, while populating the results. The base
        directory on the remote end is determined from the site catalog and
        the property pegasus.dir.storage.</para>

        <para>To this base directory, the relative submit directory structure
        ( $user/$vogroup/$label/runxxxx ) is appended.</para>

        <para>$storage = $base + $relative_submit_directory</para>

        <para>Depending on the number of files being staged to the remote site
        a Hashed File Structure is created that ensures that only 256 files
        reside in one directory.</para>

        <para>To create this directory structure on the storage site, Pegasus
        relies on the directory creation feature of the Grid FTP server, which
        appeared in globus 4.0.x</para>
      </section>

      <!-- end section -->

      <section id="Propertiespegasus.dir.create.strategy">
        <title>pegasus.dir.create.strategy</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>System:</entry>

                  <entry>Pegasus</entry>
                </row>

                <row>
                  <entry>Since:</entry>

                  <entry>2.2</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>enumeration</entry>
                </row>

                <row>
                  <entry>Value[0]:</entry>

                  <entry>HourGlass</entry>
                </row>

                <row>
                  <entry>Value[1]:</entry>

                  <entry>Tentacles</entry>
                </row>

                <row>
                  <entry>Default:</entry>

                  <entry>Tentacles</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para />

        <para>If the <screen>--randomdir</screen> option is given to the
        Planner at runtime, the Pegasus planner adds nodes that create the
        random directories at the remote pool sites, before any jobs are
        actually run. The two modes determine the placement of these nodes and
        their dependencies to the rest of the graph.</para>

        <para>
          <variablelist>
            <varlistentry>
              <term>HourGlass</term>

              <listitem>It adds a make directory node at the top level of the
              graph, and all these concat to a single dummy job before
              branching out to the root nodes of the original/ concrete dag so
              far. So we introduce a classic X shape at the top of the graph.
              Hence the name HourGlass.</listitem>
            </varlistentry>

            <varlistentry>
              <term>Tentacles</term>

              <listitem>This option places the jobs creating directories at
              the top of the graph. However instead of constricting it to an
              hour glass shape, this mode links the top node to all the
              relevant nodes for which the create dir job is necessary. It
              looks as if the node spreads its tentacleas all around. This
              puts more load on the DAGMan because of the added dependencies
              but removes the restriction of the plan progressing only when
              all the create directory jobs have progressed on the remote
              pools, as is the case in the HourGlass model.</listitem>
            </varlistentry>
          </variablelist>
        </para>
      </section>

      <!-- end section -->

      <section id="Propertiespegasus.dir.create.impl">
        <title>pegasus.dir.create.impl</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>System:</entry>

                  <entry>Pegasus</entry>
                </row>

                <row>
                  <entry>Since:</entry>

                  <entry>2.2</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>enumeration</entry>
                </row>

                <row>
                  <entry>Value[0]:</entry>

                  <entry>DefaultImplementation</entry>
                </row>

                <row>
                  <entry>Value[1]:</entry>

                  <entry>S3</entry>
                </row>

                <row>
                  <entry>Default:</entry>

                  <entry>DefaultImpelmentation</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para />

        <para>This property is used to select the executable that is used to
        create the working directory on the compute sites.</para>

        <para>
          <variablelist>
            <varlistentry>
              <term>DefaultImplementation</term>

              <listitem>The default executable that is used to create a
              directory is the dirmanager executable shipped with Pegasus. It
              is found at $PEGASUS_HOME/bin/dirmanager in the pegasus
              distribution. An entry for transformation pegasus::dirmanager
              needs to exist in the Transformation Catalog or the PEGASUS_HOME
              environment variable should be specified in the site catalog for
              the sites for this mode to work.</listitem>
            </varlistentry>

            <varlistentry>
              <term>S3</term>

              <listitem>This option is used to create buckets in S3 instead of
              a directory. This should be set when running workflows on Amazon
              EC2. This implementation relies on s3cmd command line client to
              create the bucket. An entry for transformation amazon::s3cmd
              needs to exist in the Transformation Catalog for this to
              work.</listitem>
            </varlistentry>
          </variablelist>
        </para>

        <para />
      </section>

      <!-- end section -->
    </section>

    <!-- end section -->

    <section id="PropertiesSchemaFileLocationProperties">
      <title>Schema File Location Properties</title>

      <para>This section defines the location of XML schema files that are
      used to parse the various XML document instances in the PEGASUS. The
      schema backups in the installed file-system permit PEGASUS operations
      without being online.</para>

      <section id="Propertiespegasus.schema.dax">
        <title>pegasus.schema.dax</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>Systems:</entry>

                  <entry>Pegasus</entry>
                </row>

                <row>
                  <entry>Since:</entry>

                  <entry>2.0</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>XML schema file location string</entry>
                </row>

                <row>
                  <entry>Value[0]:</entry>

                  <entry>${pegasus.home.sysconfdir}/dax-3.2.xsd</entry>
                </row>

                <row>
                  <entry>Default:</entry>

                  <entry>${pegasus.home.sysconfdir}/dax-3.2.xsd</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para />

        <para>This file is a copy of the XML schema that describes abstract
        DAG files that are the result of the abstract planning process, and
        input into any concrete planning. Providing a copy of the schema
        enables the parser to use the local copy instead of reaching out to
        the internet, and obtaining the latest version from the GriPhyN
        website dynamically.</para>
      </section>

      <!-- end section -->

      <section id="Propertiespegasus.schema.sc">
        <title>pegasus.schema.sc</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>Systems:</entry>

                  <entry>Pegasus</entry>
                </row>

                <row>
                  <entry>Since:</entry>

                  <entry>2.0</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>XML schema file location string</entry>
                </row>

                <row>
                  <entry>Value[0]:</entry>

                  <entry>${pegasus.home.sysconfdir}/sc-3.0.xsd</entry>
                </row>

                <row>
                  <entry>Default:</entry>

                  <entry>${pegasus.home.sysconfdir}/sc-3.0.xsd</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para />

        <para>This file is a copy of the XML schema that describes the xml
        description of the site catalog, that is generated as a result of
        using genpoolconfig command. Providing a copy of the schema enables
        the parser to use the local copy instead of reaching out to the
        internet, and obtaining the latest version from the GriPhyN website
        dynamically.</para>
      </section>

      <!-- end section -->

      <section id="Propertiespegasus.schema.ivr">
        <title>pegasus.schema.ivr</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>Systems:</entry>

                  <entry>all</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>XML schema file location string</entry>
                </row>

                <row>
                  <entry>Value[0]:</entry>

                  <entry>${pegasus.home.sysconfdir}/iv-2.0.xsd</entry>
                </row>

                <row>
                  <entry>Default:</entry>

                  <entry>${pegasus.home.sysconfdir}/iv-2.0.xsd</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para />

        <para>This file is a copy of the XML schema that describes invocation
        record files that are the result of the a grid launch in a remote or
        local site. Providing a copy of the schema enables the parser to use
        the local copy instead of reaching out to the internet, and obtaining
        the latest version from the GriPhyN website dynamically.</para>
      </section>

      <!-- end section -->
    </section>

    <!-- end section -->

    <section id="PropertiesDatabaseDriversForAllRelationalCatalogs">
      <title>Database Drivers For All Relational Catalogs</title>

      <para />

      <para />

      <section id="Propertiespegasus.catalog.*.db.driver">
        <title>pegasus.catalog.*.db.driver</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>System:</entry>

                  <entry>Pegasus</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>Java class name</entry>
                </row>

                <row>
                  <entry>Value[0]:</entry>

                  <entry>Postgres</entry>
                </row>

                <row>
                  <entry>Value[1]:</entry>

                  <entry>MySQL</entry>
                </row>

                <row>
                  <entry>Value[2]:</entry>

                  <entry>SQLServer2000 (not yet implemented!)</entry>
                </row>

                <row>
                  <entry>Value[3]:</entry>

                  <entry>Oracle (not yet implemented!)</entry>
                </row>

                <row>
                  <entry>Default:</entry>

                  <entry>(no default)</entry>
                </row>

                <row>
                  <entry>See also:</entry>

                  <entry>pegasus.catalog.provenance</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para>The database driver class is dynamically loaded, as required by
        the schema. Currently, only PostGreSQL 7.3 and MySQL 4.0 are
        supported. Their respective JDBC3 driver is provided as part and
        parcel of the PEGASUS.</para>

        <para>A user may provide their own implementation, derived from
        org.griphyn.vdl.dbdriver.DatabaseDriver, to talk to a database of
        their choice.</para>

        <para>For each schema in ptc and tc, a driver is instantiated
        separately, which has the same prefix as the schema. This may result
        in multiple connections to the database backend. As fallback, the
        schema "*" driver is attempted.</para>

        <para>The * in the property name can be replaced by a catalog name to
        apply the property only for that catalog. Valid catalog names
        are</para>

        <para>
          <screen>
replica
transformation
provenance
work
</screen>
        </para>

        <para />
      </section>

      <!-- end section -->

      <section id="Propertiespegasus.catalog.*.db.url">
        <title>pegasus.catalog.*.db.url</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>System:</entry>

                  <entry>PTC, TC, ...</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>JDBC database URI string</entry>
                </row>

                <row>
                  <entry>Default:</entry>

                  <entry>(no default)</entry>
                </row>

                <row>
                  <entry>Example:</entry>

                  <entry>jdbc:postgresql:${user.name}</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para>Each database has its own string to contact the database on a
        given host, port, and database. Although most driver URLs allow to
        pass arbitrary arguments, please use the
        pegasus.catalog.[catalog-name].db.* keys or pegasus.catalog.*.db.* to
        preload these arguments. <warning>
            <emphasis role="bold">THE URL IS A MANDATORY PROPERTY FOR ANY DBMS
            BACKEND.</emphasis>
          </warning></para>

        <para>
          <screen>
Postgres : jdbc:postgresql:[//hostname[:port]/]database
MySQL    : jdbc:mysql://hostname[:port]]/database
SQLServer: jdbc:microsoft:sqlserver://hostname:port
Oracle   : jdbc:oracle:thin:[user/password]@//host[:port]/service
</screen>
        </para>

        <para>The * in the property name can be replaced by a catalog name to
        apply the property only for that catalog. Valid catalog names
        are</para>

        <para>
          <screen>
replica
transformation
provenance
work
</screen>
        </para>

        <para />
      </section>

      <!-- end section -->

      <section id="Propertiespegasus.catalog.*.db.user">
        <title>pegasus.catalog.*.db.user</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>System:</entry>

                  <entry>PTC, TC, ...</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>string</entry>
                </row>

                <row>
                  <entry>Default:</entry>

                  <entry>(no default)</entry>
                </row>

                <row>
                  <entry>Example:</entry>

                  <entry>${user.name}</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para>In order to access a database, you must provide the name of your
        account on the DBMS. This property is database-independent. <warning>
            <emphasis role="bold">THE ACCOUNT NAME IS A MANDATORY PROPERTY FOR
            MANY DBMS BACKENDS</emphasis>
          </warning>.</para>

        <para>The * in the property name can be replaced by a catalog name to
        apply the property only for that catalog. Valid catalog names
        are</para>

        <para>
          <screen>
replica
transformation
provenance
work
</screen>
        </para>

        <para />
      </section>

      <!-- end section -->

      <section id="Propertiespegasus.catalog.*.db.password">
        <title>pegasus.catalog.*.db.password</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>System:</entry>

                  <entry>PTC, TC, ...</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>string</entry>
                </row>

                <row>
                  <entry>Default:</entry>

                  <entry>(no default)</entry>
                </row>

                <row>
                  <entry>Example:</entry>

                  <entry>${user.name}</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para>In order to access a database, you must provide an optional
        password of your account on the DBMS. This property is
        database-independent. <warning>
            <emphasis role="bold">THIS IS A MANDATORY PROPERTY, IF YOUR DBMS
            BACKEND ACCOUNT REQUIRES A PASSWORD</emphasis>
          </warning>.</para>

        <para>The * in the property name can be replaced by a catalog name to
        apply the property only for that catalog. Valid catalog names
        are</para>

        <para>
          <screen>
replica
transformation
provenance
work
</screen>
        </para>

        <para />
      </section>

      <!-- end section -->

      <section id="Propertiespegasus.catalog.*.db.*">
        <title>pegasus.catalog.*.db.*</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>System:</entry>

                  <entry>PTC, TC, WORK, RC</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para>Each database has a multitude of options to control in fine
        detail the further behaviour. You may want to check the JDBC3
        documentation of the JDBC driver for your database for details. The
        keys will be passed as part of the connect properties by stripping the
        "pegasus.catalog.[catalog-name].db." prefix from them. The
        catalog-name can be replaced by the following values provenance for
        Provenance Catalog (PTC) transformation for Transformation Catalog
        (TC) replica for Replica Catalog (RC) work for Workflow Catalog</para>

        <para>Postgres 7.3 parses the following properties: <screen>
pegasus.catalog.*.db.user
pegasus.catalog.*.db.password
pegasus.catalog.*.db.PGHOST
pegasus.catalog.*.db.PGPORT
pegasus.catalog.*.db.charSet
pegasus.catalog.*.db.compatible
</screen></para>

        <para>MySQL 4.0 parses the following properties:</para>

        <para>
          <screen>
pegasus.catalog.*.db.user
pegasus.catalog.*.db.password
pegasus.catalog.*.db.databaseName
pegasus.catalog.*.db.serverName
pegasus.catalog.*.db.portNumber
pegasus.catalog.*.db.socketFactory
pegasus.catalog.*.db.strictUpdates
pegasus.catalog.*.db.ignoreNonTxTables
pegasus.catalog.*.db.secondsBeforeRetryMaster
pegasus.catalog.*.db.queriesBeforeRetryMaster
pegasus.catalog.*.db.allowLoadLocalInfile
pegasus.catalog.*.db.continueBatchOnError
pegasus.catalog.*.db.pedantic
pegasus.catalog.*.db.useStreamLengthsInPrepStmts
pegasus.catalog.*.db.useTimezone
pegasus.catalog.*.db.relaxAutoCommit
pegasus.catalog.*.db.paranoid
pegasus.catalog.*.db.autoReconnect
pegasus.catalog.*.db.capitalizeTypeNames
pegasus.catalog.*.db.ultraDevHack
pegasus.catalog.*.db.strictFloatingPoint
pegasus.catalog.*.db.useSSL
pegasus.catalog.*.db.useCompression
pegasus.catalog.*.db.socketTimeout
pegasus.catalog.*.db.maxReconnects
pegasus.catalog.*.db.initialTimeout
pegasus.catalog.*.db.maxRows
pegasus.catalog.*.db.useHostsInPrivileges
pegasus.catalog.*.db.interactiveClient
pegasus.catalog.*.db.useUnicode
pegasus.catalog.*.db.characterEncoding
</screen>
        </para>

        <para>MS SQL Server 2000 support the following properties (keys are
        case-insensitive, e.g. both "user" and "User" are valid):</para>

        <para>
          <screen>
pegasus.catalog.*.db.User
pegasus.catalog.*.db.Password
pegasus.catalog.*.db.DatabaseName
pegasus.catalog.*.db.ServerName
pegasus.catalog.*.db.HostProcess
pegasus.catalog.*.db.NetAddress
pegasus.catalog.*.db.PortNumber
pegasus.catalog.*.db.ProgramName
pegasus.catalog.*.db.SendStringParametersAsUnicode
pegasus.catalog.*.db.SelectMethod
</screen>
        </para>

        <para>The * in the property name can be replaced by a catalog name to
        apply the property only for that catalog. Valid catalog names
        are</para>

        <para>
          <screen>
replica
transformation
provenance
work
</screen>
        </para>

        <para />
      </section>

      <!-- end section -->
    </section>

    <!-- end section -->

    <section id="PropertiesCatalogProperties">
      <title>Catalog Properties</title>

      <para />

      <section id="PropertiesReplicaCatalog">
        <title>Replica Catalog</title>

        <para />

        <section id="Propertiespegasus.catalog.replica">
          <title>pegasus.catalog.replica</title>

          <para>
            <informaltable frame="none">
              <tgroup align="left" cols="2" colsep="1" rowsep="1">
                <tbody>
                  <row>
                    <entry>System:</entry>

                    <entry>Pegasus</entry>
                  </row>

                  <row>
                    <entry>Since:</entry>

                    <entry>2.0</entry>
                  </row>

                  <row>
                    <entry>Type:</entry>

                    <entry>enumeration</entry>
                  </row>

                  <row>
                    <entry>Value[0]:</entry>

                    <entry>RLS</entry>
                  </row>

                  <row>
                    <entry>Value[1]:</entry>

                    <entry>LRC</entry>
                  </row>

                  <row>
                    <entry>Value[2]:</entry>

                    <entry>JDBCRC</entry>
                  </row>

                  <row>
                    <entry>Value[3]:</entry>

                    <entry>File</entry>
                  </row>

                  <row>
                    <entry>Value[4]:</entry>

                    <entry>MRC</entry>
                  </row>

                  <row>
                    <entry>Default:</entry>

                    <entry>RLS</entry>
                  </row>
                </tbody>
              </tgroup>
            </informaltable>
          </para>

          <para>Pegasus queries a Replica Catalog to discover the physical
          filenames (PFN) for input files specified in the DAX. Pegasus can
          interface with various types of Replica Catalogs. This property
          specifies which type of Replica Catalog to use during the planning
          process.</para>

          <para>
            <variablelist>
              <varlistentry>
                <term>RLS</term>

                <listitem>RLS (Replica Location Service) is a distributed
                replica catalog, which ships with GT4. There is an index
                service called Replica Location Index (RLI) to which 1 or more
                Local Replica Catalog (LRC) report. Each LRC can contain all
                or a subset of mappings. In this mode, Pegasus queries the
                central RLI to discover in which LRC's the mappings for a LFN
                reside. It then queries the individual LRC's for the PFN's. To
                use RLS, the user additionally needs to set the property
                pegasus.catalog.replica.url to specify the URL for the RLI to
                query. Details about RLS can be found at
                http://www.globus.org/toolkit/data/rls/</listitem>
              </varlistentry>

              <varlistentry>
                <term>LRC</term>

                <listitem>If the user does not want to query the RLI, but
                directly a single Local Replica Catalog. To use LRC, the user
                additionally needs to set the property
                pegasus.catalog.replica.url to specify the URL for the LRC to
                query. Details about RLS can be found at
                http://www.globus.org/toolkit/data/rls/</listitem>
              </varlistentry>

              <varlistentry>
                <term>JDBCRC</term>

                <listitem>In this mode, Pegasus queries a SQL based replica
                catalog that is accessed via JDBC. The sql schema's for this
                catalog can be found at $PEGASUS_HOME/sql directory. To use
                JDBCRC, the user additionally needs to set the following
                properties <orderedlist>
                    <listitem>pegasus.catalog.replica.db.url</listitem>

                    <listitem>pegasus.catalog.replica.db.user</listitem>

                    <listitem>pegasus.catalog.replica.db.password</listitem>
                  </orderedlist></listitem>
              </varlistentry>

              <varlistentry>
                <term>File</term>

                <listitem>
                  <para>In this mode, Pegasus queries a file based replica
                  catalog. It is neither transactionally safe, nor advised to
                  use for production purposes in any way. Multiple concurrent
                  instances <emphasis>will clobber</emphasis> each other!. The
                  site attribute should be specified whenever possible. The
                  attribute key for the site attribute is "pool".</para>

                  <para>The LFN may or may not be quoted. If it contains
                  linear whitespace, quotes, backslash or an equality sign, it
                  must be quoted and escaped. Ditto for the PFN. The attribute
                  key-value pairs are separated by an equality sign without
                  any whitespaces. The value may be in quoted. The LFN
                  sentiments about quoting apply.</para>

                  <para>
                    <screen>
LFN PFN
LFN PFN a=b [..]
LFN PFN a="b" [..]
"LFN w/LWS" "PFN w/LWS" [..]
</screen>
                  </para>

                  <para>To use File, the user additionally needs to specify
                  pegasus.catalog.replica.file property to specify the path to
                  the file based RC.</para>
                </listitem>
              </varlistentry>

              <varlistentry>
                <term>MRC</term>

                <listitem>
                  <para>In this mode, Pegasus queries multiple replica
                  catalogs to discover the file locations on the grid. To use
                  it set</para>

                  <para>
                    <screen>
pegasus.catalog.replica MRC
</screen>
                  </para>

                  <para>Each associated replica catalog can be configured via
                  properties as follows.</para>

                  <para>The user associates a variable name referred to as
                  [value] for each of the catalogs, where [value] is any legal
                  identifier (concretely [A-Za-z][_A-Za-z0-9]*) For each
                  associated replica catalogs the user specifies the following
                  properties.</para>

                  <para>
                    <screen>
pegasus.catalog.replica.mrc.[value]       specifies the type of replica catalog.
pegasus.catalog.replica.mrc.[value].key   specifies a property name key for a
particular catalog
</screen>
                  </para>

                  <para>For example, if a user wants to query two lrc's at the
                  same time he/she can specify as follows</para>

                  <para>
                    <screen>
pegasus.catalog.replica.mrc.lrc1 LRC
pegasus.catalog.replica.mrc.lrc2.url rls://sukhna
pegasus.catalog.replica.mrc.lrc2 LRC
pegasus.catalog.replica.mrc.lrc2.url rls://smarty
</screen>
                  </para>

                  <para>In the above example, lrc1, lrc2 are any valid
                  identifier names and url is the property key that needed to
                  be specified.</para>
                </listitem>
              </varlistentry>
            </variablelist>
          </para>

          <para />
        </section>

        <!-- end section -->

        <section id="Propertiespegasus.catalog.replica.url">
          <title>pegasus.catalog.replica.url</title>

          <para>
            <informaltable frame="none">
              <tgroup align="left" cols="2" colsep="1" rowsep="1">
                <tbody>
                  <row>
                    <entry>System:</entry>

                    <entry>Pegasus</entry>
                  </row>

                  <row>
                    <entry>Since:</entry>

                    <entry>2.0</entry>
                  </row>

                  <row>
                    <entry>Type:</entry>

                    <entry>URI string</entry>
                  </row>

                  <row>
                    <entry>Default:</entry>

                    <entry>(no default)</entry>
                  </row>
                </tbody>
              </tgroup>
            </informaltable>
          </para>

          <para>When using the modern RLS replica catalog, the URI to the
          Replica catalog must be provided to Pegasus to enable it to look up
          filenames. There is no default.</para>
        </section>

        <!-- end section -->

        <section id="Propertiespegasus.catalog.replica.chunk.size">
          <title>pegasus.catalog.replica.chunk.size</title>

          <para>
            <informaltable frame="none">
              <tgroup align="left" cols="2" colsep="1" rowsep="1">
                <tbody>
                  <row>
                    <entry>System:</entry>

                    <entry>Pegasus, rc-client</entry>
                  </row>

                  <row>
                    <entry>Since:</entry>

                    <entry>2.0</entry>
                  </row>

                  <row>
                    <entry>Type:</entry>

                    <entry>Integer</entry>
                  </row>

                  <row>
                    <entry>Default:</entry>

                    <entry>1000</entry>
                  </row>
                </tbody>
              </tgroup>
            </informaltable>
          </para>

          <para />

          <para>The rc-client takes in an input file containing the mappings
          upon which to work. This property determines, the number of lines
          that are read in at a time, and worked upon at together. This allows
          the various operations like insert, delete happen in bulk if the
          underlying replica implementation supports it.</para>

          <para />
        </section>

        <!-- end section -->

        <section id="Propertiespegasus.catalog.replica.lrc.ignore">
          <title>pegasus.catalog.replica.lrc.ignore</title>

          <para>
            <informaltable frame="none">
              <tgroup align="left" cols="2" colsep="1" rowsep="1">
                <tbody>
                  <row>
                    <entry>System:</entry>

                    <entry>Replica Catalog - RLS</entry>
                  </row>

                  <row>
                    <entry>Since:</entry>

                    <entry>2.0</entry>
                  </row>

                  <row>
                    <entry>Type:</entry>

                    <entry>comma separated list of LRC urls</entry>
                  </row>

                  <row>
                    <entry>Default:</entry>

                    <entry>(no default)</entry>
                  </row>

                  <row>
                    <entry>See also:</entry>

                    <entry>pegasus.catalog.replica.lrc.restrict</entry>
                  </row>
                </tbody>
              </tgroup>
            </informaltable>
          </para>

          <para>Certain users may like to skip some LRCs while querying for
          the physical locations of a file. If some LRCs need to be skipped
          from those found in the rli then use this property. You can define
          either the full URL or partial domain names that need to be skipped.
          E.g. If a user wants rls://smarty.isi.edu and all LRCs on usc.edu to
          be skipped then the property will be set as
          pegasus.rls.lrc.ignore=rls://smarty.isi.edu,usc.edu</para>
        </section>

        <!-- end section -->

        <section id="Propertiespegasus.catalog.replica.lrc.restrict">
          <title>pegasus.catalog.replica.lrc.restrict</title>

          <para>
            <informaltable frame="none">
              <tgroup align="left" cols="2" colsep="1" rowsep="1">
                <tbody>
                  <row>
                    <entry>System:</entry>

                    <entry>Replica Catalog - RLS</entry>
                  </row>

                  <row>
                    <entry>Since:</entry>

                    <entry>1.3.9</entry>
                  </row>

                  <row>
                    <entry>Type:</entry>

                    <entry>comma separated list of LRC urls</entry>
                  </row>

                  <row>
                    <entry>Default:</entry>

                    <entry>(no default)</entry>
                  </row>

                  <row>
                    <entry>See also:</entry>

                    <entry>pegasus.catalog.replica.lrc.ignore</entry>
                  </row>
                </tbody>
              </tgroup>
            </informaltable>
          </para>

          <para />

          <para>This property applies a tighter restriction on the results
          returned from the LRCs specified. Only those PFNs are returned that
          have a pool attribute associated with them. The property
          "pegasus.rc.lrc.ignore" has a higher priority than
          "pegasus.rc.lrc.restrict". For example, in case a LRC is specified
          in both properties, the LRC would be ignored (i.e. not queried at
          all instead of applying a tighter restriction on the results
          returned).</para>
        </section>

        <!-- end section -->

        <section id="Propertiespegasus.catalog.replica.lrc.site.[site-name]">
          <title>pegasus.catalog.replica.lrc.site.[site-name]</title>

          <para>
            <informaltable frame="none">
              <tgroup align="left" cols="2" colsep="1" rowsep="1">
                <tbody>
                  <row>
                    <entry>System:</entry>

                    <entry>Replica Catalog - RLS</entry>
                  </row>

                  <row>
                    <entry>Since:</entry>

                    <entry>2.3.0</entry>
                  </row>

                  <row>
                    <entry>Type:</entry>

                    <entry>LRC url</entry>
                  </row>

                  <row>
                    <entry>Default:</entry>

                    <entry>(no default)</entry>
                  </row>
                </tbody>
              </tgroup>
            </informaltable>
          </para>

          <para />

          <para>This property allows for the LRC url to be associated with
          site handles. Usually, a pool attribute is required to be associated
          with the PFN for Pegasus to figure out the site on which PFN
          resides. However, in the case where an LRC is responsible for only a
          single site's mappings, Pegasus can safely associate LRC url with
          the site. This association can be used to determine the pool
          attribute for all mappings returned from the LRC, if the mapping
          does not have a pool attribute associated with it.</para>

          <para>The site_name in the property should be replaced by the name
          of the site. For example <screen>
pegasus.catalog.replica.lrc.site.isi  rls://lrc.isi.edu
</screen> tells Pegasus that all PFNs returned from LRC rls://lrc.isi.edu are
          associated with site isi.</para>

          <para>The [site_name] should be the same as the site handle
          specified in the site catalog.</para>
        </section>

        <!-- end section -->

        <section id="Propertiespegasus.catalog.replica.cache.asrc">
          <title>pegasus.catalog.replica.cache.asrc</title>

          <para>
            <informaltable frame="none">
              <tgroup align="left" cols="2" colsep="1" rowsep="1">
                <tbody>
                  <row>
                    <entry>System:</entry>

                    <entry>Pegasus</entry>
                  </row>

                  <row>
                    <entry>Since:</entry>

                    <entry>2.0</entry>
                  </row>

                  <row>
                    <entry>Type:</entry>

                    <entry>Boolean</entry>
                  </row>

                  <row>
                    <entry>Value[0]:</entry>

                    <entry>false</entry>
                  </row>

                  <row>
                    <entry>Value[1]:</entry>

                    <entry>true</entry>
                  </row>

                  <row>
                    <entry>Default:</entry>

                    <entry>false</entry>
                  </row>

                  <row>
                    <entry>See also:</entry>

                    <entry>pegasus.catalog.replica</entry>
                  </row>
                </tbody>
              </tgroup>
            </informaltable>
          </para>

          <para />

          <para>This property determines whether to treat the cache file
          specified as a supplemental replica catalog or not. User can specify
          on the command line to pegasus-plan a comma separated list of cache
          files using the --cache option. By default, the LFN-&gt;PFN mappings
          contained in the cache file are treated as cache, i.e if an entry is
          found in a cache file the replica catalog is not queried. This
          results in only the entry specified in the cache file to be
          available for replica selection.</para>

          <para>Setting this property to true, results in the cache files to
          be treated as supplemental replica catalogs. This results in the
          mappings found in the replica catalog (as specified by
          pegasus.catalog.replica) to be merged with the ones found in the
          cache files. Thus, mappings for a particular LFN found in both the
          cache and the replica catalog are available for replica
          selection.</para>
        </section>

        <!-- end section -->
      </section>

      <!-- end subsection -->

      <section id="PropertiesSiteCatalog">
        <title>Site Catalog</title>

        <para />

        <para />

        <section id="Propertiespegasus.catalog.site">
          <title>pegasus.catalog.site</title>

          <para>
            <informaltable frame="none">
              <tgroup align="left" cols="2" colsep="1" rowsep="1">
                <tbody>
                  <row>
                    <entry>System:</entry>

                    <entry>Site Catalog</entry>
                  </row>

                  <row>
                    <entry>Since:</entry>

                    <entry>2.0</entry>
                  </row>

                  <row>
                    <entry>Type:</entry>

                    <entry>enumeration</entry>
                  </row>

                  <row>
                    <entry>Value[0]:</entry>

                    <entry>XML3</entry>
                  </row>

                  <row>
                    <entry>Value[1]:</entry>

                    <entry>XML</entry>
                  </row>

                  <row>
                    <entry>Value[2]:</entry>

                    <entry>Text</entry>
                  </row>

                  <row>
                    <entry>Default:</entry>

                    <entry>XML3</entry>
                  </row>
                </tbody>
              </tgroup>
            </informaltable>
          </para>

          <para>The site catalog file is available in three major flavors: The
          Text and and XML formats for the site catalog are deprecated. Users
          can use pegasus-sc-converter client to convert their site catalog to
          the newer XML3 format. <orderedlist>
              <listitem>The "XML" format is an XML-based file. The XML format
              reads site catalog conforming to the old site catalog schema
              available at
              http://pegasus.isi.edu/wms/docs/schemas/sc-2.0/sc-2.0.xsd</listitem>

              <listitem>The "XML3" format is an XML-based file. The XML format
              reads site catalog conforming to the old site catalog schema
              available at
              http://pegasus.isi.edu/wms/docs/schemas/sc-3.0/sc-3.0.xsd</listitem>

              <listitem>The "Text" format is a multiline site catalog format.
              It is described in the site catalog guide. It can be directly
              given to Pegasus starting with PEGASUS-1.4</listitem>
            </orderedlist></para>
        </section>

        <!-- end section -->

        <section id="Propertiespegasus.catalog.site.file">
          <title>pegasus.catalog.site.file</title>

          <para>
            <informaltable frame="none">
              <tgroup align="left" cols="2" colsep="1" rowsep="1">
                <tbody>
                  <row>
                    <entry>System:</entry>

                    <entry>Site Catalog</entry>
                  </row>

                  <row>
                    <entry>Since:</entry>

                    <entry>2.0</entry>
                  </row>

                  <row>
                    <entry>Type:</entry>

                    <entry>file location string</entry>
                  </row>

                  <row>
                    <entry>Default:</entry>

                    <entry>${pegasus.home.sysconfdir}/sites.xml |
                    ${pegasus.home.sysconfdir}/sites.txt</entry>
                  </row>

                  <row>
                    <entry>See also:</entry>

                    <entry>pegasus.catalog.site</entry>
                  </row>
                </tbody>
              </tgroup>
            </informaltable>
          </para>

          <para />

          <para>Running things on the grid requires an extensive description
          of the capabilities of each compute cluster, commonly termed "site".
          This property describes the location of the file that contains such
          a site description. As the format is currently in flow, please refer
          to the userguide and Pegasus for details which format is expected.
          The default value is dependant on the value specified for the
          property pegasus.sc . pegasus.sc denotes the type of site catalog
          being used.</para>
        </section>

        <!-- end section -->
      </section>

      <!-- end subsection -->

      <section id="PropertiesTransformationCatalog">
        <title>Transformation Catalog</title>

        <para />

        <para />

        <section id="Propertiespegasus.catalog.transformation">
          <title>pegasus.catalog.transformation</title>

          <para>
            <informaltable frame="none">
              <tgroup align="left" cols="2" colsep="1" rowsep="1">
                <tbody>
                  <row>
                    <entry>System:</entry>

                    <entry>Transformation Catalog</entry>
                  </row>

                  <row>
                    <entry>Since:</entry>

                    <entry>2.0</entry>
                  </row>

                  <row>
                    <entry>Type:</entry>

                    <entry>enumeration</entry>
                  </row>

                  <row>
                    <entry>Value[0]:</entry>

                    <entry>Text</entry>
                  </row>

                  <row>
                    <entry>Value[1]:</entry>

                    <entry>File</entry>
                  </row>

                  <row>
                    <entry>Value[2]:</entry>

                    <entry>Database</entry>
                  </row>

                  <row>
                    <entry>Default:</entry>

                    <entry>Text</entry>
                  </row>

                  <row>
                    <entry>See also:</entry>

                    <entry>pegasus.catalog.*.driver</entry>
                  </row>

                  <row>
                    <entry>See also:</entry>

                    <entry>pegasus.catalog.transformation.file</entry>
                  </row>
                </tbody>
              </tgroup>
            </informaltable>
          </para>

          <para>
            <variablelist>
              <varlistentry>
                <term>Text</term>

                <listitem>
                  <para>In this mode, a multiline file based format is
                  understood. The file is read and cached in memory. Any
                  modifications, as adding or deleting, causes an update of
                  the memory and hence to the file underneath. All queries are
                  done against the memory representation.</para>

                  <para>The file sample.tc.text in the etc directory contains
                  an example</para>

                  <para>Here is a sample textual format for transfomation
                  catalog containing one transformation on two sites</para>

                  <para>
                    <screen>
tr example::keg:1.0 {
#specify profiles that apply for all the sites for the transformation
#in each site entry the profile can be overriden
profile env "APP_HOME" "/tmp/karan"
profile env "JAVA_HOME" "/bin/app"
site isi {
profile env "me" "with"
profile condor "more" "test"
profile env "JAVA_HOME" "/bin/java.1.6"
pfn "/path/to/keg"
arch  "x86"
os    "linux"
osrelease "fc"
osversion "4"
type "INSTALLED"
site wind {
profile env "me" "with"
profile condor "more" "test"
pfn "/path/to/keg"
arch  "x86"
os    "linux"
osrelease "fc"
osversion "4"
type "STAGEABLE"
</screen>
                  </para>
                </listitem>
              </varlistentry>

              <varlistentry>
                <term>File</term>

                <listitem>In this mode, a file format is understood. The file
                is read and cached in memory. Any modifications, as adding or
                deleting, causes an update of the memory and hence to the file
                underneath. All queries are done against the memory
                representation. The new TC file format uses 6 columns:
                <orderedlist>
                    <listitem>The resource ID is represented in the first
                    column.</listitem>

                    <listitem>The logical transformation uses the colonized
                    format ns::name:vs.</listitem>

                    <listitem>The path to the application on the
                    system</listitem>

                    <listitem>The installation type is identified by one of
                    the following keywords - all upper case: INSTALLED,
                    STAGEABLE. If not specified, or <command>NULL</command> is
                    used, the type defaults to INSTALLED.</listitem>

                    <listitem>The system is of the format
                    ARCH::OS[:VER:GLIBC]. The following arch types are
                    understood: "INTEL32", "INTEL64", "SPARCV7", "SPARCV9".
                    The following os types are understood: "LINUX", "SUNOS",
                    "AIX". If unset or <command>NULL</command>, defaults to
                    INTEL32::LINUX.</listitem>

                    <listitem>Profiles are written in the format
                    NS::KEY=VALUE,KEY2=VALUE;NS2::KEY3=VALUE3 Multiple
                    key-values for same namespace are seperated by a comma ","
                    and multiple namespaces are seperated by a semicolon ";".
                    If any of your profile values contains a comma you must
                    not use the namespace abbreviator.</listitem>
                  </orderedlist></listitem>
              </varlistentry>

              <varlistentry>
                <term>Database</term>

                <listitem>
                  <para>In this mode, the transformation catalog is kept in a
                  relational database. Currently mysql DB and Postgre are
                  supported. To set up the the database, use the schema in
                  $PEGASUS_HOME/sql/create-my-init.sql followed by
                  $PEGASUS_HOME/sql/create-my-tc.sql .</para>

                  <para>
                    <screen>
The following properties need to be set
pegasus.catalog.transformation.db.driver = MySQL|Postgres
pegasus.catalog.transformation.db.url =
jdbc:mysql://[hostname[:port]]/database |
jdbc:postgres://[hostname[:port]]/database
pegasus.catalog.transformation.db.user = dbusername
pegasus.catalog.transformation.db.password = passoword
</screen>
                  </para>

                  <para>If the pegasus.catalog.transformation.db.* properties
                  are not defined, the database implementation picks up the
                  properties specified by pegasus.catalog.*.db.* .</para>
                </listitem>
              </varlistentry>
            </variablelist>
          </para>

          <para>Future modifications to the TC may extend the enumeration. To
          implement your own TC implementation see
          org.girphyn.cPlanner.tc.TCMechanism. To load the class set
          pegasus.catalog.transformation to the TC implementation
          class.</para>
        </section>

        <!-- end section -->

        <section id="Propertiespegasus.catalog.transformation.file">
          <title>pegasus.catalog.transformation.file</title>

          <para>
            <informaltable frame="none">
              <tgroup align="left" cols="2" colsep="1" rowsep="1">
                <tbody>
                  <row>
                    <entry>Systems:</entry>

                    <entry>Transformation Catalog</entry>
                  </row>

                  <row>
                    <entry>Type:</entry>

                    <entry>file location string</entry>
                  </row>

                  <row>
                    <entry>Default:</entry>

                    <entry>${pegasus.home.sysconfdir}/tc.data</entry>
                  </row>

                  <row>
                    <entry>See also:</entry>

                    <entry>pegasus.catalog.transformation</entry>
                  </row>
                </tbody>
              </tgroup>
            </informaltable>
          </para>

          <para />

          <para>This property is used to set the path to the textual
          transformation catalogs of type File or Text.</para>

          <para />
        </section>

        <!-- end section -->
      </section>

      <!-- end subsection -->

      <section id="PropertiesProvenanceCatalog">
        <title>Provenance Catalog</title>

        <para />

        <section id="Propertiespegasus.catalog.provenance">
          <title>pegasus.catalog.provenance</title>

          <para>
            <informaltable frame="none">
              <tgroup align="left" cols="2" colsep="1" rowsep="1">
                <tbody>
                  <row>
                    <entry>System:</entry>

                    <entry>Provenance Tracking Catalog (PTC)</entry>
                  </row>

                  <row>
                    <entry>Since:</entry>

                    <entry>2.0</entry>
                  </row>

                  <row>
                    <entry>Type:</entry>

                    <entry>Java class name</entry>
                  </row>

                  <row>
                    <entry>Value[0]:</entry>

                    <entry>InvocationSchema</entry>
                  </row>

                  <row>
                    <entry>Value[1]:</entry>

                    <entry>NXDInvSchema</entry>
                  </row>

                  <row>
                    <entry>Default:</entry>

                    <entry>(no default)</entry>
                  </row>

                  <row>
                    <entry>See also:</entry>

                    <entry>pegasus.catalog.*.db.driver</entry>
                  </row>
                </tbody>
              </tgroup>
            </informaltable>
          </para>

          <para>This property denotes the schema that is being used to access
          a PTC. The PTC is usually not a standard installation. If you use a
          database backend, you most likely have a schema that supports PTCs.
          By default, no PTC will be used.</para>

          <para>Currently only the InvocationSchema is available for storing
          the provenance tracking records. Beware, this can become a lot of
          data. The values are names of Java classes. If no absolute Java
          classname is given, "org.griphyn.vdl.dbschema." is prepended. Thus,
          by deriving from the DatabaseSchema API, and implementing the PTC
          interface, users can provide their own classes here.</para>

          <para>Alternatively, if you use a native XML database like eXist,
          you can store data using the NXDInvSchema. This will avoid using any
          of the other database driver properties.</para>
        </section>

        <!-- end section -->

        <section id="Propertiespegasus.catalog.provenance.refinement">
          <title>pegasus.catalog.provenance.refinement</title>

          <para>
            <informaltable frame="none">
              <tgroup align="left" cols="2" colsep="1" rowsep="1">
                <tbody>
                  <row>
                    <entry>System:</entry>

                    <entry>PASOA Provenance Store</entry>
                  </row>

                  <row>
                    <entry>Since:</entry>

                    <entry>2.0.1</entry>
                  </row>

                  <row>
                    <entry>Type:</entry>

                    <entry>Java class name</entry>
                  </row>

                  <row>
                    <entry>Value[0]:</entry>

                    <entry>Pasoa</entry>
                  </row>

                  <row>
                    <entry>Value[1]:</entry>

                    <entry>InMemory</entry>
                  </row>

                  <row>
                    <entry>Default:</entry>

                    <entry>InMemory</entry>
                  </row>

                  <row>
                    <entry>See also:</entry>

                    <entry>pegasus.catalog.*.db.driver</entry>
                  </row>
                </tbody>
              </tgroup>
            </informaltable>
          </para>

          <para />

          <para>This property turns on the logging of the refinement process
          that happens inside Pegasus to the PASOA store. Not all actions are
          currently captured. It is still an experimental feature.</para>

          <para>The PASOA store needs to run on localhost on port 8080
          https://localhost:8080/prserv-1.0</para>

          <para />
        </section>

        <!-- end section -->
      </section>

      <!-- end subsection -->

      <section id="PropertiesWorkCatalog">
        <title>Work Catalog</title>

        <para />

        <section id="Propertiespegasus.catalog.work">
          <title>pegasus.catalog.work</title>

          <para>
            <informaltable frame="none">
              <tgroup align="left" cols="2" colsep="1" rowsep="1">
                <tbody>
                  <row>
                    <entry>System:</entry>

                    <entry>Work Catalog</entry>
                  </row>

                  <row>
                    <entry>Since:</entry>

                    <entry>2.0</entry>
                  </row>

                  <row>
                    <entry>Type:</entry>

                    <entry>Java class name</entry>
                  </row>

                  <row>
                    <entry>Value[0]:</entry>

                    <entry>Database</entry>
                  </row>

                  <row>
                    <entry>Default:</entry>

                    <entry>Database</entry>
                  </row>

                  <row>
                    <entry>See also:</entry>

                    <entry>pegasus.catalog.*.db.driver</entry>
                  </row>
                </tbody>
              </tgroup>
            </informaltable>
          </para>

          <para>This property denotes the schema that is being used to store
          workflow monitoring entries. This catalog is populated at the end of
          the planning process by pegasus-plan and at workflow execution by
          the tailstatd daemon.</para>
        </section>

        <!-- end section -->
      </section>

      <!-- end subsection -->
    </section>

    <!-- end section -->

    <section id="PropertiesReplicaSelectionProperties">
      <title>Replica Selection Properties</title>

      <para />

      <para />

      <section id="Propertiespegasus.selector.replica">
        <title>pegasus.selector.replica</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>System:</entry>

                  <entry>Replica Selection</entry>
                </row>

                <row>
                  <entry>Since:</entry>

                  <entry>2.0</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>URI string</entry>
                </row>

                <row>
                  <entry>Default:</entry>

                  <entry>default</entry>
                </row>

                <row>
                  <entry>See also:</entry>

                  <entry>pegasus.replica.*.ignore.stagein.sites</entry>
                </row>

                <row>
                  <entry>See also:</entry>

                  <entry>pegasus.replica.*.prefer.stagein.sites</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para>Each job in the DAX maybe associated with input LFN's denoting
        the files that are required for the job to run. To determine the
        physical replica (PFN) for a LFN, Pegasus queries the replica catalog
        to get all the PFN's (replicas) associated with a LFN. Pegasus then
        calls out to a replica selector to select a replica amongst the
        various replicas returned. This property determines the replica
        selector to use for selecting the replicas.</para>

        <para>
          <variablelist>
            <varlistentry>
              <term>Default</term>

              <listitem>If a PFN that is a file URL (starting with file:///)
              and has a pool attribute matching to the site handle of the site
              where the compute is to be run is found, then that is returned.
              Else,a random PFN is selected amongst all the PFN's that have a
              pool attribute matching to the site handle of the site where a
              compute job is to be run. Else, a random pfn is selected amongst
              all the PFN's.</listitem>
            </varlistentry>

            <varlistentry>
              <term>Restricted</term>

              <listitem>
                <para>This replica selector, allows the user to specify good
                sites and bad sites for staging in data to a particular
                compute site. A good site for a compute site X, is a preferred
                site from which replicas should be staged to site X. If there
                are more than one good sites having a particular replica, then
                a random site is selected amongst these preferred
                sites.</para>

                <para>A bad site for a compute site X, is a site from which
                replica's should not be staged. The reason of not accessing
                replica from a bad site can vary from the link being down, to
                the user not having permissions on that site's data.</para>

                <para>The good | bad sites are specified by the
                properties</para>

                <para>
                  <screen>
pegasus.replica.*.prefer.stagein.sites
pegasus.replica.*.ignore.stagein.sites
</screen>
                </para>

                <para>where the * in the property name denotes the name of the
                compute site. A * in the property key is taken to mean all
                sites.</para>

                <para>The pegasus.replica.*.prefer.stagein.sites property
                takes precedence over pegasus.replica.*.ignore.stagein.sites
                property i.e. if for a site X, a site Y is specified both in
                the ignored and the preferred set, then site Y is taken to
                mean as only a preferred site for a site X.</para>
              </listitem>
            </varlistentry>

            <varlistentry>
              <term>Regex</term>

              <listitem>
                <para>This replica selector allows the user allows the user to
                specific regex expressions that can be used to rank various
                PFN's returned from the Replica Catalog for a particular LFN.
                This replica selector selects the highest ranked PFN i.e the
                replica with the lowest rank value.</para>

                <para>The regular expressions are assigned different rank,
                that determine the order in which the expressions are
                employed. The rank values for the regex can expressed in user
                properties using the property.</para>

                <para>
                  <screen>
pegasus.selector.replica.regex.rank.[value]   regex-expression
</screen>
                </para>

                <para>The value is an integer value that denotes the rank of
                an expression with a rank value of 1 being the highest
                rank.</para>

                <para>Please note that before applying any regular expressions
                on the PFN's, the file URL's that dont match the preferred
                site are explicitly filtered out.</para>
              </listitem>
            </varlistentry>

            <varlistentry>
              <term>Local</term>

              <listitem>This replica selector prefers replicas from the local
              host and that start with a file: URL scheme. It is useful, when
              users want to stagin files to a remote site from your submit
              host using the Condor file transfer mechanism.</listitem>
            </varlistentry>
          </variablelist>
        </para>

        <para />
      </section>

      <!-- end section -->

      <section id="Propertiespegasus.selector.replica.*.ignore.stagein.sites">
        <title>pegasus.selector.replica.*.ignore.stagein.sites</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>System:</entry>

                  <entry>Replica Selection</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>comma separated list of sites</entry>
                </row>

                <row>
                  <entry>Since:</entry>

                  <entry>2.0</entry>
                </row>

                <row>
                  <entry>Default:</entry>

                  <entry>no default</entry>
                </row>

                <row>
                  <entry>See also:</entry>

                  <entry>pegasus.selector.replica</entry>
                </row>

                <row>
                  <entry>See also:</entry>

                  <entry>pegasus.selector.replica.*.prefer.stagein.sites</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para>A comma separated list of storage sites from which to never
        stage in data to a compute site. The property can apply to all or a
        single compute site, depending on how the * in the property name is
        expanded.</para>

        <para>The * in the property name means all compute sites unless
        replaced by a site name.</para>

        <para>For e.g setting pegasus.selector.replica.*.ignore.stagein.sites
        to usc means that ignore all replicas from site usc for staging in to
        any compute site. Setting pegasus.replica.isi.ignore.stagein.sites to
        usc means that ignore all replicas from site usc for staging in data
        to site isi.</para>

        <para />
      </section>

      <!-- end section -->

      <section id="Propertiespegasus.selector.replica.*.prefer.stagein.sites">
        <title>pegasus.selector.replica.*.prefer.stagein.sites</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>System:</entry>

                  <entry>Replica Selection</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>comma separated list of sites</entry>
                </row>

                <row>
                  <entry>Since:</entry>

                  <entry>2.0</entry>
                </row>

                <row>
                  <entry>Default:</entry>

                  <entry>no default</entry>
                </row>

                <row>
                  <entry>See also:</entry>

                  <entry>pegasus.selector.replica</entry>
                </row>

                <row>
                  <entry>See also:</entry>

                  <entry>pegasus.selector.replica.*.ignore.stagein.sites</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para>A comma separated list of preferred storage sites from which to
        stage in data to a compute site. The property can apply to all or a
        single compute site, depending on how the * in the property name is
        expanded.</para>

        <para>The * in the property name means all compute sites unless
        replaced by a site name.</para>

        <para>For e.g setting pegasus.selector.replica.*.prefer.stagein.sites
        to usc means that prefer all replicas from site usc for staging in to
        any compute site. Setting pegasus.replica.isi.prefer.stagein.sites to
        usc means that prefer all replicas from site usc for staging in data
        to site isi.</para>

        <para />
      </section>

      <!-- end section -->

      <section id="Propertiespegasus.selector.replica.regex.rank.[value]">
        <title>pegasus.selector.replica.regex.rank.[value]</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>System:</entry>

                  <entry>Replica Selection</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>Regex Expression</entry>
                </row>

                <row>
                  <entry>Since:</entry>

                  <entry>2.3.0</entry>
                </row>

                <row>
                  <entry>Default:</entry>

                  <entry>no default</entry>
                </row>

                <row>
                  <entry>See also:</entry>

                  <entry>pegasus.selector.replica</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para>Specifies the regex expressions to be applied on the PFNs
        returned for a particular LFN. Refer to <screen>
http://java.sun.com/javase/6/docs/api/java/util/regex/Pattern.html
</screen> on information of how to construct a regex expression.</para>

        <para>The [value] in the property key is to be replaced by an int
        value that designates the rank value for the regex expression to be
        applied in the Regex replica selector.</para>

        <para>The example below indicates preference for file URL's over URL's
        referring to gridftp server at example.isi.edu</para>

        <para>
          <screen>
pegasus.selector.replica.regex.rank.1 file://.*
pegasus.selector.replica.regex.rank.2 gsiftp://example\.isi\.edu.*
</screen>
        </para>

        <para />
      </section>

      <!-- end section -->
    </section>

    <!-- end section -->

    <section id="PropertiesSiteSelectionProperties">
      <title>Site Selection Properties</title>

      <para />

      <section id="Propertiespegasus.selector.site">
        <title>pegasus.selector.site</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>System:</entry>

                  <entry>Pegasus</entry>
                </row>

                <row>
                  <entry>Since:</entry>

                  <entry>2.0</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>enumeration</entry>
                </row>

                <row>
                  <entry>Value[0]:</entry>

                  <entry>Random</entry>
                </row>

                <row>
                  <entry>Value[1]:</entry>

                  <entry>RoundRobin</entry>
                </row>

                <row>
                  <entry>Value[2]:</entry>

                  <entry>NonJavaCallout</entry>
                </row>

                <row>
                  <entry>Value[3]:</entry>

                  <entry>Group</entry>
                </row>

                <row>
                  <entry>Value[4]:</entry>

                  <entry>Heft</entry>
                </row>

                <row>
                  <entry>Default:</entry>

                  <entry>Random</entry>
                </row>

                <row>
                  <entry>See also:</entry>

                  <entry>pegasus.selector.site.path</entry>
                </row>

                <row>
                  <entry>See also:</entry>

                  <entry>pegasus.selector.site.timeout</entry>
                </row>

                <row>
                  <entry>See also:</entry>

                  <entry>pegasus.selector.site.keep.tmp</entry>
                </row>

                <row>
                  <entry>See also:</entry>

                  <entry>pegasus.selector.site.env.*</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para>The site selection in Pegasus can be on basis of any of the
        following strategies.</para>

        <para>
          <variablelist>
            <varlistentry>
              <term>Random</term>

              <listitem>In this mode, the jobs will be randomly distributed
              among the sites that can execute them.</listitem>
            </varlistentry>

            <varlistentry>
              <term>RoundRobin</term>

              <listitem>In this mode. the jobs will be assigned in a round
              robin manner amongst the sites that can execute them. Since each
              site cannot execute everytype of job, the round robin scheduling
              is done per level on a sorted list. The sorting is on the basis
              of the number of jobs a particular site has been assigned in
              that level so far. If a job cannot be run on the first site in
              the queue (due to no matching entry in the transformation
              catalog for the transformation referred to by the job), it goes
              to the next one and so on. This implementation defaults to
              classic round robin in the case where all the jobs in the
              workflow can run on all the sites.</listitem>
            </varlistentry>

            <varlistentry>
              <term>NonJavaCallout</term>

              <listitem>
                <para>In this mode, Pegasus will callout to an external site
                selector.In this mode a temporary file is prepared containing
                the job information that is passed to the site selector as an
                argument while invoking it. The path to the site selector is
                specified by setting the property pegasus.site.selector.path.
                The environment variables that need to be set to run the site
                selector can be specified using the properties with a
                pegasus.site.selector.env. prefix. The temporary file contains
                information about the job that needs to be scheduled. It
                contains key value pairs with each key value pair being on a
                new line and separated by a =.</para>

                <para>The following pairs are currently generated for the site
                selector temporary file that is generated in the
                NonJavaCallout.</para>

                <para>
                  <informaltable frame="none">
                    <tgroup align="left" cols="2" colsep="1" rowsep="1">
                      <tbody>
                        <row>
                          <entry>version</entry>

                          <entry>is the version of the site selector
                          api,currently 2.0.</entry>
                        </row>

                        <row>
                          <entry>transformation</entry>

                          <entry>is the fully-qualified definition identifier
                          for the transformation (TR)
                          namespace::name:version.</entry>
                        </row>

                        <row>
                          <entry>derivation</entry>

                          <entry>is the fully qualified definition identifier
                          for the derivation (DV),
                          namespace::name:version.</entry>
                        </row>

                        <row>
                          <entry>job.level</entry>

                          <entry>is the job's depth in the tree of the
                          workflow DAG.</entry>
                        </row>

                        <row>
                          <entry>job.id</entry>

                          <entry>is the job's ID, as used in the DAX
                          file.</entry>
                        </row>

                        <row>
                          <entry>resource.id</entry>

                          <entry>is a pool handle, followed by whitespace,
                          followed by a gridftp server. Typically, each
                          gridftp server is enumerated once, so you may have
                          multiple occurances of the same site. There can be
                          multiple occurances of this key.</entry>
                        </row>

                        <row>
                          <entry>input.lfn</entry>

                          <entry>is an input LFN, optionally followed by a
                          whitespace and file size. There can be multiple
                          occurances of this key,one for each input LFN
                          required by the job.</entry>
                        </row>

                        <row>
                          <entry>wf.name</entry>

                          <entry>label of the dax, as found in the DAX's root
                          element. wf.index is the DAX index, that is
                          incremented for each partition in case of deferred
                          planning.</entry>
                        </row>

                        <row>
                          <entry>wf.time</entry>

                          <entry>is the mtime of the workflow.</entry>
                        </row>

                        <row>
                          <entry>wf.manager</entry>

                          <entry>is the name of the workflow manager being
                          used .e.g condor</entry>
                        </row>

                        <row>
                          <entry>vo.name</entry>

                          <entry>is the name of the virtual organization that
                          is running this workflow. It is currently set to
                          NONE</entry>
                        </row>

                        <row>
                          <entry>vo.group</entry>

                          <entry>unused at present and is set to NONE.</entry>
                        </row>

                        <row>
                          <entry />
                        </row>
                      </tbody>
                    </tgroup>
                  </informaltable>
                </para>
              </listitem>
            </varlistentry>

            <varlistentry>
              <term>Group</term>

              <listitem>In this mode, a group of jobs will be assigned to the
              same site that can execute them. The use of the PEGASUS profile
              key group in the dax, associates a job with a particular group.
              The jobs that do not have the profile key associated with them,
              will be put in the default group. The jobs in the default group
              are handed over to the "Random" Site Selector for
              scheduling.</listitem>
            </varlistentry>

            <varlistentry>
              <term>Heft</term>

              <listitem>
                <para>In this mode, a version of the HEFT processor scheduling
                algorithm is used to schedule jobs in the workflow to multiple
                grid sites. The implementation assumes default data
                communication costs when jobs are not scheduled on to the same
                site. Later on this may be made more configurable.</para>

                <para>The runtime for the jobs is specified in the
                transformation catalog by associating the pegasus profile key
                runtime with the entries.</para>

                <para>The number of processors in a site is picked up from the
                attribute idle-nodes associated with the vanilla jobmanager of
                the site in the site catalog.</para>
              </listitem>
            </varlistentry>
          </variablelist>
        </para>
      </section>

      <!-- end section -->

      <section id="Propertiespegasus.selector.site.path">
        <title>pegasus.selector.site.path</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>System:</entry>

                  <entry>Site Selector</entry>
                </row>

                <row>
                  <entry>Since:</entry>

                  <entry>2.0</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>String</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para />

        <para>If one calls out to an external site selector using the
        NonJavaCallout mode, this refers to the path where the site selector
        is installed. In case other strategies are used it does not need to be
        set.</para>
      </section>

      <!-- end section -->

      <section id="Propertiespegasus.site.selector.env.*">
        <title>pegasus.site.selector.env.*</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>System:</entry>

                  <entry>Pegasus</entry>
                </row>

                <row>
                  <entry>Since:</entry>

                  <entry>1.2.3</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>String</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para />

        <para>The environment variables that need to be set while callout to
        the site selector. These are the variables that the user would set if
        running the site selector on the command line. The name of the
        environment variable is got by stripping the keys of the prefix
        "pegasus.site.selector.env." prefix from them. The value of the
        environment variable is the value of the property.</para>

        <para>e.g pegasus.site.selector.path.LD_LIBRARY_PATH /globus/lib would
        lead to the site selector being called with the LD_LIBRARY_PATH set to
        /globus/lib.</para>
      </section>

      <!-- end section -->

      <section id="Propertiespegasus.selector.site.timeout">
        <title>pegasus.selector.site.timeout</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>System:</entry>

                  <entry>Site Selector</entry>
                </row>

                <row>
                  <entry>Since:</entry>

                  <entry>2.0</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>non negative integer</entry>
                </row>

                <row>
                  <entry>Default:</entry>

                  <entry>60</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para />

        <para>It sets the number of seconds Pegasus waits to hear back from an
        external site selector using the NonJavaCallout interface before
        timing out.</para>
      </section>

      <!-- end section -->

      <section id="Propertiespegasus.selector.site.keep.tmp">
        <title>pegasus.selector.site.keep.tmp</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>System:</entry>

                  <entry>Pegasus</entry>
                </row>

                <row>
                  <entry>Since:</entry>

                  <entry>2.0</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>enumeration</entry>
                </row>

                <row>
                  <entry>Value[0]:</entry>

                  <entry>onerror</entry>
                </row>

                <row>
                  <entry>Value[1]:</entry>

                  <entry>always</entry>
                </row>

                <row>
                  <entry>Value[2]:</entry>

                  <entry>never</entry>
                </row>

                <row>
                  <entry>Default:</entry>

                  <entry>onerror</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para />

        <para>It determines whether Pegasus deletes the temporary input files
        that are generated in the temp directory or not. These temporary input
        files are passed as input to the external site selectors.</para>

        <para>A temporary input file is created for each that needs to be
        scheduled.</para>
      </section>

      <!-- end section -->
    </section>

    <!-- end section -->

    <section id="PropertiesTransferConfigurationProperties">
      <title>Transfer Configuration Properties</title>

      <para />

      <para />

      <section id="Propertiespegasus.transfer.*.impl">
        <title>pegasus.transfer.*.impl</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>System:</entry>

                  <entry>Pegasus</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>enumeration</entry>
                </row>

                <row>
                  <entry>Value[0]:</entry>

                  <entry>Transfer3</entry>
                </row>

                <row>
                  <entry>Value[1]:</entry>

                  <entry>GUC</entry>
                </row>

                <row>
                  <entry>Default:</entry>

                  <entry>Transfer3</entry>
                </row>

                <row>
                  <entry>See also:</entry>

                  <entry>pegasus.transfer.refiner</entry>
                </row>

                <row>
                  <entry>Since:</entry>

                  <entry>2.0</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para>Each compute job usually has data products that are required to
        be staged in to the execution site, materialized data products staged
        out to a final resting place, or staged to another job running at a
        different site. This property determines the underlying grid transfer
        tool that is used to manage the transfers.</para>

        <para>The * in the property name can be replaced to achieve finer
        grained control to dictate what type of transfer jobs need to be
        managed with which grid transfer tool.</para>

        <para>Usually,the arguments with which the client is invoked can be
        specified by <screen>
- the property pegasus.transfer.arguments
- associating the PEGASUS profile key transfer.arguments
</screen></para>

        <para>The table below illustrates all the possible variations of the
        property.</para>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>Property Name</entry>

                  <entry>Applies to</entry>
                </row>

                <row>
                  <entry>pegasus.transfer.stagein.impl</entry>

                  <entry>the stage in transfer jobs</entry>
                </row>

                <row>
                  <entry>pegasus.transfer.stageout.impl</entry>

                  <entry>the stage out transfer jobs</entry>
                </row>

                <row>
                  <entry>pegasus.transfer.inter.impl</entry>

                  <entry>the inter pool transfer jobs</entry>
                </row>

                <row>
                  <entry>pegasus.transfer.setup.impl</entry>

                  <entry>the setup transfer job</entry>
                </row>

                <row>
                  <entry>pegasus.transfer.*.impl</entry>

                  <entry>apply to types of transfer jobs</entry>
                </row>

                <row>
                  <entry />
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <note>
          <para>Note: Since version 2.2.0 the worker package is staged
          automatically during staging of executables to the remote site. This
          is achieved by adding a setup transfer job to the workflow. The
          setup transfer job by default uses GUC to stage the data. The
          implementation to use can be configured by setting the
          property</para>
        </note>

        <para><screen>pegasus.transfer.setup.impl </screen>property. However,
        if you have pegasus.transfer.*.impl set in your properties file, then
        you need to set pegasus.transfer.setup.impl to GUC</para>

        <para>The various grid transfer tools that can be used to manage data
        transfers are explained below</para>

        <para>
          <variablelist>
            <varlistentry>
              <term>Transfer3</term>

              <listitem>
                <para>This results in pegasus-transfer to be used for
                transferring of files. It is a python based wrapper around
                various transfer clients like globus-url-copy, lcg-copy, wget,
                cp, ln . pegasus-transfer looks at source and destination url
                and figures out automatically which underlying client to use.
                pegasus-transfer is distributed with the PEGASUS and can be
                found at $PEGASUS_HOME/bin/pegasus-transfer.</para>

                <para>For remote sites, Pegasus constructs the default path to
                pegasus-transfer on the basis of PEGASUS_HOME env profile
                specified in the site catalog. To specify a different path to
                the pegasus-transfer client , users can add an entry into the
                transformation catalog with fully qualified logical name as
                pegasus::pegasus-transfer</para>
              </listitem>
            </varlistentry>

            <varlistentry>
              <term>GUC</term>

              <listitem>This refers to the new guc client that does multiple
              file transfers per invocation. The globus-url-copy client
              distributed with Globus 4.x is compatible with this
              mode.</listitem>
            </varlistentry>
          </variablelist>
        </para>

        <para />
      </section>

      <!-- end section -->

      <section id="Propertiespegasus.transfer.refiner">
        <title>pegasus.transfer.refiner</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>System:</entry>

                  <entry>Pegasus</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>enumeration</entry>
                </row>

                <row>
                  <entry>Value[0]:</entry>

                  <entry>Default</entry>
                </row>

                <row>
                  <entry>Value[1]:</entry>

                  <entry>Bundle</entry>
                </row>

                <row>
                  <entry>Value[2]:</entry>

                  <entry>Chain</entry>
                </row>

                <row>
                  <entry>Value[3]:</entry>

                  <entry>Condor</entry>
                </row>

                <row>
                  <entry>Value[4]:</entry>

                  <entry>Cluster</entry>
                </row>

                <row>
                  <entry>Default:</entry>

                  <entry>Bundle</entry>
                </row>

                <row>
                  <entry>Since:</entry>

                  <entry>2.0</entry>
                </row>

                <row>
                  <entry>See also:</entry>

                  <entry>pegasus.transfer.*.impl</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para>This property determines how the transfer nodes are added to the
        workflow. The various refiners differ in the how they link the various
        transfer jobs, and the number of transfer jobs that are created per
        compute jobs. <variablelist>
            <varlistentry>
              <term>Default</term>

              <listitem>This is the default refinement strategy for the
              transfer tools that can handle multiple file transfers per
              invocation. In this, all files required by a particular transfer
              job will be attempted to be transferred with just one transfer
              job. This also takes care of file clobbering while stageing in
              data to a remote grid site. File Clobbering can occur when two
              jobs scheduled on the same site require the same input file to
              be staged.</listitem>
            </varlistentry>

            <varlistentry>
              <term>Bundle</term>

              <listitem>In this refinement strategy, the number of stage in
              transfer nodes that are constructed per execution site can vary.
              The number of transfer nodes can be specified, by associating
              the pegasus profile "bundle.stagein". The profile can either be
              associated with the execution site in the site catalog or with
              the "transfer" executable in the transformation catalog. The
              value in the transformation catalog overrides the one in the
              site catalog. This refinement strategy extends from the Default
              refiner, and thus takes care of file clobbering while staging in
              data.</listitem>
            </varlistentry>

            <varlistentry>
              <term>Chain</term>

              <listitem>In this refinement strategy, chains of stagein
              transfer nodes are constructed. A chain means that the jobs are
              sequentially dependant upon each other i.e. at any moment, only
              one stage in transfer job will run per chain. The number of
              chains can be specified by associating the pegasus profile
              "chain.stagein". The profile can either be associated with the
              execution site in the site catalog or with the "transfer"
              executable in the transformation catalog. The value in the
              transformation catalog overrides the one in the site catalog.
              This refinement strategy extends from the Default refiner, and
              thus takes care of file clobbering while staging in
              data.</listitem>
            </varlistentry>

            <varlistentry>
              <term>Condor</term>

              <listitem>In this refinement strategy, no additional staging
              transfer jobs are added to the workflow. Instead the compute
              jobs are modified to have the transfer_input_files and
              transfer_output_files set to pull the input data. To stage-out
              the data a separate stage-out is added. The stage-out job is a
              /bin/true job that uses the transfer_input_file and
              transfer_output_files to stage the data back to the submit host.
              This refinement strategy is used workflows are being executed on
              a Condor pool, and the submit node itself is a part of the
              Condor pool.</listitem>
            </varlistentry>

            <varlistentry>
              <term>Cluster</term>

              <listitem>
                <para>In this refinement strategy, clusters of stage-in and
                stageout jobs are created per level of the workflow. It builds
                upon the Bundle refiner. The differences between the Bundle
                and Cluster refiner are as follows. <screen>
- stagein is also clustered/bundled per level. In Bundle it was
for the whole workflow.
- keys that control the clustering ( old name bundling are )
cluster.stagein and cluster.stageout
</screen> This refinement strategy also adds dependencies between the stagein
                transfer jobs on different levels of the workflow to ensure
                that stagein for the top level happens first and so on.</para>

                <para>An image of the workflow with this refinement strategy
                can be found at <screen>
http://vtcpc.isi.edu/pegasus/index.php/ChangeLog#Added_a_Cluster_Transfer_Refiner
</screen></para>
              </listitem>
            </varlistentry>
          </variablelist></para>

        <para />
      </section>

      <!-- end section -->

      <section id="Propertiespegasus.transfer.sls.*.impl">
        <title>pegasus.transfer.sls.*.impl</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>System:</entry>

                  <entry>Pegasus</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>enumeration</entry>
                </row>

                <row>
                  <entry>Value[0]:</entry>

                  <entry>Transfer3</entry>
                </row>

                <row>
                  <entry>Value[1]:</entry>

                  <entry>S3</entry>
                </row>

                <row>
                  <entry>Default:</entry>

                  <entry>Transfer3</entry>
                </row>

                <row>
                  <entry>Since:</entry>

                  <entry>2.2.0</entry>
                </row>

                <row>
                  <entry>See also:</entry>

                  <entry>pegasus.execute.*.filesystem.local</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para>This property specifies the transfer tool to be used for Second
        Level Staging (SLS) of input and output data between the head node and
        worker node filesystems.</para>

        <para>Currently, the * in the property name CANNOT be replaced to
        achieve finer grained control to dictate what type of SLS transfers
        need to be managed with which grid transfer tool.</para>

        <para>The various grid transfer tools that can be used to manage SLS
        data transfers are explained below</para>

        <para>
          <variablelist>
            <varlistentry>
              <term>Transfer3</term>

              <listitem>
                <para>This results in pegasus-transfer to be used for
                transferring of files. It is a python based wrapper around
                various transfer clients like globus-url-copy, lcg-copy, wget,
                cp, ln . pegasus-transfer looks at source and destination url
                and figures out automatically which underlying client to use.
                pegasus-transfer is distributed with the PEGASUS and can be
                found at $PEGASUS_HOME/bin/pegasus-transfer.</para>

                <para>For remote sites, Pegasus constructs the default path to
                pegasus-transfer on the basis of PEGASUS_HOME env profile
                specified in the site catalog. To specify a different path to
                the pegasus-transfer client , users can add an entry into the
                transformation catalog with fully qualified logical name as
                pegasus::pegasus-transfer</para>
              </listitem>
            </varlistentry>

            <varlistentry>
              <term>Condor</term>

              <listitem>
                <para>This results in Condor file transfer mechanism to be
                used to transfer the input data files from the submit host
                directly to the worker node directories. This is used when
                running in pure Condor mode or in a Condor pool that does not
                have a shared filesystem between the nodes.</para>

                <para>When setting the SLS transfers to Condor make sure that
                the following properties are also set <screen>
pegasus.transfer.refiner   Condor
pegasus.selector.replica   Local
pegasus.execute.*.filesystem.local true
</screen></para>

                <para>Also make sure that pegasus.gridstart is not set.</para>

                <para>Please refer to the section on "Condor Pool Without a
                Shared Filesystem" in the chapter on Planning and
                Submitting.</para>
              </listitem>
            </varlistentry>

            <varlistentry>
              <term>S3</term>

              <listitem>
                <para>This implementation refers to the s3cmd transfer client
                that is used for second level staging of data in the cloud.
                The data can be staged between the filesystem on the worker
                nodes and the workflow specific bucket on S3.</para>

                <para>There should be an entry in the transformation catalog
                with the fully qualified name as amazon::s3cmd for the site
                corresponding to the cloud.</para>
              </listitem>
            </varlistentry>
          </variablelist>
        </para>

        <para />
      </section>

      <!-- end section -->

      <section id="Propertiespegasus.transfer.arguments">
        <title>pegasus.transfer.arguments</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>System:</entry>

                  <entry>Pegasus</entry>
                </row>

                <row>
                  <entry>Since:</entry>

                  <entry>2.0</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>String</entry>
                </row>

                <row>
                  <entry>Default:</entry>

                  <entry>(no default)</entry>
                </row>

                <row>
                  <entry>See also:</entry>

                  <entry>pegasus.transfer.sls.arguments</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para>This determines the extra arguments with which the transfer
        implementation is invoked. The transfer executable that is invoked is
        dependant upon the transfer mode that has been selected. The property
        can be overloaded by associated the pegasus profile key
        transfer.arguments either with the site in the site catalog or the
        corresponding transfer executable in the transformation
        catalog.</para>

        <para />
      </section>

      <!-- end section -->

      <section id="Propertiespegasus.transfer.sls.arguments">
        <title>pegasus.transfer.sls.arguments</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>System:</entry>

                  <entry>Pegasus</entry>
                </row>

                <row>
                  <entry>Since:</entry>

                  <entry>2.4</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>String</entry>
                </row>

                <row>
                  <entry>Default:</entry>

                  <entry>(no default)</entry>
                </row>

                <row>
                  <entry>See also:</entry>

                  <entry>pegasus.transfer.arguments</entry>
                </row>

                <row>
                  <entry>See also:</entry>

                  <entry>pegasus.transfer.sls.*.impl</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para>This determines the extra arguments with which the SLS transfer
        implementation is invoked. The transfer executable that is invoked is
        dependant upon the SLS transfer implementation that has been
        selected.</para>

        <para />
      </section>

      <!-- end section -->

      <section id="Propertiespegasus.transfer.stage.sls.file">
        <title>pegasus.transfer.stage.sls.file</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>System:</entry>

                  <entry>Pegasus</entry>
                </row>

                <row>
                  <entry>Since:</entry>

                  <entry>3.0</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>Boolean</entry>
                </row>

                <row>
                  <entry>Default:</entry>

                  <entry>(no default)</entry>
                </row>

                <row>
                  <entry>See also:</entry>

                  <entry>pegasus.gridstart</entry>
                </row>

                <row>
                  <entry>See also:</entry>

                  <entry>pegasus.execute.*.filesystem.local</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para>For executing jobs on the local filesystem, Pegasus creates sls
        files for each compute jobs. These sls files list the files that need
        to be staged to the worker node and the output files that need to be
        pushed out from the worker node after completion of the job. By
        default, pegasus will stage these SLS files to the shared filesystem
        on the head node as part of first level data stagein jobs. However, in
        the case where there is no shared filesystem between head nodes and
        the worker nodes, the user can set this property to false. This will
        result in the sls files to be transferred using the Condor File
        Transfer from the submit host.</para>

        <para />
      </section>

      <!-- end section -->

      <section id="Propertiespegasus.transfer.worker.package">
        <title>pegasus.transfer.worker.package</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>System:</entry>

                  <entry>Pegasus</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>boolean</entry>
                </row>

                <row>
                  <entry>Default:</entry>

                  <entry>false</entry>
                </row>

                <row>
                  <entry>Since:</entry>

                  <entry>3.0</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para>By default, Pegasus relies on the worker package to be installed
        on the shared filesystem of the remote sites . Pegasus uses the value
        of PEGASUS_HOME environment profile in the site catalog for the remote
        sites, to then construct paths to pegasus auxillary executables like
        kickstart, pegasus-transfer, seqexec etc.</para>

        <para>If the Pegasus worker package is not installed on the remote
        sites users can set this property to true to get Pegasus to deploy
        worker package shared scratch directory for the workflow on the remote
        sites.</para>

        <para />
      </section>

      <!-- end section -->

      <section id="Propertiespegasus.transfer.links">
        <title>pegasus.transfer.links</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>System:</entry>

                  <entry>Pegasus</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>boolean</entry>
                </row>

                <row>
                  <entry>Default:</entry>

                  <entry>false</entry>
                </row>

                <row>
                  <entry>Since:</entry>

                  <entry>2.0</entry>
                </row>

                <row>
                  <entry>See also:</entry>

                  <entry>pegasus.transfer</entry>
                </row>

                <row>
                  <entry>See also:</entry>

                  <entry>pegasus.transfer.force</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para>If this is set, and the transfer implementation is set to
        Transfer i.e. using the transfer executable distributed with the
        PEGASUS. On setting this property, if Pegasus while fetching data from
        the RLS sees a pool attribute associated with the PFN that matches the
        execution pool on which the data has to be transferred to, Pegasus
        instead of the URL returned by the RLS replaces it with a file based
        URL. This supposes that the if the pools match the filesystems are
        visible to the remote execution directory where input data resides. On
        seeing both the source and destination urls as file based URLs the
        transfer executable spawns a job that creates a symbolic link by
        calling ln -s on the remote pool. This ends up bypassing the GridFTP
        server and reduces the load on it, and is much faster.</para>

        <para />
      </section>

      <!-- end section -->

      <section id="Propertiespegasus.transfer.*.remote.sites">
        <title>pegasus.transfer.*.remote.sites</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>System:</entry>

                  <entry>Pegasus</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>comma separated list of sites</entry>
                </row>

                <row>
                  <entry>Default:</entry>

                  <entry>no default</entry>
                </row>

                <row>
                  <entry>Since:</entry>

                  <entry>2.0</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para>By default Pegasus looks at the source and destination URL's for
        to determine whether the associated transfer job runs on the submit
        host or the head node of a remote site, with preference set to run a
        transfer job to run on submit host.</para>

        <para>Pegasus will run transfer jobs on the remote sites</para>

        <para>
          <screen>
-  if the file server for the compute site is a file server i.e url prefix file://
-  symlink jobs need to be added that require the symlink transfer jobs to
be run remotely.
</screen>
        </para>

        <para>This property can be used to change the default behaviour of
        Pegasus and force pegasus to run different types of transfer jobs for
        the sites specified on the remote site.</para>

        <para>The table below illustrates all the possible variations of the
        property.</para>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>Property Name</entry>

                  <entry>Applies to</entry>
                </row>

                <row>
                  <entry>pegasus.transfer.stagein.remote.sites</entry>

                  <entry>the stage in transfer jobs</entry>
                </row>

                <row>
                  <entry>pegasus.transfer.stageout.remote.sites</entry>

                  <entry>the stage out transfer jobs</entry>
                </row>

                <row>
                  <entry>pegasus.transfer.inter.remote.sites</entry>

                  <entry>the inter pool transfer jobs</entry>
                </row>

                <row>
                  <entry>pegasus.transfer.*.remote.sites</entry>

                  <entry>apply to types of transfer jobs</entry>
                </row>

                <row>
                  <entry />
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para>In addition * can be specified as a property value, to designate
        that it applies to all sites.</para>
      </section>

      <!-- end section -->

      <section id="Propertiespegasus.transfer.staging.delimiter">
        <title>pegasus.transfer.staging.delimiter</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>System:</entry>

                  <entry>Pegasus</entry>
                </row>

                <row>
                  <entry>Since:</entry>

                  <entry>2.0</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>String</entry>
                </row>

                <row>
                  <entry>Default:</entry>

                  <entry>:</entry>
                </row>

                <row>
                  <entry>See also:</entry>

                  <entry>pegasus.transformation.selector</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para />

        <para>Pegasus supports executable staging as part of the workflow.
        Currently staging of statically linked executables is supported only.
        An executable is normally staged to the work directory for the
        workflow/partition on the remote site. The basename of the staged
        executable is derived from the namespace,name and version of the
        transformation in the transformation catalog. This property sets the
        delimiter that is used for the construction of the name of the staged
        executable.</para>

        <para />
      </section>

      <!-- end section -->

      <section id="Propertiespegasus.transfer.disable.chmod.sites">
        <title>pegasus.transfer.disable.chmod.sites</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>System:</entry>

                  <entry>Pegasus</entry>
                </row>

                <row>
                  <entry>Since:</entry>

                  <entry>2.0</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>comma separated list of sites</entry>
                </row>

                <row>
                  <entry>Default:</entry>

                  <entry>no default</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para>During staging of executables to remote sites, chmod jobs are
        added to the workflow. These jobs run on the remote sites and do a
        chmod on the staged executable. For some sites, this maynot be
        required. The permissions might be preserved, or there maybe an
        automatic mechanism that does it.</para>

        <para>This property allows you to specify the list of sites, where you
        do not want the chmod jobs to be executed. For those sites, the chmod
        jobs are replaced by NoOP jobs. The NoOP jobs are executed by Condor,
        and instead will immediately have a terminate event written to the job
        log file and removed from the queue.</para>

        <para />
      </section>

      <!-- end section -->

      <section id="Propertiespegasus.transfer.proxy">
        <title>pegasus.transfer.proxy</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>System:</entry>

                  <entry>Pegasus</entry>
                </row>

                <row>
                  <entry>Since:</entry>

                  <entry>2.0</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>Boolean</entry>
                </row>

                <row>
                  <entry>Default:</entry>

                  <entry>false</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para>By default, CondorG transfers a limited proxy to the remote
        site, while running jobs in the grid universe. However, certain grid
        ftp servers (like those in front of SRB) require a fully user proxy.
        In this case, the planners need to transfer the proxy along with the
        transfer job using Condor file transfer mechanisms. This property
        triggers Pegasus into creating the appropriate condor commands, that
        transfer the proxy from the submit host to the remote host. The source
        location is determined from the value of the X509_USER_KEY env profile
        key , that is associated with site local in the site catalog.</para>

        <para />
      </section>

      <!-- end section -->

      <section id="Propertiespegasus.transfer.setup.source.base.url ">
        <title>pegasus.transfer.setup.source.base.url</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>System:</entry>

                  <entry>Pegasus</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>URL</entry>
                </row>

                <row>
                  <entry>Default:</entry>

                  <entry>no default</entry>
                </row>

                <row>
                  <entry>Since:</entry>

                  <entry>2.3</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para>This property specifies the base URL to the directory containing
        the Pegasus worker package builds. During Staging of Executable, the
        Pegasus Worker Package is also staged to the remote site. The worker
        packages are by default pulled from the http server at
        pegasus.isi.edu. This property can be used to override the location
        from where the worker package are staged. This maybe required if the
        remote computes sites don't allows files transfers from a http
        server.</para>
      </section>

      <!-- end section -->
    </section>

    <!-- end section -->

    <section id="PropertiesGridstartAndExitcodeProperties">
      <title>Gridstart And Exitcode Properties</title>

      <para />

      <para />

      <section id="Propertiespegasus.gridstart">
        <title>pegasus.gridstart</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>System:</entry>

                  <entry>Pegasus</entry>
                </row>

                <row>
                  <entry>Since:</entry>

                  <entry>2.0</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>enumeration</entry>
                </row>

                <row>
                  <entry>Value[0]:</entry>

                  <entry>Kickstart</entry>
                </row>

                <row>
                  <entry>Value[1]:</entry>

                  <entry>None</entry>
                </row>

                <row>
                  <entry>Value[2]:</entry>

                  <entry>SeqExec</entry>
                </row>

                <row>
                  <entry>Default:</entry>

                  <entry>Kickstart</entry>
                </row>

                <row>
                  <entry>See also:</entry>

                  <entry>pegasus.execute.*.filesystem.local</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para>Jobs that are launched on the grid maybe wrapped in a wrapper
        executable/script that enables information about about the execution,
        resource consumption, and - most importantly - the exitcode of the
        remote application. At present, a job scheduled on a remote site is
        launched with a gridstart if site catalog has the corresponding
        gridlaunch attribute set and the job being launched is not MPI.</para>

        <para>Users can explicitly decide what gridstart to use for a job, by
        associating the pegasus profile key named gridstart with the
        job.</para>

        <para>
          <variablelist>
            <varlistentry>
              <term>Kickstart</term>

              <listitem>In this mode, all the jobs are lauched via kickstart.
              The kickstart executable is a light-weight program which
              connects the stdin,stdout and stderr filehandles for PEGASUS
              jobs on the remote site. Kickstart is an executable distributed
              with PEGASUS that can generally be found at
              ${pegasus.home.bin}/kickstart.</listitem>
            </varlistentry>

            <varlistentry>
              <term>None</term>

              <listitem>In this mode, all the jobs are launched directly on
              the remote site. Each job's stdin,stdout and stderr are
              connected to condor commands in a manner to ensure that they are
              sent back to the submit host.</listitem>
            </varlistentry>

            <varlistentry>
              <term>SeqExec</term>

              <listitem>In this mode, all the jobs are launched on the remote
              site via SeqExec clustering executable. This is useful when user
              wants to run on the local filesystem of the worker nodes. The
              generated input file to seqexec contains commands to create the
              directory on the worker node, pull data from the head node,
              execute the job , push data to the head node, and remove the
              directory on the worker node. This is still an experimental mode
              and will be refined more for 3.1.</listitem>
            </varlistentry>
          </variablelist>
        </para>

        <para>Support for a new gridstart (K2) is expected to be added
        soon.</para>
      </section>

      <!-- end section -->

      <section id="Propertiespegasus.gridstart.kickstart.set.xbit">
        <title>pegasus.gridstart.kickstart.set.xbit</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>System:</entry>

                  <entry>Pegasus</entry>
                </row>

                <row>
                  <entry>Since:</entry>

                  <entry>2.4</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>Boolean</entry>
                </row>

                <row>
                  <entry>Default:</entry>

                  <entry>false</entry>
                </row>

                <row>
                  <entry>See also:</entry>

                  <entry>pegasus.transfer.disable.chmod.sites</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para />

        <para>Kickstart has an option to set the X bit on an executable before
        it launches it on the remote site. In case of staging of executables,
        by default chmod jobs are launched that set the x bit of the user
        executables staged to a remote site.</para>

        <para>On setting this property to true, kickstart gridstart module
        adds a -X option to kickstart arguments. The -X arguments tells
        kickstart to set the x bit of the executable before launching
        it.</para>

        <para>User should usually disable the chmod jobs by setting the
        property pegasus.transfer.disable.chmod.sites , if they set this
        property to true.</para>

        <para />
      </section>

      <!-- end section -->

      <section id="Propertiespegasus.gridstart.kickstart.stat">
        <title>pegasus.gridstart.kickstart.stat</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>System:</entry>

                  <entry>Pegasus</entry>
                </row>

                <row>
                  <entry>Since:</entry>

                  <entry>2.1</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>Boolean</entry>
                </row>

                <row>
                  <entry>Default:</entry>

                  <entry>false</entry>
                </row>

                <row>
                  <entry>See also:</entry>

                  <entry>pegasus.gridstart.generate.lof</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para>Kickstart has an option to stat the input files and the output
        files. The stat information is collected in the XML record generated
        by kickstart. Since stat is an expensive operation, it is not turned
        on by on. Set this property to true if you want to see stat
        information for the input files and output files of a job in it's
        kickstart output.</para>

        <para />
      </section>

      <!-- end section -->

      <section id="Propertiespegasus.gridstart.generate.lof">
        <title>pegasus.gridstart.generate.lof</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>System:</entry>

                  <entry>Pegasus</entry>
                </row>

                <row>
                  <entry>Since:</entry>

                  <entry>2.1</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>Boolean</entry>
                </row>

                <row>
                  <entry>Default:</entry>

                  <entry>false</entry>
                </row>

                <row>
                  <entry>See also:</entry>

                  <entry>pegasus.gridstart.kickstart.stat</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para>For the stat option for kickstart, we generate 2 lof ( list of
        filenames ) files for each job. One lof file containing the input
        lfn's for the job, and the other containing output lfn's for the job.
        In some cases, it maybe beneficial to have these lof files generated
        but not do the actual stat. This property allows you to generate the
        lof files without triggering the stat in kickstart invocations.</para>

        <para />
      </section>

      <!-- end section -->

      <section id="Propertiespegasus.gridstart.invoke.always">
        <title>pegasus.gridstart.invoke.always</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>System:</entry>

                  <entry>Pegasus</entry>
                </row>

                <row>
                  <entry>Since:</entry>

                  <entry>2.0</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>Boolean</entry>
                </row>

                <row>
                  <entry>Default:</entry>

                  <entry>false</entry>
                </row>

                <row>
                  <entry>See also:</entry>

                  <entry>pegasus.gridstart.invoke.length</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para>Condor has a limit in it, that restricts the length of arguments
        to an executable to 4K. To get around this limit, you can trigger
        Kickstart to be invoked with the -I option. In this case, an arguments
        file is prepared per job that is transferred to the remote end via the
        Condor file transfer mechanism. This way the arguments to the
        executable are not specified in the condor submit file for the job.
        This property specifies whether you want to use the invoke option
        always for all jobs, or want it to be triggered only when the argument
        string is determined to be greater than a certain limit.</para>

        <para />
      </section>

      <!-- end section -->

      <section id="Propertiespegasus.gridstart.invoke.length">
        <title>pegasus.gridstart.invoke.length</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>System:</entry>

                  <entry>Pegasus</entry>
                </row>

                <row>
                  <entry>Since:</entry>

                  <entry>2.0</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>Long</entry>
                </row>

                <row>
                  <entry>Default:</entry>

                  <entry>4000</entry>
                </row>

                <row>
                  <entry>See also:</entry>

                  <entry>pegasus.gridstart.invoke.always</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para>Gridstart is automatically invoked with the -I option, if it is
        determined that the length of the arguments to be specified is going
        to be greater than a certain limit. By default this limit is set to
        4K. However, it can overriden by specifying this property.</para>

        <para />
      </section>

      <!-- end section -->
    </section>

    <!-- end section -->

    <section id="PropertiesInterfaceToCondorAndCondorDagman">
      <title>Interface To Condor And Condor Dagman</title>

      <para>The Condor DAGMan facility is usually activate using the
      condor_submit_dag command. However, many shapes of workflows have the
      ability to either overburden the submit host, or overflow remote
      gatekeeper hosts. While DAGMan provides throttles, unfortunately these
      can only be supplied on the command-line. Thus,PEGASUS provides a
      versatile wrapper to invoke DAGMan, called pegasus-submit-dag. It can be
      configured from the command-line, from user- and system properties, and
      by defaults.</para>

      <section id="Propertiespegasus.condor.logs.symlink">
        <title>pegasus.condor.logs.symlink</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>System:</entry>

                  <entry>Condor</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>Boolean</entry>
                </row>

                <row>
                  <entry>Default:</entry>

                  <entry>true</entry>
                </row>

                <row>
                  <entry>Since:</entry>

                  <entry>3.0</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para />

        <para>By default pegasus has the Condor common log [dagname]-0.log in
        the submit file as a symlink to a location in /tmp . This is to ensure
        that condor common log does not get written to a shared filesystem. If
        the user knows for sure that the workflow submit directory is not on
        the shared filesystem, then they can opt to turn of the symlinking of
        condor common log file by setting this property to false.</para>

        <para />
      </section>

      <!-- end section -->

      <section id="Propertiespegasus.condor.arguments.quote">
        <title>pegasus.condor.arguments.quote</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>System:</entry>

                  <entry>Condor</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>Boolean</entry>
                </row>

                <row>
                  <entry>Default:</entry>

                  <entry>true</entry>
                </row>

                <row>
                  <entry>Since:</entry>

                  <entry>2.0</entry>
                </row>

                <row>
                  <entry>Old Name:</entry>

                  <entry>pegasus.condor.arguments.quote</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para>This property determines whether to apply the new Condor quoting
        rules for quoting the argument string. The new argument quoting rules
        appeared in Condor 6.7.xx series. We have verified it for 6.7.19
        version. If you are using an old condor at the submit host, set this
        property to false.</para>

        <para />
      </section>

      <!-- end section -->

      <section id="Propertiespegasus.dagman.nofity">
        <title>pegasus.dagman.nofity</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>System:</entry>

                  <entry>DAGman wrapper</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>Case-insensitive enumeration</entry>
                </row>

                <row>
                  <entry>Value[0]:</entry>

                  <entry>Complete</entry>
                </row>

                <row>
                  <entry>Value[1]:</entry>

                  <entry>Error</entry>
                </row>

                <row>
                  <entry>Value[2]:</entry>

                  <entry>Never</entry>
                </row>

                <row>
                  <entry>Default:</entry>

                  <entry>Error</entry>
                </row>

                <row>
                  <entry>Document:</entry>

                  <entry>http://www.cs.wisc.edu/condor/manual/v6.9/condor_submit_dag.html</entry>
                </row>

                <row>
                  <entry>Document:</entry>

                  <entry>http://www.cs.wisc.edu/condor/manual/v6.9/condor_submit.html</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para>The pegasus-submit-dag wrapper processes properties to set
        DAGMan commandline arguments. The argument sets the e-mail
        notification for DAGMan itself. This information will be used within
        the Condor submit description file for DAGMan. This file is produced
        by the the condor_submit_dag. See notification within the section of
        submit description file commands in the condor_submit manual page for
        specification of value. Many users prefer the value NEVER.</para>
      </section>

      <!-- end section -->

      <section id="Propertiespegasus.dagman.verbose">
        <title>pegasus.dagman.verbose</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>System:</entry>

                  <entry>DAGman wrapper</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>Boolean</entry>
                </row>

                <row>
                  <entry>Value[0]:</entry>

                  <entry>false</entry>
                </row>

                <row>
                  <entry>Value[1]:</entry>

                  <entry>true</entry>
                </row>

                <row>
                  <entry>Default:</entry>

                  <entry>false</entry>
                </row>

                <row>
                  <entry>Document:</entry>

                  <entry>http://www.cs.wisc.edu/condor/manual/v6.9/condor_submit_dag.html</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para />

        <para>The pegasus-submit-dag wrapper processes properties to set
        DAGMan commandline arguments. If set and true, the argument activates
        verbose output in case of DAGMan errors.</para>
      </section>

      <!-- end section -->

      <section id="Propertiespegasus.dagman.[category].maxjobs">
        <title>pegasus.dagman.[category].maxjobs</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>System:</entry>

                  <entry>DAGman wrapper</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>Integer</entry>
                </row>

                <row>
                  <entry>Since:</entry>

                  <entry>2.2</entry>
                </row>

                <row>
                  <entry>Default:</entry>

                  <entry>no default</entry>
                </row>

                <row>
                  <entry>Document:</entry>

                  <entry>http://vtcpc.isi.edu/pegasus/index.php/ChangeLog\#Support_for_DAGMan_node_categories</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para />

        <para>DAGMan now allows for the nodes in the DAG to be grouped in
        category. The tuning parameters like maxjobs then can be applied per
        category instead of being applied to the whole workflow. To use this
        facility users need to associate the dagman profile key named category
        with their jobs. The value of the key is the category to which the job
        belongs to.</para>

        <para>You can then use this property to specify the value for a
        category. For the above example you will set
        pegasus.dagman.short-running.maxjobs</para>
      </section>

      <!-- end section -->
    </section>

    <!-- end section -->

    <section id="PropertiesMonitoringProperties">
      <title>Monitoring Properties</title>

      <para />

      <para />

      <section id="Propertiespegasus.monitord.events">
        <title>pegasus.monitord.events</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>System:</entry>

                  <entry>Pegasus-monitord</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>Boolean</entry>
                </row>

                <row>
                  <entry>Default:</entry>

                  <entry>true</entry>
                </row>

                <row>
                  <entry>Since:</entry>

                  <entry>3.0.2</entry>
                </row>

                <row>
                  <entry>See Also:</entry>

                  <entry>pegasus.monitord.output</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para>This property determines whether pegasus-monitord generates log
        events. If log events are disabled using this property, no bp file, or
        database will be created, even if the pegasus.monitord.output property
        is specified.</para>

        <para />
      </section>

      <!-- end section -->

      <section id="Propertiespegasus.monitord.output">
        <title>pegasus.monitord.output</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>System:</entry>

                  <entry>Pegasus-monitord</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>String</entry>
                </row>

                <row>
                  <entry>Since:</entry>

                  <entry>3.0.2</entry>
                </row>

                <row>
                  <entry>See Also:</entry>

                  <entry>pegasus.monitord.events</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para>This property specifies the destination for generated log events
        in pegasus-monitord. By default, events are stored in a sqlite
        database in the workflow directory, which will be created with the
        workflow's name, and a ".stampede.db" extension. Users can specify an
        alternative database by using a SQLAlchemy connection string. Details
        are available at: <screen>
http://www.sqlalchemy.org/docs/05/reference/dialects/index.html
</screen></para>

        <note>
          <para>Users will need to have the appropriate db interface library
          installed. Which is to say, SQLAlchemy is a wrapper around the mysql
          interface library (for instance), it does not provide a MySQL driver
          itself. The Pegasus distribution includes both SQLAlchemy and the
          SQLite Python driver. As a final note, it is important to mention
          that unlike when using SQLite databases, using SQLAlchemy with other
          database servers, e.g. MySQL or Postgres , the target database needs
          to exist. Users can also specify a file name using this property in
          order to create a file with the log events.</para>
        </note>

        <para>Example values for the SQLAlchemy connection string for various
        end points are listed below</para>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>SQL Alchemy End Point</entry>

                  <entry>Example Value</entry>
                </row>

                <row>
                  <entry>Netlogger BP File</entry>

                  <entry>file:///submit/dir/myworkflow.bp</entry>
                </row>

                <row>
                  <entry>SQL Lite Database</entry>

                  <entry>sqlite:///submit/dir/myworkflow.db</entry>
                </row>

                <row>
                  <entry>MySQL Database</entry>

                  <entry>mysql://user:password@host:port/databasename</entry>
                </row>

                <row>
                  <entry />
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para />
      </section>

      <!-- end section -->
    </section>

    <!-- end section -->

    <section id="PropertiesJobClusteringProperties">
      <title>Job Clustering Properties</title>

      <para />

      <section id="Propertiespegasus.clusterer.job.aggregator">
        <title>pegasus.clusterer.job.aggregator</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>System:</entry>

                  <entry>Job Clustering</entry>
                </row>

                <row>
                  <entry>Since:</entry>

                  <entry>2.0</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>String</entry>
                </row>

                <row>
                  <entry>Value[0]:</entry>

                  <entry>seqexec</entry>
                </row>

                <row>
                  <entry>Value[1]:</entry>

                  <entry>mpiexec</entry>
                </row>

                <row>
                  <entry>Default:</entry>

                  <entry>seqexec</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para>A large number of workflows executed through the Virtual Data
        System, are composed of several jobs that run for only a few seconds
        or so. The overhead of running any job on the grid is usually 60
        seconds or more. Hence, it makes sense to collapse small independent
        jobs into a larger job. This property determines, the executable that
        will be used for running the larger job on the remote site.</para>

        <para>
          <variablelist>
            <varlistentry>
              <term>seqexec</term>

              <listitem>In this mode, the executable used to run the merged
              job is seqexec that runs each of the smaller jobs sequentially
              on the same node. The executable "seqexec" is a PEGASUS tool
              distributed in the PEGASUS worker package, and can be usually
              found at {pegasus.home}/bin/seqexec.</listitem>
            </varlistentry>

            <varlistentry>
              <term>mpiexec</term>

              <listitem>In this mode, the executable used to run the merged
              job is mpiexec that runs the smaller jobs via mpi on n nodes
              where n is the nodecount associated with the merged job. The
              executable "mpiexec" is a PEGASUS tool distributed in the
              PEGASUS worker package, and can be usually found at
              {pegasus.home}/bin/mpiexec.</listitem>
            </varlistentry>
          </variablelist>
        </para>

        <para />
      </section>

      <!-- end section -->

      <section id="Propertiespegasus.clusterer.job.aggregator.seqexec.log">
        <title>pegasus.clusterer.job.aggregator.seqexec.log</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>System:</entry>

                  <entry>Job Clustering</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>Boolean</entry>
                </row>

                <row>
                  <entry>Default:</entry>

                  <entry>false</entry>
                </row>

                <row>
                  <entry>Since:</entry>

                  <entry>2.3</entry>
                </row>

                <row>
                  <entry>See also:</entry>

                  <entry>pegasus.clusterer.job.aggregator</entry>
                </row>

                <row>
                  <entry>See also:</entry>

                  <entry>pegasus.clusterer.job.aggregator.seqexec.log.global</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para>Seqexec logs the progress of the jobs that are being run by it
        in a progress file on the remote cluster where it is executed.</para>

        <para>This property sets the Boolean flag, that indicates whether to
        turn on the logging or not.</para>

        <para />
      </section>

      <!-- end section -->

      <section id="Propertiespegasus.clusterer.job.aggregator.seqexec.log.global">
        <title>pegasus.clusterer.job.aggregator.seqexec.log.global</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>System:</entry>

                  <entry>Job Clustering</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>Boolean</entry>
                </row>

                <row>
                  <entry>Default:</entry>

                  <entry>true</entry>
                </row>

                <row>
                  <entry>Since:</entry>

                  <entry>2.3</entry>
                </row>

                <row>
                  <entry>See also:</entry>

                  <entry>pegasus.clusterer.job.aggregator</entry>
                </row>

                <row>
                  <entry>See also:</entry>

                  <entry>pegasus.clusterer.job.aggregator.seqexec.log</entry>
                </row>

                <row>
                  <entry>Old Name:</entry>

                  <entry>pegasus.clusterer.job.aggregator.seqexec.hasgloballog</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para>Seqexec logs the progress of the jobs that are being run by it
        in a progress file on the remote cluster where it is executed. The
        progress log is useful for you to track the progress of your
        computations and remote grid debugging. The progress log file can be
        shared by multiple seqexec jobs that are running on a particular
        cluster as part of the same workflow. Or it can be per job.</para>

        <para>This property sets the Boolean flag, that indicates whether to
        have a single global log for all the seqexec jobs on a particular
        cluster or progress log per job.</para>

        <para />
      </section>

      <!-- end section -->

      <section id="Propertiespegasus.clusterer.job.aggregator.seqexec.firstjobfail">
        <title>pegasus.clusterer.job.aggregator.seqexec.firstjobfail</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>System:</entry>

                  <entry>Job Clustering</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>Boolean</entry>
                </row>

                <row>
                  <entry>Default:</entry>

                  <entry>true</entry>
                </row>

                <row>
                  <entry>Since:</entry>

                  <entry>2.2</entry>
                </row>

                <row>
                  <entry>See also:</entry>

                  <entry>pegasus.clusterer.job.aggregator</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para>By default seqexec does not stop execution even if one of the
        clustered jobs it is executing fails. This is because seqexec tries to
        get as much work done as possible.</para>

        <para>This property sets the Boolean flag, that indicates whether to
        make seqexec stop on the first job failure it detects.</para>

        <para />
      </section>

      <!-- end section -->

      <section id="Propertiespegasus.clusterer.label.key">
        <title>pegasus.clusterer.label.key</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>System:</entry>

                  <entry>Job Clustering</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>String</entry>
                </row>

                <row>
                  <entry>Default:</entry>

                  <entry>label</entry>
                </row>

                <row>
                  <entry>Since:</entry>

                  <entry>2.0</entry>
                </row>

                <row>
                  <entry>See also:</entry>

                  <entry>pegasus.partitioner.label.key</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para>While clustering jobs in the workflow into larger jobs, you can
        optionally label your graph to control which jobs are clustered and to
        which clustered job they belong. This done using a label based
        clustering scheme and is done by associating a profile/label key in
        the PEGASUS namespace with the jobs in the DAX. Each job that has the
        same value/label value for this profile key, is put in the same
        clustered job.</para>

        <para>This property allows you to specify the PEGASUS profile key that
        you want to use for label based clustering.</para>

        <para />
      </section>

      <!-- end section -->
    </section>

    <!-- end section -->

    <section id="PropertiesLoggingProperties">
      <title>Logging Properties</title>

      <para />

      <section id="Propertiespegasus.log.manager">
        <title>pegasus.log.manager</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>System:</entry>

                  <entry>Pegasus</entry>
                </row>

                <row>
                  <entry>Since:</entry>

                  <entry>2.2.0</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>Enumeration</entry>
                </row>

                <row>
                  <entry>Value[0]:</entry>

                  <entry>Default</entry>
                </row>

                <row>
                  <entry>Value[1]:</entry>

                  <entry>Log4j</entry>
                </row>

                <row>
                  <entry>Default:</entry>

                  <entry>Default</entry>
                </row>

                <row>
                  <entry>See also:</entry>

                  <entry>pegasus.log.manager.formatter</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para>This property sets the logging implementation to use for
        logging.</para>

        <para>
          <variablelist>
            <varlistentry>
              <term>Default</term>

              <listitem>This implementation refers to the legacy Pegasus
              logger, that logs directly to stdout and stderr. It however,
              does have the concept of levels similar to log4j or
              syslog.</listitem>
            </varlistentry>

            <varlistentry>
              <term>Log4j</term>

              <listitem>This implementation, uses Log4j to log messages. The
              log4j properties can be specified in a properties file, the
              location of which is specified by the property <screen>
pegasus.log.manager.log4j.conf
</screen></listitem>
            </varlistentry>
          </variablelist>
        </para>

        <para />
      </section>

      <!-- end section -->

      <section id="Propertiespegasus.log.manager.formatter">
        <title>pegasus.log.manager.formatter</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>System:</entry>

                  <entry>Pegasus</entry>
                </row>

                <row>
                  <entry>Since:</entry>

                  <entry>2.2.0</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>Enumeration</entry>
                </row>

                <row>
                  <entry>Value[0]:</entry>

                  <entry>Simple</entry>
                </row>

                <row>
                  <entry>Value[1]:</entry>

                  <entry>Netlogger</entry>
                </row>

                <row>
                  <entry>Default:</entry>

                  <entry>Simple</entry>
                </row>

                <row>
                  <entry>See also:</entry>

                  <entry>pegasus.log.manager.formatter</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para>This property sets the formatter to use for formatting the log
        messages while logging.</para>

        <para>
          <variablelist>
            <varlistentry>
              <term>Simple</term>

              <listitem>This formats the messages in a simple format. The
              messages are logged as is with minimal formatting. Below are
              sample log messages in this format while ranking a dax according
              to performance. <screen>
event.pegasus.ranking dax.id se18-gda.dax  - STARTED
event.pegasus.parsing.dax dax.id se18-gda-nested.dax  - STARTED
event.pegasus.parsing.dax dax.id se18-gda-nested.dax  - FINISHED
job.id jobGDA
job.id jobGDA query.name getpredicted performace time 10.00
event.pegasus.ranking dax.id se18-gda.dax  - FINISHED
</screen></listitem>
            </varlistentry>

            <varlistentry>
              <term>Netlogger</term>

              <listitem>
                <para>This formats the messages in the Netlogger format , that
                is based on key value pairs. The netlogger format is useful
                for loading the logs into a database to do some meaningful
                analysis. Below are sample log messages in this format while
                ranking a dax according to performance. <screen>
ts=2008-09-06T12:26:20.100502Z event=event.pegasus.ranking.start \
msgid=6bc49c1f-112e-4cdb-af54-3e0afb5d593c \
eventId=event.pegasus.ranking_8d7c0a3c-9271-4c9c-a0f2-1fb57c6394d5 \
dax.id=se18-gda.dax prog=Pegasus
ts=2008-09-06T12:26:20.100750Z event=event.pegasus.parsing.dax.start \
msgid=fed3ebdf-68e6-4711-8224-a16bb1ad2969 \
eventId=event.pegasus.parsing.dax_887134a8-39cb-40f1-b11c-b49def0c5232\
dax.id=se18-gda-nested.dax prog=Pegasus
ts=2008-09-06T12:26:20.100894Z event=event.pegasus.parsing.dax.end \
msgid=a81e92ba-27df-451f-bb2b-b60d232ed1ad \
eventId=event.pegasus.parsing.dax_887134a8-39cb-40f1-b11c-b49def0c5232
ts=2008-09-06T12:26:20.100395Z event=event.pegasus.ranking \
msgid=4dcecb68-74fe-4fd5-aa9e-ea1cee88727d \
eventId=event.pegasus.ranking_8d7c0a3c-9271-4c9c-a0f2-1fb57c6394d5 \
job.id="jobGDA"
ts=2008-09-06T12:26:20.100395Z event=event.pegasus.ranking \
msgid=4dcecb68-74fe-4fd5-aa9e-ea1cee88727d \
eventId=event.pegasus.ranking_8d7c0a3c-9271-4c9c-a0f2-1fb57c6394d5 \
job.id="jobGDA" query.name="getpredicted performace" time="10.00"
ts=2008-09-06T12:26:20.102003Z event=event.pegasus.ranking.end \
msgid=31f50f39-efe2-47fc-9f4c-07121280cd64 \
eventId=event.pegasus.ranking_8d7c0a3c-9271-4c9c-a0f2-1fb57c6394d5
</screen></para>
              </listitem>
            </varlistentry>
          </variablelist>
        </para>

        <para />
      </section>

      <!-- end section -->

      <section id="Propertiespegasus.log.* ">
        <title>pegasus.log.*</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>System:</entry>

                  <entry>Pegasus</entry>
                </row>

                <row>
                  <entry>Since:</entry>

                  <entry>2.0</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>String</entry>
                </row>

                <row>
                  <entry>Default:</entry>

                  <entry>No default</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para>This property sets the path to the file where all the logging
        for Pegasus can be redirected to. Both stdout and stderr are logged to
        the file specified.</para>

        <para />
      </section>

      <!-- end section -->

      <section id="Propertiespegasus.log.metrics">
        <title>pegasus.log.metrics</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>System:</entry>

                  <entry>Pegasus</entry>
                </row>

                <row>
                  <entry>Since:</entry>

                  <entry>2.1.0</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>Boolean</entry>
                </row>

                <row>
                  <entry>Default:</entry>

                  <entry>true</entry>
                </row>

                <row>
                  <entry>See also:</entry>

                  <entry>pegasus.log.metrics.file</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para>This property enables the logging of certain planning and
        workflow metrics to a global log file. By default the file to which
        the metrics are logged is ${pegasus.home}/var/pegasus.log.</para>

        <para />
      </section>

      <!-- end section -->

      <section id="Propertiespegasus.log.metrics.file">
        <title>pegasus.log.metrics.file</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>System:</entry>

                  <entry>Pegasus</entry>
                </row>

                <row>
                  <entry>Since:</entry>

                  <entry>2.1.0</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>Boolean</entry>
                </row>

                <row>
                  <entry>Default:</entry>

                  <entry>${pegasus.home}/var/pegasus.log</entry>
                </row>

                <row>
                  <entry>See also:</entry>

                  <entry>pegasus.log.metrics</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para>This property determines the file to which the workflow and
        planning metrics are logged if enabled.</para>

        <para />
      </section>

      <!-- end section -->
    </section>

    <!-- end section -->

    <section id="PropertiesMiscellaneousProperties">
      <title>Miscellaneous Properties</title>

      <para />

      <section id="Propertiespegasus.code.generator">
        <title>pegasus.code.generator</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>System:</entry>

                  <entry>Pegasus</entry>
                </row>

                <row>
                  <entry>Since:</entry>

                  <entry>3.0</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>enumeration</entry>
                </row>

                <row>
                  <entry>Value[0]:</entry>

                  <entry>Condor</entry>
                </row>

                <row>
                  <entry>Value[1]:</entry>

                  <entry>Shell</entry>
                </row>

                <row>
                  <entry>Default:</entry>

                  <entry>Condor</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para>This property is used to load the appropriate Code Generator to
        use for writing out the executable workflow.</para>

        <para>
          <variablelist>
            <varlistentry>
              <term>Condor</term>

              <listitem>This is the default code generator for Pegasus . This
              generator generates the executable workflow as a Condor DAG file
              and associated job submit files. The Condor DAG file is passed
              as input to Condor DAGMan for job execution.</listitem>
            </varlistentry>

            <varlistentry>
              <term>Shell</term>

              <listitem>This Code Generator generates the executable workflow
              as a shell script that can be executed on the submit host. While
              using this code generator, all the jobs should be mapped to site
              local i.e specify --sites local to pegasus-plan.</listitem>
            </varlistentry>
          </variablelist>
        </para>

        <para />
      </section>

      <!-- end section -->

      <section id="Propertiespegasus.file.cleanup.strategy">
        <title>pegasus.file.cleanup.strategy</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>System:</entry>

                  <entry>Pegasus</entry>
                </row>

                <row>
                  <entry>Since:</entry>

                  <entry>2.2</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>enumeration</entry>
                </row>

                <row>
                  <entry>Value[0]:</entry>

                  <entry>InPlace</entry>
                </row>

                <row>
                  <entry>Default:</entry>

                  <entry>InPlace</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para>This property is used to select the strategy of how the the
        cleanup nodes are added to the executable workflow.</para>

        <para>
          <variablelist>
            <varlistentry>
              <term>InPlace</term>

              <listitem>This is the only mode available .</listitem>
            </varlistentry>
          </variablelist>
        </para>

        <para />
      </section>

      <!-- end section -->

      <section id="Propertiespegasus.file.cleanup.impl">
        <title>pegasus.file.cleanup.impl</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>System:</entry>

                  <entry>Pegasus</entry>
                </row>

                <row>
                  <entry>Since:</entry>

                  <entry>2.2</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>enumeration</entry>
                </row>

                <row>
                  <entry>Value[0]:</entry>

                  <entry>Cleanup</entry>
                </row>

                <row>
                  <entry>Value[1]:</entry>

                  <entry>RM</entry>
                </row>

                <row>
                  <entry>Value[2]:</entry>

                  <entry>S3</entry>
                </row>

                <row>
                  <entry>Default:</entry>

                  <entry>Cleanup</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para>This property is used to select the executable that is used to
        create the working directory on the compute sites.</para>

        <para>
          <variablelist>
            <varlistentry>
              <term>Cleanup</term>

              <listitem>The default executable that is used to delete files is
              the dirmanager executable shipped with Pegasus. It is found at
              $PEGASUS_HOME/bin/dirmanager in the pegasus distribution. An
              entry for transformation pegasus::dirmanager needs to exist in
              the Transformation Catalog or the PEGASUS_HOME environment
              variable should be specified in the site catalog for the sites
              for this mode to work.</listitem>
            </varlistentry>

            <varlistentry>
              <term>RM</term>

              <listitem>This mode results in the rm executable to be used to
              delete files from remote directories. The rm executable is
              standard on *nix systems and is usually found at /bin/rm
              location.</listitem>
            </varlistentry>

            <varlistentry>
              <term>S3</term>

              <listitem>This mode is used to delete files/objects from the
              buckets in S3 instead of a directory. This should be set when
              running workflows on Amazon EC2. This implementation relies on
              s3cmd command line client to create the bucket. An entry for
              transformation amazon::s3cmd needs to exist in the
              Transformation Catalog for this to work.</listitem>
            </varlistentry>
          </variablelist>
        </para>

        <para />
      </section>

      <!-- end section -->

      <section id="Propertiespegasus.file.cleanup.scope">
        <title>pegasus.file.cleanup.scope</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>System:</entry>

                  <entry>Pegasus</entry>
                </row>

                <row>
                  <entry>Since:</entry>

                  <entry>2.3.0</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>enumeration</entry>
                </row>

                <row>
                  <entry>Value[0]:</entry>

                  <entry>fullahead</entry>
                </row>

                <row>
                  <entry>Value[1]:</entry>

                  <entry>deferred</entry>
                </row>

                <row>
                  <entry>Default:</entry>

                  <entry>fullahead</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para>By default in case of deferred planning InPlace file cleanup is
        turned OFF. This is because the cleanup algorithm does not work across
        partitions. This property can be used to turn on the cleanup in case
        of deferred planning. <variablelist>
            <varlistentry>
              <term>fullahead</term>

              <listitem>This is the default scope. The pegasus cleanup
              algorithm does not work across partitions in deferred planning.
              Hence the cleanup is always turned OFF , when deferred planning
              occurs and cleanup scope is set to full ahead.</listitem>
            </varlistentry>

            <varlistentry>
              <term>deferred</term>

              <listitem>If the scope is set to deferred, then Pegasus will not
              disable file cleanup in case of deferred planning. This is
              useful for scenarios where the partitions themselves are
              independant ( i.e. dont share files ). Even if the scope is set
              to deferred, users can turn off cleanup by specifying
              --nocleanup option to pegasus-plan.</listitem>
            </varlistentry>
          </variablelist></para>

        <para />
      </section>

      <!-- end section -->

      <section id="Propertiespegasus.catalog.transformation.mapper">
        <title>pegasus.catalog.transformation.mapper</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>System:</entry>

                  <entry>Staging of Executables</entry>
                </row>

                <row>
                  <entry>Since:</entry>

                  <entry>2.0</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>enumeration</entry>
                </row>

                <row>
                  <entry>Value[0]:</entry>

                  <entry>All</entry>
                </row>

                <row>
                  <entry>Value[1]:</entry>

                  <entry>Installed</entry>
                </row>

                <row>
                  <entry>Value[2]:</entry>

                  <entry>Staged</entry>
                </row>

                <row>
                  <entry>Value[3]:</entry>

                  <entry>Submit</entry>
                </row>

                <row>
                  <entry>Default:</entry>

                  <entry>All</entry>
                </row>

                <row>
                  <entry>See also:</entry>

                  <entry>pegasus.transformation.selector</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para>Pegasus now supports transfer of statically linked executables
        as part of the concrete workflow. At present, there is only support
        for staging of executables referred to by the compute jobs specified
        in the DAX file. Pegasus determines the source locations of the
        binaries from the transformation catalog, where it searches for
        entries of type STATIC_BINARY for a particular architecture type. The
        PFN for these entries should refer to a globus-url-copy valid and
        accessible remote URL. For transfer of executables, Pegasus constructs
        a soft state map that resides on top of the transformation catalog,
        that helps in determining the locations from where an executable can
        be staged to the remote site.</para>

        <para>This property determines, how that map is created. <variablelist>
            <varlistentry>
              <term>All</term>

              <listitem>In this mode, all sources with entries of type
              STATIC_BINARY for a particular transformation are considered
              valid sources for the transfer of executables. This the most
              general mode, and results in the constructing the map as a
              result of the cartesian product of the matches.</listitem>
            </varlistentry>

            <varlistentry>
              <term>Installed</term>

              <listitem>In this mode, only entries that are of type INSTALLED
              are used while constructing the soft state map. This results in
              Pegasus never doing any transfer of executables as part of the
              workflow. It always prefers the installed executables at the
              remote sites.</listitem>
            </varlistentry>

            <varlistentry>
              <term>Staged</term>

              <listitem>In this mode, only entries that are of type
              STATIC_BINARY are used while constructing the soft state map.
              This results in the concrete workflow referring only to the
              staged executables, irrespective of the fact that the
              executables are already installed at the remote end.</listitem>
            </varlistentry>

            <varlistentry>
              <term>Submit</term>

              <listitem>In this mode, only entries that are of type
              STATIC_BINARY and reside at the submit host (pool local), are
              used while constructing the soft state map. This is especially
              helpful, when the user wants to use the latest compute code for
              his computations on the grid and that relies on his submit
              host.</listitem>
            </varlistentry>
          </variablelist></para>
      </section>

      <!-- end section -->

      <section id="Propertiespegasus.selector.transformation">
        <title>pegasus.selector.transformation</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>System:</entry>

                  <entry>Staging of Executables</entry>
                </row>

                <row>
                  <entry>Since:</entry>

                  <entry>2.0</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>enumeration</entry>
                </row>

                <row>
                  <entry>Value[0]:</entry>

                  <entry>Random</entry>
                </row>

                <row>
                  <entry>Value[1]:</entry>

                  <entry>Installed</entry>
                </row>

                <row>
                  <entry>Value[2]:</entry>

                  <entry>Staged</entry>
                </row>

                <row>
                  <entry>Value[3]:</entry>

                  <entry>Submit</entry>
                </row>

                <row>
                  <entry>Default:</entry>

                  <entry>Random</entry>
                </row>

                <row>
                  <entry>See also:</entry>

                  <entry>pegasus.catalog.transformation</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para />

        <para>In case of transfer of executables, Pegasus could have various
        transformations to select from when it schedules to run a particular
        compute job at a remote site. For e.g it can have the choice of
        staging an executable from a particular remote pool, from the local
        (submit host) only, use the one that is installed on the remote site
        only.</para>

        <para>This property determines, how a transformation amongst the
        various candidate transformations is selected, and is applied after
        the property pegasus.tc has been applied. For e.g specifying
        pegasus.tc as Staged and then pegasus.transformation.selector as
        INSTALLED does not work, as by the time this property is applied, the
        soft state map only has entries of type STAGED.</para>

        <para>
          <variablelist>
            <varlistentry>
              <term>Random</term>

              <listitem>In this mode, a random matching candidate
              transformation is selected to be staged to the remote execution
              pool.</listitem>
            </varlistentry>

            <varlistentry>
              <term>Installed</term>

              <listitem>In this mode, only entries that are of type INSTALLED
              are selected. This means that the concrete workflow only refers
              to the transformations already pre installed on the remote
              pools.</listitem>
            </varlistentry>

            <varlistentry>
              <term>Staged</term>

              <listitem>In this mode, only entries that are of type
              STATIC_BINARY are selected, ignoring the ones that are installed
              at the remote site.</listitem>
            </varlistentry>

            <varlistentry>
              <term>Submit</term>

              <listitem>In this mode, only entries that are of type
              STATIC_BINARY and reside at the submit host (pool local), are
              selected as sources for staging the executables to the remote
              execution pools.</listitem>
            </varlistentry>
          </variablelist>
        </para>
      </section>

      <!-- end section -->

      <section id="Propertiespegasus.execute.*.filesystem.local">
        <title>pegasus.execute.*.filesystem.local</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>System:</entry>

                  <entry>Pegasus</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>Boolean</entry>
                </row>

                <row>
                  <entry>Default:</entry>

                  <entry>false</entry>
                </row>

                <row>
                  <entry>Since:</entry>

                  <entry>2.1.0</entry>
                </row>

                <row>
                  <entry>See also:</entry>

                  <entry>pegasus.gridstart</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para />

        <para>Normally, Pegasus transfers the data to and from a directory on
        the shared filesystem on the head node of a compute site. The
        directory needs to be visible to both the head node and the worker
        nodes for the compute jobs to execute correctly.</para>

        <para>By setting this property to true, you can get Pegasus to execute
        jobs on the worker node filesystem. In this case, when the jobs are
        launched on the worker nodes, the jobs grab the input data from the
        workflow specific execution directory on the compute site and push the
        output data to the same directory after completion. The transfer of
        data to and from the worker node directory is referred to as Second
        Level Staging ( SLS ).</para>

        <para>It is recommended that a user when setting this property to
        true, also sets the following property, unless the user is running in
        a Condor Pool without a shared filesystem and is relying on Condor to
        transfer the files. <screen>
pegasus.gridstart    SeqExec
</screen></para>

        <para />
      </section>

      <!-- end section -->

      <section id="Propertiespegasus.parser.dax.preserver.linebreaks">
        <title>pegasus.parser.dax.preserver.linebreaks</title>

        <para>
          <informaltable frame="none">
            <tgroup align="left" cols="2" colsep="1" rowsep="1">
              <tbody>
                <row>
                  <entry>System:</entry>

                  <entry>Pegasus</entry>
                </row>

                <row>
                  <entry>Type:</entry>

                  <entry>Boolean</entry>
                </row>

                <row>
                  <entry>Default:</entry>

                  <entry>false</entry>
                </row>

                <row>
                  <entry>Since:</entry>

                  <entry>2.2.0</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
        </para>

        <para>The DAX Parser normally does not preserve line breaks while
        parsing the CDATA section that appears in the arguments section of the
        job element in the DAX. On setting this to true, the DAX Parser
        preserves any line line breaks that appear in the CDATA
        section.</para>
      </section>

      <!-- end section -->
    </section>

    <!-- end section -->
  </section>

  <section id="profiles">
    <title>Profiles</title>

    <para>The Pegasus Workflow Mapper uses the concept of profiles to
    encapsulate configurations for various aspects of dealing with the Grid
    infrastructure. Profiles provide an abstract yet uniform interface to
    specify configuration options for various layers from planner/mapper
    behavior to remote environment settings. At various stages during the
    mapping process, profiles may be added associated with the job.</para>

    <para>This document describes various types of profiles, levels of
    priorities for intersecting profiles, and how to specify profiles in
    different contexts.</para>

    <section>
      <title>Profile Structure Heading</title>

      <para>All profiles are triples comprised of a namespace, a name or key,
      and a value. The namespace is a simple identifier. The key has only
      meaning within its namespace, and it&amp;rsquor;s yet another
      identifier. There are no constraints on the contents of a value</para>

      <para>Profiles may be represented with different syntaxes in different
      context. However, each syntax will describe the underlying
      triple.</para>
    </section>

    <section>
      <title>Profile Namespaces</title>

      <para>Each namespace refers to a different aspect of a job&amp;rsquor;s
      runtime settings. A profile&amp;rsquor;s representation in the concrete
      plan (e.g. the Condor submit files) depends its namespace. Pegasus
      supports the following Namespaces for profiles:</para>

      <itemizedlist>
        <listitem>
          <para><emphasis role="bold">env</emphasis> permits remote
          environment variables to be set.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">globus</emphasis> sets Globus RSL
          parameters.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">condor</emphasis> sets Condor
          configuration parameters for the submit file.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">dagman</emphasis> introduces Condor
          DAGMan configuration parameters.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">pegasus</emphasis> configures the
          behaviour of various planner/mapper components.</para>
        </listitem>
      </itemizedlist>

      <section>
        <title>The env Profile Namespace</title>

        <para>The <emphasis>env</emphasis> namespace allows users to specify
        environment variables of remote jobs. Globus transports the
        environment variables, and ensure that they are set before the job
        starts.</para>

        <para>The key used in conjunction with an <emphasis>env</emphasis>
        profile denotes the name of the environment variable. The value of the
        profile becomes the value of the remote environment variable.</para>

        <para>Grid jobs usually only set a minimum of environment variables by
        virtue of Globus. You cannot compare the environment variables visible
        from an interactive login with those visible to a grid job. Thus, it
        often becomes necessary to set environment variables like
        LD_LIBRARY_PATH for remote jobs.</para>

        <para>If you use any of the Pegasus worker package tools like transfer
        or the rc-client, it becomes necessary to set PEGASUS_HOME and
        GLOBUS_LOCATION even for jobs that run locally</para>

        <table>
          <title>Table 1: Useful Environment Settings</title>

          <tgroup cols="2">
            <tbody>
              <row>
                <entry><emphasis role="bold">Environment
                Variable</emphasis></entry>

                <entry><emphasis role="bold">Description</emphasis></entry>
              </row>

              <row>
                <entry>PEGASUS_HOME</entry>

                <entry>Used by auxillary jobs created by Pegasus both on
                remote site and local site. Should be set usually set in the
                Site Catalog for the sites</entry>
              </row>

              <row>
                <entry>GLOBUS_LOCATION</entry>

                <entry>Used by auxillary jobs created by Pegasus both on
                remote site and local site. Should be set usually set in the
                Site Catalog for the sites</entry>
              </row>

              <row>
                <entry>LD_LIBRARY_PATH</entry>

                <entry>Point this to $GLOBUS_LOCATION/lib, except you cannot
                use the dollar variable. You must use the full path. Applies
                to both, local and remote jobs that use Globus components and
                should be usually set in the site catalog for the
                sites</entry>
              </row>
            </tbody>
          </tgroup>
        </table>

        <para>Even though Condor and Globus both permit environment variable
        settings through their profiles, all remote environment variables must
        be set through the means of <emphasis>env</emphasis> profiles.</para>
      </section>

      <section>
        <title>The Globus Profile Namespace</title>

        <para>The <emphasis>globus</emphasis> profile namespace encapsulates
        Globus resource specification language (RSL) instructions. The RSL
        configures settings and behavior of the remote scheduling system. Some
        systems require queue name to schedule jobs, a project name for
        accounting purposes, or a run-time estimate to schedule jobs. The
        Globus RSL addresses all these issues.</para>

        <para>A key in the <emphasis>globus</emphasis> namespace denotes the
        command name of an RLS instruction. The profile value becomes the RSL
        value. Even though Globus RSL is typically shown using parentheses
        around the instruction, the out pair of parentheses is not necessary
        in globus profile specifications</para>

        <para>Table 2 shows some commonly used RSL instructions. For an
        authoritative list of all possible RSL instructions refer to the
        Globus RSL specification.</para>

        <table>
          <title>Table 2: Useful Globus RSL Instructions</title>

          <tgroup cols="2">
            <tbody>
              <row>
                <entry><emphasis role="bold">Key</emphasis></entry>

                <entry><emphasis role="bold">Description</emphasis></entry>
              </row>

              <row>
                <entry>count</entry>

                <entry>the number of times an executable is started.</entry>
              </row>

              <row>
                <entry>jobtype</entry>

                <entry>specifies how the job manager should start the remote
                job. While Pegasus defaults to single, use mpi when running
                MPI jobs.</entry>
              </row>

              <row>
                <entry>maxcputime</entry>

                <entry>the max cpu time for a single execution of a
                job.</entry>
              </row>

              <row>
                <entry>maxmemory</entry>

                <entry>the maximum memory in MB required for the job</entry>
              </row>

              <row>
                <entry>maxtime</entry>

                <entry>the maximum time or walltime for a single execution of
                a job.</entry>
              </row>

              <row>
                <entry>maxwalltime</entry>

                <entry>the maximum walltime for a single execution of a
                job.</entry>
              </row>

              <row>
                <entry>minmemory</entry>

                <entry>the minumum amount of memory required for this
                job</entry>
              </row>

              <row>
                <entry>project</entry>

                <entry>associates an account with a job at the remote
                end.</entry>
              </row>

              <row>
                <entry>queue</entry>

                <entry>the remote queue in which the job should be run. Used
                when remote scheduler is PBS that supports queues.</entry>
              </row>
            </tbody>
          </tgroup>
        </table>

        <para>Pegasus prevents the user from specifying certain RSL
        instructions as globus profiles, because they are either automatically
        generated or can be overridden through some different means. For
        instance, if you need to specify remote environment settings, do not
        use the environment key in the globus profiles. Use one or more env
        profiles instead.</para>

        <table>
          <title>Table 3: RSL Instructions that are not permissible</title>

          <tgroup cols="2">
            <tbody>
              <row>
                <entry><emphasis role="bold">Key</emphasis></entry>

                <entry><emphasis role="bold">Reason for
                Prohibition</emphasis></entry>
              </row>

              <row>
                <entry>arguments</entry>

                <entry>you specify arguments in the arguments section for a
                job in the DAX</entry>
              </row>

              <row>
                <entry>directory</entry>

                <entry>the site catalog and properties determine which
                directory a job will run in.</entry>
              </row>

              <row>
                <entry>environment</entry>

                <entry>use multiple env profiles instead</entry>
              </row>

              <row>
                <entry>executable</entry>

                <entry>the physical executable to be used is specified in the
                transformation catalog and is also dependant on the gridstart
                module being used. If you are launching jobs via kickstart
                then the executable created is the path to kickstart and the
                application executable path appears in the arguments for
                kickstart</entry>
              </row>

              <row>
                <entry>stdin</entry>

                <entry>you specify in the DAX for the job</entry>
              </row>

              <row>
                <entry>stdout</entry>

                <entry>you specify in the DAX for the job</entry>
              </row>

              <row>
                <entry>stderr</entry>

                <entry>you specify in the DAX for the job</entry>
              </row>
            </tbody>
          </tgroup>
        </table>
      </section>

      <section>
        <title>The Condor Profile Namespace</title>

        <para>The Condor submit file controls every detail how and where a job
        is run. The <emphasis>condor</emphasis> profiles permit to add or
        overwrite instructions in the Condor submit file.</para>

        <para>The <emphasis>condor</emphasis> namespace directly sets commands
        in the Condor submit file for a job the profile applies to. Keys in
        the <emphasis>condor</emphasis> profile namespace denote the name of
        the Condor command. The profile value becomes the command's argument.
        All <emphasis>condor</emphasis> profiles are translated into key=value
        lines in the Condor submit file</para>

        <para>Some of the common condor commands that a user may need to
        specify are listed below. For an authoritative list refer to the
        online condor documentation. Note: Pegasus Workflow Planner/Mapper by
        default specify a lot of condor commands in the submit files depending
        upon the job, and where it is being run.</para>

        <table>
          <title>Table 4: Useful Condor Commands</title>

          <tgroup cols="2">
            <tbody>
              <row>
                <entry><emphasis role="bold">Key</emphasis></entry>

                <entry><emphasis role="bold">Description</emphasis></entry>
              </row>

              <row>
                <entry>universe</entry>

                <entry>Pegasus defaults to either globus or scheduler
                universes. Set to standard for compute jobs that require
                standard universe. Set to vanilla to run natively in a condor
                pool, or to run on resources grabbed via condor
                glidein.</entry>
              </row>

              <row>
                <entry>periodic_release</entry>

                <entry>is the number of times job is released back to the
                queue if it goes to HOLD, e.g. due to Globus errors. Pegasus
                defaults to 3.</entry>
              </row>

              <row>
                <entry>periodic_remove</entry>

                <entry>is the number of times a job is allowed to get into
                HOLD state before being removed from the queue. Pegasus
                defaults to 3.</entry>
              </row>

              <row>
                <entry>filesystemdomain</entry>

                <entry>Useful for Condor glide-ins to pin a job to a remote
                site.</entry>
              </row>

              <row>
                <entry>stream_error</entry>

                <entry>boolean to turn on the streaming of the stderr of the
                remote job back to submit host.</entry>
              </row>

              <row>
                <entry>stream_output</entry>

                <entry>boolean to turn on the streaming of the stdout of the
                remote job back to submit host.</entry>
              </row>

              <row>
                <entry>priority</entry>

                <entry>integer value to assign the priority of a job. Higher
                value means higher priority. The priorities are only applied
                for vanilla / standard/ local universe jobs. Determines the
                order in which a users own jobs are executed.</entry>
              </row>
            </tbody>
          </tgroup>
        </table>

        <para>Other useful condor keys, that advanced users may find useful
        and can be set by profiles are</para>

        <orderedlist>
          <listitem>
            <para>should_transfer_files</para>
          </listitem>

          <listitem>
            <para>transfer_output</para>
          </listitem>

          <listitem>
            <para>transfer_error</para>
          </listitem>

          <listitem>
            <para>whentotransferoutput</para>
          </listitem>

          <listitem>
            <para>requirements</para>
          </listitem>

          <listitem>
            <para>rank</para>
          </listitem>
        </orderedlist>

        <para>Pegasus prevents the user from specifying certain Condor
        commands in condor profiles, because they are automatically generated
        or can be overridden through some different means. Table 5 shows
        prohibited Condor commands.</para>

        <table>
          <title>Table 5: Condor commands prohibited in condor
          profiles</title>

          <tgroup cols="2">
            <tbody>
              <row>
                <entry><emphasis role="bold">Key</emphasis></entry>

                <entry><emphasis role="bold">Reason for
                Prohibition</emphasis></entry>
              </row>

              <row>
                <entry>arguments</entry>

                <entry>you specify arguments in the arguments section for a
                job in the DAX</entry>
              </row>

              <row>
                <entry>environment</entry>

                <entry>use multiple env profiles instead</entry>
              </row>

              <row>
                <entry>executable</entry>

                <entry>the physical executable to be used is specified in the
                transformation catalog and is also dependant on the gridstart
                module being used. If you are launching jobs via kickstart
                then the executable created is the path to kickstart and the
                application executable path appears in the arguments for
                kickstart</entry>
              </row>
            </tbody>
          </tgroup>
        </table>
      </section>

      <section>
        <title>The Dagman Profile Namespace</title>

        <para>DAGMan is Condor's workflow manager. While planners generate
        most of DAGMan's configuration, it is possible to tweak certain
        job-related characteristics using dagman profiles. A dagman profile
        can be used to specify a DAGMan pre- or post-script.</para>

        <para>Pre- and post-scripts execute on the submit machine. Both
        inherit the environment settings from the submit host when
        pegasus-submit-dag or pegasus-run is invoked.</para>

        <para>By default, kickstart launches all jobs except standard universe
        and MPI jobs. Kickstart tracks the execution of the job, and returns
        usage statistics for the job. A DAGMan post-script starts the Pegasus
        application exitcode to determine, if the job succeeded. DAGMan
        receives the success indication as exit status from exitcode.</para>

        <para>If you need to run your own post-script, you have to take over
        the job success parsing. The planner is set up to pass the file name
        of the remote job's stdout, usually the output from kickstart, as sole
        argument to the post-script.</para>

        <para>Table 6 shows the keys in the dagman profile domain that are
        understood by Pegasus and can be associated at a per job basis.</para>

        <para><table>
            <title>Table 6: Useful dagman Commands that can be associated at a
            per job basis</title>

            <tgroup cols="2">
              <tbody>
                <row>
                  <entry><emphasis role="bold">Key</emphasis></entry>

                  <entry><emphasis role="bold">Description</emphasis></entry>
                </row>

                <row>
                  <entry>PRE</entry>

                  <entry>is the path to the pre-script. DAGMan executes the
                  pre-script before it runs the job.</entry>
                </row>

                <row>
                  <entry>PRE.ARGUMENTS</entry>

                  <entry>are command-line arguments for the pre-script, if
                  any.</entry>
                </row>

                <row>
                  <entry>POST</entry>

                  <entry>is the postscript type/mode that a user wants to
                  associate with a job. <orderedlist>
                      <listitem>
                        <para><emphasis
                        role="bold">pegasus-exitcode</emphasis> - pegasus will
                        by default associate this postscript with all jobs
                        launched via kickstart, as long the POST.SCOPE value
                        is not set to NONE.</para>
                      </listitem>

                      <listitem>
                        <para><emphasis role="bold">none</emphasis> -means
                        that no postscript is generated for the jobs. This is
                        useful for MPI jobs that are not launched via
                        kickstart currently.</para>
                      </listitem>

                      <listitem>
                        <para><emphasis role="bold">any legal
                        identifier</emphasis> - Any other identifier of the
                        form ([_A-Za-z][_A-Za-z0-9]*), than one of the 2
                        reserved keywords above, signifies a user postscript.
                        This allows the user to specify their own postscript
                        for the jobs in the workflow. The path to the
                        postscript can be specified by the dagman profile
                        <emphasis role="bold">POST.PATH.[value</emphasis>]
                        where [value] is this legal identifier specified. The
                        user postscript is passed the name of the .out file of
                        the job as the last argument on the command
                        line.</para>

                        <para>For e.g. if the following dagman profiles were
                        associated with a job X</para>

                        <orderedlist>
                          <listitem>
                            <para>POST with value user_script
                            /bin/user_postscript</para>
                          </listitem>

                          <listitem>
                            <para>POST.PATH.user_script with value
                            /path/to/user/script</para>
                          </listitem>

                          <listitem>
                            <para>POST.ARGUMENTS with value -verbose</para>
                          </listitem>
                        </orderedlist>

                        <para>then the following postscript will be associated
                        with the job X in the .dag file</para>

                        <para>/path/to/user/script -verbose X.out where X.out
                        contains the stdout of the job X</para>
                      </listitem>
                    </orderedlist></entry>
                </row>

                <row>
                  <entry>POST.PATH.* ( where * is replaced by the value of the
                  POST Profile )</entry>

                  <entry>the path to the post script on the submit
                  host.</entry>
                </row>

                <row>
                  <entry>POST.ARGUMENTS</entry>

                  <entry>are the command line arguments for the post script,
                  if any.</entry>
                </row>

                <row>
                  <entry>RETRY</entry>

                  <entry>is the number of times DAGMan retries the full job
                  cycle from pre-script through post-script, if failure was
                  detected.</entry>
                </row>

                <row>
                  <entry>CATEGORY</entry>

                  <entry>the DAGMan category the job belongs to.</entry>
                </row>

                <row>
                  <entry>PRIORITY</entry>

                  <entry>the priority to apply to a job. DAGMan uses this to
                  select what jobs to release when MAXJOBS is enforced for the
                  DAG.</entry>
                </row>
              </tbody>
            </tgroup>
          </table></para>

        <para></para>

        <para>Table 7 shows the keys in the dagman profile domain that are
        understood by Pegasus and can be used to apply to the whole workflow.
        These are used to control DAGMan's behavior at the workflow level, and
        are recommended to be specified in the properties file.</para>

        <table>
          <title>Table 7: Useful dagman Commands that can be specified in the
          properties file.</title>

          <tgroup cols="2">
            <tbody>
              <row>
                <entry><emphasis role="bold">Key</emphasis></entry>

                <entry><emphasis role="bold">Description</emphasis></entry>
              </row>

              <row>
                <entry>MAXPRE</entry>

                <entry>sets the maximum number of PRE scripts within the DAG
                that may be running at one time</entry>
              </row>

              <row>
                <entry>MAXPOST</entry>

                <entry>sets the maximum number of PRE scripts within the DAG
                that may be running at one time</entry>
              </row>

              <row>
                <entry>MAXJOBS</entry>

                <entry>sets the maximum number of jobs within the DAG that
                will be submitted to Condor at one time.</entry>
              </row>

              <row>
                <entry>MAXIDLE</entry>

                <entry>sets the maximum number of idle jobs within the DAG
                that will be submitted to Condor at one time.</entry>
              </row>

              <row>
                <entry>[CATEGORY-NAME].MAXJOBS</entry>

                <entry>is the value of maxjobs for a particular category.
                Users can associate different categories to the jobs at a per
                job basis. However, the value of a dagman knob for a category
                can only be specified at a per workflow basis in the
                properties.</entry>
              </row>

              <row>
                <entry>POST.SCOPE</entry>

                <entry>scope for the postscripts. <orderedlist>
                    <listitem>
                      <para>If set to <emphasis role="bold">all</emphasis> ,
                      means each job in the workflow will have a postscript
                      associated with it.</para>
                    </listitem>

                    <listitem>
                      <para>If set to <emphasis role="bold">none</emphasis> ,
                      means no job has postscript associated with it. None
                      mode should be used if you are running vanilla /
                      standard/ local universe jobs, as in those cases Condor
                      traps the remote exitcode correctly. None scope is not
                      recommended for grid universe jobs.</para>
                    </listitem>

                    <listitem>
                      <para>If set to <emphasis
                      role="bold">essential</emphasis>, means only essential
                      jobs have post scripts associated with them. At present
                      the only non essential job is the replica registration
                      job.</para>
                    </listitem>
                  </orderedlist></entry>
              </row>
            </tbody>
          </tgroup>
        </table>
      </section>

      <section>
        <title>The Pegasus Profile Namespace</title>

        <para>The <emphasis>pegasus</emphasis> profiles allow users to
        configure extra options to the Pegasus Workflow Planner that can be
        applied selectively to a job or a group of jobs. Site selectors may
        use a sub-set of <emphasis>pegasus</emphasis> profiles for their
        decision-making.</para>

        <para>Table 8 shows some of the useful configuration option Pegasus
        understands.</para>

        <table>
          <title>Table 8: Useful pegasus Profiles.</title>

          <tgroup cols="2">
            <tbody>
              <row>
                <entry><emphasis role="bold">Key</emphasis></entry>

                <entry><emphasis role="bold">Description</emphasis></entry>
              </row>

              <row>
                <entry>workdir</entry>

                <entry>Sets the remote initial dir for a Condor-G job.
                Overrides the work directory algorithm that uses the site
                catalog and properties.</entry>
              </row>

              <row>
                <entry>clusters.num</entry>

                <entry>Please refer to the Pegasus Clustering Guide for
                detailed description. This option determines the total number
                of clusters per level. Jobs are evenly spread across
                clusters.</entry>
              </row>

              <row>
                <entry>clusters.size</entry>

                <entry>Please refer to the Pegasus Clustering Guide for
                detailed description. This profile determines the number of
                jobs in each cluster. The number of clusters depends on the
                total number of jobs on the level.</entry>
              </row>

              <row>
                <entry>collapser</entry>

                <entry>Indicates the clustering executable that is used to run
                the clustered job on the remote site.</entry>
              </row>

              <row>
                <entry>gridstart</entry>

                <entry>Determines the executable for launching a job. Possible
                values are <emphasis role="bold"><emphasis>Kickstart |
                NoGridStart</emphasis></emphasis> at the moment.</entry>
              </row>

              <row>
                <entry>gridstart.path</entry>

                <entry>Sets the path to the gridstart . This profile is best
                set in the Site Catalog.</entry>
              </row>

              <row>
                <entry>gridstart.arguments</entry>

                <entry>Sets the arguments with which GridStart is used to
                launch a job on the remote site.</entry>
              </row>

              <row>
                <entry>stagein.clusters</entry>

                <entry>This key determines the maximum number of
                <emphasis>stage-in</emphasis> jobs that are can executed
                locally or remotely per compute site per workflow. This is
                used to configure the <emphasis>Bundle</emphasis> Transfer
                Refiner, which is the Default Refiner used in Pegasus. This
                profile is best set in the Site Catalog or in the Properties
                file</entry>
              </row>

              <row>
                <entry>stagein.local.clusters</entry>

                <entry>This key provides finer grained control in determining
                the number of stage-in jobs that are executed locally and are
                responsible for staging data to a particular remote site. This
                profile is best set in the Site Catalog or in the Properties
                file</entry>
              </row>

              <row>
                <entry>stagein.remote.clusters</entry>

                <entry>This key provides finer grained control in determining
                the number of stage-in jobs that are executed remotely on the
                remote site and are responsible for staging data to it. This
                profile is best set in the Site Catalog or in the Properties
                file</entry>
              </row>

              <row>
                <entry>stageout.clusters</entry>

                <entry>This key determines the maximum number of
                <emphasis>stage-out</emphasis> jobs that are can executed
                locally or remotely per compute site per workflow. This is
                used to configure the <emphasis>Bundle</emphasis> Transfer
                Refiner, , which is the Default Refiner used in
                Pegasus.</entry>
              </row>

              <row>
                <entry>stageout.local.clusters</entry>

                <entry>This key provides finer grained control in determining
                the number of stage-out jobs that are executed locally and are
                responsible for staging data from a particular remote site.
                This profile is best set in the Site Catalog or in the
                Properties file</entry>
              </row>

              <row>
                <entry>stageout.remote.clusters</entry>

                <entry>This key provides finer grained control in determining
                the number of stage-out jobs that are executed remotely on the
                remote site and are responsible for staging data from it. This
                profile is best set in the Site Catalog or in the Properties
                file</entry>
              </row>

              <row>
                <entry>group</entry>

                <entry>Tags a job with an arbitrary group identifier. The
                group site selector makes use of the tag.</entry>
              </row>

              <row>
                <entry>change.dir</entry>

                <entry>If true, tells <emphasis>kickstart</emphasis> to change
                into the remote working directory. Kickstart itself is
                executed in whichever directory the remote scheduling system
                chose for the job.</entry>
              </row>

              <row>
                <entry>create.dir</entry>

                <entry>If true, tells <emphasis>kickstart</emphasis> to create
                the the remote working directory before changing into the
                remote working directory. Kickstart itself is executed in
                whichever directory the remote scheduling system chose for the
                job.</entry>
              </row>

              <row>
                <entry>transfer.proxy</entry>

                <entry>If true, tells Pegasus to explicitly transfer the proxy
                for transfer jobs to the remote site. This is useful, when you
                want to use a full proxy at the remote end, instead of the
                limited proxy that is transferred by CondorG.</entry>
              </row>

              <row>
                <entry>transfer.arguments</entry>

                <entry>Allows the user to specify the arguments with which the
                transfer executable is invoked. However certain options are
                always generated for the transfer executable(base-uri
                se-mount-point).</entry>
              </row>

              <row>
                <entry>style</entry>

                <entry>Sets the condor submit file style. If set to globus,
                submit file generated refers to CondorG job submissions. If
                set to condor, submit file generated refers to direct Condor
                submission to the local Condor pool. It applies for glidein,
                where nodes from remote grid sites are glided into the local
                condor pool. The default style that is applied is
                globus.</entry>
              </row>
            </tbody>
          </tgroup>
        </table>
      </section>
    </section>

    <section>
      <title>Sources for Profiles</title>

      <para>Profiles may enter the job-processing stream at various stages.
      Depending on the requirements and scope a profile is to apply, profiles
      can be associated at</para>

      <itemizedlist>
        <listitem>
          <para>as user property settings.</para>
        </listitem>

        <listitem>
          <para>dax level</para>
        </listitem>

        <listitem>
          <para>in the site catalog</para>
        </listitem>

        <listitem>
          <para>in the transformation catalog</para>
        </listitem>
      </itemizedlist>

      <para>Unfortunately, a different syntax applies to each level and
      context. This section shows the different profile sources and syntaxes.
      However, at the foundation of each profile lies the triple of namespace,
      key and value.</para>

      <section>
        <title>User Profiles in Properties</title>

        <para>Users can specify all profiles in the properties files where the
        property name is <emphasis role="bold">[namespace].key</emphasis> and
        <emphasis role="bold">value</emphasis> of the property is the value of
        the profile.</para>

        <para>Namespace can be env|condor|globus|dagman|pegasus</para>

        <para>Any profile specified as a property applies to the whole
        workflow unless overridden at the DAX level , Site Catalog ,
        Transformation Catalog Level.</para>

        <para>Some profiles that they can be set in the properties file are
        listed below</para>

        <programlisting>env.JAVA_HOME "/software/bin/java"

condor.periodic_release 5
condor.periodic_remove  my_own_expression
condor.stream_error true
condor.stream_output fa

globus.maxwalltime  1000
globus.maxtime      900
globus.maxcputime   10
globus.project      test_project
globus.queue        main_queue

dagman.post.arguments --test arguments
dagman.retry  4
dagman.post simple_exitcode
dagman.post.path.simple_exitcode  /bin/exitcode/exitcode.sh
dagman.post.scope all
dagman.maxpre  12
dagman.priority 13

dagman.bigjobs.maxjobs 1


pegasus.clusters.size 5

pegasus.stagein.clusters 3</programlisting>
      </section>

      <section>
        <title>Profiles in DAX</title>

        <para>The user can associate profiles with logical transformations in
        DAX. Environment settings required by a job's application, or a
        maximum estimate on the run-time are examples for profiles at this
        stage.</para>

        <programlisting>&lt;job id="ID000001" namespace="asdf" name="preprocess" version="1.0"
 level="3" dv-namespace="voeckler" dv-name="top" dv-version="1.0"&gt;
  &lt;argument&gt;-a top -T10  -i &lt;filename file="voeckler.f.a"/&gt;
 -o &lt;filename file="voeckler.f.b1"/&gt;
 &lt;filename file="voeckler.f.b2"/&gt;&lt;/argument&gt;
  <emphasis role="bold">&lt;profile namespace="pegasus" key="walltime"&gt;2&lt;/profile&gt;
  &lt;profile namespace="pegasus" key="diskspace"&gt;1&lt;/profile&gt;</emphasis>
  &amp;mldr;
&lt;/job&gt;
</programlisting>
      </section>

      <section>
        <title>Profiles in Site Catalog</title>

        <para>If it becomes necessary to limit the scope of a profile to a
        single site, these profiles should go into the site catalog. A profile
        in the site catalog applies to all jobs and all application run at the
        site. Commonly, site catalog profiles set environment settings like
        the LD_LIBRARY_PATH, or globus rsl parameters like queue and project
        names.</para>

        <para>Currently, there is no tool to manipulate the site catalog, e.g.
        by adding profiles. Modifying the site catalog requires that you load
        it into your editor.</para>

        <para>The XML version of the site catalog uses the following
        syntax:</para>

        <programlisting><emphasis role="bold">&lt;profile namespace=</emphasis>"<emphasis>namespace</emphasis>" <emphasis
            role="bold">key=</emphasis>"<emphasis>key</emphasis>"&gt;<emphasis>value</emphasis><emphasis
            role="bold">&lt;/profile&gt;</emphasis></programlisting>

        <para>The XML schema requires that profiles are the first children of
        a pool element. If the element ordering is wrong, the XML parser will
        produce errors and warnings:</para>

        <programlisting>&lt;pool handle="isi_condor" gridlaunch="/home/shared/pegasus/bin/kickstart"&gt;
  <emphasis role="bold">&lt;profile namespace="env"
   key="GLOBUS_LOCATION"&gt;/home/shared/globus/&lt;/profile&gt;
  &lt;profile namespace="env"
   key="LD_LIBRARY_PATH" &gt;/home/shared/globus/lib&lt;/profile&gt;</emphasis>
  &lt;lrc url="rls://sukhna.isi.edu" /&gt;
  &amp;mldr;
&lt;/pool&gt;
</programlisting>

        <para>The multi-line textual version of the site catalog uses the
        following syntax:</para>

        <programlisting><emphasis role="bold">profile</emphasis> <emphasis>namespace "key" "value"</emphasis></programlisting>

        <para>The order within the textual pool definition is not important.
        Profiles can appear anywhere:</para>

        <programlisting>pool isi_condor {
  gridlaunch "/home/shared/pegasus/bin/kickstart"
  <emphasis role="bold">profile env "GLOBUS_LOCATION" "/home/shared/globus"
  profile env "LD_LIBRARY_PATH" "/home/shared/globus/lib"</emphasis>
  &amp;mldr;
}
</programlisting>
      </section>

      <section>
        <title>Profiles in Transformation Catalog</title>

        <para>Some profiles require a narrower scope than the site catalog
        offers. Some profiles only apply to certain applications on certain
        sites, or change with each application and site.
        Transformation-specific and CPU-specific environment variables, or job
        clustering profiles are good candidates. Such profiles are best
        specified in the transformation catalog.</para>

        <para>Profiles associate with a physical transformation and site in
        the transformation catalog. The Database version of the transformation
        catalog also permits the convenience of connecting a transformation
        with a profile.</para>

        <para>The Pegasus tc-client tool is a convenient helper to associate
        profiles with transformation catalog entries. As benefit, the user
        does not have to worry about formats of profiles in the various
        transformation catalog instances.</para>

        <programlisting>tc-client -a -P -E -p /home/shared/executables/analyze -t INSTALLED -r isi_condor -e env::GLOBUS_LOCATION=&amp;rdquor;/home/shared/globus&amp;rdquor;</programlisting>

        <para>The above example adds an environment variable GLOBUS_LOCATION
        to the application /home/shared/executables/analyze on site
        isi_condor. The transformation catalog guide has more details on the
        usage of the tc-client.</para>
      </section>
    </section>

    <section>
      <title>Profiles Conflict Resolution</title>

      <para>Irrespective of where the profiles are specified, eventually the
      profiles are associated with jobs. Multiple sources may specify the same
      profile for the same job. For instance, DAX may specify an environment
      variable X. The site catalog may also specify an environment variable X
      for the chosen site. The transformation catalog may specify an
      environment variable X for the chosen site and application. When the job
      is concretized, these three conflicts need to be resolved.</para>

      <para>Pegasus defines a priority ordering of profiles. The higher
      priority takes precedence (overwrites) a profile of a lower
      priority.</para>

      <orderedlist>
        <listitem>
          <para>Transformation Catalog Profiles</para>
        </listitem>

        <listitem>
          <para>Site Catalog Profiles</para>
        </listitem>

        <listitem>
          <para>DAX Profiles</para>
        </listitem>

        <listitem>
          <para>Profiles in Properties</para>
        </listitem>
      </orderedlist>
    </section>

    <section>
      <title>Details of Profile Handling</title>

      <para>The previous sections omitted some of the finer details for the
      sake of clarity. To understand some of the constraints that Pegasus
      imposes, it is required to look at the way profiles affect jobs.</para>

      <section>
        <title>Details of env Profiles</title>

        <para>Profiles in the env namespace are translated to a
        semicolon-separated list of key-value pairs. The list becomes the
        argument for the Condor environment command in the job's submit
        file.</para>

        <programlisting>######################################################################
# Pegasus WMS  SUBMIT FILE GENERATOR
# DAG : black-diamond, Index = 0, Count = 1
# SUBMIT FILE NAME : findrange_ID000002.sub
######################################################################
globusrsl = (jobtype=single)
<emphasis role="bold">environment=GLOBUS_LOCATION=/shared/globus;LD_LIBRARY_PATH=/shared/globus/lib;</emphasis>
executable = /shared/software/linux/pegasus/default/bin/kickstart
globusscheduler = columbus.isi.edu/jobmanager-condor
remote_initialdir = /shared/CONDOR/workdir/isi_hourglass
universe = globus
&amp;mldr;
queue
######################################################################
# END OF SUBMIT FILE
</programlisting>

        <para>Condor-G, in turn, will translate the
        <emphasis>environment</emphasis> command for any remote job into
        Globus RSL environment settings, and append them to any existing RSL
        syntax it generates. To permit proper mixing, all
        <emphasis>environment</emphasis> setting should solely use the env
        profiles, and none of the Condor nor Globus environment
        settings.</para>

        <para>If <emphasis>kickstart</emphasis> starts a job, it may make use
        of environment variables in its executable and arguments
        setting.</para>
      </section>

      <section>
        <title>Details of globus Profiles</title>

        <para>Profiles in the <emphasis>globus</emphasis> Namespaces are
        translated into a list of paranthesis-enclosed equal-separated
        key-value pairs. The list becomes the value for the Condor
        <emphasis>globusrsl</emphasis> setting in the job's submit
        file:</para>

        <programlisting>######################################################################
# Pegasus WMS SUBMIT FILE GENERATOR
# DAG : black-diamond, Index = 0, Count = 1
# SUBMIT FILE NAME : findrange_ID000002.sub
######################################################################
<emphasis role="bold">globusrsl = (jobtype=single)(queue=fast)(project=nvo)</emphasis>
executable = /shared/software/linux/pegasus/default/bin/kickstart
globusscheduler = columbus.isi.edu/jobmanager-condor
remote_initialdir = /shared/CONDOR/workdir/isi_hourglass
universe = globus
&amp;mldr;
queue
######################################################################
# END OF SUBMIT FILE
</programlisting>

        <para>For this reason, Pegasus prohibits the use of the
        <emphasis>globusrsl</emphasis> key in the <emphasis>condor</emphasis>
        profile namespace.</para>
      </section>
    </section>
  </section>

  <section id="replica_selection">
    <title>Replica Selection</title>

    <para>Each job in the DAX maybe associated with input LFN&amp;rsquor;s
    denoting the files that are required for the job to run. To determine the
    physical replica (PFN) for a LFN, Pegasus queries the Replica catalog to
    get all the PFN&amp;rsquor;s (replicas) associated with a LFN. The Replica
    Catalog may return multiple PFN's for each of the LFN's queried. Hence,
    Pegasus needs to select a single PFN amongst the various PFN's returned
    for each LFN. This process is known as replica selection in Pegasus. Users
    can specify the replica selector to use in the properties file.</para>

    <para>This document describes the various Replica Selection Strategies in
    Pegasus.</para>

    <section>
      <title>Configuration</title>

      <para>The user properties determine what replica selector Pegasus
      Workflow Mapper uses. The property <emphasis
      role="bold">pegasus.selector.replica</emphasis> is used to specify the
      replica selection strategy. Currently supported Replica Selection
      strategies are</para>

      <orderedlist>
        <listitem>
          <para>Default</para>
        </listitem>

        <listitem>
          <para>Restricted</para>
        </listitem>

        <listitem>
          <para>Regex</para>
        </listitem>
      </orderedlist>

      <para>The values are case sensitive. For example the following property
      setting will throw a Factory Exception .</para>

      <programlisting>pegasus.selector.replica  default</programlisting>

      <para>The correct way to specify is</para>

      <programlisting>pegasus.selector.replica  Default</programlisting>
    </section>

    <section>
      <title>Supported Replica Selectors</title>

      <para>The various Replica Selectors supported in Pegasus Workflow Mapper
      are explained below</para>

      <section>
        <title>Default</title>

        <para>This is the default replica selector used in the Pegasus
        Workflow Mapper. If the property pegasus.selector.replica is not
        defined in properties, then Pegasus uses this selector.</para>

        <para>This selector looks at each PFN returned for a LFN and checks to
        see if</para>

        <orderedlist>
          <listitem>
            <para>the PFN is a file URL (starting with file:///)</para>
          </listitem>

          <listitem>
            <para>the PFN has a pool attribute matching to the site handle of
            the site where the compute job that requires the input file is to
            be run.</para>
          </listitem>
        </orderedlist>

        <para>If a PFN matching the conditions above exists then that is
        returned by the selector .</para>

        <para><emphasis role="bold">Else,</emphasis> a random PFN is selected
        amongst all the PFN&amp;rsquor;s that have a pool attribute matching
        to the site handle of the site where a compute job is to be
        run.</para>

        <para><emphasis role="bold">Else,</emphasis> a random pfn is selected
        amongst all the PFN&amp;rsquor;s</para>

        <para>To use this replica selector set the following
        property<programlisting>pegasus.selector.replica                  Default</programlisting></para>
      </section>

      <section>
        <title>Restricted</title>

        <para>This replica selector, allows the user to specify good sites and
        bad sites for staging in data to a particular compute site. A good
        site for a compute site X, is a preferred site from which replicas
        should be staged to site X. If there are more than one good sites
        having a particular replica, then a random site is selected amongst
        these preferred sites.</para>

        <para>A bad site for a compute site X, is a site from which
        replica&amp;rsquor;s should not be staged. The reason of not accessing
        replica from a bad site can vary from the link being down, to the user
        not having permissions on that site&amp;rsquor;s data.</para>

        <para>The good | bad sites are specified by the following
        properties</para>

        <programlisting>pegasus.replica.*.prefer.stagein.sites
pegasus.replica.*.ignore.stagein.sites</programlisting>

        <para>where the * in the property name denotes the name of the compute
        site. A * in the property key is taken to mean all sites. The value to
        these properties is a comma separated list of sites.</para>

        <para>For example the following settings</para>

        <programlisting>pegasus.selector.replica.*.prefer.stagein.sites            usc
pegasus.replica.uwm.prefer.stagein.sites                   isi,cit
</programlisting>

        <para>means that prefer all replicas from site usc for staging in to
        any compute site. However, for uwm use a tighter constraint and prefer
        only replicas from site isi or cit. The pool attribute associated with
        the PFN's tells the replica selector to what site a replica/PFN is
        associated with.</para>

        <para>The pegasus.replica.*.prefer.stagein.sites property takes
        precedence over pegasus.replica.*.ignore.stagein.sites property i.e.
        if for a site X, a site Y is specified both in the ignored and the
        preferred set, then site Y is taken to mean as only a preferred site
        for a site X.</para>

        <para>To use this replica selector set the following property</para>

        <programlisting>pegasus.selector.replica                  Restricted</programlisting>
      </section>

      <section>
        <title>Regex</title>

        <para>This replica selector allows the user allows the user to
        specific regex expressions that can be used to rank various
        PFN&amp;rsquor;s returned from the Replica Catalog for a particular
        LFN. This replica selector selects the highest ranked PFN i.e the
        replica with the lowest rank value.</para>

        <para>The regular expressions are assigned different rank, that
        determine the order in which the expressions are employed. The rank
        values for the regex can expressed in user properties using the
        property.</para>

        <programlisting>pegasus.selector.replica.regex.rank.<emphasis
            role="bold">[value]</emphasis>                  regex-expression</programlisting>

        <para>The <emphasis role="bold">[value]</emphasis> in the above
        property is an integer value that denotes the rank of an expression
        with a rank value of 1 being the highest rank.</para>

        <para>For example, a user can specify the following regex expressions
        that will ask Pegasus to prefer file URL's over gsiftp url's from
        example.isi.edu</para>

        <programlisting>pegasus.selector.replica.regex.rank.1                       file://.*
pegasus.selector.replica.regex.rank.2                       gsiftp://example\.isi\.edu.*</programlisting>

        <para>User can specify as many regex expressions as they want.</para>

        <para>Since Pegasus is in Java , the regex expression support is what
        Java supports. It is pretty close to what is supported by Perl. More
        details can be found at
        http://java.sun.com/j2se/1.5.0/docs/api/java/util/regex/Pattern.html</para>

        <para>Before applying any regular expressions on the PFN&amp;rsquor;s
        for a particular LFN that has to be staged to a site X, the file
        URL&amp;rsquor;s that don't match the site X are explicitly filtered
        out.</para>

        <para>To use this replica selector set the following
        property<programlisting>pegasus.selector.replica                  Regex</programlisting></para>
      </section>

      <section>
        <title>Local</title>

        <para>This replica selector always prefers replicas from the local
        host ( pool attribute set to local ) and that start with a file: URL
        scheme. It is useful, when users want to stagein files to a remote
        site from the submit host using the Condor file transfer
        mechanism.</para>

        <para>To use this replica selector set the following
        property<programlisting>pegasus.selector.replica                  Default</programlisting></para>
      </section>
    </section>
  </section>

  <section id="job_clustering">
    <title>Job Clustering</title>

    <para>A large number of workflows executed through the Pegasus Workflow
    Management System, are composed of several jobs that run for only a few
    seconds or so. The overhead of running any job on the grid is usually 60
    seconds or more. Hence, it makes sense to cluster small independent jobs
    into a larger job. This is done while mapping an abstract workflow to a
    concrete workflow. Site specific or transformation specific criteria are
    taken into consideration while clustering smaller jobs into a larger job
    in the concrete workflow. The user is allowed to control the granularity
    of this clustering on a per transformation per site basis.</para>

    <section>
      <title>Overview</title>

      <para>The abstract workflow is mapped onto the various sites by the Site
      Selector. This semi executable workflow is then passed to the clustering
      module. The clustering of the workflow can be either be</para>

      <itemizedlist>
        <listitem>
          <para>level based (horizontal clustering )</para>
        </listitem>

        <listitem>
          <para>label based (label clustering)</para>
        </listitem>
      </itemizedlist>

      <para>The clustering module clusters the jobs into larger/clustered
      jobs, that can then be executed on the remote sites. The execution can
      either be sequential on a single node or on multiple nodes using MPI. To
      specify which clustering technique to use the user has to pass the
      <emphasis role="bold">--cluster</emphasis> option to <emphasis
      role="bold">pegasus-plan</emphasis> .</para>

      <section>
        <title>Generating Clustered Concrete DAG</title>

        <para>The clustering of a workflow is activated by passing the
        <emphasis role="bold">--cluster|-C</emphasis> option to <emphasis
        role="bold">pegasus-plan</emphasis>. The clustering granularity of a
        particular logical transformation on a particular site is dependant
        upon the clustering techniques being used. The executable that is used
        for running the clustered job on a particular site is determined as
        explained in section 7.<programlisting>#Running pegasus-plan to generate clustered workflows

$ <emphasis>pegasus-plan &amp;ndash;-dax example.dax --dir ./dags &amp;ndash;p siteX &amp;ndash;-output local
               --cluster [ comma separated list of clustering techniques]  &amp;ndash;verbose
</emphasis>
Valid clustering techniques are horizontal and label.</programlisting></para>

        <para>The naming convention of submit files of the clustered jobs
        is<emphasis role="bold"> merge_NAME_IDX.sub</emphasis> . The NAME is
        derived from the logical transformation name. The IDX is an integer
        number between 1 and the total number of jobs in a cluster. Each of
        the submit files has a corresponding input file, following the naming
        convention <emphasis role="bold">merge_NAME_IDX.in </emphasis>. The
        input file contains the respective execution targets and the arguments
        for each of the jobs that make up the clustered job.</para>

        <section>
          <title>Horizontal Clustering</title>

          <para>In case of horizontal clustering, each job in the workflow is
          associated with a level. The levels of the workflow are determined
          by doing a modified Breadth First Traversal of the workflow starting
          from the root nodes. The level associated with a node, is the
          furthest distance of it from the root node instead of it being the
          shortest distance as in normal BFS. For each level the jobs are
          grouped by the site on which they have been scheduled by the Site
          Selector. Only jobs of same type (txnamespace, txname, txversion)
          can be clustered into a larger job. To use horizontal clustering the
          user needs to set the <emphasis role="bold">--cluster</emphasis>
          option of <emphasis role="bold">pegasus-plan to
          horizontal</emphasis> .</para>

          <section>
            <title>Controlling Clustering Granularity</title>

            <para>The number of jobs that have to be clustered into a single
            large job, is determined by the value of two parameters associated
            with the smaller jobs. Both these parameters are specified by the
            use of a PEGASUS namespace profile keys. The keys can be specified
            at any of the placeholders for the profiles (abstract
            transformation in the DAX, site in the site catalog,
            transformation in the transformation catalog). The normal
            overloading semantics apply i.e. profile in transformation catalog
            overrides the one in the site catalog and that in turn overrides
            the one in the DAX. The two parameters are described below.</para>

            <itemizedlist>
              <listitem>
                <para><emphasis role="bold">clusters.size
                factor</emphasis></para>

                <para>The clusters.size factor denotes how many jobs need to
                be merged into a single clustered job. It is specified via the
                use of a PEGASUS namespace profile key
                &amp;ldquo;clusters.size&amp;rdquor;. for e.g. if at a
                particular level, say 4 jobs referring to logical
                transformation B have been scheduled to a siteX. The
                clusters.size factor associated with job B for siteX is say 3.
                This will result in 2 clustered jobs, one composed of 3 jobs
                and another of 2 jobs. The clusters.size factor can be
                specified in the transformation catalog as follows</para>

                <programlisting><emphasis role="bold">#site   transformation   pfn            type               architecture  profiles
</emphasis>
siteX    B     /shared/PEGASUS/bin/jobB INSTALLED       INTEL32::LINUX  PEGASUS::clusters.size=3
siteX    C     /shared/PEGASUS/bin/jobC INSTALLED       INTEL32::LINUX  PEGASUS::clusters.size=2
</programlisting>

                <figure>
                  <title></title>

                  <mediaobject>
                    <imageobject>
                      <imagedata fileref="images/advanced-clustering-1.png" />
                    </imageobject>
                  </mediaobject>
                </figure>
              </listitem>

              <listitem>
                <para><emphasis role="bold">clusters.num
                factor</emphasis></para>

                <para>The clusters.num factor denotes how many clustered jobs
                does the user want to see per level per site. It is specified
                via the use of a PEGASUS namespace profile key
                &amp;ldquo;clusters.num&amp;rdquor;. for e.g. if at a
                particular level, say 4 jobs referring to logical
                transformation B have been scheduled to a siteX. The
                &amp;ldquo;clusters.num&amp;rdquor; factor associated with job
                B for siteX is say 3. This will result in 3 clustered jobs,
                one composed of 2 jobs and others of a single job each. The
                clusters.num factor in the transformation catalog can be
                specified as follows</para>

                <programlisting><emphasis role="bold">#site  transformation      pfn           type            architecture    profiles
</emphasis>
siteX    B     /shared/PEGASUS/bin/jobB INSTALLED       INTEL32::LINUX  PEGASUS::clusters.num=3
siteX    C     /shared/PEGASUS/bin/jobC INSTALLED       INTEL32::LINUX  PEGASUS::clusters.num=2
</programlisting>

                <para>In the case, where both the factors are associated with
                the job, the clusters.num value supersedes the clusters.size
                value.</para>

                <programlisting><emphasis role="bold">#site  transformation   pfn             type             architecture   profiles
</emphasis>
siteX    B     /shared/PEGASUS/bin/jobB INSTALLED       INTEL32::LINUX PEGASUS::clusters.size=3,clusters.num=3
</programlisting>

                <para>In the above case the jobs referring to logical
                transformation B scheduled on siteX will be clustered on the
                basis of &amp;ldquo;clusters.num&amp;rdquor; value. Hence, if
                there are 4 jobs referring to logical transformation B
                scheduled to siteX, then 3 clustered jobs will be
                created.</para>

                <figure>
                  <title></title>

                  <mediaobject>
                    <imageobject>
                      <imagedata fileref="images/advanced-clustering-2.png" />
                    </imageobject>
                  </mediaobject>
                </figure>
              </listitem>
            </itemizedlist>
          </section>
        </section>

        <section>
          <title>Label Clustering</title>

          <para>In label based clustering, the user labels the workflow. All
          jobs having the same label value are clustered into a single
          clustered job. This allows the user to create clusters or use a
          clustering technique that is specific to his workflows. If there is
          no label associated with the job, the job is not clustered and is
          executed as is<figure>
              <title></title>

              <mediaobject>
                <imageobject>
                  <imagedata fileref="images/advanced-clustering-3.png" />
                </imageobject>
              </mediaobject>
            </figure></para>

          <para>Since, the jobs in a cluster in this case are not independent,
          it is important the jobs are executed in the correct order. This is
          done by doing a topological sort on the jobs in each cluster. To use
          label based clustering the user needs to set the <emphasis
          role="bold">--cluster</emphasis> option of <emphasis
          role="bold">pegasus-plan</emphasis> to label.</para>

          <section>
            <title>Labelling the Workflow</title>

            <para>The labels for the jobs in the workflow are specified by
            associated <emphasis role="bold">pegasus</emphasis> profile keys
            with the jobs during the DAX generation process. The user can
            choose which profile key to use for labeling the workflow. By
            default, it is assumed that the user is using the PEGASUS profile
            key label to associate the labels. To use another key, in the
            <emphasis role="bold">pegasus</emphasis> namespace the user needs
            to set the following property</para>

            <itemizedlist>
              <listitem>
                <para>pegasus.clusterer.label.key</para>
              </listitem>
            </itemizedlist>

            <para>For example if the user sets <emphasis
            role="bold">pegasus.clusterer.label.key </emphasis>to <emphasis
            role="bold">user_label</emphasis> then the job description in the
            DAX looks as follows</para>

            <programlisting>&lt;adag &gt;
&amp;mldr;
  &lt;job id="ID000004" namespace="app" name="analyze" version="1.0" level="1" &gt;
    &lt;argument&gt;-a bottom -T60  -i &lt;filename file="user.f.c1"/&gt;  -o &lt;filename file="user.f.d"/&gt;&lt;/argument&gt;
    &lt;profile namespace=&amp;ldquo;pegasus&amp;rdquor; key=&amp;ldquo;user_label&amp;rdquor;&gt;p1&lt;/profile&gt;
    &lt;uses file="user.f.c1" link="input" dontRegister="false" dontTransfer="false"/&gt;
    &lt;uses file="user.f.c2" link="input" dontRegister="false" dontTransfer="false"/&gt;
    &lt;uses file="user.f.d" link="output" dontRegister="false" dontTransfer="false"/&gt;
  &lt;/job&gt;
&amp;mldr;
&lt;/adag&gt;</programlisting>

            <itemizedlist>
              <listitem>
                <para>The above states that the <emphasis
                role="bold">pegasus</emphasis> profiles with key as <emphasis
                role="bold">user_label</emphasis> are to be used for
                designating clusters.</para>
              </listitem>

              <listitem>
                <para>Each job with the same value for <emphasis
                role="bold">pegasus</emphasis> profile key <emphasis
                role="bold">user_label </emphasis>appears in the same
                cluster.</para>
              </listitem>
            </itemizedlist>
          </section>
        </section>

        <section>
          <title>Recursive Clustering</title>

          <para>In some cases, a user may want to use a combination of
          clustering techniques. For e.g. a user may want some jobs in the
          workflow to be horizontally clustered and some to be label
          clustered. This can be achieved by specifying a comma separated list
          of clustering techniques to the<emphasis role="bold">
          &amp;ndash;-cluster</emphasis> option of <emphasis
          role="bold">pegasus-plan</emphasis>. In this case the clustering
          techniques are applied one after the other on the workflow in the
          order specified on the command line.</para>

          <para>For example</para>

          <programlisting>$ <emphasis>pegasus-plan &amp;ndash;-dax example.dax --dir ./dags --cluster label,horizontal &amp;ndash;s siteX &amp;ndash;-output local --verbose</emphasis></programlisting>

          <figure>
            <title></title>

            <mediaobject>
              <imageobject>
                <imagedata fileref="images/advanced-clustering-4.png" />
              </imageobject>
            </mediaobject>
          </figure>
        </section>
      </section>

      <section>
        <title>Execution of the Clustered Job</title>

        <para>The execution of the clustered job on the remote site, involves
        the execution of the smaller constituent jobs either</para>

        <itemizedlist>
          <listitem>
            <para><emphasis role="bold">sequentially on a single node of the
            remote site</emphasis></para>

            <para>The clustered job is executed using <emphasis
            role="bold">seqexec</emphasis>, a wrapper tool written in C that
            is distributed as part of the PEGASUS. It takes in the jobs passed
            to it, and ends up executing them sequentially on a single node.
            To use &amp;ldquo;<emphasis
            role="bold">seqexec</emphasis>&amp;rdquor; for executing any
            clustered job on a siteX, there needs to be an entry in the
            transformation catalog for an executable with the logical name
            seqexec and namespace as pegasus.</para>

            <programlisting><emphasis role="bold">#site  transformation   pfn            type                 architecture    profiles</emphasis>

siteX    pegasus::seqexec     /shared/PEGASUS/bin/seqexec INSTALLED       INTEL32::LINUX NULL</programlisting>

            <para>By default, the entry for seqexec on a site is automatically
            picked up if $PEGASUS_HOME or $VDS_HOME is specified in the site
            catalog for that site.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">On multiple nodes of the remote site
            using MPI</emphasis></para>

            <para>The clustered job is executed using mpiexec, a wrapper mpi
            program written in C that is distributed as part of the PEGASUS.
            It is only distributed as source not as binary. The wrapper ends
            up being run on every mpi node, with the first one being the
            master and the rest of the ones as workers. The number of
            instances of mpiexec that are invoked is equal to the value of the
            globus rsl key nodecount. The master distributes the smaller
            constituent jobs to the workers.</para>

            <para>For e.g. If there were 10 jobs in the merged job and
            nodecount was 5, then one node acts as master, and the 10 jobs are
            distributed amongst the 4 slaves on demand. The master hands off a
            job to the slave node as and when it gets free. So initially all
            the 4 nodes are given a single job each, and then as and when they
            get done are handed more jobs till all the 10 jobs have been
            executed.</para>

            <para>To use &amp;ldquo;mpiexec&amp;rdquor; for executing the
            clustered job on a siteX, there needs to be an entry in the
            transformation catalog for an executable with the logical name
            mpiexec and namespace as pegasus.</para>

            <programlisting><emphasis role="bold">#site  transformation   pfn            type                 architecture    profiles</emphasis>

siteX    pegasus::seqexec     /shared/PEGASUS/bin/mpiexec INSTALLED       INTEL32::LINUX NULL</programlisting>

            <para>Another added advantage of using mpiexec, is that regular
            non mpi code can be run via MPI.</para>

            <para>Both the clustered job and the smaller constituent jobs are
            invoked via kickstart, unless the clustered job is being run via
            mpi (mpiexec). Kickstart is unable to launch mpi jobs. If
            kickstart is not installed on a particular site i.e. the
            gridlaunch attribute for site is not specified in the site
            catalog, the jobs are invoked directly.</para>
          </listitem>
        </itemizedlist>

        <section>
          <title>Specification of Method of Execution for Clustered
          Jobs</title>

          <para>The method execution of the clustered job(whether to launch
          via mpiexec or seqexec) can be specified</para>

          <orderedlist>
            <listitem>
              <para><emphasis role="bold">globally in the properties
              file</emphasis></para>

              <para>The user can set a property in the properties file that
              results in all the clustered jobs of the workflow being executed
              by the same type of executable.</para>

              <programlisting><emphasis role="bold">#PEGASUS PROPERTIES FILE</emphasis>
pegasus.clusterer.job.aggregator seqexec|mpiexec</programlisting>

              <para>In the above example, all the clustered jobs on the remote
              sites are going to be launched via the property value, as long
              as the property value is not overridden in the site
              catalog.</para>
            </listitem>

            <listitem>
              <para><emphasis role="bold">associating profile key
              &amp;ldquo;collapser&amp;rdquor; with the site in the site
              catalog</emphasis></para>

              <programlisting>&lt;site handle="siteX" gridlaunch = "/shared/PEGASUS/bin/kickstart"&gt;
    &lt;profile namespace="env" key="GLOBUS_LOCATION" &gt;/home/shared/globus&lt;/profile&gt;
    &lt;profile namespace="env" key="LD_LIBRARY_PATH"&gt;/home/shared/globus/lib&lt;/profile&gt;
    &lt;profile namespace="pegasus" key="collapser" &gt;seqexec&lt;/profile&gt;
    &lt;lrc url="rls://siteX.edu" /&gt;
    &lt;gridftp  url="gsiftp://siteX.edu/" storage="/home/shared/work" major="2" minor="4" patch="0" /&gt;
    &lt;jobmanager universe="transfer" url="siteX.edu/jobmanager-fork" major="2" minor="4" patch="0" /&gt;
    &lt;jobmanager universe="vanilla" url="siteX.edu/jobmanager-condor" major="2" minor="4" patch="0" /&gt;
    &lt;workdirectory &gt;/home/shared/storage&lt;/workdirectory&gt;
  &lt;/site&gt;</programlisting>

              <para>In the above example, all the clustered jobs on a siteX
              are going to be executed via seqexec, as long as the value is
              not overridden in the transformation catalog.</para>
            </listitem>

            <listitem>
              <para><emphasis role="bold">associating profile key
              &amp;ldquo;collapser&amp;rdquor; with the transformation that is
              being clustered, in the transformation catalog</emphasis></para>

              <programlisting><emphasis role="bold">#site  transformation   pfn            type                architecture profiles
</emphasis>
siteX    B     /shared/PEGASUS/bin/jobB INSTALLED       INTEL32::LINUX pegasus::clusters.size=3,collapser=mpiexec</programlisting>

              <para>In the above example, all the clustered jobs that consist
              of transformation B on siteX will be executed via
              mpiexec.</para>

              <note>
                <para><emphasis role="bold"> The clustering of jobs on a site
                only happens only if </emphasis></para>
              </note>

              <itemizedlist>
                <listitem>
                  <para>there exists an entry in the transformation catalog
                  for the clustering executable that has been determined by
                  the above 3 rules</para>
                </listitem>

                <listitem>
                  <para>the number of jobs being clustered on the site are
                  more than 1</para>
                </listitem>
              </itemizedlist>
            </listitem>
          </orderedlist>
        </section>
      </section>

      <section>
        <title>Outstanding Issues</title>

        <orderedlist>
          <listitem>
            <para><emphasis role="bold">Label Clustering</emphasis></para>

            <para>More rigorous checks are required to ensure that the
            labeling scheme applied by the user is valid.</para>
          </listitem>
        </orderedlist>
      </section>
    </section>
  </section>

  <section id="transfer">
    <title>Transfers</title>

    <para>As part of the Workflow Mapping Process, Pegasus does data
    management for the executable workflow . It queries a Replica Catalog to
    discover the locations of the input datasets and adds data movement and
    registration nodes in the workflow to</para>

    <orderedlist>
      <listitem>
        <para>stage-in input data to the compute sites ( where the jobs in the
        workflow are executed )</para>
      </listitem>

      <listitem>
        <para>stage-out output data generated by the workflow to the final
        storage site.</para>
      </listitem>

      <listitem>
        <para>stage-in intermediate data between compute sites if
        required.</para>
      </listitem>

      <listitem>
        <para>data registration nodes to catalog the locations of the output
        data on the final storage site into the replica catalog.</para>
      </listitem>
    </orderedlist>

    <para>The separate data movement jobs that are added to the executable
    workflow are responsible for staging data to a workflow specific directory
    accessible to the staging server on a staging site associated with the
    compute sites. Currently, the staging site for a compute site is the
    compute site itself. In the default case, the staging server is usually on
    the headnode of the compute site and has access to the shared filesystem
    between the worker nodes and the head node. Pegasus adds a directory
    creation job in the executable workflow that creates the workflow specific
    directory on the staging server.</para>

    <figure>
      <title>Default Transfer Case : Input Data To Workflow Specific Directory
      on Shared File System</title>

      <mediaobject>
        <imageobject>
          <imagedata fileref="images/default-transfer-sharedfs.png" />
        </imageobject>
      </mediaobject>
    </figure>

    <para>In addition to data, Pegasus does transfer user executables to the
    compute sites if the executables are not installed on the remote sites
    before hand. This chapter gives an overview of how transfers of data and
    executables is managed in Pegasus.</para>

    <section>
      <title>Local versus Remote Transfers</title>

      <para>As far as possible, Pegasus will ensure that the transfer jobs
      added to the executable workflow are executed on the submit host. By
      default, Pegasus will schedule a transfer to be executed on the remote
      compute site only if there is no way to execute it on the submit host.
      For e.g if the file server specified for the compute site is a file
      server, then Pegasus will schedule all the stage in data movement jobs
      on the compute site to stage-in the input data for the workflow. Another
      case would be if a user has symlinking turned on. In that case, the
      transfer jobs that symlink against the input data on the compute site,
      will be executed remotely ( on the compute site ).</para>

      <para>Users can specify the property <emphasis
      role="bold">pegasus.transfer.*.remote.sites</emphasis> to change the
      default behaviour of Pegasus and force pegasus to run different types of
      transfer jobs for the sites specified on the remote site. The value of
      the property is a comma separated list of compute sites for which you
      want the transfer jobs to run remotely.</para>

      <para>The table below illustrates all the possible variations of the
      property.</para>

      <table>
        <title>Property Variations for pegasus.transfer.*.remote.sites</title>

        <tgroup cols="2">
          <thead>
            <row>
              <entry>Property Name</entry>

              <entry>Applies to</entry>
            </row>
          </thead>

          <tbody>
            <row>
              <entry>pegasus.transfer.stagein.remote.sites</entry>

              <entry>the stage in transfer jobs</entry>
            </row>

            <row>
              <entry>pegasus.transfer.stageout.remote.sites</entry>

              <entry>the stage out transfer jobs</entry>
            </row>

            <row>
              <entry>pegasus.transfer.inter.remote.sites</entry>

              <entry>the inter site transfer jobs</entry>
            </row>

            <row>
              <entry>pegasus.transfer.*.remote.sites</entry>

              <entry>all types of transfer jobs</entry>
            </row>
          </tbody>
        </tgroup>
      </table>

      <para>The prefix for the transfer job name indicates whether the
      transfer job is to be executed locallly ( on the submit host ) or
      remotely ( on the compute site ). For example stage_in_local_ in a
      transfer job name stage_in_local_isi_viz_0 indicates that the transfer
      job is a stage in transfer job that is executed locally and is used to
      transfer input data to compute site isi_viz. The prefix naming scheme
      for the transfer jobs is <emphasis
      role="bold">[stage_in|stage_out|inter]_[local|remote]_</emphasis>
      .</para>
    </section>

    <section>
      <title>Symlinking Against Input Data</title>

      <para>If input data for a job already exists on a compute site, then it
      is possible for Pegasus to symlink against that data. In this case, the
      remote stage in transfer jobs that Pegasus adds to the executable
      workflow will symlink instead of doing a copy of the data.</para>

      <para>Pegasus determines whether a file is on the same site as the
      compute site, by inspecting the pool attribute associated with the URL
      in the Replica Catalog. If the pool attribute of an input file location
      matches the compute site where the job is scheduled, then that
      particular input file is a candidate for symlinking.</para>

      <para>For Pegasus to symlink against existing input data on a compute
      site, following must be true</para>

      <orderedlist>
        <listitem>
          <para>Property <emphasis
          role="bold">pegasus.transfer.links</emphasis> is set to <emphasis
          role="bold">true</emphasis></para>
        </listitem>

        <listitem>
          <para>The input file location in the Replica Catalog has the pool
          attribute matching the compute site.</para>
        </listitem>
      </orderedlist>

      <tip>
        <para>To confirm if a particular input file is symlinked instead of
        being copied, look for the destination URL for that file in
        stage_in_remote*.in file. The destination URL will start with
        symlink:// .</para>
      </tip>

      <para>In the symlinking case, Pegasus strips out URL prefix from a URL
      and replaces it with a file URL.</para>

      <para>For example if a user has the following URL catalogued in the
      Replica Catalog for an input file f.input</para>

      <programlisting>f.input   gsiftp://server.isi.edu/shared/storage/input/data/f.input pool="isi"</programlisting>

      <para>and the compute job that requires this file executes on a compute
      site named isi , then if symlinking is turned on the data stage in job
      (stage_in_remote_viz_0 ) will have the following source and destination
      specified for the file</para>

      <programlisting>#viz viz
file:///shared/storage/input/data/f.input  symlink://shared-scratch/workflow-exec-dir/f.input
</programlisting>
    </section>

    <section>
      <title>Addition of Separate Data Movement Nodes to Executable
      Workflow</title>

      <para>Pegasus relies on a Transfer Refiner that comes up with the
      strategy on how many data movement nodes are added to the executable
      workflow. All the compute jobs scheduled to a site share the same
      workflow specific directory. The transfer refiners ensure that only one
      copy of the input data is transferred to the workflow execution
      directory. This is to prevent data clobbering . Data clobbering can
      occur when compute jobs of a workflow share some input files, and have
      different stage in transfer jobs associated with them that are staging
      the shared files to the same destination workflow execution
      directory.</para>

      <para>The default Transfer Refiner used in Pegasus is the Bundle Refiner
      that allows the user to specify how many local|remote stagein|stageout
      jobs are created per execution site.</para>

      <para>The behavior of the refiner is controlled by specifying certain
      pegasus profiles</para>

      <orderedlist>
        <listitem>
          <para>either with the execution sites in the site catalog</para>
        </listitem>

        <listitem>
          <para>OR globally in the properties file</para>
        </listitem>
      </orderedlist>

      <table>
        <title>Pegasus Profile Keys For the Bundle Transfer Refiner</title>

        <tgroup cols="2">
          <thead>
            <row>
              <entry>Profile Key</entry>

              <entry>Description</entry>
            </row>
          </thead>

          <tbody>
            <row>
              <entry>stagein.clusters</entry>

              <entry>This key determines the maximum number of stage-in jobs
              that are can executed locally or remotely per compute site per
              workflow.</entry>
            </row>

            <row>
              <entry>stagein.local.clusters</entry>

              <entry>This key provides finer grained control in determining
              the number of stage-in jobs that are executed locally and are
              responsible for staging data to a particular remote
              site.</entry>
            </row>

            <row>
              <entry>stagein.remote.clusters</entry>

              <entry>This key provides finer grained control in determining
              the number of stage-in jobs that are executed remotely on the
              remote site and are responsible for staging data to it.</entry>
            </row>

            <row>
              <entry>stageout.clusters</entry>

              <entry>This key determines the maximum number of stage-out jobs
              that are can executed locally or remotely per compute site per
              workflow.</entry>
            </row>

            <row>
              <entry>stageout.local.clusters</entry>

              <entry>This key provides finer grained control in determining
              the number of stage-out jobs that are executed locally and are
              responsible for staging data from a particular remote
              site.</entry>
            </row>

            <row>
              <entry>stageout.remote.clusters</entry>

              <entry>This key provides finer grained control in determining
              the number of stage-out jobs that are executed remotely on the
              remote site and are responsible for staging data from
              it.</entry>
            </row>
          </tbody>
        </tgroup>
      </table>

      <figure>
        <title>Default Transfer Case : Input Data To Workflow Specific
        Directory on Shared File System</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="images/bundle-transfer-refiner.png" lang="" />
          </imageobject>
        </mediaobject>
      </figure>
    </section>

    <section>
      <title>Executable Used for Transfer Jobs</title>

      <para>Pegasus refers to a python script called <emphasis
      role="bold">pegasus-transfer</emphasis> as the executable in the
      transfer jobs to transfer the data. pegasus-transfer is a python based
      wrapper around various transfer clients . pegasus-transfer looks at
      source and destination url and figures out automatically which
      underlying client to use. pegasus-transfer is distributed with the
      PEGASUS and can be found at $PEGASUS_HOME/bin/pegasus-transfer.</para>

      <para>Currently, pegasus-transfer interfaces with the following transfer
      clients</para>

      <table>
        <title>Transfer Clients interfaced to by pegasus-transfer</title>

        <tgroup cols="2">
          <thead>
            <row>
              <entry>Transfer Client</entry>

              <entry>Used For</entry>
            </row>
          </thead>

          <tbody>
            <row>
              <entry>globus-url-copy</entry>

              <entry>staging files to and from a gridftp server.</entry>
            </row>

            <row>
              <entry>lcg-copy</entry>

              <entry>staging files to and from a SRM server.</entry>
            </row>

            <row>
              <entry>wget</entry>

              <entry>staging files from a HTTP server.</entry>
            </row>

            <row>
              <entry>cp</entry>

              <entry>copying files from a POSIX filesystem .</entry>
            </row>

            <row>
              <entry>ln</entry>

              <entry>symlinking against input files.</entry>
            </row>
          </tbody>
        </tgroup>
      </table>

      <para>For remote sites, Pegasus constructs the default path to
      pegasus-transfer on the basis of PEGASUS_HOME env profile specified in
      the site catalog. To specify a different path to the pegasus-transfer
      client , users can add an entry into the transformation catalog with
      fully qualified logical name as <emphasis
      role="bold">pegasus::pegasus-transfer</emphasis></para>
    </section>

    <section>
      <title>Staging of Executables</title>

      <para>Users can get Pegasus to stage the user executables ( executables
      that the jobs in the DAX refer to ) as part of the transfer jobs to the
      workflow specific execution directory on the compute site. The URL
      locations of the executables need to be specified in the transformation
      catalog as the PFN and the type of executable needs to be set to
      <emphasis role="bold">STAGEABLE</emphasis> .</para>

      <para>The location of a transformation can be specified either in</para>

      <itemizedlist>
        <listitem>
          <para>DAX in the executables section. More details <link
          linkend="dax_transformation_catalog">here</link> .</para>
        </listitem>

        <listitem>
          <para>Transformation Catalog. More details <link
          linkend="transformation">here</link> .</para>
        </listitem>
      </itemizedlist>

      <para>A particular transformation catalog entry of type STAGEABLE is
      compatible with a compute site only if all the System Information
      attributes associated with the entry match with the System Information
      attributes for the compute site in the Site Catalog. The following
      attributes make up the System Information attributes</para>

      <orderedlist>
        <listitem>
          <para>arch</para>
        </listitem>

        <listitem>
          <para>os</para>
        </listitem>

        <listitem>
          <para>osrelease</para>
        </listitem>

        <listitem>
          <para>osversion</para>
        </listitem>
      </orderedlist>

      <section>
        <title>Transformation Mappers</title>

        <para>Pegasus has a notion of transformation mappers that determines
        what type of executables are picked up when a job is executed on a
        remote compute site. For transfer of executables, Pegasus constructs a
        soft state map that resides on top of the transformation catalog, that
        helps in determining the locations from where an executable can be
        staged to the remote site.</para>

        <para>Users can specify the following property to pick up a specific
        transformation mapper</para>

        <programlisting><emphasis role="bold">pegasus.catalog.transformation.mapper</emphasis> </programlisting>

        <para>Currently, the following transformation mappers are
        supported.</para>

        <table>
          <title>Transformation Mappers Supported in Pegasus</title>

          <tgroup cols="2">
            <thead>
              <row>
                <entry>Transformation Mapper</entry>

                <entry>Description</entry>
              </row>
            </thead>

            <tbody>
              <row>
                <entry>Installed</entry>

                <entry>This mapper only relies on transformation catalog
                entries that are of type INSTALLED to construct the soft state
                map. This results in Pegasus never doing any transfer of
                executables as part of the workflow. It always prefers the
                installed executables at the remote sites</entry>
              </row>

              <row>
                <entry>Staged</entry>

                <entry>This mapper only relies on matching transformation
                catalog entries that are of type STAGEABLE to construct the
                soft state map. This results in the executable workflow
                referring only to the staged executables, irrespective of the
                fact that the executables are already installed at the remote
                end</entry>
              </row>

              <row>
                <entry>All</entry>

                <entry>This mapper relies on all matching transformation
                catalog entries of type STAGEABLE or INSTALLED for a
                particular transformation as valid sources for the transfer of
                executables. This the most general mode, and results in the
                constructing the map as a result of the cartesian product of
                the matches.</entry>
              </row>

              <row>
                <entry>Submit</entry>

                <entry>This mapper only on matching transformation catalog
                entries that are of type STAGEABLE and reside at the submit
                host (pool local), are used while constructing the soft state
                map. This is especially helpful, when the user wants to use
                the latest compute code for his computations on the grid and
                that relies on his submit host.</entry>
              </row>
            </tbody>
          </tgroup>
        </table>
      </section>
    </section>

    <section>
      <title>Staging of Pegasus Worker Package</title>

      <para>Pegasus can optionally stage the pegasus worker package as part of
      the executable workflow to remote workflow specific execution directory.
      The pegasus worker package contains the pegasus auxillary executables
      that are required on the remote site. If the worker package is not
      staged as part of the executable workflow, then Pegasus relies on the
      installed version of the worker package on the remote site. To determine
      the location of the installed version of the worker package on a remote
      site, Pegasus looks for an environment profile PEGASUS_HOME for the site
      in the Site Catalog.</para>

      <para>Users can set the following property to true to turn on worker
      package staging</para>

      <programlisting><emphasis role="bold">pegasus.transfer.worker.package          true</emphasis> </programlisting>

      <para>By default, when worker package staging is turned on pegasus pulls
      the compatible worker package from the Pegasus Website. To specify a
      different worker package location, users can specify the transformation
      <emphasis role="bold">pegasus::worker</emphasis> in the transformation
      catalog with</para>

      <itemizedlist>
        <listitem>
          <para>type set to STAGEABLE</para>
        </listitem>

        <listitem>
          <para>System Information attributes of the transformation catalog
          entry match the System Information attributes of the compute
          site.</para>
        </listitem>

        <listitem>
          <para>the PFN specified should be a remote URL that can be pulled to
          the compute site.</para>
        </listitem>
      </itemizedlist>
    </section>

    <section>
      <title>Second Level Staging</title>

      <para>By default, Pegasus executes the jobs in the workflow specific
      directory created on the shared filesystem of a compute site. However,
      if a user wants Pegasus can execute the jobs on the worker nodes
      filesystem. When the jobs are executed on the worker node, they pull the
      input data for the job from the workflow specific directory on the
      staging server ( usually the shared filesystem on the compute site ) to
      a directory on the worker node filesystem, and after the job has
      completed stages out the output files from the worker node to the
      workflow specific execution directory.</para>

      <para>The separate data stagein and stageout jobs are still added to the
      workflow. They are responsible for getting the input data to the
      workflow specific directory on the staging server ( usually the shared
      filesystem on the compute site ) , and pushing out the output data to
      final storage site from that directory.</para>

      <figure>
        <title>Second Level Staging : Getting Data to and from a directory on
        the worker nodes</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="./images/sls-transfer-worker.png" />
          </imageobject>
        </mediaobject>
      </figure>

      <para>This mode is especially useful for running in the cloud
      environments where you don't want to setup a shared filesystem between
      the worker nodes. Running in that mode is explained in detail <link
      linkend="running_on_cloud">here.</link></para>

      <para>To turn on second level staging for the workflows users should set
      the following properties</para>

      <programlisting><emphasis role="bold">pegasus.execute.*.filesystem.local = true   </emphasis>    # Turn on second-level staging (SLS)
<emphasis role="bold">pegasus.transfer.sls.s3.stage.sls.file = false</emphasis>  # Do not transfer .sls files via transfer jobs
<emphasis role="bold">pegasus.gridstart = SeqExec </emphasis>                    # Use SeqExec to launch the jobs</programlisting>
    </section>
  </section>

  <section>
    <title>Hierarchical Workflow</title>

    <section>
      <title>Introduction</title>

      <para>The Abstract Workflow in addition to containing compute jobs, can
      also contain jobs that refer to other workflows. This is useful for
      running large workflows or ensembles of workflows.</para>

      <para>Users can embed two types of workflow jobs in the DAX</para>

      <orderedlist>
        <listitem>
          <para>daxjob - refers to a sub workflow represented as a DAX. During
          the planning of a workflow, the DAX jobs are mapped to condor dagman
          jobs that have pegasus plan invocation on the dax ( referred to in
          the DAX job ) as the prescript.</para>

          <figure>
            <title>Planning of a DAX Job</title>

            <mediaobject>
              <imageobject>
                <imagedata fileref="./images/daxjob-mapping.png" />
              </imageobject>
            </mediaobject>
          </figure>
        </listitem>

        <listitem>
          <para>dagjob - refers to a sub workflow represented as a DAG. During
          the planning of a workflow, the DAG jobs are mapped to condor dagman
          and refer to the DAG file mentioned in the DAG job.</para>

          <figure>
            <title>Planning of a DAG Job</title>

            <mediaobject>
              <imageobject>
                <imagedata fileref="./images/dagjob-mapping.png" />
              </imageobject>
            </mediaobject>
          </figure>
        </listitem>
      </orderedlist>
    </section>

    <section>
      <title>Specifying a DAX Job in the DAX</title>

      <para>Specifying a DAXJob in a DAX is pretty similar to how normal
      compute jobs are specified. There are minor differences in terms of the
      xml element name ( dax vs job ) and the attributes specified. DAXJob XML
      specification is described in detail in the <link linkend="api">chapter
      on DAX API</link> . An example DAX Job in a DAX is shown below</para>

      <programlisting id="dax_job_example" language="">  &lt;dax id="ID000002" name="black.dax" node-label="bar" &gt;
    &lt;profile namespace="dagman" key="maxjobs"&gt;10&lt;/profile&gt;
    &lt;argument&gt;-Xmx1024 -Xms512 -Dpegasus.dir.storage=storagedir  -Dpegasus.dir.exec=execdir -o local -vvvvv --force -s dax_site &lt;/argument&gt;
  &lt;/dax&gt;</programlisting>

      <section>
        <title>DAX File Locations</title>

        <para>The name attribute in the dax element refers to the LFN (
        Logical File Name ) of the dax file. The location of the DAX file can
        be catalogued either in the</para>

        <para><orderedlist>
            <listitem>
              <para>Replica Catalog</para>
            </listitem>

            <listitem>
              <para>Replica Catalog Section in the <link
              linkend="dax_replica_catalog">DAX</link> .</para>

              <note>
                <para>Currently, only file url's on the local site ( submit
                host ) can be specified as DAX file locations.</para>
              </note>
            </listitem>
          </orderedlist></para>
      </section>

      <section>
        <title>Arguments for a DAX Job</title>

        <para>Users can specify specific arguments to the DAX Jobs. The
        arguments specified for the DAX Jobs are passed to the pegasus-plan
        invocation in the prescript for the corresponding condor dagman job in
        the executable workflow.</para>

        <para>The following options for pegasus-plan are inherited from the
        pegasus-plan invocation of the parent workflow. If an option is
        specified in the arguments section for the DAX Job then that overrides
        what is inherited.</para>

        <table>
          <title>Options inherited from parent workflow</title>

          <tgroup cols="2">
            <thead>
              <row>
                <entry>Option Name</entry>

                <entry>Description</entry>
              </row>
            </thead>

            <tbody>
              <row>
                <entry>--sites</entry>

                <entry>list of execution sites.</entry>
              </row>
            </tbody>
          </tgroup>
        </table>

        <para>It is highly recommended that users <emphasis role="bold">dont
        specify</emphasis> directory related options in the arguments section
        for the DAX Jobs. Pegasus assigns values to these options for the sub
        workflows automatically.</para>

        <orderedlist>
          <listitem>
            <para>--relative-dir</para>
          </listitem>

          <listitem>
            <para>--dir</para>
          </listitem>

          <listitem>
            <para>--relative-submit-dir</para>
          </listitem>
        </orderedlist>
      </section>

      <section>
        <title>Profiles for DAX Job</title>

        <para>Users can choose to specify dagman profiles with the DAX Job to
        control the behavior of the corresponding condor dagman instance in
        the executable workflow. In the example <link
        linkend="dax_job_example">above</link> maxjobs is set to 10 for the
        sub workflow.</para>
      </section>

      <section>
        <title>Execution of the PRE script and Condor DAGMan instance</title>

        <para>The pegasus plan that is invoked as part of the prescript to the
        condor dagman job is executed on the submit host. The log from the
        output of pegasus plan is redirected to a file ( ending with suffix
        pre.log ) in the submit directory of the workflow that contains the
        DAX Job. The path to pegasus-plan is automatically determined.</para>

        <para>The DAX Job maps to a Condor DAGMan job. The path to condor
        dagman binary is determined according to the following rules -</para>

        <orderedlist>
          <listitem>
            <para>entry in the transformation catalog for condor::dagman for
            site local, else</para>
          </listitem>

          <listitem>
            <para>pick up the value of CONDOR_HOME from the environment if
            specified and set path to condor dagman as
            $CONDOR_HOME/bin/condor_dagman , else</para>
          </listitem>

          <listitem>
            <para>pick up the value of CONDOR_LOCATION from the environment if
            specified and set path to condor dagman as
            $CONDOR_LOCATION/bin/condor_dagman , else</para>
          </listitem>

          <listitem>
            <para>pick up the path to condor dagman from what is defined in
            the user's PATH</para>
          </listitem>
        </orderedlist>

        <tip>
          <para>It is recommended that user dagman.maxpre in their properties
          file to control the maximum number of pegasus plan instances
          launched by each running dagman instance.</para>
        </tip>
      </section>
    </section>

    <section>
      <title>Specifying a DAG Job in the DAX</title>

      <para>Specifying a DAGJob in a DAX is pretty similar to how normal
      compute jobs are specified. There are minor differences in terms of the
      xml element name ( dag vs job ) and the attributes specified. For DAGJob
      XML details,see the <link linkend="api"> API Reference </link> chapter .
      An example DAG Job in a DAX is shown below</para>

      <programlisting id="dag_job_example">  &lt;dag id="ID000003" name="black.dag" node-label="foo" &gt;
    &lt;profile namespace="dagman" key="maxjobs"&gt;10&lt;/profile&gt;
    &lt;profile namespace="dagman" key="DIR"&gt;/dag-dir/test&lt;/profile&gt;
  &lt;/dag&gt;</programlisting>

      <section>
        <title>DAG File Locations</title>

        <para>The name attribute in the dag element refers to the LFN (
        Logical File Name ) of the dax file. The location of the DAX file can
        be catalogued either in the</para>

        <para><orderedlist>
            <listitem>
              <para>Replica Catalog</para>
            </listitem>

            <listitem>
              <para>Replica Catalog Section in the DAX.</para>

              <note>
                <para>Currently, only file url's on the local site ( submit
                host ) can be specified as DAG file locations.</para>
              </note>
            </listitem>
          </orderedlist></para>
      </section>

      <section>
        <title>Profiles for DAG Job</title>

        <para>Users can choose to specify dagman profiles with the DAX Job to
        control the behavior of the corresponding condor dagman instance in
        the executable workflow. In the example above, maxjobs is set to 10
        for the sub workflow.</para>

        <para>The dagman profile DIR allows users to specify the directory in
        which they want the condor dagman instance to execute. In the example
        <link linkend="dag_job_example">above</link> black.dag is set to be
        executed in directory /dag-dir/test . The /dag-dir/test should be
        created beforehand.</para>
      </section>
    </section>

    <section>
      <title>File Dependencies Across DAX Jobs</title>

      <para>In hierarchal workflows , if a sub workflow generates some output
      files required by another sub workflow then there should be an edge
      connecting the two dax jobs. Pegasus will ensure that the prescript for
      the child sub-workflow, has the path to the cache file generated during
      the planning of the parent sub workflow. The cache file in the submit
      directory for a workflow is a textual replica catalog that lists the
      locations of all the output files created in the remote workflow
      execution directory when the workflow executes.</para>

      <para>This automatic passing of the cache file to a child sub-workflow
      ensures that the datasets from the same workflow run are used. However,
      the passing of the locations in a cache file also ensures that Pegasus
      will prefer them over all other locations in the Replica Catalog. If you
      need the Replica Selection to consider locations in the Replica Catalog
      also, then set the following property.</para>

      <programlisting><emphasis role="bold">pegasus.catalog.replica.cache.asrc  true</emphasis></programlisting>

      <para>The above is useful in the case, where you are staging out the
      output files to a storage site, and you want the child sub workflow to
      stage these files from the storage output site instead of the workflow
      execution directory where the files were originally created.</para>
    </section>

    <section>
      <title>Recursion in Hierarchal Workflows</title>

      <para>It is possible for a user to add a dax jobs to a dax that already
      contain dax jobs in them. Pegasus does not place a limit on how many
      levels of recursion a user can have in their workflows. From Pegasus
      perspective recursion in hierarchal workflows ends when a DAX with only
      compute jobs is encountered . However, the levels of recursion are
      limited by the system resources consumed by the DAGMan processes that
      are running (each level of nesting produces another DAGMan process)
      .</para>

      <para>The figure below illustrates an example with recursion 2 levels
      deep.</para>

      <figure>
        <title>Recursion in Hierarchal Workflows</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="./images/recursion_in_hierarchal_workflows.png" />
          </imageobject>
        </mediaobject>
      </figure>

      <para>The execution time-line of the various jobs in the above figure is
      illustrated below.</para>

      <figure>
        <title>Execution Time-line for Hierarchal Workflows</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="./images/hierarchal_workflows_execution_timeline.png" />
          </imageobject>
        </mediaobject>
      </figure>
    </section>

    <section>
      <title>Example</title>

      <para>The Galactic Plane workflow is a Hierarchical workflow of many
      Montage workflows. For details, see <link
      linkend="example_workflows">Workflow of Workflows</link>.</para>
    </section>
  </section>

  <section id="api">
    <title>API Reference</title>

    <section>
      <title>DAX XML Schema</title>

      <para>The DAX format is described by the XML schema instance document
      <ulink
      url="http://pegasus.isi.edu/wms/docs/schemas/dax-3.2/dax-3.2.xsd">dax-3.2.xsd</ulink>.
      A local copy of the schema definition is provided in the
      <quote>etc</quote> directory. The documentation of the XML schema and
      its elements can be found in <ulink
      url="http://pegasus.isi.edu/wms/docs/schemas/dax-3.2/dax-3.2.html">dax-3.2.html</ulink>
      as well as locally in
      <filename>doc/schemas/dax-3.2/dax-3.2.html</filename> in your Pegasus
      distribution.</para>

      <section>
        <title>DAX XML Schema In Detail</title>

        <para>The DAX file format has three major sections, with the first
        section sub-divided into more sub-sections. The DAX format works on
        the abstract or logical level, letting you focus on the shape of the
        workflows, what to do and what to work upon.</para>

        <orderedlist>
          <listitem>
            <para>Catalogs</para>

            <para>The first section deals with included catalogs. While we do
            recommend to use external replica- and transformation catalogs, it
            is possible to include some replicas and transformations into the
            DAX file itself. Any DAX-included entry takes precedence over
            regular RC and TC entries.</para>

            <para>The first section (and any of its sub-sections) is
            completely optional.</para>

            <orderedlist>
              <listitem>
                <para>The first sub-section deals with included replica
                descriptions.</para>
              </listitem>

              <listitem>
                <para>The second sub-section deals with included
                transformation descriptions.</para>
              </listitem>

              <listitem>
                <para>The third sub-section declares multi-item
                executables.</para>
              </listitem>
            </orderedlist>
          </listitem>

          <listitem>
            <para>Job List</para>

            <para>The second section defines the job- or task descriptions.
            For each task to conduct, a three-part logical name declares the
            task and aides identifying it in the transformation catalog or one
            of the <emphasis>executable</emphasis> section above. During
            planning, the logical name is translated into the physical
            executable location on the chosen target site. By declaring jobs
            abstractly, physical layout consideration of the target sites do
            not matter. The job's <emphasis>id</emphasis> uniquley identifies
            the job within this workflow.</para>

            <para>The arguments declare what command-line arguments to pass to
            the job. If you are passing filenames, you should refer to the
            logical filename using the <emphasis>file</emphasis> element in
            the argument list.</para>

            <para>Important for properly planning the task is the list of
            files consumed by the task, its input files, and the files
            produced by the task, its output files. Each file is described
            with a <emphasis>uses</emphasis> element inside the task.</para>

            <para>Elements exist to link a logical file to any of the stdio
            file descriptors. The profile element is Pegasus's way to abstract
            site-specific data.</para>

            <para>Jobs are nodes in the workflow graph.</para>
          </listitem>

          <listitem>
            <para>Control-flow Dependencies</para>

            <para>The third section lists the dependencies between the tasks.
            The relationships are defined as child parent relationships, and
            thus impacts the order in which tasks are run. No cyclic
            dependencies are permitted.</para>

            <para>Dependencies are directed edges in the workflow
            graph.</para>
          </listitem>
        </orderedlist>

        <section>
          <title>XML Intro</title>

          <para>If you have seen the DAX schema before, not a lot of new items
          in the root element. However, we did retire the (old) attributes
          ending in <emphasis>Count</emphasis>.</para>

          <programlisting>&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;!-- generated: 2010-11-20T03:10:52Z --&gt;
&lt;adag xmlns="http://pegasus.isi.edu/schema/DAX" 
      xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
      xsi:schemaLocation="http://pegasus.isi.edu/schema/DAX http://pegasus.isi.edu/schema/dax-3.2.xsd" 
      version="3.2" 
      name="diamond" 
      index="0" 
      count="1"&gt;</programlisting>

          <para>The following attributes are supported for the root element
          <emphasis>adag</emphasis>.</para>

          <table>
            <tgroup cols="4">
              <thead>
                <row>
                  <entry>attribute</entry>

                  <entry>optional?</entry>

                  <entry>type</entry>

                  <entry>meaning</entry>
                </row>
              </thead>

              <tbody>
                <row>
                  <entry>version</entry>

                  <entry>required</entry>

                  <entry>
                    <emphasis>VersionPattern</emphasis>
                  </entry>

                  <entry>Version number of DAX instance document.</entry>
                </row>

                <row>
                  <entry>name</entry>

                  <entry>required</entry>

                  <entry>string</entry>

                  <entry>name of this DAX (or set of DAXes). Must be
                  3.2.</entry>
                </row>

                <row>
                  <entry>count</entry>

                  <entry>optional</entry>

                  <entry>positiveInteger</entry>

                  <entry>size of list of DAXes with this
                  <emphasis>name</emphasis>. Defaults to 1.</entry>
                </row>

                <row>
                  <entry>index</entry>

                  <entry>optional</entry>

                  <entry>nonNegativeInteger</entry>

                  <entry>current index of DAX with same
                  <emphasis>name</emphasis>. Defaults to 0.</entry>
                </row>

                <row>
                  <entry>fileCount</entry>

                  <entry>removed</entry>

                  <entry>nonNegativeInteger</entry>

                  <entry>Old 2.1 attribute, removed, do not use.</entry>
                </row>

                <row>
                  <entry>jobCount</entry>

                  <entry>removed</entry>

                  <entry>positiveInteger</entry>

                  <entry>Old 2.1 attribute, removed, do not use.</entry>
                </row>

                <row>
                  <entry>childCount</entry>

                  <entry>removed</entry>

                  <entry>nonNegativeInteger</entry>

                  <entry>Old 2.1 attribute, removed, do not use.</entry>
                </row>
              </tbody>
            </tgroup>
          </table>

          <para>The <emphasis>version</emphasis> attribute is restricted to
          the regular expression <code>\d+(\.\d+(\.\d+)?)?</code>.This
          expression represents the <emphasis>VersionPattern</emphasis> type
          that is used in other places, too. It is a more restrictive
          expression than before, but allows us to compute comparable version
          number using the following formula:</para>

          <informaltable border="1">
            <tr>
              <td>version1: a.b.c</td>

              <td>version2: d.e.f</td>
            </tr>

            <tr>
              <td>n = a * 1,000,000 + b * 1,000 + c</td>

              <td>m = d * 1,000,000 + e * 1,000 + f</td>
            </tr>

            <tr>
              <td colspan="2">version1 &gt; version2 if n &gt; m</td>
            </tr>
          </informaltable>
        </section>

        <section>
          <title>The Catalogs Section</title>

          <para>The initial section features three sub-sections:</para>

          <orderedlist>
            <listitem>
              <para>a catalog of files used,</para>
            </listitem>

            <listitem>
              <para>a catalog of transformations used, and</para>
            </listitem>

            <listitem>
              <para>compound transformation declarations.</para>
            </listitem>
          </orderedlist>

          <section id="dax_replica_catalog">
            <title>The Replica Catalog Section</title>

            <para>The file section acts as in in-file replica catalog (RC).
            Any files declared in this section take precedence over files in
            external replica catalogs during planning.</para>

            <programlisting>  &lt;!-- part 1.1: included replica catalog --&gt;
  &lt;file name="example.a" &gt;
    &lt;!-- profiles are optional --&gt;
    &lt;!-- The "stat" namespace is ONLY AN EXAMPLE --&gt;
    &lt;profile namespace="stat" key="size"&gt;/* integer to be defined */&lt;/profile&gt;
    &lt;profile namespace="stat" key="md5sum"&gt;/* 32 char hex string */&lt;/profile&gt;
    &lt;profile namespace="stat" key="mtime"&gt;/* ISO-8601 timestamp */&lt;/profile&gt;

    &lt;!-- metadata is currently NOT SUPPORTED --&gt;
    &lt;metadata key="timestamp" type="int"&gt;/* ISO-8601 *or* 20100417134523:int */&lt;/metadata&gt;
    &lt;metadata key="origin" type="string"&gt;ocean&lt;/metadata&gt;
    
    &lt;!-- PFN to by-pass replica catalog --&gt;
    &lt;!-- The "site attribute is optional --&gt;
    &lt;pfn url="file:///tmp/example.a" site="local"&gt;
      &lt;profile namespace="stat" key="owner"&gt;voeckler&lt;/profile&gt;
    &lt;/pfn&gt;
    &lt;pfn url="file:///storage/funky.a" site="local"/&gt;    
  &lt;/file&gt;

  &lt;!-- a more typical example from the black diamond --&gt;
  &lt;file name="f.a"&gt;
    &lt;pfn url="file:///Users/voeckler/f.a" site="local"/&gt;
  &lt;/file&gt;</programlisting>

            <para>The first <emphasis>file</emphasis> entry above is an
            example of a data file with two replicas. The
            <emphasis>file</emphasis> element requires a logical file
            <emphasis>name</emphasis>. Each logical filename may have
            additional information associated with it, enumerated by
            <emphasis>profile</emphasis> elements. Each file entry may have 0
            or more <emphasis>metadata</emphasis> associated with it. Each
            piece of metadata has a <emphasis>key</emphasis> string and
            <emphasis>type</emphasis> attribute describing the element's
            value.</para>

            <warning>
              <para>The <emphasis>metadata</emphasis> element is not support
              as of this writing! Details may change in the future.</para>
            </warning>

            <para>The <emphasis>file</emphasis> element can provide 0 or more
            <emphasis>pfn</emphasis> locations, taking precedence over the
            replica catalog. A <emphasis>file</emphasis> element that does not
            name any <emphasis>pfn</emphasis> children-elements will still
            require look-ups in external replica catalogs. Each
            <emphasis>pfn</emphasis> element names a concrete location of a
            file. Multiple locations constitute replicas of the same file, and
            are assumed to be usable interchangably. The
            <emphasis>url</emphasis> attribute is mandatory, and typically
            would use a file schema URL. The <emphasis>site</emphasis>
            attribute is optional, and defaults to value
            <emphasis>local</emphasis> if missing. A <emphasis>pfn</emphasis>
            element may have <emphasis>profile</emphasis> children-elements,
            which refer to attributes of the physical file. The file-level
            profiles refer to attributes of the logical file.</para>

            <note>
              <para>The <literal>stat</literal> profile namespace is ony an
              example, and details about stat are not yet implemented. The
              proper namespaces <literal>pegasus</literal>,
              <literal>condor</literal>, <literal>dagman</literal>,
              <literal>env</literal>, <literal>hints</literal>,
              <literal>globus</literal> and <literal>selector</literal> enjoy
              full support.</para>
            </note>

            <para>The second <emphasis>file</emphasis> entry above shows a
            usage example from the black-diamond example workflow that you are
            more likely to encouter or write.</para>

            <para>The presence of an in-file replica catalog lets you declare
            a couple of interesting advanced features. The DAG and DAX file
            declarations are just files for all practical purposes. For
            deferred planning, the location of the site catalog (SC) can be
            captured in a file, too, that is passed to the job dealing with
            the deferred planning as logical filename.</para>

            <programlisting>  &lt;file name="black.dax" &gt;
    &lt;!-- specify the location of the DAX file --&gt;
    &lt;pfn url="file:///Users/vahi/Pegasus/work/dax-3.0/blackdiamond_dax.xml" site="local"/&gt;
  &lt;/file&gt;

  &lt;file name="black.dag" &gt;
    &lt;!-- specify the location of the DAG file --&gt;
    &lt;pfn url="file:///Users/vahi/Pegasus/work/dax-3.0/blackdiamond.dag" site="local"/&gt;
  &lt;/file&gt;
  
  &lt;file name="sites.xml" &gt;
    &lt;!-- specify the location of a site catalog to use for deferred planning --&gt;
    &lt;pfn url="file:///Users/vahi/Pegasus/work/dax-3.0/conf/sites.xml" site="local"/&gt;
  &lt;/file&gt;</programlisting>
          </section>

          <section id="dax_transformation_catalog" label="">
            <title>The Transformation Catalog Section</title>

            <para>The executable section acts as an in-file transformation
            catalog (TC). Any transformations declared in this section take
            precedence over the external transformation catalog during
            planning.</para>

            <programlisting>  &lt;!-- part 1.2: included transformation catalog --&gt;
  &lt;executable namespace="example" name="mDiffFit" version="1.0" 
              arch="x86_64" os="linux" installed="true" &gt;
    &lt;!-- profiles are optional --&gt;
    &lt;!-- The "stat" namespace is ONLY AN EXAMPLE! --&gt;
    &lt;profile namespace="stat" key="size"&gt;5000&lt;/profile&gt;
    &lt;profile namespace="stat" key="md5sum"&gt;AB454DSSDA4646DS&lt;/profile&gt;
    &lt;profile namespace="stat" key="mtime"&gt;2010-11-22T10:05:55.470606000-0800&lt;/profile&gt;

    &lt;!-- metadata is currently NOT SUPPORTED! --&gt;
    &lt;metadata key="timestamp" type="int"&gt;/* see above */&lt;/metadata&gt;
    &lt;metadata key="origin" type="string"&gt;ocean&lt;/metadata&gt;
 
    &lt;!-- PFN to by-pass transformation catalog --&gt;
    &lt;!-- The "site" attribute is optional --&gt;
    &lt;pfn url="file:///tmp/mDiffFit"          site="local"/&gt;     
    &lt;pfn url="file:///tmp/storage/mDiffFit"  site="local"/&gt;     
  &lt;/executable&gt;

  &lt;!-- to be used in compound transformation later --&gt;
  &lt;executable namespace="example" name="mDiff" version="1.0" 
              arch="x86_64" os="linux" installed="true" &gt;
    &lt;pfn url="file:///tmp/mDiff" site="local"/&gt;        
  &lt;/executable&gt;

  &lt;!-- to be used in compound transformation later --&gt;
  &lt;executable namespace="example" name="mFitplane" version="1.0"
              arch="x86_64" os="linux" installed="true" &gt;
    &lt;pfn url="file:///tmp/mDiffFitplane"  site="local"&gt;
      &lt;profile namespace="stat" key="md5sum"&gt;0a9c38b919c7809cb645fc09011588a6&lt;/profile&gt;
    &lt;/pfn&gt;
  &lt;/executable&gt;

  &lt;!-- a more likely example from the black diamond --&gt;
  &lt;executable namespace="diamond" name="preprocess" version="2.0" 
              arch="x86_64"
              os="linux" 
              osversion="2.6.18"&gt;
    &lt;pfn url="file:///opt/pegasus/default/bin/keg" site="local" /&gt;
  &lt;/executable&gt;</programlisting>

            <para>Logical filenames pertaining to a single executables in the
            transformation catalog use the <emphasis>executable</emphasis>
            element. Any <emphasis>executable</emphasis> element features the
            optional <emphasis>namespace</emphasis> attribute, a mandatory
            <emphasis>name</emphasis> attribute, and an optional
            <emphasis>version</emphasis> attribute. The
            <emphasis>version</emphasis> attribute defaults to "1.0" when
            absent. An executable typically needs additional attributes to
            describe it properly, like the architecture, OS release and other
            flags typically seen with transformations, or found in the
            transformation catalog.</para>

            <table>
              <tgroup cols="4">
                <thead>
                  <row>
                    <entry>attribute</entry>

                    <entry>optional?</entry>

                    <entry>type</entry>

                    <entry>meaning</entry>
                  </row>
                </thead>

                <tbody>
                  <row>
                    <entry>name</entry>

                    <entry>required</entry>

                    <entry>string</entry>

                    <entry>logical transformation name</entry>
                  </row>

                  <row>
                    <entry>namespace</entry>

                    <entry>optional</entry>

                    <entry>string</entry>

                    <entry>namespace of logical transformation, default to
                    <emphasis>null</emphasis> value.</entry>
                  </row>

                  <row>
                    <entry>version</entry>

                    <entry>optional</entry>

                    <entry>VersionPattern</entry>

                    <entry>version of logical transformation, defaults to
                    "1.0".</entry>
                  </row>

                  <row>
                    <entry>installed</entry>

                    <entry>optional</entry>

                    <entry>boolean</entry>

                    <entry>whether to stage the file (false), or not (true,
                    default).</entry>
                  </row>

                  <row>
                    <entry>arch</entry>

                    <entry>optional</entry>

                    <entry>Architecture</entry>

                    <entry>restricted set of tokens, see schema definition
                    file.</entry>
                  </row>

                  <row>
                    <entry>os</entry>

                    <entry>optional</entry>

                    <entry>OSType</entry>

                    <entry>restricted set of tokens, see schema definition
                    file.</entry>
                  </row>

                  <row>
                    <entry>osversion</entry>

                    <entry>optional</entry>

                    <entry>VersionPattern</entry>

                    <entry>kernel version as beginning of `uname -r`.</entry>
                  </row>

                  <row>
                    <entry>glibc</entry>

                    <entry>optional</entry>

                    <entry>VersionPattern</entry>

                    <entry>version of libc.</entry>
                  </row>
                </tbody>
              </tgroup>
            </table>

            <para>The rationale for giving these flags in the
            <emphasis>executable</emphasis> element header is that PFNs are
            just identical replicas or instances of a given LFN. If you need a
            different 32/64 bit-ed-ness or OS release, the underlying PFN
            would be different, and thus the LFN for it should be different,
            too.</para>

            <note>
              <para>We are still discussing some details and implications of
              this decision.</para>
            </note>

            <para>The initial examples come with the same caveats as for the
            included replica catalog.</para>

            <warning>
              <para>The <emphasis>metadata</emphasis> element is not support
              as of this writing! Details may change in the future.</para>
            </warning>

            <para>Similar to the replica catalog, each
            <emphasis>executable</emphasis> element may have 0 or more
            <emphasis>profile</emphasis> elements abstracting away
            site-specific details, zero or more <emphasis>metadata</emphasis>
            elements, and zero or more <emphasis>pfn</emphasis> elements. If
            there are no <emphasis>pfn</emphasis> elements, the transformation
            must still be searched for in the external transformation catalog.
            As before, the <emphasis>pfn</emphasis> element may have
            <emphasis>profile</emphasis> children-elements, referring to
            attributes of the physical filename itself.</para>

            <para>The last example above comes from the black diamond example
            workflow, and presents the kind and extend of attributes you are
            most likely to see and use in your own workflows.</para>
          </section>

          <section>
            <title>The Compound Transformation Section</title>

            <para>The compound transformation section declares a
            transformation that comprises multiple plain transformation. You
            can think of a compound transformation like a script interpreter
            and the script itself. In order to properly run the application,
            you must start both, the script interpreter and the script passed
            to it. The compound transformation helps Pegasus to properly deal
            with this case, especially when it needs to stage
            executables.</para>

            <programlisting>  &lt;transformation namespace="example" version="1.0" name="mDiffFit" &gt;
    &lt;uses name="mDiffFit" /&gt;
    &lt;uses name="mDiff" namespace="example" version="2.0" /&gt;
    &lt;uses name="mFitPlane" /&gt;
    &lt;uses name="mDiffFit.config" executable="false" /&gt;
  &lt;/transformation&gt;</programlisting>

            <para>A <emphasis>transformation</emphasis> element declares a set
            of purely logical entities, executables and config (data) files,
            that are all required together for the same job. Being purely
            logical entities, the lookup happens only when the transformation
            element is referenced (or instantiated) by a job element later
            on.</para>

            <para>The <emphasis>namespace</emphasis> and
            <emphasis>version</emphasis> attributes of the transformation
            element are optional, and provide the defaults for the inner uses
            elements. They are also essential for matching the transformation
            with a job.</para>

            <para>The <emphasis>transformation</emphasis> is made up of 1 or
            more <emphasis>uses</emphasis> element. Each
            <emphasis>uses</emphasis> has a boolean attribute
            <emphasis>executable</emphasis>, <literal>true</literal> by
            default, or <literal>false</literal> to indicate a data file. The
            <emphasis>name</emphasis> is a mandatory attribute, refering to an
            LFN declared previously in the File Catalog
            (<emphasis>executable</emphasis> is <literal>false</literal>),
            Executable Catalog (<emphasis>executable</emphasis> is
            <literal>true</literal>), or to be looked up as necessary at
            instantiation time. The lookup catalog is determined by the
            <emphasis>executable</emphasis> attribute.</para>

            <para>The <emphasis>namespace</emphasis> and
            <emphasis>version</emphasis> attributes' default values inside
            <emphasis>uses</emphasis> elements are inherited from the
            <emphasis>transformation</emphasis> attributes of the same name.
            There is no such inheritance for <emphasis>uses</emphasis>
            elements with <emphasis>executable</emphasis> attribute of
            <literal>false</literal>.</para>
          </section>
        </section>

        <section id="api-graph-nodes">
          <title>Graph Nodes</title>

          <para>The nodes in the DAX comprise regular job nodes, already
          instantiated sub-workflows as dag nodes, and still to be
          instantiated dax nodes. Each of the graph nodes can has a mandatory
          <emphasis>id</emphasis> attribute. The <emphasis>id</emphasis>
          attribute is currently a restriction of type
          <code>xsd:NMTOKEN</code> type.The <emphasis>level</emphasis>
          attribute is being phased out, as the planner will trust its own
          re-computation more than user input. The
          <emphasis>node-label</emphasis> attribute is optional. It applies to
          the use-case when every transformation has the same name, but its
          arguments determine what it really does. In the presence of a
          <emphasis>node-label</emphasis> value, a workflow grapher could use
          the label value to show graph nodes to the user. It may also come in
          handy while debugging.</para>

          <para>Any job-like graph node has the following set of children
          elements, as defined in the <emphasis>AbstractJobType</emphasis>
          declaration in the schema definition:</para>

          <itemizedlist>
            <listitem>
              <para>0 or 1 <emphasis>argument</emphasis> element to declare
              the command-line of the job's invocation.</para>
            </listitem>

            <listitem>
              <para>0 or more <emphasis>profile</emphasis> elements to
              abstract away site-specific or job-specific details.</para>
            </listitem>

            <listitem>
              <para>0 or 1 <emphasis>stdin</emphasis> element to link a
              logical file the the job's standard input.</para>
            </listitem>

            <listitem>
              <para>0 or 1 <emphasis>stdout</emphasis> element to link a
              logical file to the job's standard output.</para>
            </listitem>

            <listitem>
              <para>0 or 1 <emphasis>stderr</emphasis> element to link a
              logical file to the job's standard error.</para>
            </listitem>

            <listitem>
              <para>0 or more <emphasis>uses</emphasis> elements to declare
              consumed data files and produced data files.</para>
            </listitem>

            <listitem>
              <para>0 or more <emphasis>invoke</emphasis> elements</para>
            </listitem>
          </itemizedlist>

          <warning>
            <para>The invoke element is not supported as of this
            writing.</para>
          </warning>

          <section id="api-job-nodes">
            <title>Job Nodes</title>

            <para>A job element has a number of attributes. In addition to the
            <emphasis>id</emphasis> and <emphasis>node-label</emphasis>
            described in (Graph Nodes)above, the optional
            <emphasis>namespace</emphasis>, mandatory
            <emphasis>name</emphasis> and optional
            <emphasis>version</emphasis> identify the transformation, and
            provide the look-up handle: first in the DAX's
            <emphasis>transformation</emphasis> elements, then in the
            <emphasis>executable</emphasis> elements, and finally in an
            external transformation catalog.</para>

            <programlisting>  &lt;!-- part 2: definition of all jobs (at least one) --&gt;
  &lt;job id="ID000001" namespace="example" name="mDiffFit" version="1.0" 
       node-label="preprocess" &gt;
    &lt;argument&gt;-a top -T 6  -i &lt;file name="f.a"/&gt;  -o &lt;file name="f.b1"/&gt;&lt;/argument&gt;

    &lt;!-- profiles are optional --&gt;
    &lt;profile namespace="execution" key="site"&gt;isi_viz&lt;/profile&gt;
    &lt;profile namespace="condor" key="getenv"&gt;true&lt;/profile&gt;

    &lt;uses name="f.a" link="input"  register="false" transfer="true" type="data" /&gt;
    &lt;uses name="f.b" link="output" register="false" transfer="true" type="data" /&gt;
    
    &lt;!-- 'WHEN' enumeration: never, start, on_error, on_success, on_end, all --&gt;
    &lt;!-- PEGASUS_* env-vars: event, status, submit dir, wf/job id, stdout, stderr --&gt;
    &lt;invoke when="start"&gt;/path/to arg arg&lt;/invoke&gt;
    &lt;invoke when="on_success"&gt;&lt;![CDATA[/path/to arg arg]]&gt;&lt;/invoke&gt;
    &lt;invoke when="on_end"&gt;&lt;![CDATA[/path/to arg arg]]&gt;&lt;/invoke&gt;
  &lt;/job&gt;</programlisting>

            <para>The <emphasis>argument</emphasis> element contains the
            complete command-line that is needed to invoke the executable. The
            only variable components are logical filenames, as included
            <emphasis>file</emphasis> elements.</para>

            <para>The <emphasis>profile</emphasis> argument lets you
            encapsulate site-specific knowledge .</para>

            <para>The <emphasis>stdin</emphasis>, <emphasis>stdout</emphasis>
            and <emphasis>stderr</emphasis> element permits you to connect a
            stdio file descriptor to a logical filename. Note that you will
            still have to declare these files in the <emphasis>uses</emphasis>
            section below.</para>

            <para>The <emphasis>uses</emphasis> element enumerates all the
            files that the task consumes or produces. While it is not
            necessary nor required to have all files appear on the
            command-line, it is imperative that you declare even hidden files
            that your task requires in this section, so that the proper
            ancilliary staging- and clean-up tasks can be generated during
            planning.</para>

            <warning>
              <para>The <emphasis>invoke</emphasis> element is not supported
              as of this writing!</para>
            </warning>

            <para>The invoke element may be specified multiple times, as
            needed. It has a mandatory when attribute with the following value
            set:</para>

            <table>
              <tgroup cols="3">
                <thead>
                  <row>
                    <entry align="center">keyword</entry>

                    <entry align="center">when</entry>

                    <entry align="center">meaning</entry>
                  </row>
                </thead>

                <tbody>
                  <row>
                    <entry>never</entry>

                    <entry>never</entry>

                    <entry>(default). Never notify of anything. This is useful
                    to temporarily disable an existing notifications.</entry>
                  </row>

                  <row>
                    <entry>start</entry>

                    <entry>submit</entry>

                    <entry>create a notification when the job is
                    submitted.</entry>
                  </row>

                  <row>
                    <entry>start</entry>

                    <entry>submit</entry>

                    <entry>create a notification when the job is
                    submitted.</entry>
                  </row>

                  <row>
                    <entry>on_error</entry>

                    <entry>end</entry>

                    <entry>after a job finishes with failure (exitcode !=
                    0).</entry>
                  </row>

                  <row>
                    <entry>on_success</entry>

                    <entry>end</entry>

                    <entry>after a job finishes with success (exitcode ==
                    0).</entry>
                  </row>

                  <row>
                    <entry>at_end</entry>

                    <entry>end</entry>

                    <entry>after a job finishes, regardless of
                    exitcode.</entry>
                  </row>

                  <row>
                    <entry>all</entry>

                    <entry>always</entry>

                    <entry>like start and at_end combined.</entry>
                  </row>
                </tbody>
              </tgroup>
            </table>

            <para>Again, even though we define the <emphasis>invoke</emphasis>
            element, at this point, it is not supported. Each
            <emphasis>invoke</emphasis> is a simple local invocation of an
            executable or script with the specified arguments. Pegasus will
            plan any notification into the pre- and post-script of DAGMan. The
            executable inside the invoke body will see the following
            environment variables:</para>

            <table>
              <tgroup cols="3">
                <thead>
                  <row>
                    <entry align="center">variable</entry>

                    <entry align="center">when</entry>

                    <entry align="center">meaning</entry>
                  </row>
                </thead>

                <tbody>
                  <row>
                    <entry>PEGASUS_EVENT</entry>

                    <entry>always</entry>

                    <entry>The value of the when attribute</entry>
                  </row>

                  <row>
                    <entry>PEGASUS_STATUS</entry>

                    <entry>end</entry>

                    <entry>The exit status of the graph node. Only available
                    for end notifications.</entry>
                  </row>

                  <row>
                    <entry>PEGASUS_SUBMIT_DIR</entry>

                    <entry>always</entry>

                    <entry>In which directory to find the job (or
                    workflow).</entry>
                  </row>

                  <row>
                    <entry>PEGASUS_JOBID</entry>

                    <entry>always</entry>

                    <entry>The job (or workflow) identifier. This is
                    potentially more than merely the value of the
                    <emphasis>id</emphasis> attribute.</entry>
                  </row>

                  <row>
                    <entry>PEGASUS_STDOUT</entry>

                    <entry>always</entry>

                    <entry>The filename where <emphasis>stdout</emphasis>
                    goes. Empty and possibly non-existent at submit time
                    (though we still have the filename). The kickstart record
                    for job nodes.</entry>
                  </row>

                  <row>
                    <entry>PEGASUS_STDERR</entry>

                    <entry>always</entry>

                    <entry>The filename where <emphasis>stderr</emphasis>
                    goes. Empty and possibly non-existent at submit time
                    (though we still have the filename).</entry>
                  </row>
                </tbody>
              </tgroup>
            </table>

            <para>Generators should use CDATA encapsulated values to the
            invoke element to minimize interference. Unfortunately, CDATA
            cannot be nested, so if the user invocation contains a CDATA
            section, we suggest that they use careful XML-entity escaped
            strings.</para>
          </section>

          <section>
            <title>DAG Nodes</title>

            <para>A workflow that has already been concretized, either by an
            earlier run of Pegasus, or otherwise constructed for DAGMan
            execution, can be included into the current workflow using the
            <emphasis>dag</emphasis> element.</para>

            <programlisting>  &lt;dag id="ID000003" name="black.dag" node-label="foo" &gt;
    &lt;profile namespace="dagman" key="DIR"&gt;/dag-dir/test&lt;/profile&gt;
    &lt;invoke&gt; &lt;!-- optional, should be possible --&gt; &lt;/invoke&gt;
    &lt;uses file="sites.xml" link="input" register="false" transfer="true" type="data"/&gt;     
  &lt;/dag&gt;</programlisting>

            <para>The <emphasis>id</emphasis> and
            <emphasis>node-label</emphasis> attributes were described <link
            linkend="api-graph-nodes">previously</link>. The
            <emphasis>name</emphasis> attribute refers to a file from the File
            Catalog that provides the actual DAGMan DAG as data content. The
            <emphasis>dag</emphasis> element features optional
            <emphasis>profile</emphasis> elements. These would most likely
            pertain to the <literal>dagman</literal> and
            <literal>env</literal> profile namespaces. It should be possible
            to have the optional <emphasis>notify</emphasis> element in the
            same manner as for jobs.</para>

            <warning>
              <para>The <emphasis>invoke</emphasis> element is not supported
              as of this writing!</para>
            </warning>

            <para>A graph node that is a dag instead of a job would just use a
            different submit file generator to create a DAGMan invocation.
            There can be an <emphasis>argument</emphasis> element to modify
            the command-line passed to DAGMan.</para>
          </section>

          <section>
            <title>DAX Nodes</title>

            <para>A still to be planned workflow incurs an invocation of the
            Pegasus planner as part of the workflow. This still abstract
            sub-workflow uses the <emphasis>dax</emphasis> element.</para>

            <programlisting>  &lt;dax id="ID000002" name="black.dax" node-label="bar" &gt;
    &lt;profile namespace="env" key="foo"&gt;bar&lt;/profile&gt;
    &lt;argument&gt;-Xmx1024 -Xms512 -Dpegasus.dir.storage=storagedir  -Dpegasus.dir.exec=execdir -o local --dir ./datafind -vvvvv --force -s dax_site &lt;/argument&gt;
    &lt;invoke&gt; &lt;!-- optional, may not be possible here --&gt; &lt;/invoke&gt;
    &lt;uses file="sites.xml" link="input" register="false" transfer="true" type="data" /&gt;
  &lt;/dax&gt;</programlisting>

            <para>In addition to the <emphasis>id</emphasis> and
            <emphasis>node-label</emphasis> attributes, See <link
            linkend="api-graph-nodes">Graph Nodes</link>. The
            <emphasis>name</emphasis> attribute refers to a file from the File
            Catalog that provides the to be planned DAX as external file data
            content. The <emphasis>dax</emphasis> element features optional
            <emphasis>profile</emphasis> elements. These would most likely
            pertain to the <literal>pegasus</literal>,
            <literal>dagman</literal> and <literal>env</literal> profile
            namespaces. It may be possible to have the optional
            <emphasis>notify</emphasis> element in the same manner as for
            jobs.</para>

            <warning>
              <para>The <emphasis>invoke</emphasis> element is not supported
              as of this writing!</para>
            </warning>

            <para>A graph node that is a <emphasis>dax</emphasis> instead of a
            job would just use yet another submit file and pre-script
            generator to create a DAGMan invocation. The
            <emphasis>argument</emphasis> string pertains to the command line
            of the to-be-generated DAGMan invocation.</para>
          </section>

          <section>
            <title>Inner ADAG Nodes</title>

            <para>While completeness would argue to have a recursive nesting
            of <emphasis>adag</emphasis> elements, such recursive nestings are
            currently not supported, not even in the schema. If you need to
            nest workflows, please use the <emphasis>dax</emphasis> or
            <emphasis>dag</emphasis> element to achieve the same goal.</para>
          </section>
        </section>

        <section>
          <title>The Dependency Section</title>

          <para>This section describes the dependencies between the
          jobs.</para>

          <programlisting>  &lt;!-- part 3: list of control-flow dependencies --&gt;
  &lt;child ref="ID000002"&gt;
    &lt;parent ref="ID000001" edge-label="edge1" /&gt;
  &lt;/child&gt;
  &lt;child ref="ID000003"&gt;
    &lt;parent ref="ID000001" edge-label="edge2" /&gt;
  &lt;/child&gt;
  &lt;child ref="ID000004"&gt;
    &lt;parent ref="ID000002" edge-label="edge3" /&gt;
    &lt;parent ref="ID000003" edge-label="edge4" /&gt;
  &lt;/child&gt;</programlisting>

          <para>Each <emphasis>child</emphasis> element contains one or more
          <emphasis>parent</emphasis> element. Either element refers to a
          <emphasis>job</emphasis>, <emphasis>dag</emphasis> or
          <emphasis>dax</emphasis> element id attribute using the
          <emphasis>ref</emphasis> attribute. In this version, we relaxed the
          <code>xs:IDREF</code> constraint in favor of a restriction on the
          <code>xs:NMTOKEN</code> type to permit a larger set of
          identifiers.</para>

          <para>The <emphasis>parent</emphasis> element has an optional
          <emphasis>edge-label</emphasis> attribute.</para>

          <warning>
            <para>The <emphasis>edge-label</emphasis> attribute is currently
            unused.</para>
          </warning>

          <para>Its goal is to annotate edges when drawing workflow
          graphs.</para>
        </section>

        <section>
          <title>Closing</title>

          <para>As any XML element, the root element needs to be
          closed.</para>

          <programlisting>&lt;/adag&gt;</programlisting>
        </section>
      </section>

      <section>
        <title>DAX XML Schema Example</title>

        <para>The following code example shows the XML instance document
        representing the diamond workflow.</para>

        <programlisting>&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;!-- generated: 2010-11-19T20:00:29Z --&gt;
&lt;adag xmlns="http://pegasus.isi.edu/schema/DAX" 
      xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
      xsi:schemaLocation="http://pegasus.isi.edu/schema/DAX http://pegasus.isi.edu/schema/dax-3.2.xsd"
      version="3.2" 
      name="diamond" 
      index="0" 
      count="1"&gt;
  &lt;!-- part 1.1: included replica catalog --&gt;
  &lt;file name="f.a"&gt;
    &lt;pfn url="file:///home/voeckler/f.a" site="local" /&gt;
  &lt;/file&gt;
  &lt;!-- part 1.2: included transformation catalog --&gt;
  &lt;executable namespace="diamond" name="preprocess" version="2.0" 
              arch="x86_64" os="linux" osversion="2.6.18"&gt;
    &lt;pfn url="file:///opt/pegasus/default/bin/keg" site="local" /&gt;
  &lt;/executable&gt;
  &lt;executable namespace="diamond" name="findrange" version="2.0" 
              arch="x86_64" os="linux" osversion="2.6.18"&gt;
    &lt;pfn url="file:///opt/pegasus/default/bin/keg" site="local" /&gt;
  &lt;/executable&gt;
  &lt;executable namespace="diamond" name="analyze" version="2.0" 
              arch="x86_64" os="linux" osversion="2.6.18"&gt;
    &lt;pfn url="file:///opt/pegasus/default/bin/keg" site="local" /&gt;
  &lt;/executable&gt;
  &lt;!-- part 2: definition of all jobs (at least one) --&gt;
  &lt;job namespace="diamond" name="preprocess" version="2.0" id="ID000001"&gt;
    &lt;argument&gt;-a preprocess -T60 -i &lt;file name="f.a" /&gt; -o &lt;file name="f.b1" /&gt; &lt;file name="f.b2" /&gt;&lt;/argument&gt;
    &lt;uses name="f.b2" link="output" register="false" transfer="false" /&gt;
    &lt;uses name="f.b1" link="output" register="false" transfer="false" /&gt;
    &lt;uses name="f.a" link="input" /&gt;
  &lt;/job&gt;
  &lt;job namespace="diamond" name="findrange" version="2.0" id="ID000002"&gt;
    &lt;argument&gt;-a findrange -T60 -i &lt;file name="f.b1" /&gt; -o &lt;file name="f.c1" /&gt;&lt;/argument&gt;
    &lt;uses name="f.b1" link="input" register="false" transfer="false" /&gt;
    &lt;uses name="f.c1" link="output" register="false" transfer="false" /&gt;
  &lt;/job&gt;
  &lt;job namespace="diamond" name="findrange" version="2.0" id="ID000003"&gt;
    &lt;argument&gt;-a findrange -T60 -i &lt;file name="f.b2" /&gt; -o &lt;file name="f.c2" /&gt;&lt;/argument&gt;
    &lt;uses name="f.c2" link="output" register="false" transfer="false" /&gt;
    &lt;uses name="f.b2" link="input" register="false" transfer="false" /&gt;
  &lt;/job&gt;
  &lt;job namespace="diamond" name="analyze" version="2.0" id="ID000004"&gt;
    &lt;argument&gt;-a analyze -T60 -i &lt;file name="f.c1" /&gt; &lt;file name="f.c2" /&gt; -o &lt;file name="f.d" /&gt;&lt;/argument&gt;
    &lt;uses name="f.c2" link="input" register="false" transfer="false" /&gt;
    &lt;uses name="f.d" link="output" register="false" transfer="true" /&gt;
    &lt;uses name="f.c1" link="input" register="false" transfer="false" /&gt;
  &lt;/job&gt;
  &lt;!-- part 3: list of control-flow dependencies --&gt;
  &lt;child ref="ID000002"&gt;
    &lt;parent ref="ID000001" /&gt;
  &lt;/child&gt;
  &lt;child ref="ID000003"&gt;
    &lt;parent ref="ID000001" /&gt;
  &lt;/child&gt;
  &lt;child ref="ID000004"&gt;
    &lt;parent ref="ID000002" /&gt;
    &lt;parent ref="ID000003" /&gt;
  &lt;/child&gt;
&lt;/adag&gt;</programlisting>

        <para>The above workflow defines the black diamond from the abstract
        workflow section of the <link linkend="about">Introduction</link>
        chapter. It will require minimal configuration, because the catalog
        sections include all necessary declarations.</para>

        <para>The file element defines the location of the required input file
        in terms of the local machine. Please note that</para>

        <itemizedlist>
          <listitem>
            <para>The <emphasis role="bold">file</emphasis> element declares
            the required input file "f.a" in terms of the local machine.
            Please note that if you plan the workflow for a remote site, the
            has to be some way for the file to be staged from the local site
            to the remote site. While Pegasus will augment the workflow with
            such ancillary jobs, the site catalog as well as local and remote
            site have to be set up properlyl. For a locally run workflow you
            don't need to do anything.</para>
          </listitem>

          <listitem>
            <para>The <emphasis role="bold">executable</emphasis> elements
            declare the same executable keg that is to be run for each the
            logical transformation in terms of the local site. To declare it
            for a remote site, you would have to adjust the
            <emphasis>site</emphasis> attribute's value to the proper remote
            site handle. This section also shows that the same executable may
            come in different guises as transformation.</para>
          </listitem>

          <listitem>
            <para>The <emphasis role="bold">job</emphasis> elements define the
            workflow's logical constituents, the way to invoke the
            <literal>keg</literal> command, where to put filenames on the
            commandline, and what files are consumed or produced. In addition
            to the direction of files, further attributes determine whether to
            register the file with a replica catalog and whether to transfer
            it to the output site in case of a product. We are only interested
            in the final data product "f.d" in this workflow, and not any
            intermediary files. Typically, you would also want to register the
            data products in the replica catalog, especially in larger
            scenarios.</para>
          </listitem>

          <listitem>
            <para>The <emphasis role="bold">child</emphasis> elements define
            the control flow between the jobs.</para>
          </listitem>
        </itemizedlist>
      </section>
    </section>

    <section>
      <title>DAX Generator API</title>

      <para>The DAX generating APIs support Java, Perl and Python. This
      section will show in each language the necessary code, using
      Pegasus-provided libraries, to generate the diamond DAX example above.
      There may be minor differences in details, e.g. to show-case certain
      features, but effectively all generate the same basic diamond.</para>

      <section id="api-java">
        <title>The Java DAX Generator API</title>

        <para>The Java DAX API provided with the Pegasus distribution allows
        easy creation of complex and huge workflows. This API is used by
        several applications to generate their abstract DAX. SCEC, which is
        Southern California Earthquake Center, uses this API in their
        CyberShake workflow generator to generate huge DAX containing
        10&amp;rsquor;s of thousands of tasks with 100&amp;rsquor;s of
        thousands of input and output files. The <ulink
        url="http://pegasus.isi.edu/wms/docs/3.0/javadoc/index.html">Java
        API</ulink> is well documented using <ulink
        url="http://pegasus.isi.edu/wms/docs/3.0/javadoc/edu/isi/pegasus/planner/dax/ADAG.html">Javadoc
        for ADAGs</ulink> .</para>

        <para>The steps involved in creating a DAX using the API are</para>

        <orderedlist>
          <listitem>
            <para>Create a new <emphasis>ADAG</emphasis> object</para>
          </listitem>

          <listitem>
            <para>Create <emphasis>File</emphasis> objects as necessary. You
            can augment the files with physical information, if you want to
            include them into your DAX. Otherwise, the physical information is
            determined from the replica catalog.</para>
          </listitem>

          <listitem>
            <para>(Optional) Create <emphasis>Executable</emphasis> objects,
            if you want to include your transformation catalog into your DAX.
            Otherwise, the translation of a job/task into executable location
            happens with the transformation catalog.</para>
          </listitem>

          <listitem>
            <para>Create a new <emphasis>Job</emphasis> object.</para>
          </listitem>

          <listitem>
            <para>Add arguments, files, profiles and other information to the
            <emphasis>Job</emphasis> object</para>
          </listitem>

          <listitem>
            <para>Add the job object to the <emphasis>ADAG</emphasis>
            object</para>
          </listitem>

          <listitem>
            <para>Repeat step 4-6 as necessary.</para>
          </listitem>

          <listitem>
            <para>Add all dependencies to the <emphasis>ADAG</emphasis>
            object.</para>
          </listitem>

          <listitem>
            <para>Call the <emphasis>writeToFile()</emphasis> method on the
            <emphasis>ADAG</emphasis> object to render the XML DAX
            file.</para>
          </listitem>
        </orderedlist>

        <para>An example Java code that generates the diamond dax show above
        is listed below. This same code can be found in the Pegasus
        distribution in the <filename
        class="directory">examples/grid-blackdiamond-java</filename> directory
        as <filename>BlackDiamonDAX.java</filename>:</para>

        <programlisting>/**
 *  Copyright 2007-2008 University Of Southern California
 *
 *  Licensed under the Apache License, Version 2.0 (the "License");
 *  you may not use this file except in compliance with the License.
 *  You may obtain a copy of the License at
 *
 *  http://www.apache.org/licenses/LICENSE-2.0
 *
 *  Unless required by applicable law or agreed to in writing,
 *  software distributed under the License is distributed on an "AS IS" BASIS,
 *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 *  See the License for the specific language governing permissions and
 *  limitations under the License.
 */

import edu.isi.pegasus.planner.dax.*;

public class BlackDiamondDAX {

    /**
     * Create an example DIAMOND DAX
     * @param args
     */
    public static void main(String[] args) {
        if (args.length != 3) {
            System.out.println("Usage: java ADAG &lt;site_handle&gt; &lt;pegasus_location&gt; &lt;filename.dax&gt;");
            System.exit(1);
        }

        try {
            Diamond(args[0], args[1]).writeToFile(args[2]);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

    }

    private static ADAG Diamond(String site_handle, String pegasus_location) throws Exception {

        java.io.File cwdFile = new java.io.File (".");
        String cwd = cwdFile.getCanonicalPath(); 

        ADAG dax = new ADAG("blackdiamond");

        File fa = new File("f.a");
        fa.addPhysicalFile("file://" + cwd + "/f.a", "local");
        dax.addFile(fa);

        File fb1 = new File("f.b1");
        File fb2 = new File("f.b2");
        File fc1 = new File("f.c1");
        File fc2 = new File("f.c2");
        File fd = new File("f.d");
        fd.setRegister(true);

        Executable preprocess = new Executable("pegasus", "preprocess", "4.0");
        preprocess.setArchitecture(Executable.ARCH.X86).setOS(Executable.OS.LINUX);
        preprocess.setInstalled(true);
        preprocess.addPhysicalFile("file://" + pegasus_location + "/bin/keg", site_handle);

        Executable findrange = new Executable("pegasus", "findrange", "4.0");
        findrange.setArchitecture(Executable.ARCH.X86).setOS(Executable.OS.LINUX);
        findrange.setInstalled(true);
        findrange.addPhysicalFile("file://" + pegasus_location + "/bin/keg", site_handle);

        Executable analyze = new Executable("pegasus", "analyze", "4.0");
        analyze.setArchitecture(Executable.ARCH.X86).setOS(Executable.OS.LINUX);
        analyze.setInstalled(true);
        analyze.addPhysicalFile("file://" + pegasus_location + "/bin/keg", site_handle);

        dax.addExecutable(preprocess).addExecutable(findrange).addExecutable(analyze);

        // Add a preprocess job
        Job j1 = new Job("j1", "pegasus", "preprocess", "4.0");
        j1.addArgument("-a preprocess -T 60 -i ").addArgument(fa);
        j1.addArgument("-o ").addArgument(fb1);
        j1.addArgument(" ").addArgument(fb2);
        j1.uses(fa, File.LINK.INPUT);
        j1.uses(fb1, File.LINK.OUTPUT);
        j1.uses(fb2, File.LINK.OUTPUT);
        dax.addJob(j1);

        // Add left Findrange job
        Job j2 = new Job("j2", "pegasus", "findrange", "4.0");
        j2.addArgument("-a findrange -T 60 -i ").addArgument(fb1);
        j2.addArgument("-o ").addArgument(fc1);
        j2.uses(fb1, File.LINK.INPUT);
        j2.uses(fc1, File.LINK.OUTPUT);
        dax.addJob(j2);

        // Add right Findrange job
        Job j3 = new Job("j3", "pegasus", "findrange", "4.0");
        j3.addArgument("-a findrange -T 60 -i ").addArgument(fb2);
        j3.addArgument("-o ").addArgument(fc2);
        j3.uses(fb2, File.LINK.INPUT);
        j3.uses(fc2, File.LINK.OUTPUT);
        dax.addJob(j3);

        // Add analyze job
        Job j4 = new Job("j4", "pegasus", "analyze", "4.0");
        j4.addArgument("-a analyze -T 60 -i ").addArgument(fc1);
        j4.addArgument(" ").addArgument(fc2);
        j4.addArgument("-o ").addArgument(fd);
        j4.uses(fc1, File.LINK.INPUT);
        j4.uses(fc2, File.LINK.INPUT);
        j4.uses(fd, File.LINK.OUTPUT);
        dax.addJob(j4);

        dax.addDependency("j1", "j2");
        dax.addDependency("j1", "j3");
        dax.addDependency("j2", "j4");
        dax.addDependency("j3", "j4");
        return dax;
    }
}</programlisting>

        <para>Of course, you will have to set up some catalogs and properties
        to run this example. The details are catpured in the examples
        directory <filename
        class="directory">examples/grid-blackdiamond-java</filename>.</para>
      </section>

      <section id="api-python">
        <title>The Python DAX Generator API</title>

        <para>Refer to the <ulink
        url="http://pegasus.isi.edu/wms/docs/3.0/python/">auto-generated
        python documentation</ulink> explaining this API.</para>

        <programlisting>#!/usr/bin/env python

from Pegasus.DAX3 import *
import sys
import os

if len(sys.argv) != 2:
        print "Usage: %s PEGASUS_HOME" % (sys.argv[0])
        sys.exit(1)

# Create a abstract dag
diamond = ADAG("diamond")

# Add input file to the DAX-level replica catalog
a = File("f.a")
a.addPFN(PFN("file://" + os.getcwd() + "/f.a", "local"))
diamond.addFile(a)
        
# Add executables to the DAX-level replica catalog
# In this case the binary is keg, which is shipped with Pegasus, so we use
# the remote PEGASUS_HOME to build the path.
e_preprocess = Executable(namespace="diamond", name="preprocess", version="4.0", os="linux", arch="x86_64")
e_preprocess.addPFN(PFN("file://" + sys.argv[1] + "/bin/keg", "TestCluster"))
diamond.addExecutable(e_preprocess)
        
e_findrange = Executable(namespace="diamond", name="findrange", version="4.0", os="linux", arch="x86_64")
e_findrange.addPFN(PFN("file://" + sys.argv[1] + "/bin/keg", "TestCluster"))
diamond.addExecutable(e_findrange)
        
e_analyze = Executable(namespace="diamond", name="analyze", version="4.0", os="linux", arch="x86_64")
e_analyze.addPFN(PFN("file://" + sys.argv[1] + "/bin/keg", "TestCluster"))
diamond.addExecutable(e_analyze)

# Add a preprocess job
preprocess = Job(namespace="diamond", name="preprocess", version="4.0")
b1 = File("f.b1")
b2 = File("f.b2")
preprocess.addArguments("-a preprocess","-T60","-i",a,"-o",b1,b2)
preprocess.uses(a, link=Link.INPUT)
preprocess.uses(b1, link=Link.OUTPUT)
preprocess.uses(b2, link=Link.OUTPUT)
diamond.addJob(preprocess)

# Add left Findrange job
frl = Job(namespace="diamond", name="findrange", version="4.0")
c1 = File("f.c1")
frl.addArguments("-a findrange","-T60","-i",b1,"-o",c1)
frl.uses(b1, link=Link.INPUT)
frl.uses(c1, link=Link.OUTPUT)
diamond.addJob(frl)

# Add right Findrange job
frr = Job(namespace="diamond", name="findrange", version="4.0")
c2 = File("f.c2")
frr.addArguments("-a findrange","-T60","-i",b2,"-o",c2)
frr.uses(b2, link=Link.INPUT)
frr.uses(c2, link=Link.OUTPUT)
diamond.addJob(frr)

# Add Analyze job
analyze = Job(namespace="diamond", name="analyze", version="4.0")
d = File("f.d")
analyze.addArguments("-a analyze","-T60","-i",c1,c2,"-o",d)
analyze.uses(c1, link=Link.INPUT)
analyze.uses(c2, link=Link.INPUT)
analyze.uses(d, link=Link.OUTPUT, register=True)
diamond.addJob(analyze)

# Add control-flow dependencies
diamond.addDependency(parent=preprocess, child=frl)
diamond.addDependency(parent=preprocess, child=frr)
diamond.addDependency(parent=frl, child=analyze)
diamond.addDependency(parent=frr, child=analyze)

# Write the DAX to stdout
diamond.writeXML(sys.stdout)</programlisting>
      </section>

      <section id="api-perl">
        <title>The Perl DAX Generator</title>

        <para>The Perl API example below can be found in file
        <filename>blackdiamond.pl</filename> in directory <filename
        class="directory">examples/grid-blackdiamond-perl</filename>. It
        requires that you set the environment variable
        <envar>PEGASUS_HOME</envar> to the installation directory of Pegasus,
        and include into <envar>PERL5LIB</envar> the path to the directory
        <filename class="directory">lib/perl</filename> of the Pegasus
        installation. The actual code is longer, and will not require these
        settings, only the example below does. The Perl API is documented
        using <ulink
        url="http://pegasus.isi.edu/wms/docs/3.0/perl/">perldoc</ulink>. For
        each of the modules you can invoke <application>perldoc</application>,
        if your <envar>PERL5LIB</envar> variable is set.</para>

        <para>The steps to generate a DAX from Perl are similar to the Java
        steps. However, since most methods to the classes are deeply within
        the Perl class modules, the convenience module
        <code>Perl::DAX::Factory</code> makes most constructors accessible
        without you needing to type your fingers raw:</para>

        <orderedlist>
          <listitem>
            <para>Create a new <emphasis>ADAG</emphasis> object.</para>
          </listitem>

          <listitem>
            <para>Create <emphasis>Job</emphasis> objects as necessary.</para>
          </listitem>

          <listitem>
            <para>As example, the required input file "f.a" is declared as
            <emphasis>File</emphasis> object and linked to the
            <emphasis>ADAG</emphasis> object.</para>
          </listitem>

          <listitem>
            <para>The first job arguments and files are filled into the job,
            and the job is added to the <emphasis>ADAG</emphasis>
            object.</para>
          </listitem>

          <listitem>
            <para>Repeat step 4 for the remaining jobs.</para>
          </listitem>

          <listitem>
            <para>Add dependencies for all jobs. You have the option of
            assigning label text to edges, though these are not used
            (yet).</para>
          </listitem>

          <listitem>
            <para>To generate the DAX file, invoke the
            <emphasis>toXML()</emphasis> method on the
            <emphasis>ADAG</emphasis> object. The first argument is an opened
            file handle or <code>IO::Handle</code> descriptor scalar to write
            to, the second the default indentation for the root element, and
            the third the XML namespace to use for elements and attributes.
            The latter is typically unused unless you want to include your
            output into another XML document.</para>
          </listitem>
        </orderedlist>

        <programlisting>#!/usr/bin/env perl
#
use 5.006;
use strict;
use IO::Handle; 
use Cwd;
##!! PEGASUS_HOME set to install dir of Pegasus
##!! PERL5LIB includes $PEGASUS_HOME/lib/perl
use Pegasus::DAX::Factory qw(:all); 

use constant NS =&gt; 'diamond'; 

my $adag = newADAG( name =&gt; NS ); 
my $job1 = newJob( namespace =&gt; NS, name =&gt; 'preprocess', version =&gt; '2.0' );
my $job2 = newJob( namespace =&gt; NS, name =&gt; 'findrange', version =&gt; '2.0' );
my $job3 = newJob( namespace =&gt; NS, name =&gt; 'findrange', version =&gt; '2.0' );
my $job4 = newJob( namespace =&gt; NS, name =&gt; 'analyze', version =&gt; '2.0' );

my $file = newFile( name =&gt; 'f.a' );
$file-&gt;addPFN( newPFN( url =&gt; 'file://' . Cwd::abs_path(getcwd()) . '/f.a', 
                       site =&gt; 'local' ) ); 
$adag-&gt;addFile($file); 

if ( exists $ENV{'PEGASUS_HOME'} ) {
    use File::Spec;
    use POSIX (); 
    my $keg = File::Spec-&gt;catfile( $ENV{'PEGASUS_HOME'}, 'bin', 'keg' ); 
    my @os = POSIX::uname(); 
    $os[2] =~ s/^(\d+(\.\d+(\.\d+)?)?).*/$1/;
    if ( -x $keg ) { 
        my $app1 = newExecutable( namespace =&gt; NS, name =&gt; 'preprocess', version =&gt; '2.0',
                                  arch =&gt; $os[4], os =&gt; lc($^O), osversion =&gt; $os[2] ); 
        $app1-&gt;addPFN( newPFN( url =&gt; "file://$keg", site =&gt; 'local' ) );
        $adag-&gt;addExecutable($app1); 
    }
}

my %hash = ( link =&gt; LINK_OUT, register =&gt; 'false', transfer =&gt; 'true' ); 
my $fna = newFilename( name =&gt; $file-&gt;name, link =&gt; LINK_IN );
my $fnb1 = newFilename( name =&gt; 'f.b1', %hash );
my $fnb2 = newFilename( name =&gt; 'f.b2', %hash ); 
$job1-&gt;addArgument( '-a', $job1-&gt;name, '-T60', '-i', $fna,
                    '-o', $fnb1, $fnb2 ); 
$adag-&gt;addJob($job1); 

my $fnc1 = newFilename( name =&gt; 'f.c1', %hash );
$fnb1-&gt;link( LINK_IN ); 
$job2-&gt;addArgument( '-a', $job2-&gt;name, '-T60', '-i', $fnb1, 
                    '-o', $fnc1 ); 
$adag-&gt;addJob($job2);

my $fnc2 = newFilename( name =&gt; 'f.c2', %hash );
$fnb2-&gt;link( LINK_IN ); 
$job3-&gt;addArgument( '-a', $job3-&gt;name, '-T60', '-i', $fnb2, 
                    '-o', $fnc2 ); 
$adag-&gt;addJob($job3);

# yeah, you can create multiple children for the same parent
# string labels are distinguished from jobs, no problem
$adag-&gt;addDependency( $job1, $job2, 'edge1', $job3, 'edge2' ); 

my $fnd = newFilename( name =&gt; 'f.d', %hash ); 
$fnc1-&gt;link( LINK_IN );
$fnc2-&gt;link( LINK_IN ); 
$job4-&gt;separator('');                 # just to show the difference wrt default
$job4-&gt;addArgument( '-a ', $job4-&gt;name, ' -T60 -i ', $fnc1, ' ', $fnc2, 
                    ' -o ', $fnd );
$adag-&gt;addJob($job4);
# this is a convenience function -- easier than overloading addDependency?
$adag-&gt;addInverse( $job4, $job2, 'edge3', $job3, 'edge4' );

my $xmlns = shift; 
$adag-&gt;toXML( \*STDOUT, '', $xmlns );</programlisting>
      </section>
    </section>

    <section>
      <title>DAX Generator without a Pegasus DAX API</title>

      <para>If you are using some other scripting or programming environment,
      you can directly write out the DAX format using the provided schema
      using any language. For instance, LIGO, the Laser Interferometer
      Gravitational Wave Observatory, generate their DAX files as XML using
      their own Python code, not using our provided API.</para>

      <para>If you write your own XML, you <emphasis>must</emphasis> ensure
      that the generated XML is well formed and valid with respect to the DAX
      schema. You can use the <command>pegasus-dax-validator</command> to
      verify the validity of your generated file. Typically, you generate a
      smallish test file to, validate that your generator creates valid XML
      using the validator, and then ramp it up to produce the full workflow(s)
      you want to run. At this point the
      <command>pegasus-dax-validator</command> is a very simple program that
      will only take exactly one argument, the name of the file to check.The
      following snippet checks a black-diamond file that uses an improper
      <emphasis>osversion</emphasis> attribute in its
      <emphasis>executable</emphasis> element:</para>

      <screen><prompt>$</prompt> <command>pegasus-dax-validator <replaceable>blackdiamond.dax</replaceable></command>
ERROR: cvc-pattern-valid: Value '2.6.18-194.26.1.el5' is not facet-valid
 with respect to pattern '[0-9]+(\.[0-9]+(\.[0-9]+)?)?' for type 'VersionPattern'.
ERROR: cvc-attribute.3: The value '2.6.18-194.26.1.el5' of attribute 'osversion'
 on element 'executable' is not valid with respect to its type, 'VersionPattern'.

0 warnings, 2 errors, and 0 fatal errors detected.</screen>

      <para>We are working on improving this program, e.g. provide output with
      regards to the line number where the issue occurred. However, it will
      return with a non-zero exit code whenever errors were detected.</para>
    </section>
  </section>

  <section>
    <title>Command Reference</title>

    <para>This chapter contains reference material for all the command-line
    tools distributed with Pegasus.</para>

    <section>
      <title>pegasus-version</title>

      <para>pegasus-version is a simple command-line tool that reports the
      version number of the Pegasus distribution being used.</para>

      <para>In its most basic invocation, it will show the current version of
      the Pegasus software you have installed: </para>

      <para><screen>$ <command>pegasus-version</command>
3.1.0cvs</screen>If you want to know more details about the installed version,
      i.e. which system it was compiled for and when, use the
      <emphasis>long</emphasis> or <emphasis>full</emphasis> mode: </para>

      <para><screen>$ <command>pegasus-version</command> <command><replaceable>-f</replaceable></command>
3.1.0cvs-x86_64_cent_5.6-20110706191019Z</screen>The reported version may
      sometimes not be the version you would expect, if the Pegasus jar file
      got updated but not the remainder of the installation environment. To
      check this case, you use <emphasis>match</emphasis> mode:</para>

      <para><screen>$ <command>pegasus-version</command> <command><replaceable>-m</replaceable></command>
Compiled into PEGASUS: 20110706191019Z x86_64_cent_5.6
Installation provides: 20110706191019Z x86_64_cent_5.6
OK: Internal version matches installation.
Complete version info: 3.1.0cvs-x86_64_cent_5.6-20110706191019Z</screen></para>

      <para>The <emphasis>match</emphasis> mode implies
      <emphasis>long</emphasis> mode, and will thus show the full version as
      part of its output. </para>

      <para>In <emphasis>quiet</emphasis> mode, which can only be applied to
      <emphasis>match</emphasis> mode, no output is written unless there is an
      error. You would use <emphasis>quiet</emphasis> mode to check the exit
      code of <literal>pegasus-version</literal> to determine a problem while
      being in a scripted environment. </para>

      <para><screen>$ <command>pegasus-version</command> <command><replaceable>-mq</replaceable></command>
$ <command>echo $?</command>
0</screen>The manual page for <literal>pegasus-version</literal> will show
      more details, and long options availble to the user. </para>
    </section>

    <section>
      <title>pegasus-plan</title>

      <para>pegasus-plan generates a concrete, executable workflow from an
      abstract workflow description (DAX).</para>
    </section>

    <section>
      <title>pegasus-run</title>

      <para>pegasus-run executes a workflow that has been planned using
      pegasus-plan.</para>
    </section>

    <section>
      <title>pegasus-remove</title>

      <para>pegasus-remove is used to abort a running workflow.</para>
    </section>

    <section>
      <title>pegasus-status</title>

      <para>pegasus-status reports on the status of a workflow.</para>
    </section>

    <section>
      <title>pegasus-analyzer</title>

      <para>pegasus-analyzer is used to debug failed workflows.</para>
    </section>

    <section>
      <title>pegasus-statistics</title>

      <para>pegasus-statistics reports statistics about a workflow.</para>
    </section>

    <section>
      <title>pegasus-plots</title>

      <para>pegasus-plots generates charts and graphs that illustrate the
      statistics and execution of a workflow.</para>
    </section>

    <section>
      <title>pegasus-transfer</title>

      <para>pegasus-transfer is a wrapper for several file transfer
      clients.</para>
    </section>

    <section>
      <title>pegasus-sc-client</title>

      <para>pegasus-sc-client is used to generate and modify site
      catalogs.</para>
    </section>

    <section>
      <title>pegasus-tc-client</title>

      <para>pegasus-tc-client is used to generate and modify transformation
      catalogs.</para>
    </section>

    <section>
      <title>pegasus-s3</title>

      <para>pegasus-s3 is a client for the Amazon S3 object storage service
      and any other storage services that conform to the Amazon S3 API, such
      as Eucalyptus Walrus.</para>

      <section>
        <title>URL Format</title>

        <para>All URLs for objects stored in S3 should be specified in the
        following format:</para>

        <programlisting>s3[s]://USER@SITE[/BUCKET[/KEY]]</programlisting>

        <para>The protocol part can be s3:// or s3s://. If s3s:// is used,
        then pegasus-s3 will force the connection to use SSL and override the
        setting in the configuration file. If s3:// is used, then whether the
        connection uses SSL or not is determined by the value of the
        'endpoint' variable in the configuration for the site.</para>

        <para>The <emphasis>USER@SITE</emphasis> part is required, but the
        <emphasis>BUCKET</emphasis> and <emphasis>KEY</emphasis> parts may be
        optional depending on the context.</para>

        <para>The <emphasis>USER@SITE</emphasis> portion is referred to as the
        'identity', and the <emphasis>SITE</emphasis> portion is referred to
        as the site. Both the identity and the site are looked up in the
        configuration file (see pegasus-s3 Configuration) to determine the
        parameters to use when establishing a connection to the service. The
        site portion is used to find the host and port, whether to use SSL,
        and other things. The identity portion is used to determine which
        authentication tokens to use. This format is designed to enable users
        to easily use multiple services with multiple authentication tokens.
        Note that neither the <emphasis>USER</emphasis> nor the
        <emphasis>SITE</emphasis> portion of the URL have any meaning outside
        of pegasus-s3. They do not refer to real usernames or hostnames, but
        are rather handles used to look up configuration values in the
        configuration file.</para>

        <para>The <emphasis>BUCKET</emphasis> portion of the URL is the part
        between the 3rd and 4th slashes. Buckets are part of a global
        namespace that is shared with other users of the storage service. As
        such, they should be unique.</para>

        <para>The <emphasis>KEY</emphasis> portion of the URL is anything
        after the 4th slash. Keys can include slashes, but S3-like storage
        services do not have the concept of a directory like regular file
        systems. Instead, keys are treated like opaque identifiers for
        individual objects. So, for example, the keys 'a/b' and 'a/c' have a
        common prefix, but cannot be said to be in the same
        'directory'.</para>

        <para>Some example URLs are:</para>

        <programlisting>s3://ewa@amazon
s3://juve@skynet/gideon.isi.edu
s3://juve@magellan/pegasus-images/centos-5.5-x86_64-20101101.part.1
s3s://ewa@amazon/pegasus-images/data.tar.gz</programlisting>
      </section>

      <section>
        <title>Subcommands</title>

        <para>pegasus-s3 has several subcommands for different storage service
        operations.</para>

        <section>
          <title>help</title>

          <para><emphasis role="bold">pegasus-s3 help</emphasis></para>

          <para>The <emphasis role="bold">help</emphasis> subcommand lists all
          available subcommands.</para>
        </section>

        <section>
          <title>ls</title>

          <para><emphasis role="bold">pegasus-s3 ls [options]
          URL...</emphasis></para>

          <para>The <emphasis role="bold">ls</emphasis> subcommand lists the
          contents of a URL. If the URL does not contain a bucket, then all
          the buckets owned by the user are listed. If the URL contains a
          bucket, but no key, then all the keys in the bucket are listed. If
          the URL contains a bucket and a key, then all keys in the bucket
          that begin with the specified key are listed.</para>
        </section>

        <section>
          <title>mkdir</title>

          <para><emphasis role="bold">pegasus-s3 mkdir [options]
          URL...</emphasis></para>

          <para>The <emphasis role="bold">mkdir</emphasis> subcommand creates
          one or more buckets.</para>
        </section>

        <section>
          <title>rmdir</title>

          <para><emphasis role="bold">pegasus-s3 rmdir [options]
          URL...</emphasis></para>

          <para>The <emphasis role="bold">rmdir</emphasis> subcommand deletes
          one or more buckets from the storage service. In order to delete a
          bucket, the bucket must be empty.</para>
        </section>

        <section>
          <title>rm</title>

          <para><emphasis role="bold">pegasus-s3 rm [options]
          URL...</emphasis></para>

          <para>The <emphasis role="bold">rm</emphasis> subcommand deletes one
          or more keys from the storage service.</para>
        </section>

        <section>
          <title>put</title>

          <para><emphasis role="bold">pegasus-s3 put [options] FILE
          URL</emphasis></para>

          <para>The <emphasis role="bold">put</emphasis> subcommand stores the
          file specified by <emphasis>FILE</emphasis> in the storage service
          under the bucket and key specified by <emphasis>URL</emphasis>. If
          the URL contains a bucket, but not a key, then the file name is used
          as the key.</para>

          <para>If a transient failure occurs, then the upload will be retried
          several times before pegasus-s3 gives up and fails.</para>

          <para>The put subcommand can do both chunked and parallel uploads if
          the service supports multipart uploads (see multipart_uploads in the
          configuration). Currently only Amazon S3 supports multipart
          uploads.</para>

          <para>This subcommand will check the size of the file to make sure
          it can be stored before attempting to store it.</para>

          <para>Chunked uploads are useful to reduce the probability of an
          upload failing. If an upload is chunked, then pegasus-s3 issues
          separate PUT requests for each chunk of the file. Specifying smaller
          chunks (using --chunksize) will reduce the chances of an upload
          failing due to a transient error. Chunksizes can range from 5 MB to
          1GB (chunk sizes smaller than 5 MB produced incomplete uploads on
          Amazon S3). The maximum number of chunks for any single file is
          10,000, so if a large file is being uploaded with a small chunksize,
          then the chunksize will be increased to fit within the 10,000 chunk
          limit. By default, the file will be split into 10 MB chunks if the
          storage service supports multipart uploads. Chunked uploads can be
          disabled by specifying a chunksize of 0. If the upload is chunked,
          then each chunk is retried independently under transient failures.
          If any chunk fails permanently, then the upload is aborted.</para>

          <para>Parallel uploads can increase performance for services that
          support multipart uploads. In a parallel upload the file is split
          into N chunks and each chunk is uploaded concurrently by one of M
          threads in first-come, first-served fashion. If the chunksize is set
          to 0, then parallel uploads are disabled. If M &gt; N, then the
          actual number of threads used will be reduced to N. The number of
          threads can be specified using the --parallel argument. If
          --parallel is 0 or 1, then only a single thread is used. The default
          value is 0. There is no maximum number of threads, but it is likely
          that the link will be saturated by ~4 threads. Very high-bandwidth,
          long-delay links may get better results with up to ~8
          threads.</para>

          <note>
            <para>Under certain circumstances, when a multipart upload fails
            it could leave behind data on the server. When a failure occurs
            the put subcommand will attempt to abort the upload. If the upload
            cannot be aborted, then a partial upload may remain on the server.
            To check for partial uploads run the <emphasis
            role="bold">lsup</emphasis> subcommand. If you see an upload that
            failed in the output of lsup, then run the <emphasis
            role="bold">rmup</emphasis> subcommand to remove it.</para>
          </note>
        </section>

        <section>
          <title>get</title>

          <para><emphasis role="bold">pegasus-s3 get [options] URL
          [FILE]</emphasis></para>

          <para>The <emphasis role="bold">get</emphasis> subcommand retrives
          an object from the storage service identified by
          <emphasis>URL</emphasis> and stores it in the file specified by
          <emphasis>FILE</emphasis>. If FILE is not specified, then the key is
          used as the file name (Note: if the key has slashes, then the file
          name will be a relative subdirectory, but pegasus-s3 will not create
          the subdirectory if it does not exist).</para>

          <para>If a transient failure occurs, then the download will be
          retried several times before pegasus-s3 gives up and fails.</para>

          <para>The get subcommand can do both chunked and parallel downloads
          if the service supports ranged downloads (see ranged_downloads in
          the configuration). Currently only Amazon S3 has good support for
          ranged downloads. Eucalyptus Walrus supports ranged downloads, but
          the current release, 1.6, is inconsistent with the Amazon interface
          and has a bug that causes ranged downloads to hang in some cases. It
          is recommended that ranged downloads not be used with Eucalyptus
          until these issues are resolved.</para>

          <para>Chunked downloads can be used to reduce the probability of a
          download failing. When a download is chunked, pegasus-s3 issues
          separate GET requests for each chunk of the file. Specifying smaller
          chunks (uisng --chunksize) will reduce the chances that a download
          will fail to do a transient error. Chunk sizes can range from 1 MB
          to 1 GB. By default, a download will be split into 10 MB chunks if
          the site supports ranged downloads. Chunked downloads can be
          disabled by specifying a chunksize of 0. If a download is chunked,
          then each chunk is retried independently under transient failures.
          If any chunk fails permanently, then the download is aborted.</para>

          <para>Parallel downloads can increase performance for services that
          support ranged downloads. In a parallel download, the file to be
          retrieved is split into N chunks and each chunk is downloaded
          concurrently by one of M threads in a first-come, first-served
          fashion. If the chunksize is 0, then parallel downloads are
          disabled. If M &gt; N, then the actual number of threads used will
          be reduced to N. The number of threads can be specified using the
          --parallel argument. If --parallel is 0 or 1, then only a single
          thread is used. The default value is 0. There is no maximum number
          of threads, but it is likely that the link will be saturated by ~4
          threads. Very high-bandwidth, long-delay links may get better
          results with up to ~8 threads.</para>
        </section>

        <section>
          <title>lsup</title>

          <para><emphasis role="bold">pegasus-s3 lsup [options]
          URL</emphasis></para>

          <para>The <emphasis role="bold">lsup</emphasis> subcommand lists
          active uploads. The URL specified should point to a bucket. This
          command is only valid if the site supports multipart uploads. The
          output of this command is a list of keys and upload IDs.</para>

          <para>This subcommand is used with <emphasis
          role="bold">rmup</emphasis> to help recover from failures of
          multipart uploads.</para>
        </section>

        <section>
          <title>rmup</title>

          <para><emphasis role="bold">pegasus-s3 rmup [options] URL
          UPLOAD</emphasis></para>

          <para>The <emphasis role="bold">rmup</emphasis> subcommand cancels
          and active upload. The <emphasis>URL</emphasis> specified should
          point to a bucket, and <emphasis>UPLOAD</emphasis> is the long,
          complicated upload ID shown by the <emphasis
          role="bold">lsup</emphasis> subcommand.</para>

          <para>This subcommand is used with <emphasis
          role="bold">lsup</emphasis> to recover from failures of multipart
          uploads.</para>
        </section>
      </section>

      <section>
        <title>pegasus-s3 Configuration</title>

        <para>Each user should specify a configuration file that pegasus-s3
        will use to look up connection parameters and authentication
        tokens.</para>

        <section>
          <title>Configuration file search path</title>

          <para>This client will look in the following locations, in order, to
          locate the user's configuration file:</para>

          <orderedlist>
            <listitem>
               The -C/--conf argument 
            </listitem>

            <listitem>
               The S3CFG environment variable 
            </listitem>

            <listitem>
               ~/.s3cfg 
            </listitem>
          </orderedlist>

          <para>If it does not find the configuration file in one of these
          locations it will fail with an error.</para>
        </section>

        <section>
          <title>Configuration file format</title>

          <para>The configuration file is in INI format and contains two types
          of entries.</para>

          <para>The first type of entry is a <emphasis role="bold">site
          entry</emphasis>, which specifies the configuration for a storage
          service. This entry specifies the service endpoint that pegasus-s3
          should connect to for the site, and some optional features that the
          site may support. Here is an example of a site entry for Amazon
          S3:</para>

          <programlisting>[amazon]
endpoint = http://s3.amazonaws.com/</programlisting>

          <para>The other type of entry is an <emphasis role="bold">identity
          entry</emphasis>, which specifies the authentication information for
          a user at a particular site. Here is an example of an identity
          entry:</para>

          <programlisting>[pegasus@amazon]
access_key = 90c4143642cb097c88fe2ec66ce4ad4e
secret_key = a0e3840e5baee6abb08be68e81674dca</programlisting>

          <para>It is important to note that user names and site names used
          are only logical--they do not correspond to actual hostnames or
          usernames, but are simply used as a convenient way to refer to the
          services and identities used by the client.</para>

          <para>The configuration file should be saved with limited
          permissions. Only the owner of the file should be able to read from
          it and write to it (i.e. it should have permissions of 0600 or
          0400). If the file has more liberal permissions, then pegasus-s3
          will fail with an error message. The purpose of this is to prevent
          the authentication tokens stored in the configuration file from
          being accessed by other users.</para>
        </section>

        <section>
          <title>Configuration variables</title>

          <table>
            <tgroup cols="3">
              <thead>
                <row>
                  <entry>Variable</entry>

                  <entry>Scope</entry>

                  <entry>Description</entry>
                </row>
              </thead>

              <tbody>
                <row>
                  <entry>endpoint</entry>

                  <entry>site</entry>

                  <entry>The URL of the web service endpoint. If the URL
                  begins with 'https', then SSL will be used.</entry>
                </row>

                <row>
                  <entry>max_object_size</entry>

                  <entry>site</entry>

                  <entry>The maximum size of an object in GB (default:
                  5GB)</entry>
                </row>

                <row>
                  <entry>multipart_uploads</entry>

                  <entry>site</entry>

                  <entry>Does the service support multipart uploads
                  (True/False, default: False)</entry>
                </row>

                <row>
                  <entry>ranged_downloads</entry>

                  <entry>site</entry>

                  <entry>Does the service support ranged downloads?
                  (True/False, default: False)</entry>
                </row>

                <row>
                  <entry>access_key</entry>

                  <entry>identity</entry>

                  <entry>The access key for the identity</entry>
                </row>

                <row>
                  <entry>secret_key</entry>

                  <entry>identity</entry>

                  <entry>The secret key for the identity</entry>
                </row>
              </tbody>
            </tgroup>
          </table>
        </section>

        <section>
          <title>Example configuration</title>

          <para>This is an example configuration that specifies a single site
          (amazon) and a single identity (pegasus@amazon). For this site the
          maximum object size is 5TB, and the site supports both multipart
          uploads and ranged downloads, so both uploads and downloads can be
          done in parallel.</para>

          <programlisting>[amazon]
endpoint = https://s3.amazonaws.com/
max_object_size = 5120
multipart_uploads = True
ranged_downloads = True

[pegasus@amazon]
access_key = 90c4143642cb097c88fe2ec66ce4ad4e
secret_key = a0e3840e5baee6abb08be68e81674dca

[magellan]
# NERSC Magellan is a Eucalyptus site. It doesn't support multipart uploads,
# or ranged downloads (the defaults), and the maximum object size is 5GB
# (also the default)
endpoint = https://128.55.69.235:8773/services/Walrus

[juve@magellan]
access_key = quwefahsdpfwlkewqjsdoijldsdf
secret_key = asdfa9wejalsdjfljasldjfasdfa

[voeckler@magellan]
# Each site can have multiple associated identities
access_key = asdkfaweasdfbaeiwhkjfbaqwhei
secret_key = asdhfuinakwjelfuhalsdflahsdl</programlisting>
        </section>
      </section>
    </section>

    <section>
      <title>pegasus-exitcode</title>

      <para>pegasus-exitcode is a utility that examines the STDOUT of a job to
      determine if the job failed, and renames the STDOUT and STDERR files of
      a job to preserve them in case the job is retried.</para>

      <para>Pegasus uses pegasus-exitcode as the DAGMan postscript for all
      jobs submitted via Globus GRAM. This tool exists as a workaround to a
      known problem with Globus where the exitcodes of GRAM jobs are not
      returned. This is a problem because Pegasus uses the exitcode of a job
      to determine if the job failed or not.</para>

      <para>In order to get around the exitcode problem, Pegasus wraps all
      GRAM jobs with Kickstart, which records the exitcode of the job in an
      XML invocation record, which it writes to the job's STDOUT. The STDOUT
      is transferred from the execution host back to the submit host when the
      job terminates. After the job terminates, DAGMan runs the job's
      postscript, which Pegasus sets to be pegasus-exitcode. pegasus-exitcode
      looks at the invocation record generated by kickstart to see if the job
      succeeded or failed. If the invocation record indicates a failure, then
      pegasus-exitcode returns a non-zero result, which indicates to DAGMan
      that the job has failed. If the invocation record indicates that the job
      succeeded, then pegasus-exitcode returns 0, which tells DAGMan that the
      job succeeeded.</para>

      <para>pegasus-exitcode performs several checks to determine whether a
      job failed or not. These checks include:</para>

      <orderedlist>
        <listitem>
          <para>Is STDOUT empty? If it is empty, then the job failed.</para>
        </listitem>

        <listitem>
          <para>Are there any &lt;status&gt; tags with a non-zero value? If
          there are, then the job failed. Note that, if this is a clustered
          job, there could be multiple &lt;status&gt; tags, one for each task.
          If any of them are non-zero, then the job failed.</para>
        </listitem>

        <listitem>
          <para>Is there at least one &lt;status&gt; tag with a zero value?
          There must be at least one successful invocation or the job has
          failed.</para>
        </listitem>
      </orderedlist>

      <para>In addition, pegasus-exitcode allows the caller to specify the
      exitcode returned by Condor using the --return argument. This can be
      passed to pegasus-exitcode in a DAGMan post script by using the $RETURN
      variable. If this value is non-zero, then pegasus-exitcode returns a
      non-zero result before performing any other checks. For GRAM jobs, the
      value of $RETURN will always be 0 regardless of whether the job failed
      or not.</para>

      <para>Also, pegasus-exitcode allows the caller to specify the number of
      successful tasks it should see using the --tasks argument. If
      pegasus-exitcode does not see N successful tasks, where N is set by
      --tasks, then it will return a non-zero result. The default value is 1.
      This can be used to detect failures in clustered jobs where, for any
      number of reasons, invocation records do not get generated for all the
      tasks in the clustered job.</para>

      <para>In addition to checking the success/failure of a job,
      pegasus-exitcode also renames the STDOUT and STDERR files of the job so
      that if the job is retried, the STDOUT and STDERR of the previous run
      are not lost. It does this by appending a sequence number to the end of
      the files. For example, if the STDOUT file is called "job.out", then the
      first time the job is run pegasus-exitcode will rename the file
      "job.out.000". If the job is run again, then pegasus-exitcode sees that
      "job.out.000" already exists and renames the file "job.out.001". It will
      continue to rename the file by incrementing the sequence number every
      time the job is executed.</para>
    </section>

    <section>
      <title>Kickstart</title>

      <para>Kickstart is a job wrapper that collects data about a job's
      execution environment, performance, and output.</para>
    </section>
  </section>
</chapter>
