<?xml version="1.0" encoding="US-ASCII"?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN"
"http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<chapter id="reference">
  <title>Reference Manual</title>

  <!-- first section is autogenerated from properties file -->

  <xi:include href="advanced_concepts_properties.xml"
              xmlns:xi="http://www.w3.org/2001/XInclude" />

  <section>
    <title>Profiles</title>

    <para>The Pegasus Workflow Mapper uses the concept of profiles to
    encapsulate configurations for various aspects of dealing with the Grid
    infrastructure. Profiles provide an abstract yet uniform interface to
    specify configuration options for various layers from planner/mapper
    behavior to remote environment settings. At various stages during the
    mapping process, profiles may be added associated with the job.</para>

    <para>This document describes various types of profiles, levels of
    priorities for intersecting profiles, and how to specify profiles in
    different contexts.</para>

    <section>
      <title>Profile Structure Heading</title>

      <para>All profiles are triples comprised of a namespace, a name or key,
      and a value. The namespace is a simple identifier. The key has only
      meaning within its namespace, and it&rsquor;s yet another identifier.
      There are no constraints on the contents of a value</para>

      <para>Profiles may be represented with different syntaxes in different
      context. However, each syntax will describe the underlying
      triple.</para>
    </section>

    <section>
      <title>Profile Namespaces</title>

      <para>Each namespace refers to a different aspect of a job&rsquor;s
      runtime settings. A profile&rsquor;s representation in the concrete plan
      (e.g. the Condor submit files) depends its namespace. Pegasus supports
      the following Namespaces for profiles:</para>

      <itemizedlist>
        <listitem>
          <para><emphasis role="bold">env</emphasis> permits remote
          environment variables to be set.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">globus</emphasis> sets Globus RSL
          parameters.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">condor</emphasis> sets Condor
          configuration parameters for the submit file.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">dagman</emphasis> introduces Condor
          DAGMan configuration parameters.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">pegasus</emphasis> configures the
          behaviour of various planner/mapper components.</para>
        </listitem>
      </itemizedlist>

      <section>
        <title>The env Profile Namespace</title>

        <para>The <emphasis>env</emphasis> namespace allows users to specify
        environment variables of remote jobs. Globus transports the
        environment variables, and ensure that they are set before the job
        starts.</para>

        <para>The key used in conjunction with an <emphasis>env</emphasis>
        profile denotes the name of the environment variable. The value of the
        profile becomes the value of the remote environment variable.</para>

        <para>Grid jobs usually only set a minimum of environment variables by
        virtue of Globus. You cannot compare the environment variables visible
        from an interactive login with those visible to a grid job. Thus, it
        often becomes necessary to set environment variables like
        LD_LIBRARY_PATH for remote jobs.</para>

        <para>If you use any of the Pegasus worker package tools like transfer
        or the rc-client, it becomes necessary to set PEGASUS_HOME and
        GLOBUS_LOCATION even for jobs that run locally</para>

        <table>
          <title>Table 1: Useful Environment Settings</title>

          <tgroup cols="2">
            <tbody>
              <row>
                <entry><emphasis role="bold">Environment
                Variable</emphasis></entry>

                <entry><emphasis role="bold">Description</emphasis></entry>
              </row>

              <row>
                <entry>PEGASUS_HOME</entry>

                <entry>Used by auxillary jobs created by Pegasus both on
                remote site and local site. Should be set usually set in the
                Site Catalog for the sites</entry>
              </row>

              <row>
                <entry>GLOBUS_LOCATION</entry>

                <entry>Used by auxillary jobs created by Pegasus both on
                remote site and local site. Should be set usually set in the
                Site Catalog for the sites</entry>
              </row>

              <row>
                <entry>LD_LIBRARY_PATH</entry>

                <entry>Point this to $GLOBUS_LOCATION/lib, except you cannot
                use the dollar variable. You must use the full path. Applies
                to both, local and remote jobs that use Globus components and
                should be usually set in the site catalog for the
                sites</entry>
              </row>
            </tbody>
          </tgroup>
        </table>

        <para>Even though Condor and Globus both permit environment variable
        settings through their profiles, all remote environment variables must
        be set through the means of <emphasis>env</emphasis> profiles.</para>
      </section>

      <section>
        <title>The globus Profile Namespace</title>

        <para>The <emphasis>globus</emphasis> profile namespace encapsulates
        Globus resource specification language (RSL) instructions. The RSL
        configures settings and behavior of the remote scheduling system. Some
        systems require queue name to schedule jobs, a project name for
        accounting purposes, or a run-time estimate to schedule jobs. The
        Globus RSL addresses all these issues.</para>

        <para>A key in the <emphasis>globus</emphasis> namespace denotes the
        command name of an RLS instruction. The profile value becomes the RSL
        value. Even though Globus RSL is typically shown using parentheses
        around the instruction, the out pair of parentheses is not necessary
        in globus profile specifications</para>

        <para>Table 2 shows some commonly used RSL instructions. For an
        authoritative list of all possible RSL instructions refer to the
        Globus RSL specification.</para>

        <table>
          <title>Table 2: Useful Globus RSL Instructions</title>

          <tgroup cols="2">
            <tbody>
              <row>
                <entry><emphasis role="bold">Key</emphasis></entry>

                <entry><emphasis role="bold">Description</emphasis></entry>
              </row>

              <row>
                <entry>count</entry>

                <entry>the number of times an executable is started.</entry>
              </row>

              <row>
                <entry>jobtype</entry>

                <entry>specifies how the job manager should start the remote
                job. While Pegasus defaults to single, use mpi when running
                MPI jobs.</entry>
              </row>

              <row>
                <entry>maxcputime</entry>

                <entry>the max cpu time for a single execution of a
                job.</entry>
              </row>

              <row>
                <entry>maxmemory</entry>

                <entry>the maximum memory in MB required for the job</entry>
              </row>

              <row>
                <entry>maxtime</entry>

                <entry>the maximum time or walltime for a single execution of
                a job.</entry>
              </row>

              <row>
                <entry>maxwalltime</entry>

                <entry>the maximum walltime for a single execution of a
                job.</entry>
              </row>

              <row>
                <entry>minmemory</entry>

                <entry>the minumum amount of memory required for this
                job</entry>
              </row>

              <row>
                <entry>project</entry>

                <entry>associates an account with a job at the remote
                end.</entry>
              </row>

              <row>
                <entry>queue</entry>

                <entry>the remote queue in which the job should be run. Used
                when remote scheduler is PBS that supports queues.</entry>
              </row>
            </tbody>
          </tgroup>
        </table>

        <para>Pegasus prevents the user from specifying certain RSL
        instructions as globus profiles, because they are either automatically
        generated or can be overridden through some different means. For
        instance, if you need to specify remote environment settings, do not
        use the environment key in the globus profiles. Use one or more env
        profiles instead.</para>

        <table>
          <title>Table 3: RSL Instructions that are not permissible</title>

          <tgroup cols="2">
            <tbody>
              <row>
                <entry><emphasis role="bold">Key</emphasis></entry>

                <entry><emphasis role="bold">Reason for
                Prohibition</emphasis></entry>
              </row>

              <row>
                <entry>arguments</entry>

                <entry>you specify arguments in the arguments section for a
                job in the DAX</entry>
              </row>

              <row>
                <entry>directory</entry>

                <entry>the site catalog and properties determine which
                directory a job will run in.</entry>
              </row>

              <row>
                <entry>environment</entry>

                <entry>use multiple env profiles instead</entry>
              </row>

              <row>
                <entry>executable</entry>

                <entry>the physical executable to be used is specified in the
                transformation catalog and is also dependant on the gridstart
                module being used. If you are launching jobs via kickstart
                then the executable created is the path to kickstart and the
                application executable path appears in the arguments for
                kickstart</entry>
              </row>

              <row>
                <entry>stdin</entry>

                <entry>you specify in the DAX for the job</entry>
              </row>

              <row>
                <entry>stdout</entry>

                <entry>you specify in the DAX for the job</entry>
              </row>

              <row>
                <entry>stderr</entry>

                <entry>you specify in the DAX for the job</entry>
              </row>
            </tbody>
          </tgroup>
        </table>
      </section>

      <section>
        <title>The condor Profile Namespace</title>

        <para>The Condor submit file controls every detail how and where a job
        is run. The <emphasis>condor</emphasis> profiles permit to add or
        overwrite instructions in the Condor submit file.</para>

        <para>The <emphasis>condor</emphasis> namespace directly sets commands
        in the Condor submit file for a job the profile applies to. Keys in
        the <emphasis>condor</emphasis> profile namespace denote the name of
        the Condor command. The profile value becomes the command's argument.
        All <emphasis>condor</emphasis> profiles are translated into key=value
        lines in the Condor submit file</para>

        <para>Some of the common condor commands that a user may need to
        specify are listed below. For an authoritative list refer to the
        online condor documentation. Note: Pegasus Workflow Planner/Mapper by
        default specify a lot of condor commands in the submit files depending
        upon the job, and where it is being run.</para>

        <table>
          <title>Table 4: Useful Condor Commands</title>

          <tgroup cols="2">
            <tbody>
              <row>
                <entry><emphasis role="bold">Key</emphasis></entry>

                <entry><emphasis role="bold">Description</emphasis></entry>
              </row>

              <row>
                <entry>universe</entry>

                <entry>Pegasus defaults to either globus or scheduler
                universes. Set to standard for compute jobs that require
                standard universe. Set to vanilla to run natively in a condor
                pool, or to run on resources grabbed via condor
                glidein.</entry>
              </row>

              <row>
                <entry>periodic_release</entry>

                <entry>is the number of times job is released back to the
                queue if it goes to HOLD, e.g. due to Globus errors. Pegasus
                defaults to 3.</entry>
              </row>

              <row>
                <entry>periodic_remove</entry>

                <entry>is the number of times a job is allowed to get into
                HOLD state before being removed from the queue. Pegasus
                defaults to 3.</entry>
              </row>

              <row>
                <entry>filesystemdomain</entry>

                <entry>Useful for Condor glide-ins to pin a job to a remote
                site.</entry>
              </row>

              <row>
                <entry>stream_error</entry>

                <entry>boolean to turn on the streaming of the stderr of the
                remote job back to submit host.</entry>
              </row>

              <row>
                <entry>stream_output</entry>

                <entry>boolean to turn on the streaming of the stdout of the
                remote job back to submit host.</entry>
              </row>

              <row>
                <entry>priority</entry>

                <entry>integer value to assign the priority of a job. Higher
                value means higher priority. The priorities are only applied
                for vanilla / standard/ local universe jobs. Determines the
                order in which a users own jobs are executed.</entry>
              </row>
            </tbody>
          </tgroup>
        </table>

        <para>Other useful condor keys, that advanced users may find useful
        and can be set by profiles are</para>

        <orderedlist>
          <listitem>
            <para>should_transfer_files</para>
          </listitem>

          <listitem>
            <para>transfer_output</para>
          </listitem>

          <listitem>
            <para>transfer_error</para>
          </listitem>

          <listitem>
            <para>whentotransferoutput</para>
          </listitem>

          <listitem>
            <para>requirements</para>
          </listitem>

          <listitem>
            <para>rank</para>
          </listitem>
        </orderedlist>

        <para>Pegasus prevents the user from specifying certain Condor
        commands in condor profiles, because they are automatically generated
        or can be overridden through some different means. Table 5 shows
        prohibited Condor commands.</para>

        <table>
          <title>Table 5: Condor commands prohibited in condor
          profiles</title>

          <tgroup cols="2">
            <tbody>
              <row>
                <entry><emphasis role="bold">Key</emphasis></entry>

                <entry><emphasis role="bold">Reason for
                Prohibition</emphasis></entry>
              </row>

              <row>
                <entry>arguments</entry>

                <entry>you specify arguments in the arguments section for a
                job in the DAX</entry>
              </row>

              <row>
                <entry>environment</entry>

                <entry>use multiple env profiles instead</entry>
              </row>

              <row>
                <entry>executable</entry>

                <entry>the physical executable to be used is specified in the
                transformation catalog and is also dependant on the gridstart
                module being used. If you are launching jobs via kickstart
                then the executable created is the path to kickstart and the
                application executable path appears in the arguments for
                kickstart</entry>
              </row>
            </tbody>
          </tgroup>
        </table>
      </section>

      <section>
        <title>The dagman Profile Namespace</title>

        <para>DAGMan is Condor's workflow manager. While planners generate
        most of DAGMan's configuration, it is possible to tweak certain
        job-related characteristics using dagman profiles. A dagman profile
        can be used to specify a DAGMan pre- or post-script.</para>

        <para>Pre- and post-scripts execute on the submit machine. Both
        inherit the environment settings from the submit host when
        pegasus-submit-dag or pegasus-run is invoked.</para>

        <para>By default, kickstart launches all jobs except standard universe
        and MPI jobs. Kickstart tracks the execution of the job, and returns
        usage statistics for the job. A DAGMan post-script starts the Pegasus
        application exitcode to determine, if the job succeeded. DAGMan
        receives the success indication as exit status from exitcode.</para>

        <para>If you need to run your own post-script, you have to take over
        the job success parsing. The planner is set up to pass the file name
        of the remote job's stdout, usually the output from kickstart, as sole
        argument to the post-script.</para>

        <para>Table 6 shows the keys in the dagman profile domain that are
        understood by Pegasus and can be associated at a per job basis.</para>

        <para><table>
            <title>Table 6: Useful dagman Commands that can be associated at a
            per job basis</title>

            <tgroup cols="2">
              <tbody>
                <row>
                  <entry><emphasis role="bold">Key</emphasis></entry>

                  <entry><emphasis role="bold">Description</emphasis></entry>
                </row>

                <row>
                  <entry>PRE</entry>

                  <entry>is the path to the pre-script. DAGMan executes the
                  pre-script before it runs the job.</entry>
                </row>

                <row>
                  <entry>PRE.ARGUMENTS</entry>

                  <entry>are command-line arguments for the pre-script, if
                  any.</entry>
                </row>

                <row>
                  <entry>POST</entry>

                  <entry>is the postscript type/mode that a user wants to
                  associate with a job. <orderedlist>
                      <listitem>
                        <para><emphasis
                        role="bold">pegasus-exitcode</emphasis> - pegasus will
                        by default associate this postscript with all jobs
                        launched via kickstart, as long the POST.SCOPE value
                        is not set to NONE.</para>
                      </listitem>

                      <listitem>
                        <para><emphasis role="bold">none</emphasis> -means
                        that no postscript is generated for the jobs. This is
                        useful for MPI jobs that are not launched via
                        kickstart currently.</para>
                      </listitem>

                      <listitem>
                        <para><emphasis role="bold">any legal
                        identifier</emphasis> - Any other identifier of the
                        form ([_A-Za-z][_A-Za-z0-9]*), than one of the 2
                        reserved keywords above, signifies a user postscript.
                        This allows the user to specify their own postscript
                        for the jobs in the workflow. The path to the
                        postscript can be specified by the dagman profile
                        <emphasis role="bold">POST.PATH.[value</emphasis>]
                        where [value] is this legal identifier specified. The
                        user postscript is passed the name of the .out file of
                        the job as the last argument on the command
                        line.</para>

                        <para>For e.g. if the following dagman profiles were
                        associated with a job X</para>

                        <orderedlist>
                          <listitem>
                            <para>POST with value user_script
                            /bin/user_postscript</para>
                          </listitem>

                          <listitem>
                            <para>POST.PATH.user_script with value
                            /path/to/user/script</para>
                          </listitem>

                          <listitem>
                            <para>POST.ARGUMENTS with value -verbose</para>
                          </listitem>
                        </orderedlist>

                        <para>then the following postscript will be associated
                        with the job X in the .dag file</para>

                        <para>/path/to/user/script -verbose X.out where X.out
                        contains the stdout of the job X</para>
                      </listitem>
                    </orderedlist></entry>
                </row>

                <row>
                  <entry>POST.PATH.* ( where * is replaced by the value of the
                  POST Profile )</entry>

                  <entry>the path to the post script on the submit
                  host.</entry>
                </row>

                <row>
                  <entry>POST.ARGUMENTS</entry>

                  <entry>are the command line arguments for the post script,
                  if any.</entry>
                </row>

                <row>
                  <entry>RETRY</entry>

                  <entry>is the number of times DAGMan retries the full job
                  cycle from pre-script through post-script, if failure was
                  detected.</entry>
                </row>

                <row>
                  <entry>CATEGORY</entry>

                  <entry>the DAGMan category the job belongs to.</entry>
                </row>

                <row>
                  <entry>PRIORITY</entry>

                  <entry>the priority to apply to a job. DAGMan uses this to
                  select what jobs to release when MAXJOBS is enforced for the
                  DAG.</entry>
                </row>
              </tbody>
            </tgroup>
          </table></para>

        <para></para>

        <para>Table 7 shows the keys in the dagman profile domain that are
        understood by Pegasus and can be used to apply to the whole workflow.
        These are used to control DAGMan's behavior at the workflow level, and
        are recommended to be specified in the properties file.</para>

        <table>
          <title>Table 7: Useful dagman Commands that can be specified in the
          properties file.</title>

          <tgroup cols="2">
            <tbody>
              <row>
                <entry><emphasis role="bold">Key</emphasis></entry>

                <entry><emphasis role="bold">Description</emphasis></entry>
              </row>

              <row>
                <entry>MAXPRE</entry>

                <entry>sets the maximum number of PRE scripts within the DAG
                that may be running at one time</entry>
              </row>

              <row>
                <entry>MAXPOST</entry>

                <entry>sets the maximum number of PRE scripts within the DAG
                that may be running at one time</entry>
              </row>

              <row>
                <entry>MAXJOBS</entry>

                <entry>sets the maximum number of jobs within the DAG that
                will be submitted to Condor at one time.</entry>
              </row>

              <row>
                <entry>MAXIDLE</entry>

                <entry>sets the maximum number of idle jobs within the DAG
                that will be submitted to Condor at one time.</entry>
              </row>

              <row>
                <entry>[CATEGORY-NAME].MAXJOBS</entry>

                <entry>is the value of maxjobs for a particular category.
                Users can associate different categories to the jobs at a per
                job basis. However, the value of a dagman knob for a category
                can only be specified at a per workflow basis in the
                properties.</entry>
              </row>

              <row>
                <entry>POST.SCOPE</entry>

                <entry>scope for the postscripts. <orderedlist>
                    <listitem>
                      <para>If set to <emphasis role="bold">all</emphasis> ,
                      means each job in the workflow will have a postscript
                      associated with it.</para>
                    </listitem>

                    <listitem>
                      <para>If set to <emphasis role="bold">none</emphasis> ,
                      means no job has postscript associated with it. None
                      mode should be used if you are running vanilla /
                      standard/ local universe jobs, as in those cases Condor
                      traps the remote exitcode correctly. None scope is not
                      recommended for grid universe jobs.</para>
                    </listitem>

                    <listitem>
                      <para>If set to <emphasis
                      role="bold">essential</emphasis>, means only essential
                      jobs have post scripts associated with them. At present
                      the only non essential job is the replica registration
                      job.</para>
                    </listitem>
                  </orderedlist></entry>
              </row>
            </tbody>
          </tgroup>
        </table>
      </section>

      <section>
        <title>The pegasus Profile Namespace</title>

        <para>The <emphasis>pegasus</emphasis> profiles allow users to
        configure extra options to the Pegasus Workflow Planner that can be
        applied selectively to a job or a group of jobs. Site selectors may
        use a sub-set of <emphasis>pegasus</emphasis> profiles for their
        decision-making.</para>

        <para>Table 8 shows some of the useful configuration option Pegasus
        understands.</para>

        <table>
          <title>Table 8: Useful pegasus Profiles.</title>

          <tgroup cols="2">
            <tbody>
              <row>
                <entry><emphasis role="bold">Key</emphasis></entry>

                <entry><emphasis role="bold">Description</emphasis></entry>
              </row>

              <row>
                <entry>workdir</entry>

                <entry>Sets the remote initial dir for a Condor-G job.
                Overrides the work directory algorithm that uses the site
                catalog and properties.</entry>
              </row>

              <row>
                <entry>clusters.num</entry>

                <entry>Please refer to the Pegasus Clustering Guide for
                detailed description. This option determines the total number
                of clusters per level. Jobs are evenly spread across
                clusters.</entry>
              </row>

              <row>
                <entry>clusters.size</entry>

                <entry>Please refer to the Pegasus Clustering Guide for
                detailed description. This profile determines the number of
                jobs in each cluster. The number of clusters depends on the
                total number of jobs on the level.</entry>
              </row>

              <row>
                <entry>collapser</entry>

                <entry>Indicates the clustering executable that is used to run
                the clustered job on the remote site.</entry>
              </row>

              <row>
                <entry>gridstart</entry>

                <entry>Determines the executable for launching a job. Possible
                values are <emphasis role="bold"><emphasis>Kickstart |
                NoGridStart</emphasis></emphasis> at the moment.</entry>
              </row>

              <row>
                <entry>gridstart.path</entry>

                <entry>Sets the path to the gridstart . This profile is best
                set in the Site Catalog.</entry>
              </row>

              <row>
                <entry>gridstart.arguments</entry>

                <entry>Sets the arguments with which GridStart is used to
                launch a job on the remote site.</entry>
              </row>

              <row>
                <entry>stagein.clusters</entry>

                <entry>This key determines the maximum number of
                <emphasis>stage-in</emphasis> jobs that are can executed
                locally or remotely per compute site per workflow. This is
                used to configure the <emphasis>Bundle</emphasis> Transfer
                Refiner, which is the Default Refiner used in Pegasus. This
                profile is best set in the Site Catalog or in the Properties
                file</entry>
              </row>

              <row>
                <entry>stagein.local.clusters</entry>

                <entry>This key provides finer grained control in determining
                the number of stage-in jobs that are executed locally and are
                responsible for staging data to a particular remote site. This
                profile is best set in the Site Catalog or in the Properties
                file</entry>
              </row>

              <row>
                <entry>stagein.remote.clusters</entry>

                <entry>This key provides finer grained control in determining
                the number of stage-in jobs that are executed remotely on the
                remote site and are responsible for staging data to it. This
                profile is best set in the Site Catalog or in the Properties
                file</entry>
              </row>

              <row>
                <entry>stageout.clusters</entry>

                <entry>This key determines the maximum number of
                <emphasis>stage-out</emphasis> jobs that are can executed
                locally or remotely per compute site per workflow. This is
                used to configure the <emphasis>Bundle</emphasis> Transfer
                Refiner, , which is the Default Refiner used in
                Pegasus.</entry>
              </row>

              <row>
                <entry>stageout.local.clusters</entry>

                <entry>This key provides finer grained control in determining
                the number of stage-out jobs that are executed locally and are
                responsible for staging data from a particular remote site.
                This profile is best set in the Site Catalog or in the
                Properties file</entry>
              </row>

              <row>
                <entry>stageout.remote.clusters</entry>

                <entry>This key provides finer grained control in determining
                the number of stage-out jobs that are executed remotely on the
                remote site and are responsible for staging data from it. This
                profile is best set in the Site Catalog or in the Properties
                file</entry>
              </row>

              <row>
                <entry>group</entry>

                <entry>Tags a job with an arbitrary group identifier. The
                group site selector makes use of the tag.</entry>
              </row>

              <row>
                <entry>change.dir</entry>

                <entry>If true, tells <emphasis>kickstart</emphasis> to change
                into the remote working directory. Kickstart itself is
                executed in whichever directory the remote scheduling system
                chose for the job.</entry>
              </row>

              <row>
                <entry>create.dir</entry>

                <entry>If true, tells <emphasis>kickstart</emphasis> to create
                the the remote working directory before changing into the
                remote working directory. Kickstart itself is executed in
                whichever directory the remote scheduling system chose for the
                job.</entry>
              </row>

              <row>
                <entry>transfer.proxy</entry>

                <entry>If true, tells Pegasus to explicitly transfer the proxy
                for transfer jobs to the remote site. This is useful, when you
                want to use a full proxy at the remote end, instead of the
                limited proxy that is transferred by CondorG.</entry>
              </row>

              <row>
                <entry>transfer.arguments</entry>

                <entry>Allows the user to specify the arguments with which the
                transfer executable is invoked. However certain options are
                always generated for the transfer executable(base-uri
                se-mount-point).</entry>
              </row>

              <row>
                <entry>style</entry>

                <entry>Sets the condor submit file style. If set to globus,
                submit file generated refers to CondorG job submissions. If
                set to condor, submit file generated refers to direct Condor
                submission to the local Condor pool. It applies for glidein,
                where nodes from remote grid sites are glided into the local
                condor pool. The default style that is applied is
                globus.</entry>
              </row>
            </tbody>
          </tgroup>
        </table>
      </section>
    </section>

    <section>
      <title>Sources for Profiles</title>

      <para>Profiles may enter the job-processing stream at various stages.
      Depending on the requirements and scope a profile is to apply, profiles
      can be associated at</para>

      <itemizedlist>
        <listitem>
          <para>as user property settings.</para>
        </listitem>

        <listitem>
          <para>dax level</para>
        </listitem>

        <listitem>
          <para>in the site catalog</para>
        </listitem>

        <listitem>
          <para>in the transformation catalog</para>
        </listitem>
      </itemizedlist>

      <para>Unfortunately, a different syntax applies to each level and
      context. This section shows the different profile sources and syntaxes.
      However, at the foundation of each profile lies the triple of namespace,
      key and value.</para>

      <section>
        <title>User Profiles in Properties</title>

        <para>Users can specify all profiles in the properties files where the
        property name is <emphasis role="bold">[namespace].key</emphasis> and
        <emphasis role="bold">value</emphasis> of the property is the value of
        the profile.</para>

        <para>Namespace can be env|condor|globus|dagman|pegasus</para>

        <para>Any profile specified as a property applies to the whole
        workflow unless overridden at the DAX level , Site Catalog ,
        Transformation Catalog Level.</para>

        <para>Some profiles that they can be set in the properties file are
        listed below</para>

        <programlisting>env.JAVA_HOME "/software/bin/java"

condor.periodic_release 5
condor.periodic_remove  my_own_expression
condor.stream_error true
condor.stream_output fa

globus.maxwalltime  1000
globus.maxtime      900
globus.maxcputime   10
globus.project      test_project
globus.queue        main_queue

dagman.post.arguments --test arguments
dagman.retry  4
dagman.post simple_exitcode
dagman.post.path.simple_exitcode  /bin/exitcode/exitcode.sh
dagman.post.scope all
dagman.maxpre  12
dagman.priority 13

dagman.bigjobs.maxjobs 1


pegasus.clusters.size 5

pegasus.stagein.clusters 3</programlisting>
      </section>

      <section>
        <title>Profiles in DAX</title>

        <para>The user can associate profiles with logical transformations in
        DAX. Environment settings required by a job's application, or a
        maximum estimate on the run-time are examples for profiles at this
        stage.</para>

        <programlisting>&lt;job id="ID000001" namespace="asdf" name="preprocess" version="1.0"
 level="3" dv-namespace="voeckler" dv-name="top" dv-version="1.0"&gt;
  &lt;argument&gt;-a top -T10  -i &lt;filename file="voeckler.f.a"/&gt;
 -o &lt;filename file="voeckler.f.b1"/&gt;
 &lt;filename file="voeckler.f.b2"/&gt;&lt;/argument&gt;
  <emphasis role="bold">&lt;profile namespace="pegasus" key="walltime"&gt;2&lt;/profile&gt;
  &lt;profile namespace="pegasus" key="diskspace"&gt;1&lt;/profile&gt;</emphasis>
  &mldr;
&lt;/job&gt;
</programlisting>
      </section>

      <section>
        <title>Profiles in Site Catalog</title>

        <para>If it becomes necessary to limit the scope of a profile to a
        single site, these profiles should go into the site catalog. A profile
        in the site catalog applies to all jobs and all application run at the
        site. Commonly, site catalog profiles set environment settings like
        the LD_LIBRARY_PATH, or globus rsl parameters like queue and project
        names.</para>

        <para>Currently, there is no tool to manipulate the site catalog, e.g.
        by adding profiles. Modifying the site catalog requires that you load
        it into your editor.</para>

        <para>The XML version of the site catalog uses the following
        syntax:</para>

        <programlisting><emphasis role="bold">&lt;profile namespace=</emphasis>"<emphasis>namespace</emphasis>" <emphasis
            role="bold">key=</emphasis>"<emphasis>key</emphasis>"&gt;<emphasis>value</emphasis><emphasis
            role="bold">&lt;/profile&gt;</emphasis></programlisting>

        <para>The XML schema requires that profiles are the first children of
        a pool element. If the element ordering is wrong, the XML parser will
        produce errors and warnings:</para>

        <programlisting>&lt;pool handle="isi_condor" gridlaunch="/home/shared/pegasus/bin/kickstart"&gt;
  <emphasis role="bold">&lt;profile namespace="env"
   key="GLOBUS_LOCATION"&gt;/home/shared/globus/&lt;/profile&gt;
  &lt;profile namespace="env"
   key="LD_LIBRARY_PATH" &gt;/home/shared/globus/lib&lt;/profile&gt;</emphasis>
  &lt;lrc url="rls://sukhna.isi.edu" /&gt;
  &mldr;
&lt;/pool&gt;
</programlisting>

        <para>The multi-line textual version of the site catalog uses the
        following syntax:</para>

        <programlisting><emphasis role="bold">profile</emphasis> <emphasis>namespace "key" "value"</emphasis></programlisting>

        <para>The order within the textual pool definition is not important.
        Profiles can appear anywhere:</para>

        <programlisting>pool isi_condor {
  gridlaunch "/home/shared/pegasus/bin/kickstart"
  <emphasis role="bold">profile env "GLOBUS_LOCATION" "/home/shared/globus"
  profile env "LD_LIBRARY_PATH" "/home/shared/globus/lib"</emphasis>
  &mldr;
}
</programlisting>
      </section>

      <section>
        <title>Profiles in Transformation Catalog</title>

        <para>Some profiles require a narrower scope than the site catalog
        offers. Some profiles only apply to certain applications on certain
        sites, or change with each application and site.
        Transformation-specific and CPU-specific environment variables, or job
        clustering profiles are good candidates. Such profiles are best
        specified in the transformation catalog.</para>

        <para>Profiles associate with a physical transformation and site in
        the transformation catalog. The Database version of the transformation
        catalog also permits the convenience of connecting a transformation
        with a profile.</para>

        <para>The Pegasus tc-client tool is a convenient helper to associate
        profiles with transformation catalog entries. As benefit, the user
        does not have to worry about formats of profiles in the various
        transformation catalog instances.</para>

        <programlisting>tc-client -a -P -E -p /home/shared/executables/analyze -t INSTALLED -r isi_condor -e env::GLOBUS_LOCATION=&rdquor;/home/shared/globus&rdquor;</programlisting>

        <para>The above example adds an environment variable GLOBUS_LOCATION
        to the application /home/shared/executables/analyze on site
        isi_condor. The transformation catalog guide has more details on the
        usage of the tc-client.</para>
      </section>
    </section>

    <section>
      <title>Profiles Conflict Resolution</title>

      <para>Irrespective of where the profiles are specified, eventually the
      profiles are associated with jobs. Multiple sources may specify the same
      profile for the same job. For instance, DAX may specify an environment
      variable X. The site catalog may also specify an environment variable X
      for the chosen site. The transformation catalog may specify an
      environment variable X for the chosen site and application. When the job
      is concretized, these three conflicts need to be resolved.</para>

      <para>Pegasus defines a priority ordering of profiles. The higher
      priority takes precedence (overwrites) a profile of a lower
      priority.</para>

      <orderedlist>
        <listitem>
          <para>Transformation Catalog Profiles</para>
        </listitem>

        <listitem>
          <para>Site Catalog Profiles</para>
        </listitem>

        <listitem>
          <para>DAX Profiles</para>
        </listitem>

        <listitem>
          <para>Profiles in Properties</para>
        </listitem>
      </orderedlist>
    </section>

    <section>
      <title>Details of Profile Handling</title>

      <para>The previous sections omitted some of the finer details for the
      sake of clarity. To understand some of the constraints that Pegasus
      imposes, it is required to look at the way profiles affect jobs.</para>

      <section>
        <title>Details of env Profiles</title>

        <para>Profiles in the env namespace are translated to a
        semicolon-separated list of key-value pairs. The list becomes the
        argument for the Condor environment command in the job's submit
        file.</para>

        <programlisting>######################################################################
# Pegasus WMS  SUBMIT FILE GENERATOR
# DAG : black-diamond, Index = 0, Count = 1
# SUBMIT FILE NAME : findrange_ID000002.sub
######################################################################
globusrsl = (jobtype=single)
<emphasis role="bold">environment=GLOBUS_LOCATION=/shared/globus;LD_LIBRARY_PATH=/shared/globus/lib;</emphasis>
executable = /shared/software/linux/pegasus/default/bin/kickstart
globusscheduler = columbus.isi.edu/jobmanager-condor
remote_initialdir = /shared/CONDOR/workdir/isi_hourglass
universe = globus
&mldr;
queue
######################################################################
# END OF SUBMIT FILE
</programlisting>

        <para>Condor-G, in turn, will translate the
        <emphasis>environment</emphasis> command for any remote job into
        Globus RSL environment settings, and append them to any existing RSL
        syntax it generates. To permit proper mixing, all
        <emphasis>environment</emphasis> setting should solely use the env
        profiles, and none of the Condor nor Globus environment
        settings.</para>

        <para>If <emphasis>kickstart</emphasis> starts a job, it may make use
        of environment variables in its executable and arguments
        setting.</para>
      </section>

      <section>
        <title>Details of globus Profiles</title>

        <para>Profiles in the <emphasis>globus</emphasis> Namespaces are
        translated into a list of paranthesis-enclosed equal-separated
        key-value pairs. The list becomes the value for the Condor
        <emphasis>globusrsl</emphasis> setting in the job's submit
        file:</para>

        <programlisting>######################################################################
# Pegasus WMS SUBMIT FILE GENERATOR
# DAG : black-diamond, Index = 0, Count = 1
# SUBMIT FILE NAME : findrange_ID000002.sub
######################################################################
<emphasis role="bold">globusrsl = (jobtype=single)(queue=fast)(project=nvo)</emphasis>
executable = /shared/software/linux/pegasus/default/bin/kickstart
globusscheduler = columbus.isi.edu/jobmanager-condor
remote_initialdir = /shared/CONDOR/workdir/isi_hourglass
universe = globus
&mldr;
queue
######################################################################
# END OF SUBMIT FILE
</programlisting>

        <para>For this reason, Pegasus prohibits the use of the
        <emphasis>globusrsl</emphasis> key in the <emphasis>condor</emphasis>
        profile namespace.</para>
      </section>
    </section>
  </section>

  <section>
    <title>Replica Selection</title>

    <para>Each job in the DAX maybe associated with input LFN&rsquor;s
    denoting the files that are required for the job to run. To determine the
    physical replica (PFN) for a LFN, Pegasus queries the Replica catalog to
    get all the PFN&rsquor;s (replicas) associated with a LFN. The Replica
    Catalog may return multiple PFN's for each of the LFN's queried. Hence,
    Pegasus needs to select a single PFN amongst the various PFN's returned
    for each LFN. This process is known as replica selection in Pegasus. Users
    can specify the replica selector to use in the properties file.</para>

    <para>This document describes the various Replica Selection Strategies in
    Pegasus.</para>

    <section>
      <title>Configuration</title>

      <para>The user properties determine what replica selector Pegasus
      Workflow Mapper uses. The property <emphasis
      role="bold">pegasus.selector.replica</emphasis> is used to specify the
      replica selection strategy. Currently supported Replica Selection
      strategies are</para>

      <orderedlist>
        <listitem>
          <para>Default</para>
        </listitem>

        <listitem>
          <para>Restricted</para>
        </listitem>

        <listitem>
          <para>Regex</para>
        </listitem>
      </orderedlist>

      <para>The values are case sensitive. For example the following property
      setting will throw a Factory Exception .</para>

      <programlisting>pegasus.selector.replica  default</programlisting>

      <para>The correct way to specify is</para>

      <programlisting>pegasus.selector.replica  Default</programlisting>
    </section>

    <section>
      <title>Supported Replica Selectors</title>

      <para>The various Replica Selectors supported in Pegasus Workflow Mapper
      are explained below</para>

      <section>
        <title>Default</title>

        <para>This is the default replica selector used in the Pegasus
        Workflow Mapper. If the property pegasus.selector.replica is not
        defined in properties, then Pegasus uses this selector.</para>

        <para>This selector looks at each PFN returned for a LFN and checks to
        see if</para>

        <orderedlist>
          <listitem>
            <para>the PFN is a file URL (starting with file:///)</para>
          </listitem>

          <listitem>
            <para>the PFN has a pool attribute matching to the site handle of
            the site where the compute job that requires the input file is to
            be run.</para>
          </listitem>
        </orderedlist>

        <para>If a PFN matching the conditions above exists then that is
        returned by the selector .</para>

        <para><emphasis role="bold">Else,</emphasis> a random PFN is selected
        amongst all the PFN&rsquor;s that have a pool attribute matching to
        the site handle of the site where a compute job is to be run.</para>

        <para><emphasis role="bold">Else,</emphasis> a random pfn is selected
        amongst all the PFN&rsquor;s</para>

        <para>To use this replica selector set the following
        property<programlisting>pegasus.selector.replica                  Default</programlisting></para>
      </section>

      <section>
        <title>Restricted</title>

        <para>This replica selector, allows the user to specify good sites and
        bad sites for staging in data to a particular compute site. A good
        site for a compute site X, is a preferred site from which replicas
        should be staged to site X. If there are more than one good sites
        having a particular replica, then a random site is selected amongst
        these preferred sites.</para>

        <para>A bad site for a compute site X, is a site from which
        replica&rsquor;s should not be staged. The reason of not accessing
        replica from a bad site can vary from the link being down, to the user
        not having permissions on that site&rsquor;s data.</para>

        <para>The good | bad sites are specified by the following
        properties</para>

        <programlisting>pegasus.replica.*.prefer.stagein.sites
pegasus.replica.*.ignore.stagein.sites</programlisting>

        <para>where the * in the property name denotes the name of the compute
        site. A * in the property key is taken to mean all sites. The value to
        these properties is a comma separated list of sites.</para>

        <para>For example the following settings</para>

        <programlisting>pegasus.selector.replica.*.prefer.stagein.sites            usc
pegasus.replica.uwm.prefer.stagein.sites                   isi,cit
</programlisting>

        <para>means that prefer all replicas from site usc for staging in to
        any compute site. However, for uwm use a tighter constraint and prefer
        only replicas from site isi or cit. The pool attribute associated with
        the PFN's tells the replica selector to what site a replica/PFN is
        associated with.</para>

        <para>The pegasus.replica.*.prefer.stagein.sites property takes
        precedence over pegasus.replica.*.ignore.stagein.sites property i.e.
        if for a site X, a site Y is specified both in the ignored and the
        preferred set, then site Y is taken to mean as only a preferred site
        for a site X.</para>

        <para>To use this replica selector set the following property</para>

        <programlisting>pegasus.selector.replica                  Restricted</programlisting>
      </section>

      <section>
        <title>Regex</title>

        <para>This replica selector allows the user allows the user to
        specific regex expressions that can be used to rank various
        PFN&rsquor;s returned from the Replica Catalog for a particular LFN.
        This replica selector selects the highest ranked PFN i.e the replica
        with the lowest rank value.</para>

        <para>The regular expressions are assigned different rank, that
        determine the order in which the expressions are employed. The rank
        values for the regex can expressed in user properties using the
        property.</para>

        <programlisting>pegasus.selector.replica.regex.rank.<emphasis
            role="bold">[value]</emphasis>                  regex-expression</programlisting>

        <para>The <emphasis role="bold">[value]</emphasis> in the above
        property is an integer value that denotes the rank of an expression
        with a rank value of 1 being the highest rank.</para>

        <para>For example, a user can specify the following regex expressions
        that will ask Pegasus to prefer file URL's over gsiftp url's from
        example.isi.edu</para>

        <programlisting>pegasus.selector.replica.regex.rank.1                       file://.*
pegasus.selector.replica.regex.rank.2                       gsiftp://example\.isi\.edu.*</programlisting>

        <para>User can specify as many regex expressions as they want.</para>

        <para>Since Pegasus is in Java , the regex expression support is what
        Java supports. It is pretty close to what is supported by Perl. More
        details can be found at
        http://java.sun.com/j2se/1.5.0/docs/api/java/util/regex/Pattern.html</para>

        <para>Before applying any regular expressions on the PFN&rsquor;s for
        a particular LFN that has to be staged to a site X, the file
        URL&rsquor;s that don't match the site X are explicitly filtered
        out.</para>

        <para>To use this replica selector set the following
        property<programlisting>pegasus.selector.replica                  Regex</programlisting></para>
      </section>

      <section>
        <title>Local</title>

        <para>This replica selector always prefers replicas from the local
        host ( pool attribute set to local ) and that start with a file: URL
        scheme. It is useful, when users want to stagein files to a remote
        site from the submit host using the Condor file transfer
        mechanism.</para>

        <para>To use this replica selector set the following
        property<programlisting>pegasus.selector.replica                  Default</programlisting></para>
      </section>
    </section>
  </section>

  <section id="job_clustering">
    <title>Job Clustering</title>

    <para>A large number of workflows executed through the Pegasus Workflow
    Management System, are composed of several jobs that run for only a few
    seconds or so. The overhead of running any job on the grid is usually 60
    seconds or more. Hence, it makes sense to cluster small independent jobs
    into a larger job. This is done while mapping an abstract workflow to a
    concrete workflow. Site specific or transformation specific criteria are
    taken into consideration while clustering smaller jobs into a larger job
    in the concrete workflow. The user is allowed to control the granularity
    of this clustering on a per transformation per site basis.</para>

    <section>
      <title>Overview</title>

      <para>The abstract workflow is mapped onto the various sites by the Site
      Selector. This semi executable workflow is then passed to the clustering
      module. The clustering of the workflow can be either be</para>

      <itemizedlist>
        <listitem>
          <para>level based (horizontal clustering )</para>
        </listitem>

        <listitem>
          <para>label based (label clustering)</para>
        </listitem>
      </itemizedlist>

      <para>The clustering module clusters the jobs into larger/clustered
      jobs, that can then be executed on the remote sites. The execution can
      either be sequential on a single node or on multiple nodes using MPI. To
      specify which clustering technique to use the user has to pass the
      <emphasis role="bold">--cluster</emphasis> option to <emphasis
      role="bold">pegasus-plan</emphasis> .</para>

      <section>
        <title>Generating Clustered Concrete DAG</title>

        <para>The clustering of a workflow is activated by passing the
        <emphasis role="bold">--cluster|-C</emphasis> option to <emphasis
        role="bold">pegasus-plan</emphasis>. The clustering granularity of a
        particular logical transformation on a particular site is dependant
        upon the clustering techniques being used. The executable that is used
        for running the clustered job on a particular site is determined as
        explained in section 7.<programlisting>#Running pegasus-plan to generate clustered workflows

$ <emphasis>pegasus-plan &ndash;-dax example.dax --dir ./dags &ndash;p siteX &ndash;-output local
               --cluster [ comma separated list of clustering techniques]  &ndash;verbose
</emphasis>
Valid clustering techniques are horizontal and label.</programlisting></para>

        <para>The naming convention of submit files of the clustered jobs
        is<emphasis role="bold"> merge_NAME_IDX.sub</emphasis> . The NAME is
        derived from the logical transformation name. The IDX is an integer
        number between 1 and the total number of jobs in a cluster. Each of
        the submit files has a corresponding input file, following the naming
        convention <emphasis role="bold">merge_NAME_IDX.in </emphasis>. The
        input file contains the respective execution targets and the arguments
        for each of the jobs that make up the clustered job.</para>

        <section>
          <title>Horizontal Clustering</title>

          <para>In case of horizontal clustering, each job in the workflow is
          associated with a level. The levels of the workflow are determined
          by doing a modified Breadth First Traversal of the workflow starting
          from the root nodes. The level associated with a node, is the
          furthest distance of it from the root node instead of it being the
          shortest distance as in normal BFS. For each level the jobs are
          grouped by the site on which they have been scheduled by the Site
          Selector. Only jobs of same type (txnamespace, txname, txversion)
          can be clustered into a larger job. To use horizontal clustering the
          user needs to set the <emphasis role="bold">--cluster</emphasis>
          option of <emphasis role="bold">pegasus-plan to
          horizontal</emphasis> .</para>

          <section>
            <title>Controlling Clustering Granularity</title>

            <para>The number of jobs that have to be clustered into a single
            large job, is determined by the value of two parameters associated
            with the smaller jobs. Both these parameters are specified by the
            use of a PEGASUS namespace profile keys. The keys can be specified
            at any of the placeholders for the profiles (abstract
            transformation in the DAX, site in the site catalog,
            transformation in the transformation catalog). The normal
            overloading semantics apply i.e. profile in transformation catalog
            overrides the one in the site catalog and that in turn overrides
            the one in the DAX. The two parameters are described below.</para>

            <itemizedlist>
              <listitem>
                <para><emphasis role="bold">clusters.size
                factor</emphasis></para>

                <para>The clusters.size factor denotes how many jobs need to
                be merged into a single clustered job. It is specified via the
                use of a PEGASUS namespace profile key
                &ldquo;clusters.size&rdquor;. for e.g. if at a particular
                level, say 4 jobs referring to logical transformation B have
                been scheduled to a siteX. The clusters.size factor associated
                with job B for siteX is say 3. This will result in 2 clustered
                jobs, one composed of 3 jobs and another of 2 jobs. The
                clusters.size factor can be specified in the transformation
                catalog as follows</para>

                <programlisting><emphasis role="bold">#site   transformation   pfn            type               architecture  profiles
</emphasis>
siteX    B     /shared/PEGASUS/bin/jobB INSTALLED       INTEL32::LINUX  PEGASUS::clusters.size=3
siteX    C     /shared/PEGASUS/bin/jobC INSTALLED       INTEL32::LINUX  PEGASUS::clusters.size=2
</programlisting>

                <figure>
                  <title></title>

                  <mediaobject>
                    <imageobject>
                      <imagedata fileref="images/advanced-clustering-1.png" />
                    </imageobject>
                  </mediaobject>
                </figure>
              </listitem>

              <listitem>
                <para><emphasis role="bold">clusters.num
                factor</emphasis></para>

                <para>The clusters.num factor denotes how many clustered jobs
                does the user want to see per level per site. It is specified
                via the use of a PEGASUS namespace profile key
                &ldquo;clusters.num&rdquor;. for e.g. if at a particular
                level, say 4 jobs referring to logical transformation B have
                been scheduled to a siteX. The &ldquo;clusters.num&rdquor;
                factor associated with job B for siteX is say 3. This will
                result in 3 clustered jobs, one composed of 2 jobs and others
                of a single job each. The clusters.num factor in the
                transformation catalog can be specified as follows</para>

                <programlisting><emphasis role="bold">#site  transformation      pfn           type            architecture    profiles
</emphasis>
siteX    B     /shared/PEGASUS/bin/jobB INSTALLED       INTEL32::LINUX  PEGASUS::clusters.num=3
siteX    C     /shared/PEGASUS/bin/jobC INSTALLED       INTEL32::LINUX  PEGASUS::clusters.num=2
</programlisting>

                <para>In the case, where both the factors are associated with
                the job, the clusters.num value supersedes the clusters.size
                value.</para>

                <programlisting><emphasis role="bold">#site  transformation   pfn             type             architecture   profiles
</emphasis>
siteX    B     /shared/PEGASUS/bin/jobB INSTALLED       INTEL32::LINUX PEGASUS::clusters.size=3,clusters.num=3
</programlisting>

                <para>In the above case the jobs referring to logical
                transformation B scheduled on siteX will be clustered on the
                basis of &ldquo;clusters.num&rdquor; value. Hence, if there
                are 4 jobs referring to logical transformation B scheduled to
                siteX, then 3 clustered jobs will be created.</para>

                <figure>
                  <title></title>

                  <mediaobject>
                    <imageobject>
                      <imagedata fileref="images/advanced-clustering-2.png" />
                    </imageobject>
                  </mediaobject>
                </figure>
              </listitem>
            </itemizedlist>
          </section>
        </section>

        <section>
          <title>Label Clustering</title>

          <para>In label based clustering, the user labels the workflow. All
          jobs having the same label value are clustered into a single
          clustered job. This allows the user to create clusters or use a
          clustering technique that is specific to his workflows. If there is
          no label associated with the job, the job is not clustered and is
          executed as is<figure>
              <title></title>

              <mediaobject>
                <imageobject>
                  <imagedata fileref="images/advanced-clustering-3.png" />
                </imageobject>
              </mediaobject>
            </figure></para>

          <para>Since, the jobs in a cluster in this case are not independent,
          it is important the jobs are executed in the correct order. This is
          done by doing a topological sort on the jobs in each cluster. To use
          label based clustering the user needs to set the <emphasis
          role="bold">--cluster</emphasis> option of <emphasis
          role="bold">pegasus-plan</emphasis> to label.</para>

          <section>
            <title>Labelling the Workflow</title>

            <para>The labels for the jobs in the workflow are specified by
            associated <emphasis role="bold">pegasus</emphasis> profile keys
            with the jobs during the DAX generation process. The user can
            choose which profile key to use for labeling the workflow. By
            default, it is assumed that the user is using the PEGASUS profile
            key label to associate the labels. To use another key, in the
            <emphasis role="bold">pegasus</emphasis> namespace the user needs
            to set the following property</para>

            <itemizedlist>
              <listitem>
                <para>pegasus.clusterer.label.key</para>
              </listitem>
            </itemizedlist>

            <para>For example if the user sets <emphasis
            role="bold">pegasus.clusterer.label.key </emphasis>to <emphasis
            role="bold">user_label</emphasis> then the job description in the
            DAX looks as follows</para>

            <programlisting>&lt;adag &gt;
&mldr;
  &lt;job id="ID000004" namespace="app" name="analyze" version="1.0" level="1" &gt;
    &lt;argument&gt;-a bottom -T60  -i &lt;filename file="user.f.c1"/&gt;  -o &lt;filename file="user.f.d"/&gt;&lt;/argument&gt;
    &lt;profile namespace=&ldquo;pegasus&rdquor; key=&ldquo;user_label&rdquor;&gt;p1&lt;/profile&gt;
    &lt;uses file="user.f.c1" link="input" dontRegister="false" dontTransfer="false"/&gt;
    &lt;uses file="user.f.c2" link="input" dontRegister="false" dontTransfer="false"/&gt;
    &lt;uses file="user.f.d" link="output" dontRegister="false" dontTransfer="false"/&gt;
  &lt;/job&gt;
&mldr;
&lt;/adag&gt;</programlisting>

            <itemizedlist>
              <listitem>
                <para>The above states that the <emphasis
                role="bold">pegasus</emphasis> profiles with key as <emphasis
                role="bold">user_label</emphasis> are to be used for
                designating clusters.</para>
              </listitem>

              <listitem>
                <para>Each job with the same value for <emphasis
                role="bold">pegasus</emphasis> profile key <emphasis
                role="bold">user_label </emphasis>appears in the same
                cluster.</para>
              </listitem>
            </itemizedlist>
          </section>
        </section>

        <section>
          <title>Recursive Clustering</title>

          <para>In some cases, a user may want to use a combination of
          clustering techniques. For e.g. a user may want some jobs in the
          workflow to be horizontally clustered and some to be label
          clustered. This can be achieved by specifying a comma separated list
          of clustering techniques to the<emphasis role="bold">
          &ndash;-cluster</emphasis> option of <emphasis
          role="bold">pegasus-plan</emphasis>. In this case the clustering
          techniques are applied one after the other on the workflow in the
          order specified on the command line.</para>

          <para>For example</para>

          <programlisting>$ <emphasis>pegasus-plan &ndash;-dax example.dax --dir ./dags --cluster label,horizontal &ndash;s siteX &ndash;-output local --verbose</emphasis></programlisting>

          <figure>
            <title></title>

            <mediaobject>
              <imageobject>
                <imagedata fileref="images/advanced-clustering-4.png" />
              </imageobject>
            </mediaobject>
          </figure>
        </section>
      </section>

      <section>
        <title>Execution of the Clustered Job</title>

        <para>The execution of the clustered job on the remote site, involves
        the execution of the smaller constituent jobs either</para>

        <itemizedlist>
          <listitem>
            <para><emphasis role="bold">sequentially on a single node of the
            remote site</emphasis></para>

            <para>The clustered job is executed using <emphasis
            role="bold">seqexec</emphasis>, a wrapper tool written in C that
            is distributed as part of the PEGASUS. It takes in the jobs passed
            to it, and ends up executing them sequentially on a single node.
            To use &ldquo;<emphasis role="bold">seqexec</emphasis>&rdquor; for
            executing any clustered job on a siteX, there needs to be an entry
            in the transformation catalog for an executable with the logical
            name seqexec and namespace as pegasus.</para>

            <programlisting><emphasis role="bold">#site  transformation   pfn            type                 architecture    profiles</emphasis>

siteX    pegasus::seqexec     /shared/PEGASUS/bin/seqexec INSTALLED       INTEL32::LINUX NULL</programlisting>

            <para>By default, the entry for seqexec on a site is automatically
            picked up if $PEGASUS_HOME or $VDS_HOME is specified in the site
            catalog for that site.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">On multiple nodes of the remote site
            using MPI</emphasis></para>

            <para>The clustered job is executed using mpiexec, a wrapper mpi
            program written in C that is distributed as part of the PEGASUS.
            It is only distributed as source not as binary. The wrapper ends
            up being run on every mpi node, with the first one being the
            master and the rest of the ones as workers. The number of
            instances of mpiexec that are invoked is equal to the value of the
            globus rsl key nodecount. The master distributes the smaller
            constituent jobs to the workers.</para>

            <para>For e.g. If there were 10 jobs in the merged job and
            nodecount was 5, then one node acts as master, and the 10 jobs are
            distributed amongst the 4 slaves on demand. The master hands off a
            job to the slave node as and when it gets free. So initially all
            the 4 nodes are given a single job each, and then as and when they
            get done are handed more jobs till all the 10 jobs have been
            executed.</para>

            <para>To use &ldquo;mpiexec&rdquor; for executing the clustered
            job on a siteX, there needs to be an entry in the transformation
            catalog for an executable with the logical name mpiexec and
            namespace as pegasus.</para>

            <programlisting><emphasis role="bold">#site  transformation   pfn            type                 architecture    profiles</emphasis>

siteX    pegasus::seqexec     /shared/PEGASUS/bin/mpiexec INSTALLED       INTEL32::LINUX NULL</programlisting>

            <para>Another added advantage of using mpiexec, is that regular
            non mpi code can be run via MPI.</para>

            <para>Both the clustered job and the smaller constituent jobs are
            invoked via kickstart, unless the clustered job is being run via
            mpi (mpiexec). Kickstart is unable to launch mpi jobs. If
            kickstart is not installed on a particular site i.e. the
            gridlaunch attribute for site is not specified in the site
            catalog, the jobs are invoked directly.</para>
          </listitem>
        </itemizedlist>

        <section>
          <title>Specification of Method of Execution for Clustered
          Jobs</title>

          <para>The method execution of the clustered job(whether to launch
          via mpiexec or seqexec) can be specified</para>

          <orderedlist>
            <listitem>
              <para><emphasis role="bold">globally in the properties
              file</emphasis></para>

              <para>The user can set a property in the properties file that
              results in all the clustered jobs of the workflow being executed
              by the same type of executable.</para>

              <programlisting><emphasis role="bold">#PEGASUS PROPERTIES FILE</emphasis>
pegasus.clusterer.job.aggregator seqexec|mpiexec</programlisting>

              <para>In the above example, all the clustered jobs on the remote
              sites are going to be launched via the property value, as long
              as the property value is not overridden in the site
              catalog.</para>
            </listitem>

            <listitem>
              <para><emphasis role="bold">associating profile key
              &ldquo;collapser&rdquor; with the site in the site
              catalog</emphasis></para>

              <programlisting>&lt;site handle="siteX" gridlaunch = "/shared/PEGASUS/bin/kickstart"&gt;
    &lt;profile namespace="env" key="GLOBUS_LOCATION" &gt;/home/shared/globus&lt;/profile&gt;
    &lt;profile namespace="env" key="LD_LIBRARY_PATH"&gt;/home/shared/globus/lib&lt;/profile&gt;
    &lt;profile namespace="pegasus" key="collapser" &gt;seqexec&lt;/profile&gt;
    &lt;lrc url="rls://siteX.edu" /&gt;
    &lt;gridftp  url="gsiftp://siteX.edu/" storage="/home/shared/work" major="2" minor="4" patch="0" /&gt;
    &lt;jobmanager universe="transfer" url="siteX.edu/jobmanager-fork" major="2" minor="4" patch="0" /&gt;
    &lt;jobmanager universe="vanilla" url="siteX.edu/jobmanager-condor" major="2" minor="4" patch="0" /&gt;
    &lt;workdirectory &gt;/home/shared/storage&lt;/workdirectory&gt;
  &lt;/site&gt;</programlisting>

              <para>In the above example, all the clustered jobs on a siteX
              are going to be executed via seqexec, as long as the value is
              not overridden in the transformation catalog.</para>
            </listitem>

            <listitem>
              <para><emphasis role="bold">associating profile key
              &ldquo;collapser&rdquor; with the transformation that is being
              clustered, in the transformation catalog</emphasis></para>

              <programlisting><emphasis role="bold">#site  transformation   pfn            type                architecture profiles
</emphasis>
siteX    B     /shared/PEGASUS/bin/jobB INSTALLED       INTEL32::LINUX pegasus::clusters.size=3,collapser=mpiexec</programlisting>

              <para>In the above example, all the clustered jobs that consist
              of transformation B on siteX will be executed via
              mpiexec.</para>

              <para><emphasis role="bold">Note: The clustering of jobs on a
              site only happens only if </emphasis></para>

              <itemizedlist>
                <listitem>
                  <para>there exists an entry in the transformation catalog
                  for the clustering executable that has been determined by
                  the above 3 rules</para>
                </listitem>

                <listitem>
                  <para>the number of jobs being clustered on the site are
                  more than 1</para>
                </listitem>
              </itemizedlist>
            </listitem>
          </orderedlist>
        </section>
      </section>

      <section>
        <title>Outstanding Issues</title>

        <orderedlist>
          <listitem>
            <para><emphasis role="bold">Label Clustering</emphasis></para>

            <para>More rigorous checks are required to ensure that the
            labeling scheme applied by the user is valid.</para>
          </listitem>
        </orderedlist>
      </section>
    </section>
  </section>

  <section>
    <title>Transfers</title>

    <para>As part of the Workflow Mapping Process, Pegasus does data
    management for the executable workflow . It queries a Replica Catalog to
    discover the locations of the input datasets and adds data movement and
    registration nodes in the workflow to</para>

    <orderedlist>
      <listitem>
        <para>stage-in input data to the compute sites ( where the jobs in the
        workflow are executed )</para>
      </listitem>

      <listitem>
        <para>stage-out output data generated by the workflow to the final
        storage site.</para>
      </listitem>

      <listitem>
        <para>stage-in intermediate data between compute sites if
        required.</para>
      </listitem>

      <listitem>
        <para>data registration nodes to catalog the locations of the output
        data on the final storage site into the replica catalog.</para>
      </listitem>
    </orderedlist>

    <para>The separate data movement jobs that are added to the executable
    workflow are responsible for staging data to a workflow specific directory
    accessible to the staging server on a staging site associated with the
    compute sites. Currently, the staging site for a compute site is the
    compute site itself. In the default case, the staging server is usually on
    the headnode of the compute site and has access to the shared filesystem
    between the worker nodes and the head node. Pegasus adds a directory
    creation job in the executable workflow that creates the workflow specific
    directory on the staging server.</para>

    <figure>
      <title>Default Transfer Case : Input Data To Workflow Specific Directory
      on Shared File System</title>

      <mediaobject>
        <imageobject>
          <imagedata fileref="images/default-transfer-sharedfs.png" />
        </imageobject>
      </mediaobject>
    </figure>

    <para>In addition to data, Pegasus does transfer user executables to the
    compute sites if the executables are not installed on the remote sites
    before hand. This chapter gives an overview of how transfers of data and
    executables is managed in Pegasus.</para>

    <section>
      <title>Local versus Remote Transfers</title>

      <para>As far as possible, Pegasus will ensure that the transfer jobs
      added to the executable workflow are executed on the submit host. By
      default, Pegasus will schedule a transfer to be executed on the remote
      compute site only if there is no way to execute it on the submit host.
      For e.g if the file server specified for the compute site is a file
      server, then Pegasus will schedule all the stage in data movement jobs
      on the compute site to stage-in the input data for the workflow. Another
      case would be if a user has symlinking turned on. In that case, the
      transfer jobs that symlink against the input data on the compute site,
      will be executed remotely ( on the compute site ).</para>

      <para>Users can specify the property <emphasis
      role="bold">pegasus.transfer.*.remote.sites</emphasis> to change the
      default behaviour of Pegasus and force pegasus to run different types of
      transfer jobs for the sites specified on the remote site. The value of
      the property is a comma separated list of compute sites for which you
      want the transfer jobs to run remotely.</para>

      <para>The table below illustrates all the possible variations of the
      property.</para>

      <table>
        <title>Property Variations for pegasus.transfer.*.remote.sites</title>

        <tgroup cols="2">
          <thead>
            <row>
              <entry>Property Name</entry>

              <entry>Applies to</entry>
            </row>
          </thead>

          <tbody>
            <row>
              <entry>pegasus.transfer.stagein.remote.sites</entry>

              <entry>the stage in transfer jobs</entry>
            </row>

            <row>
              <entry>pegasus.transfer.stageout.remote.sites</entry>

              <entry>the stage out transfer jobs</entry>
            </row>

            <row>
              <entry>pegasus.transfer.inter.remote.sites</entry>

              <entry>the inter site transfer jobs</entry>
            </row>

            <row>
              <entry>pegasus.transfer.*.remote.sites</entry>

              <entry>all types of transfer jobs</entry>
            </row>
          </tbody>
        </tgroup>
      </table>

      <para>The prefix for the transfer job name indicates whether the
      transfer job is to be executed locallly ( on the submit host ) or
      remotely ( on the compute site ). For example stage_in_local_ in a
      transfer job name stage_in_local_isi_viz_0 indicates that the transfer
      job is a stage in transfer job that is executed locally and is used to
      transfer input data to compute site isi_viz. The prefix naming scheme
      for the transfer jobs is <emphasis
      role="bold">[stage_in|stage_out|inter]_[local|remote]_</emphasis>
      .</para>
    </section>

    <section>
      <title>Symlinking Against Input Data</title>

      <para>If input data for a job already exists on a compute site, then it
      is possible for Pegasus to symlink against that data. In this case, the
      remote stage in transfer jobs that Pegasus adds to the executable
      workflow will symlink instead of doing a copy of the data.</para>

      <para>Pegasus determines whether a file is on the same site as the
      compute site, by inspecting the pool attribute associated with the URL
      in the Replica Catalog. If the pool attribute of an input file location
      matches the compute site where the job is scheduled, then that
      particular input file is a candidate for symlinking.</para>

      <para>For Pegasus to symlink against existing input data on a compute
      site, following must be true</para>

      <orderedlist>
        <listitem>
          <para>Property <emphasis
          role="bold">pegasus.transfer.links</emphasis> is set to <emphasis
          role="bold">true</emphasis></para>
        </listitem>

        <listitem>
          <para>The input file location in the Replica Catalog has the pool
          attribute matching the compute site.</para>
        </listitem>
      </orderedlist>

      <tip>
        <para>To confirm if a particular input file is symlinked instead of
        being copied, look for the destination URL for that file in
        stage_in_remote*.in file. The destination URL will start with
        symlink:// .</para>
      </tip>

      <para>In the symlinking case, Pegasus strips out URL prefix from a URL
      and replaces it with a file URL.</para>

      <para>For example if a user has the following URL catalogued in the
      Replica Catalog for an input file f.input</para>

      <programlisting>f.input   gsiftp://server.isi.edu/shared/storage/input/data/f.input pool="isi"</programlisting>

      <para>and the compute job that requires this file executes on a compute
      site named isi , then if symlinking is turned on the data stage in job
      (stage_in_remote_viz_0 ) will have the following source and destination
      specified for the file</para>

      <programlisting>#viz viz
file:///shared/storage/input/data/f.input  symlink://shared-scratch/workflow-exec-dir/f.input
</programlisting>
    </section>

    <section>
      <title>Addition of Separate Data Movement Nodes to Executable
      Workflow</title>

      <para>Pegasus relies on a Transfer Refiner that comes up with the
      strategy on how many data movement nodes are added to the executable
      workflow. All the compute jobs scheduled to a site share the same
      workflow specific directory. The transfer refiners ensure that only one
      copy of the input data is transferred to the workflow execution
      directory. This is to prevent data clobbering . Data clobbering can
      occur when compute jobs of a workflow share some input files, and have
      different stage in transfer jobs associated with them that are staging
      the shared files to the same destination workflow execution
      directory.</para>

      <para>The default Transfer Refiner used in Pegasus is the Bundle Refiner
      that allows the user to specify how many local|remote stagein|stageout
      jobs are created per execution site.</para>

      <para>The behavior of the refiner is controlled by specifying certain
      pegasus profiles</para>

      <orderedlist>
        <listitem>
          <para>either with the execution sites in the site catalog</para>
        </listitem>

        <listitem>
          <para>OR globally in the properties file</para>
        </listitem>
      </orderedlist>

      <table>
        <title>Pegasus Profile Keys For the Bundle Transfer Refiner</title>

        <tgroup cols="2">
          <thead>
            <row>
              <entry>Profile Key</entry>

              <entry>Description</entry>
            </row>
          </thead>

          <tbody>
            <row>
              <entry>stagein.clusters</entry>

              <entry>This key determines the maximum number of stage-in jobs
              that are can executed locally or remotely per compute site per
              workflow.</entry>
            </row>

            <row>
              <entry>stagein.local.clusters</entry>

              <entry>This key provides finer grained control in determining
              the number of stage-in jobs that are executed locally and are
              responsible for staging data to a particular remote
              site.</entry>
            </row>

            <row>
              <entry>stagein.remote.clusters</entry>

              <entry>This key provides finer grained control in determining
              the number of stage-in jobs that are executed remotely on the
              remote site and are responsible for staging data to it.</entry>
            </row>

            <row>
              <entry>stageout.clusters</entry>

              <entry>This key determines the maximum number of stage-out jobs
              that are can executed locally or remotely per compute site per
              workflow.</entry>
            </row>

            <row>
              <entry>stageout.local.clusters</entry>

              <entry>This key provides finer grained control in determining
              the number of stage-out jobs that are executed locally and are
              responsible for staging data from a particular remote
              site.</entry>
            </row>

            <row>
              <entry>stageout.remote.clusters</entry>

              <entry>This key provides finer grained control in determining
              the number of stage-out jobs that are executed remotely on the
              remote site and are responsible for staging data from
              it.</entry>
            </row>
          </tbody>
        </tgroup>
      </table>

      <figure>
        <title>Default Transfer Case : Input Data To Workflow Specific
        Directory on Shared File System</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="images/bundle-transfer-refiner.png" lang="" />
          </imageobject>
        </mediaobject>
      </figure>
    </section>

    <section>
      <title>Executable Used for Transfer Jobs</title>

      <para>Pegasus refers to a python script called <emphasis
      role="bold">pegasus-transfer</emphasis> as the executable in the
      transfer jobs to transfer the data. pegasus-transfer is a python based
      wrapper around various transfer clients . pegasus-transfer looks at
      source and destination url and figures out automatically which
      underlying client to use. pegasus-transfer is distributed with the
      PEGASUS and can be found at $PEGASUS_HOME/bin/pegasus-transfer.</para>

      <para>Currently, pegasus-transfer interfaces with the following transfer
      clients</para>

      <table>
        <title>Transfer Clients interfaced to by pegasus-transfer</title>

        <tgroup cols="2">
          <thead>
            <row>
              <entry>Transfer Client</entry>

              <entry>Used For</entry>
            </row>
          </thead>

          <tbody>
            <row>
              <entry>globus-url-copy</entry>

              <entry>staging files to and from a gridftp server.</entry>
            </row>

            <row>
              <entry>lcg-copy</entry>

              <entry>staging files to and from a SRM server.</entry>
            </row>

            <row>
              <entry>wget</entry>

              <entry>staging files from a HTTP server.</entry>
            </row>

            <row>
              <entry>cp</entry>

              <entry>copying files from a POSIX filesystem .</entry>
            </row>

            <row>
              <entry>ln</entry>

              <entry>symlinking against input files.</entry>
            </row>
          </tbody>
        </tgroup>
      </table>

      <para>For remote sites, Pegasus constructs the default path to
      pegasus-transfer on the basis of PEGASUS_HOME env profile specified in
      the site catalog. To specify a different path to the pegasus-transfer
      client , users can add an entry into the transformation catalog with
      fully qualified logical name as <emphasis
      role="bold">pegasus::pegasus-transfer</emphasis></para>
    </section>

    <section>
      <title>Staging of Executables</title>

      <para>Users can get Pegasus to stage the user executables ( executables
      that the jobs in the DAX refer to ) as part of the transfer jobs to the
      workflow specific execution directory on the compute site. The URL
      locations of the executables need to be specified in the transformation
      catalog as the PFN and the type of executable needs to be set to
      <emphasis role="bold">STAGEABLE</emphasis> .</para>

      <para>The location of a transformation can be specified either in</para>

      <itemizedlist>
        <listitem>
          <para>DAX in the executables section. More details <link
          linkend="dax_transformation_catalog">here</link> .</para>
        </listitem>

        <listitem>
          <para>Transformation Catalog. More details <link
          linkend="transformation">here</link> .</para>
        </listitem>
      </itemizedlist>

      <para>A particular transformation catalog entry of type STAGEABLE is
      compatible with a compute site only if all the System Information
      attributes associated with the entry match with the System Information
      attributes for the compute site in the Site Catalog. The following
      attributes make up the System Information attributes</para>

      <orderedlist>
        <listitem>
          <para>arch</para>
        </listitem>

        <listitem>
          <para>os</para>
        </listitem>

        <listitem>
          <para>osrelease</para>
        </listitem>

        <listitem>
          <para>osversion</para>
        </listitem>
      </orderedlist>

      <section>
        <title>Transformation Mappers</title>

        <para>Pegasus has a notion of transformation mappers that determines
        what type of executables are picked up when a job is executed on a
        remote compute site. For transfer of executables, Pegasus constructs a
        soft state map that resides on top of the transformation catalog, that
        helps in determining the locations from where an executable can be
        staged to the remote site.</para>

        <para>Users can specify the following property to pick up a specific
        transformation mapper</para>

        <programlisting><emphasis role="bold">pegasus.catalog.transformation.mapper</emphasis> </programlisting>

        <para>Currently, the following transformation mappers are
        supported.</para>

        <table>
          <title>Transformation Mappers Supported in Pegasus</title>

          <tgroup cols="2">
            <thead>
              <row>
                <entry>Transformation Mapper</entry>

                <entry>Description</entry>
              </row>
            </thead>

            <tbody>
              <row>
                <entry>Installed</entry>

                <entry>This mapper only relies on transformation catalog
                entries that are of type INSTALLED to construct the soft state
                map. This results in Pegasus never doing any transfer of
                executables as part of the workflow. It always prefers the
                installed executables at the remote sites</entry>
              </row>

              <row>
                <entry>Staged</entry>

                <entry>This mapper only relies on matching transformation
                catalog entries that are of type STAGEABLE to construct the
                soft state map. This results in the executable workflow
                referring only to the staged executables, irrespective of the
                fact that the executables are already installed at the remote
                end</entry>
              </row>

              <row>
                <entry>All</entry>

                <entry>This mapper relies on all matching transformation
                catalog entries of type STAGEABLE or INSTALLED for a
                particular transformation as valid sources for the transfer of
                executables. This the most general mode, and results in the
                constructing the map as a result of the cartesian product of
                the matches.</entry>
              </row>

              <row>
                <entry>Submit</entry>

                <entry>This mapper only on matching transformation catalog
                entries that are of type STAGEABLE and reside at the submit
                host (pool local), are used while constructing the soft state
                map. This is especially helpful, when the user wants to use
                the latest compute code for his computations on the grid and
                that relies on his submit host.</entry>
              </row>
            </tbody>
          </tgroup>
        </table>
      </section>
    </section>

    <section>
      <title>Staging of Pegasus Worker Package</title>

      <para>Pegasus can optionally stage the pegasus worker package as part of
      the executable workflow to remote workflow specific execution directory.
      The pegasus worker package contains the pegasus auxillary executables
      that are required on the remote site. If the worker package is not
      staged as part of the executable workflow, then Pegasus relies on the
      installed version of the worker package on the remote site. To determine
      the location of the installed version of the worker package on a remote
      site, Pegasus looks for an environment profile PEGASUS_HOME for the site
      in the Site Catalog.</para>

      <para>Users can set the following property to true to turn on worker
      package staging</para>

      <programlisting><emphasis role="bold">pegasus.transfer.worker.package          true</emphasis> </programlisting>

      <para>By default, when worker package staging is turned on pegasus pulls
      the compatible worker package from the Pegasus Website. To specify a
      different worker package location, users can specify the transformation
      <emphasis role="bold">pegasus::worker</emphasis> in the transformation
      catalog with</para>

      <itemizedlist>
        <listitem>
          <para>type set to STAGEABLE</para>
        </listitem>

        <listitem>
          <para>System Information attributes of the transformation catalog
          entry match the System Information attributes of the compute
          site.</para>
        </listitem>

        <listitem>
          <para>the PFN specified should be a remote URL that can be pulled to
          the compute site.</para>
        </listitem>
      </itemizedlist>
    </section>

    <section>
      <title>Second Level Staging</title>

      <para>By default, Pegasus executes the jobs in the workflow specific
      directory created on the shared filesystem of a compute site. However,
      if a user wants Pegasus can execute the jobs on the worker nodes
      filesystem. When the jobs are executed on the worker node, they pull the
      input data for the job from the workflow specific directory on the
      staging server ( usually the shared filesystem on the compute site ) to
      a directory on the worker node filesystem, and after the job has
      completed stages out the output files from the worker node to the
      workflow specific execution directory.</para>

      <para>The separate data stagein and stageout jobs are still added to the
      workflow. They are responsible for getting the input data to the
      workflow specific directory on the staging server ( usually the shared
      filesystem on the compute site ) , and pushing out the output data to
      final storage site from that directory.</para>

      <figure>
        <title>Second Level Staging : Getting Data to and from a directory on
        the worker nodes</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="./images/sls-transfer-worker.png" />
          </imageobject>
        </mediaobject>
      </figure>

      <para>This mode is especially useful for running in the cloud
      environments where you don't want to setup a shared filesystem between
      the worker nodes. Running in that mode is explained in detail <link
      linkend="running_on_cloud">here.</link></para>

      <para>To turn on second level staging for the workflows users should set
      the following properties</para>

      <programlisting><emphasis role="bold">pegasus.execute.*.filesystem.local = true   </emphasis>    # Turn on second-level staging (SLS)
<emphasis role="bold">pegasus.transfer.sls.s3.stage.sls.file = false</emphasis>  # Do not transfer .sls files via transfer jobs
<emphasis role="bold">pegasus.gridstart = SeqExec </emphasis>                    # Use SeqExec to launch the jobs</programlisting>
    </section>
  </section>

  <section>
    <title>Hierarchical Workflow</title>

    <section>
      <title>Introduction</title>

      <para>The Abstract Workflow in addition to containing compute jobs, can
      also contain jobs that refer to other workflows. This is useful for
      running large workflows or ensembles of workflows.</para>

      <para>Users can embed two types of workflow jobs in the DAX</para>

      <orderedlist>
        <listitem>
          <para>daxjob - refers to a sub workflow represented as a DAX. During
          the planning of a workflow, the DAX jobs are mapped to condor dagman
          jobs that have pegasus plan invocation on the dax ( referred to in
          the DAX job ) as the prescript.</para>

          <figure>
            <title>Planning of a DAX Job</title>

            <mediaobject>
              <imageobject>
                <imagedata fileref="./images/daxjob-mapping.png" />
              </imageobject>
            </mediaobject>
          </figure>
        </listitem>

        <listitem>
          <para>dagjob - refers to a sub workflow represented as a DAG. During
          the planning of a workflow, the DAG jobs are mapped to condor dagman
          and refer to the DAG file mentioned in the DAG job.</para>

          <figure>
            <title>Planning of a DAG Job</title>

            <mediaobject>
              <imageobject>
                <imagedata fileref="./images/dagjob-mapping.png" />
              </imageobject>
            </mediaobject>
          </figure>
        </listitem>
      </orderedlist>
    </section>

    <section>
      <title>Specifying a DAX Job in the DAX</title>

      <para>Specifying a DAXJob in a DAX is pretty similar to how normal
      compute jobs are specified. There are minor differences in terms of the
      xml element name ( dax vs job ) and the attributes specified. DAXJob XML
      specification is described in detail in the <link linkend="api">chapter
      on DAX API</link> . An example DAX Job in a DAX is shown below</para>

      <programlisting id="dax_job_example" language="">  &lt;dax id="ID000002" name="black.dax" node-label="bar" &gt;
    &lt;profile namespace="dagman" key="maxjobs"&gt;10&lt;/profile&gt;
    &lt;argument&gt;-Xmx1024 -Xms512 -Dpegasus.dir.storage=storagedir  -Dpegasus.dir.exec=execdir -o local -vvvvv --force -s dax_site &lt;/argument&gt;
  &lt;/dax&gt;</programlisting>

      <section>
        <title>DAX File Locations</title>

        <para>The name attribute in the dax element refers to the LFN (
        Logical File Name ) of the dax file. The location of the DAX file can
        be catalogued either in the</para>

        <para><orderedlist>
            <listitem>
              <para>Replica Catalog</para>
            </listitem>

            <listitem>
              <para>Replica Catalog Section in the <link
              linkend="dax_replica_catalog">DAX</link> .</para>

              <note>
                <para>Currently, only file url's on the local site ( submit
                host ) can be specified as DAX file locations.</para>
              </note>
            </listitem>
          </orderedlist></para>
      </section>

      <section>
        <title>Arguments for a DAX Job</title>

        <para>Users can specify specific arguments to the DAX Jobs. The
        arguments specified for the DAX Jobs are passed to the pegasus-plan
        invocation in the prescript for the corresponding condor dagman job in
        the executable workflow.</para>

        <para>The following options for pegasus-plan are inherited from the
        pegasus-plan invocation of the parent workflow. If an option is
        specified in the arguments section for the DAX Job then that overrides
        what is inherited.</para>

        <table>
          <title>Options inherited from parent workflow</title>

          <tgroup cols="2">
            <thead>
              <row>
                <entry>Option Name</entry>

                <entry>Description</entry>
              </row>
            </thead>

            <tbody>
              <row>
                <entry>--sites</entry>

                <entry>list of execution sites.</entry>
              </row>
            </tbody>
          </tgroup>
        </table>

        <para>It is highly recommended that users <emphasis role="bold">dont
        specify</emphasis> directory related options in the arguments section
        for the DAX Jobs. Pegasus assigns values to these options for the sub
        workflows automatically.</para>

        <orderedlist>
          <listitem>
            <para>--relative-dir</para>
          </listitem>

          <listitem>
            <para>--dir</para>
          </listitem>

          <listitem>
            <para>--relative-submit-dir</para>
          </listitem>
        </orderedlist>
      </section>

      <section>
        <title>Profiles for DAX Job</title>

        <para>Users can choose to specify dagman profiles with the DAX Job to
        control the behavior of the corresponding condor dagman instance in
        the executable workflow. In the example <link
        linkend="dax_job_example">above</link> maxjobs is set to 10 for the
        sub workflow.</para>
      </section>

      <section>
        <title>Execution of the PRE script and Condor DAGMan instance</title>

        <para>The pegasus plan that is invoked as part of the prescript to the
        condor dagman job is executed on the submit host. The log from the
        output of pegasus plan is redirected to a file ( ending with suffix
        pre.log ) in the submit directory of the workflow that contains the
        DAX Job. The path to pegasus-plan is automatically determined.</para>

        <para>The DAX Job maps to a Condor DAGMan job. The path to condor
        dagman binary is determined according to the following rules -</para>

        <orderedlist>
          <listitem>
            <para>entry in the transformation catalog for condor::dagman for
            site local, else</para>
          </listitem>

          <listitem>
            <para>pick up the value of CONDOR_HOME from the environment if
            specified and set path to condor dagman as
            $CONDOR_HOME/bin/condor_dagman , else</para>
          </listitem>

          <listitem>
            <para>pick up the value of CONDOR_LOCATION from the environment if
            specified and set path to condor dagman as
            $CONDOR_LOCATION/bin/condor_dagman , else</para>
          </listitem>

          <listitem>
            <para>pick up the path to condor dagman from what is defined in
            the user's PATH</para>
          </listitem>
        </orderedlist>

        <tip>
          <para>It is recommended that user dagman.maxpre in their properties
          file to control the maximum number of pegasus plan instances
          launched by each running dagman instance.</para>
        </tip>
      </section>
    </section>

    <section>
      <title>Specifying a DAG Job in the DAX</title>

      <para>Specifying a DAGJob in a DAX is pretty similar to how normal
      compute jobs are specified. There are minor differences in terms of the
      xml element name ( dag vs job ) and the attributes specified. For DAGJob
      XML details,see the <link linkend="api"> API Reference </link> chapter .
      An example DAG Job in a DAX is shown below</para>

      <programlisting id="dag_job_example">  &lt;dag id="ID000003" name="black.dag" node-label="foo" &gt;
    &lt;profile namespace="dagman" key="maxjobs"&gt;10&lt;/profile&gt;
    &lt;profile namespace="dagman" key="DIR"&gt;/dag-dir/test&lt;/profile&gt;
  &lt;/dag&gt;</programlisting>

      <section>
        <title>DAG File Locations</title>

        <para>The name attribute in the dag element refers to the LFN (
        Logical File Name ) of the dax file. The location of the DAX file can
        be catalogued either in the</para>

        <para><orderedlist>
            <listitem>
              <para>Replica Catalog</para>
            </listitem>

            <listitem>
              <para>Replica Catalog Section in the DAX.</para>

              <note>
                <para>Currently, only file url's on the local site ( submit
                host ) can be specified as DAG file locations.</para>
              </note>
            </listitem>
          </orderedlist></para>
      </section>

      <section>
        <title>Profiles for DAG Job</title>

        <para>Users can choose to specify dagman profiles with the DAX Job to
        control the behavior of the corresponding condor dagman instance in
        the executable workflow. In the example above, maxjobs is set to 10
        for the sub workflow.</para>

        <para>The dagman profile DIR allows users to specify the directory in
        which they want the condor dagman instance to execute. In the example
        <link linkend="dag_job_example">above</link> black.dag is set to be
        executed in directory /dag-dir/test . The /dag-dir/test should be
        created beforehand.</para>
      </section>
    </section>

    <section>
      <title>File Dependencies Across DAX Jobs</title>

      <para>In hierarchal workflows , if a sub workflow generates some output
      files required by another sub workflow then there should be an edge
      connecting the two dax jobs. Pegasus will ensure that the prescript for
      the child sub-workflow, has the path to the cache file generated during
      the planning of the parent sub workflow. The cache file in the submit
      directory for a workflow is a textual replica catalog that lists the
      locations of all the output files created in the remote workflow
      execution directory when the workflow executes.</para>

      <para>This automatic passing of the cache file to a child sub-workflow
      ensures that the datasets from the same workflow run are used. However,
      the passing of the locations in a cache file also ensures that Pegasus
      will prefer them over all other locations in the Replica Catalog. If you
      need the Replica Selection to consider locations in the Replica Catalog
      also, then set the following property.</para>

      <programlisting><emphasis role="bold">pegasus.catalog.replica.cache.asrc  true</emphasis></programlisting>

      <para>The above is useful in the case, where you are staging out the
      output files to a storage site, and you want the child sub workflow to
      stage these files from the storage output site instead of the workflow
      execution directory where the files were originally created.</para>
    </section>

    <section>
      <title>Recursion in Hierarchal Workflows</title>

      <para>It is possible for a user to add a dax jobs to a dax that already
      contain dax jobs in them. Pegasus does not place a limit on how many
      levels of recursion a user can have in their workflows. From Pegasus
      perspective recursion in hierarchal workflows ends when a DAX with only
      compute jobs is encountered . However, the levels of recursion are
      limited by the system resources consumed by the DAGMan processes that
      are running (each level of nesting produces another DAGMan process)
      .</para>

      <para>The figure below illustrates an example with recursion 2 levels
      deep.</para>

      <figure>
        <title>Recursion in Hierarchal Workflows</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="./images/recursion_in_hierarchal_workflows.png" />
          </imageobject>
        </mediaobject>
      </figure>

      <para>The execution time-line of the various jobs in the above figure is
      illustrated below.</para>

      <figure>
        <title>Execution Time-line for Hierarchal Workflows</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="./images/hierarchal_workflows_execution_timeline.png" />
          </imageobject>
        </mediaobject>
      </figure>
    </section>

    <section>
      <title>Example</title>

      <para>The Galactic Plane workflow is a Hierarchical workflow of many
      Montage workflows. For details, see <link
      linkend="example_workflows">Workflow of Workflows</link>.</para>
    </section>
</section>

<section>
  <title>API Reference</title>

  <section>
    <title>DAX XML Schema</title>

    <para>The DAX format is described by the XML schema instance document
    <ulink
    url="http://pegasus.isi.edu/wms/docs/schemas/dax-3.2/dax-3.2.xsd">dax-3.2.xsd</ulink>.
    A local copy of the schema definition is provided in the
    <quote>etc</quote> directory. The documentation of the XML schema and its
    elements can be found in <ulink
    url="http://pegasus.isi.edu/wms/docs/schemas/dax-3.2/dax-3.2.html">dax-3.2.html</ulink>
    as well as locally in
    <filename>doc/schemas/dax-3.2/dax-3.2.html</filename> in your Pegasus
    distribution.</para>

    <section>
      <title>DAX XML Schema In Detail</title>

      <para>The DAX file format has three major sections, with the first
      section sub-divided into more sub-sections. The DAX format works on the
      abstract or logical level, letting you focus on the shape of the
      workflows, what to do and what to work upon.</para>

      <orderedlist>
        <listitem>
          <para>Catalogs</para>

          <para>The first section deals with included catalogs. While we do
          recommend to use external replica- and transformation catalogs, it
          is possible to include some replicas and transformations into the
          DAX file itself. Any DAX-included entry takes precedence over
          regular RC and TC entries.</para>

          <para>The first section (and any of its sub-sections) is completely
          optional.</para>

          <orderedlist>
            <listitem>
              <para>The first sub-section deals with included replica
              descriptions.</para>
            </listitem>

            <listitem>
              <para>The second sub-section deals with included transformation
              descriptions.</para>
            </listitem>

            <listitem>
              <para>The third sub-section declares multi-item
              executables.</para>
            </listitem>
          </orderedlist>
        </listitem>

        <listitem>
          <para>Job List</para>

          <para>The second section defines the job- or task descriptions. For
          each task to conduct, a three-part logical name declares the task
          and aides identifying it in the transformation catalog or one of the
          <emphasis>executable</emphasis> section above. During planning, the
          logical name is translated into the physical executable location on
          the chosen target site. By declaring jobs abstractly, physical
          layout consideration of the target sites do not matter. The job's
          <emphasis>id</emphasis> uniquley identifies the job within this
          workflow.</para>

          <para>The arguments declare what command-line arguments to pass to
          the job. If you are passing filenames, you should refer to the
          logical filename using the <emphasis>file</emphasis> element in the
          argument list.</para>

          <para>Important for properly planning the task is the list of files
          consumed by the task, its input files, and the files produced by the
          task, its output files. Each file is described with a
          <emphasis>uses</emphasis> element inside the task.</para>

          <para>Elements exist to link a logical file to any of the stdio file
          descriptors. The profile element is Pegasus's way to abstract
          site-specific data.</para>

          <para>Jobs are nodes in the workflow graph.</para>
        </listitem>

        <listitem>
          <para>Control-flow Dependencies</para>

          <para>The third section lists the dependencies between the tasks.
          The relationships are defined as child parent relationships, and
          thus impacts the order in which tasks are run. No cyclic
          dependencies are permitted.</para>

          <para>Dependencies are directed edges in the workflow graph.</para>
        </listitem>
      </orderedlist>

      <section>
        <title>XML Intro</title>

        <para>If you have seen the DAX schema before, not a lot of new items
        in the root element. However, we did retire the (old) attributes
        ending in <emphasis>Count</emphasis>.</para>

        <programlisting>&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;!-- generated: 2010-11-20T03:10:52Z --&gt;
&lt;adag xmlns="http://pegasus.isi.edu/schema/DAX" 
      xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
      xsi:schemaLocation="http://pegasus.isi.edu/schema/DAX http://pegasus.isi.edu/schema/dax-3.2.xsd" 
      version="3.2" 
      name="diamond" 
      index="0" 
      count="1"&gt;</programlisting>

        <para>The following attributes are supported for the root element
        <emphasis>adag</emphasis>.</para>

        <table>
          <tgroup cols="4">
            <thead>
              <row>
                <entry>attribute</entry>

                <entry>optional?</entry>

                <entry>type</entry>

                <entry>meaning</entry>
              </row>
            </thead>

            <tbody>
              <row>
                <entry>version</entry>

                <entry>required</entry>

                <entry>
                  <emphasis>VersionPattern</emphasis>
                </entry>

                <entry>Version number of DAX instance document.</entry>
              </row>

              <row>
                <entry>name</entry>

                <entry>required</entry>

                <entry>string</entry>

                <entry>name of this DAX (or set of DAXes). Must be
                3.2.</entry>
              </row>

              <row>
                <entry>count</entry>

                <entry>optional</entry>

                <entry>positiveInteger</entry>

                <entry>size of list of DAXes with this
                <emphasis>name</emphasis>. Defaults to 1.</entry>
              </row>

              <row>
                <entry>index</entry>

                <entry>optional</entry>

                <entry>nonNegativeInteger</entry>

                <entry>current index of DAX with same
                <emphasis>name</emphasis>. Defaults to 0.</entry>
              </row>

              <row>
                <entry>fileCount</entry>

                <entry>removed</entry>

                <entry>nonNegativeInteger</entry>

                <entry>Old 2.1 attribute, removed, do not use.</entry>
              </row>

              <row>
                <entry>jobCount</entry>

                <entry>removed</entry>

                <entry>positiveInteger</entry>

                <entry>Old 2.1 attribute, removed, do not use.</entry>
              </row>

              <row>
                <entry>childCount</entry>

                <entry>removed</entry>

                <entry>nonNegativeInteger</entry>

                <entry>Old 2.1 attribute, removed, do not use.</entry>
              </row>
            </tbody>
          </tgroup>
        </table>

        <para>The <emphasis>version</emphasis> attribute is restricted to the
        regular expression <code>\d+(\.\d+(\.\d+)?)?</code>.This expression
        represents the <emphasis>VersionPattern</emphasis> type that is used
        in other places, too. It is a more restrictive expression than before,
        but allows us to compute comparable version number using the following
        formula:</para>

        <informaltable border="1">
          <tr>
            <td>version1: a.b.c</td>

            <td>version2: d.e.f</td>
          </tr>

          <tr>
            <td>n = a * 1,000,000 + b * 1,000 + c</td>

            <td>m = d * 1,000,000 + e * 1,000 + f</td>
          </tr>

          <tr>
            <td colspan="2">version1 &gt; version2 if n &gt; m</td>
          </tr>
        </informaltable>
      </section>

      <section>
        <title>The Catalogs Section</title>

        <para>The initial section features three sub-sections:</para>

        <orderedlist>
          <listitem>
            <para>a catalog of files used,</para>
          </listitem>

          <listitem>
            <para>a catalog of transformations used, and</para>
          </listitem>

          <listitem>
            <para>compound transformation declarations.</para>
          </listitem>
        </orderedlist>

        <section id="dax_replica_catalog">
          <title>The Replica Catalog Section</title>

          <para>The file section acts as in in-file replica catalog (RC). Any
          files declared in this section take precedence over files in
          external replica catalogs during planning.</para>

          <programlisting>  &lt;!-- part 1.1: included replica catalog --&gt;
  &lt;file name="example.a" &gt;
    &lt;!-- profiles are optional --&gt;
    &lt;!-- The "stat" namespace is ONLY AN EXAMPLE --&gt;
    &lt;profile namespace="stat" key="size"&gt;/* integer to be defined */&lt;/profile&gt;
    &lt;profile namespace="stat" key="md5sum"&gt;/* 32 char hex string */&lt;/profile&gt;
    &lt;profile namespace="stat" key="mtime"&gt;/* ISO-8601 timestamp */&lt;/profile&gt;

    &lt;!-- metadata is currently NOT SUPPORTED --&gt;
    &lt;metadata key="timestamp" type="int"&gt;/* ISO-8601 *or* 20100417134523:int */&lt;/metadata&gt;
    &lt;metadata key="origin" type="string"&gt;ocean&lt;/metadata&gt;
    
    &lt;!-- PFN to by-pass replica catalog --&gt;
    &lt;!-- The "site attribute is optional --&gt;
    &lt;pfn url="file:///tmp/example.a" site="local"&gt;
      &lt;profile namespace="stat" key="owner"&gt;voeckler&lt;/profile&gt;
    &lt;/pfn&gt;
    &lt;pfn url="file:///storage/funky.a" site="local"/&gt;    
  &lt;/file&gt;

  &lt;!-- a more typical example from the black diamond --&gt;
  &lt;file name="f.a"&gt;
    &lt;pfn url="file:///Users/voeckler/f.a" site="local"/&gt;
  &lt;/file&gt;</programlisting>

          <para>The first <emphasis>file</emphasis> entry above is an example
          of a data file with two replicas. The <emphasis>file</emphasis>
          element requires a logical file <emphasis>name</emphasis>. Each
          logical filename may have additional information associated with it,
          enumerated by <emphasis>profile</emphasis> elements. Each file entry
          may have 0 or more <emphasis>metadata</emphasis> associated with it.
          Each piece of metadata has a <emphasis>key</emphasis> string and
          <emphasis>type</emphasis> attribute describing the element's
          value.</para>

          <warning>
            <para>The <emphasis>metadata</emphasis> element is not support as
            of this writing! Details may change in the future.</para>
          </warning>

          <para>The <emphasis>file</emphasis> element can provide 0 or more
          <emphasis>pfn</emphasis> locations, taking precedence over the
          replica catalog. A <emphasis>file</emphasis> element that does not
          name any <emphasis>pfn</emphasis> children-elements will still
          require look-ups in external replica catalogs. Each
          <emphasis>pfn</emphasis> element names a concrete location of a
          file. Multiple locations constitute replicas of the same file, and
          are assumed to be usable interchangably. The
          <emphasis>url</emphasis> attribute is mandatory, and typically would
          use a file schema URL. The <emphasis>site</emphasis> attribute is
          optional, and defaults to value <emphasis>local</emphasis> if
          missing. A <emphasis>pfn</emphasis> element may have
          <emphasis>profile</emphasis> children-elements, which refer to
          attributes of the physical file. The file-level profiles refer to
          attributes of the logical file.</para>

          <note>
            <para>The <literal>stat</literal> profile namespace is ony an
            example, and details about stat are not yet implemented. The
            proper namespaces <literal>pegasus</literal>,
            <literal>condor</literal>, <literal>dagman</literal>,
            <literal>env</literal>, <literal>hints</literal>,
            <literal>globus</literal> and <literal>selector</literal> enjoy
            full support.</para>
          </note>

          <para>The second <emphasis>file</emphasis> entry above shows a usage
          example from the black-diamond example workflow that you are more
          likely to encouter or write.</para>

          <para>The presence of an in-file replica catalog lets you declare a
          couple of interesting advanced features. The DAG and DAX file
          declarations are just files for all practical purposes. For deferred
          planning, the location of the site catalog (SC) can be captured in a
          file, too, that is passed to the job dealing with the deferred
          planning as logical filename.</para>

          <programlisting>  &lt;file name="black.dax" &gt;
    &lt;!-- specify the location of the DAX file --&gt;
    &lt;pfn url="file:///Users/vahi/Pegasus/work/dax-3.0/blackdiamond_dax.xml" site="local"/&gt;
  &lt;/file&gt;

  &lt;file name="black.dag" &gt;
    &lt;!-- specify the location of the DAG file --&gt;
    &lt;pfn url="file:///Users/vahi/Pegasus/work/dax-3.0/blackdiamond.dag" site="local"/&gt;
  &lt;/file&gt;
  
  &lt;file name="sites.xml" &gt;
    &lt;!-- specify the location of a site catalog to use for deferred planning --&gt;
    &lt;pfn url="file:///Users/vahi/Pegasus/work/dax-3.0/conf/sites.xml" site="local"/&gt;
  &lt;/file&gt;</programlisting>
        </section>

        <section id="dax_transformation_catalog" label="">
          <title>The Transformation Catalog Section</title>

          <para>The executable section acts as an in-file transformation
          catalog (TC). Any transformations declared in this section take
          precedence over the external transformation catalog during
          planning.</para>

          <programlisting>  &lt;!-- part 1.2: included transformation catalog --&gt;
  &lt;executable namespace="example" name="mDiffFit" version="1.0" 
              arch="x86_64" os="linux" installed="true" &gt;
    &lt;!-- profiles are optional --&gt;
    &lt;!-- The "stat" namespace is ONLY AN EXAMPLE! --&gt;
    &lt;profile namespace="stat" key="size"&gt;5000&lt;/profile&gt;
    &lt;profile namespace="stat" key="md5sum"&gt;AB454DSSDA4646DS&lt;/profile&gt;
    &lt;profile namespace="stat" key="mtime"&gt;2010-11-22T10:05:55.470606000-0800&lt;/profile&gt;

    &lt;!-- metadata is currently NOT SUPPORTED! --&gt;
    &lt;metadata key="timestamp" type="int"&gt;/* see above */&lt;/metadata&gt;
    &lt;metadata key="origin" type="string"&gt;ocean&lt;/metadata&gt;
 
    &lt;!-- PFN to by-pass transformation catalog --&gt;
    &lt;!-- The "site" attribute is optional --&gt;
    &lt;pfn url="file:///tmp/mDiffFit"          site="local"/&gt;     
    &lt;pfn url="file:///tmp/storage/mDiffFit"  site="local"/&gt;     
  &lt;/executable&gt;

  &lt;!-- to be used in compound transformation later --&gt;
  &lt;executable namespace="example" name="mDiff" version="1.0" 
              arch="x86_64" os="linux" installed="true" &gt;
    &lt;pfn url="file:///tmp/mDiff" site="local"/&gt;        
  &lt;/executable&gt;

  &lt;!-- to be used in compound transformation later --&gt;
  &lt;executable namespace="example" name="mFitplane" version="1.0"
              arch="x86_64" os="linux" installed="true" &gt;
    &lt;pfn url="file:///tmp/mDiffFitplane"  site="local"&gt;
      &lt;profile namespace="stat" key="md5sum"&gt;0a9c38b919c7809cb645fc09011588a6&lt;/profile&gt;
    &lt;/pfn&gt;
  &lt;/executable&gt;

  &lt;!-- a more likely example from the black diamond --&gt;
  &lt;executable namespace="diamond" name="preprocess" version="2.0" 
              arch="x86_64"
              os="linux" 
              osversion="2.6.18"&gt;
    &lt;pfn url="file:///opt/pegasus/default/bin/keg" site="local" /&gt;
  &lt;/executable&gt;</programlisting>

          <para>Logical filenames pertaining to a single executables in the
          transformation catalog use the <emphasis>executable</emphasis>
          element. Any <emphasis>executable</emphasis> element features the
          optional <emphasis>namespace</emphasis> attribute, a mandatory
          <emphasis>name</emphasis> attribute, and an optional
          <emphasis>version</emphasis> attribute. The
          <emphasis>version</emphasis> attribute defaults to "1.0" when
          absent. An executable typically needs additional attributes to
          describe it properly, like the architecture, OS release and other
          flags typically seen with transformations, or found in the
          transformation catalog.</para>

          <table>
            <tgroup cols="4">
              <thead>
                <row>
                  <entry>attribute</entry>

                  <entry>optional?</entry>

                  <entry>type</entry>

                  <entry>meaning</entry>
                </row>
              </thead>

              <tbody>
                <row>
                  <entry>name</entry>

                  <entry>required</entry>

                  <entry>string</entry>

                  <entry>logical transformation name</entry>
                </row>

                <row>
                  <entry>namespace</entry>

                  <entry>optional</entry>

                  <entry>string</entry>

                  <entry>namespace of logical transformation, default to
                  <emphasis>null</emphasis> value.</entry>
                </row>

                <row>
                  <entry>version</entry>

                  <entry>optional</entry>

                  <entry>VersionPattern</entry>

                  <entry>version of logical transformation, defaults to
                  "1.0".</entry>
                </row>

                <row>
                  <entry>installed</entry>

                  <entry>optional</entry>

                  <entry>boolean</entry>

                  <entry>whether to stage the file (false), or not (true,
                  default).</entry>
                </row>

                <row>
                  <entry>arch</entry>

                  <entry>optional</entry>

                  <entry>Architecture</entry>

                  <entry>restricted set of tokens, see schema definition
                  file.</entry>
                </row>

                <row>
                  <entry>os</entry>

                  <entry>optional</entry>

                  <entry>OSType</entry>

                  <entry>restricted set of tokens, see schema definition
                  file.</entry>
                </row>

                <row>
                  <entry>osversion</entry>

                  <entry>optional</entry>

                  <entry>VersionPattern</entry>

                  <entry>kernel version as beginning of `uname -r`.</entry>
                </row>

                <row>
                  <entry>glibc</entry>

                  <entry>optional</entry>

                  <entry>VersionPattern</entry>

                  <entry>version of libc.</entry>
                </row>
              </tbody>
            </tgroup>
          </table>

          <para>The rationale for giving these flags in the
          <emphasis>executable</emphasis> element header is that PFNs are just
          identical replicas or instances of a given LFN. If you need a
          different 32/64 bit-ed-ness or OS release, the underlying PFN would
          be different, and thus the LFN for it should be different,
          too.</para>

          <note>
            <para>We are still discussing some details and implications of
            this decision.</para>
          </note>

          <para>The initial examples come with the same caveats as for the
          included replica catalog.</para>

          <warning>
            <para>The <emphasis>metadata</emphasis> element is not support as
            of this writing! Details may change in the future.</para>
          </warning>

          <para>Similar to the replica catalog, each
          <emphasis>executable</emphasis> element may have 0 or more
          <emphasis>profile</emphasis> elements abstracting away site-specific
          details, zero or more <emphasis>metadata</emphasis> elements, and
          zero or more <emphasis>pfn</emphasis> elements. If there are no
          <emphasis>pfn</emphasis> elements, the transformation must still be
          searched for in the external transformation catalog. As before, the
          <emphasis>pfn</emphasis> element may have
          <emphasis>profile</emphasis> children-elements, referring to
          attributes of the physical filename itself.</para>

          <para>The last example above comes from the black diamond example
          workflow, and presents the kind and extend of attributes you are
          most likely to see and use in your own workflows.</para>
        </section>

        <section>
          <title>The Compound Transformation Section</title>

          <para>The compound transformation section declares a transformation
          that comprises multiple plain transformation. You can think of a
          compound transformation like a script interpreter and the script
          itself. In order to properly run the application, you must start
          both, the script interpreter and the script passed to it. The
          compound transformation helps Pegasus to properly deal with this
          case, especially when it needs to stage executables.</para>

          <programlisting>  &lt;transformation namespace="example" version="1.0" name="mDiffFit" &gt;
    &lt;uses name="mDiffFit" /&gt;
    &lt;uses name="mDiff" namespace="example" version="2.0" /&gt;
    &lt;uses name="mFitPlane" /&gt;
    &lt;uses name="mDiffFit.config" executable="false" /&gt;
  &lt;/transformation&gt;</programlisting>

          <para>A <emphasis>transformation</emphasis> element declares a set
          of purely logical entities, executables and config (data) files,
          that are all required together for the same job. Being purely
          logical entities, the lookup happens only when the transformation
          element is referenced (or instantiated) by a job element later
          on.</para>

          <para>The <emphasis>namespace</emphasis> and
          <emphasis>version</emphasis> attributes of the transformation
          element are optional, and provide the defaults for the inner uses
          elements. They are also essential for matching the transformation
          with a job.</para>

          <para>The <emphasis>transformation</emphasis> is made up of 1 or
          more <emphasis>uses</emphasis> element. Each
          <emphasis>uses</emphasis> has a boolean attribute
          <emphasis>executable</emphasis>, <literal>true</literal> by default,
          or <literal>false</literal> to indicate a data file. The
          <emphasis>name</emphasis> is a mandatory attribute, refering to an
          LFN declared previously in the File Catalog
          (<emphasis>executable</emphasis> is <literal>false</literal>),
          Executable Catalog (<emphasis>executable</emphasis> is
          <literal>true</literal>), or to be looked up as necessary at
          instantiation time. The lookup catalog is determined by the
          <emphasis>executable</emphasis> attribute.</para>

          <para>The <emphasis>namespace</emphasis> and
          <emphasis>version</emphasis> attributes' default values inside
          <emphasis>uses</emphasis> elements are inherited from the
          <emphasis>transformation</emphasis> attributes of the same name.
          There is no such inheritance for <emphasis>uses</emphasis> elements
          with <emphasis>executable</emphasis> attribute of
          <literal>false</literal>.</para>
        </section>
      </section>

      <section id="api-graph-nodes">
        <title>Graph Nodes</title>

        <para>The nodes in the DAX comprise regular job nodes, already
        instantiated sub-workflows as dag nodes, and still to be instantiated
        dax nodes. Each of the graph nodes can has a mandatory
        <emphasis>id</emphasis> attribute. The <emphasis>id</emphasis>
        attribute is currently a restriction of type <code>xsd:NMTOKEN</code>
        type.The <emphasis>level</emphasis> attribute is being phased out, as
        the planner will trust its own re-computation more than user input.
        The <emphasis>node-label</emphasis> attribute is optional. It applies
        to the use-case when every transformation has the same name, but its
        arguments determine what it really does. In the presence of a
        <emphasis>node-label</emphasis> value, a workflow grapher could use
        the label value to show graph nodes to the user. It may also come in
        handy while debugging.</para>

        <para>Any job-like graph node has the following set of children
        elements, as defined in the <emphasis>AbstractJobType</emphasis>
        declaration in the schema definition:</para>

        <itemizedlist>
          <listitem>
            <para>0 or 1 <emphasis>argument</emphasis> element to declare the
            command-line of the job's invocation.</para>
          </listitem>

          <listitem>
            <para>0 or more <emphasis>profile</emphasis> elements to abstract
            away site-specific or job-specific details.</para>
          </listitem>

          <listitem>
            <para>0 or 1 <emphasis>stdin</emphasis> element to link a logical
            file the the job's standard input.</para>
          </listitem>

          <listitem>
            <para>0 or 1 <emphasis>stdout</emphasis> element to link a logical
            file to the job's standard output.</para>
          </listitem>

          <listitem>
            <para>0 or 1 <emphasis>stderr</emphasis> element to link a logical
            file to the job's standard error.</para>
          </listitem>

          <listitem>
            <para>0 or more <emphasis>uses</emphasis> elements to declare
            consumed data files and produced data files.</para>
          </listitem>

          <listitem>
            <para>0 or more <emphasis>invoke</emphasis> elements</para>
          </listitem>
        </itemizedlist>

        <warning>
          <para>The invoke element is not supported as of this writing.</para>
        </warning>

        <section id="api-job-nodes">
          <title>Job Nodes</title>

          <para>A job element has a number of attributes. In addition to the
          <emphasis>id</emphasis> and <emphasis>node-label</emphasis>
          described in (Graph Nodes)above, the optional
          <emphasis>namespace</emphasis>, mandatory <emphasis>name</emphasis>
          and optional <emphasis>version</emphasis> identify the
          transformation, and provide the look-up handle: first in the DAX's
          <emphasis>transformation</emphasis> elements, then in the
          <emphasis>executable</emphasis> elements, and finally in an external
          transformation catalog.</para>

          <programlisting>  &lt;!-- part 2: definition of all jobs (at least one) --&gt;
  &lt;job id="ID000001" namespace="example" name="mDiffFit" version="1.0" 
       node-label="preprocess" &gt;
    &lt;argument&gt;-a top -T 6  -i &lt;file name="f.a"/&gt;  -o &lt;file name="f.b1"/&gt;&lt;/argument&gt;

    &lt;!-- profiles are optional --&gt;
    &lt;profile namespace="execution" key="site"&gt;isi_viz&lt;/profile&gt;
    &lt;profile namespace="condor" key="getenv"&gt;true&lt;/profile&gt;

    &lt;uses name="f.a" link="input"  register="false" transfer="true" type="data" /&gt;
    &lt;uses name="f.b" link="output" register="false" transfer="true" type="data" /&gt;
    
    &lt;!-- 'WHEN' enumeration: never, start, on_error, on_success, on_end, all --&gt;
    &lt;!-- PEGASUS_* env-vars: event, status, submit dir, wf/job id, stdout, stderr --&gt;
    &lt;invoke when="start"&gt;/path/to arg arg&lt;/invoke&gt;
    &lt;invoke when="on_success"&gt;&lt;![CDATA[/path/to arg arg]]&gt;&lt;/invoke&gt;
    &lt;invoke when="on_end"&gt;&lt;![CDATA[/path/to arg arg]]&gt;&lt;/invoke&gt;
  &lt;/job&gt;</programlisting>

          <para>The <emphasis>argument</emphasis> element contains the
          complete command-line that is needed to invoke the executable. The
          only variable components are logical filenames, as included
          <emphasis>file</emphasis> elements.</para>

          <para>The <emphasis>profile</emphasis> argument lets you encapsulate
          site-specific knowledge .</para>

          <para>The <emphasis>stdin</emphasis>, <emphasis>stdout</emphasis>
          and <emphasis>stderr</emphasis> element permits you to connect a
          stdio file descriptor to a logical filename. Note that you will
          still have to declare these files in the <emphasis>uses</emphasis>
          section below.</para>

          <para>The <emphasis>uses</emphasis> element enumerates all the files
          that the task consumes or produces. While it is not necessary nor
          required to have all files appear on the command-line, it is
          imperative that you declare even hidden files that your task
          requires in this section, so that the proper ancilliary staging- and
          clean-up tasks can be generated during planning.</para>

          <warning>
            <para>The <emphasis>invoke</emphasis> element is not supported as
            of this writing!</para>
          </warning>

          <para>The invoke element may be specified multiple times, as needed.
          It has a mandatory when attribute with the following value
          set:</para>

          <table>
            <tgroup cols="3">
              <thead>
                <row>
                  <entry align="center">keyword</entry>

                  <entry align="center">when</entry>

                  <entry align="center">meaning</entry>
                </row>
              </thead>

              <tbody>
                <row>
                  <entry>never</entry>

                  <entry>never</entry>

                  <entry>(default). Never notify of anything. This is useful
                  to temporarily disable an existing notifications.</entry>
                </row>

                <row>
                  <entry>start</entry>

                  <entry>submit</entry>

                  <entry>create a notification when the job is
                  submitted.</entry>
                </row>

                <row>
                  <entry>start</entry>

                  <entry>submit</entry>

                  <entry>create a notification when the job is
                  submitted.</entry>
                </row>

                <row>
                  <entry>on_error</entry>

                  <entry>end</entry>

                  <entry>after a job finishes with failure (exitcode !=
                  0).</entry>
                </row>

                <row>
                  <entry>on_success</entry>

                  <entry>end</entry>

                  <entry>after a job finishes with success (exitcode ==
                  0).</entry>
                </row>

                <row>
                  <entry>at_end</entry>

                  <entry>end</entry>

                  <entry>after a job finishes, regardless of exitcode.</entry>
                </row>

                <row>
                  <entry>all</entry>

                  <entry>always</entry>

                  <entry>like start and at_end combined.</entry>
                </row>
              </tbody>
            </tgroup>
          </table>

          <para>Again, even though we define the <emphasis>invoke</emphasis>
          element, at this point, it is not supported. Each
          <emphasis>invoke</emphasis> is a simple local invocation of an
          executable or script with the specified arguments. Pegasus will plan
          any notification into the pre- and post-script of DAGMan. The
          executable inside the invoke body will see the following environment
          variables:</para>

          <table>
            <tgroup cols="3">
              <thead>
                <row>
                  <entry align="center">variable</entry>

                  <entry align="center">when</entry>

                  <entry align="center">meaning</entry>
                </row>
              </thead>

              <tbody>
                <row>
                  <entry>PEGASUS_EVENT</entry>

                  <entry>always</entry>

                  <entry>The value of the when attribute</entry>
                </row>

                <row>
                  <entry>PEGASUS_STATUS</entry>

                  <entry>end</entry>

                  <entry>The exit status of the graph node. Only available for
                  end notifications.</entry>
                </row>

                <row>
                  <entry>PEGASUS_SUBMIT_DIR</entry>

                  <entry>always</entry>

                  <entry>In which directory to find the job (or
                  workflow).</entry>
                </row>

                <row>
                  <entry>PEGASUS_JOBID</entry>

                  <entry>always</entry>

                  <entry>The job (or workflow) identifier. This is potentially
                  more than merely the value of the <emphasis>id</emphasis>
                  attribute.</entry>
                </row>

                <row>
                  <entry>PEGASUS_STDOUT</entry>

                  <entry>always</entry>

                  <entry>The filename where <emphasis>stdout</emphasis> goes.
                  Empty and possibly non-existent at submit time (though we
                  still have the filename). The kickstart record for job
                  nodes.</entry>
                </row>

                <row>
                  <entry>PEGASUS_STDERR</entry>

                  <entry>always</entry>

                  <entry>The filename where <emphasis>stderr</emphasis> goes.
                  Empty and possibly non-existent at submit time (though we
                  still have the filename).</entry>
                </row>
              </tbody>
            </tgroup>
          </table>

          <para>Generators should use CDATA encapsulated values to the invoke
          element to minimize interference. Unfortunately, CDATA cannot be
          nested, so if the user invocation contains a CDATA section, we
          suggest that they use careful XML-entity escaped strings.</para>
        </section>

        <section>
          <title>DAG Nodes</title>

          <para>A workflow that has already been concretized, either by an
          earlier run of Pegasus, or otherwise constructed for DAGMan
          execution, can be included into the current workflow using the
          <emphasis>dag</emphasis> element.</para>

          <programlisting>  &lt;dag id="ID000003" name="black.dag" node-label="foo" &gt;
    &lt;profile namespace="dagman" key="DIR"&gt;/dag-dir/test&lt;/profile&gt;
    &lt;invoke&gt; &lt;!-- optional, should be possible --&gt; &lt;/invoke&gt;
    &lt;uses file="sites.xml" link="input" register="false" transfer="true" type="data"/&gt;     
  &lt;/dag&gt;</programlisting>

          <para>The <emphasis>id</emphasis> and
          <emphasis>node-label</emphasis> attributes were described <link
          linkend="api-graph-nodes">previously</link>. The
          <emphasis>name</emphasis> attribute refers to a file from the File
          Catalog that provides the actual DAGMan DAG as data content. The
          <emphasis>dag</emphasis> element features optional
          <emphasis>profile</emphasis> elements. These would most likely
          pertain to the <literal>dagman</literal> and <literal>env</literal>
          profile namespaces. It should be possible to have the optional
          <emphasis>notify</emphasis> element in the same manner as for
          jobs.</para>

          <warning>
            <para>The <emphasis>invoke</emphasis> element is not supported as
            of this writing!</para>
          </warning>

          <para>A graph node that is a dag instead of a job would just use a
          different submit file generator to create a DAGMan invocation. There
          can be an <emphasis>argument</emphasis> element to modify the
          command-line passed to DAGMan.</para>
        </section>

        <section>
          <title>DAX Nodes</title>

          <para>A still to be planned workflow incurs an invocation of the
          Pegasus planner as part of the workflow. This still abstract
          sub-workflow uses the <emphasis>dax</emphasis> element.</para>

          <programlisting>  &lt;dax id="ID000002" name="black.dax" node-label="bar" &gt;
    &lt;profile namespace="env" key="foo"&gt;bar&lt;/profile&gt;
    &lt;argument&gt;-Xmx1024 -Xms512 -Dpegasus.dir.storage=storagedir  -Dpegasus.dir.exec=execdir -o local --dir ./datafind -vvvvv --force -s dax_site &lt;/argument&gt;
    &lt;invoke&gt; &lt;!-- optional, may not be possible here --&gt; &lt;/invoke&gt;
    &lt;uses file="sites.xml" link="input" register="false" transfer="true" type="data" /&gt;
  &lt;/dax&gt;</programlisting>

          <para>In addition to the <emphasis>id</emphasis> and
          <emphasis>node-label</emphasis> attributes, See <link
          linkend="Grapgh Nodes">Graph Nodes</link>. The
          <emphasis>name</emphasis> attribute refers to a file from the File
          Catalog that provides the to be planned DAX as external file data
          content. The <emphasis>dax</emphasis> element features optional
          <emphasis>profile</emphasis> elements. These would most likely
          pertain to the <literal>pegasus</literal>, <literal>dagman</literal>
          and <literal>env</literal> profile namespaces. It may be possible to
          have the optional <emphasis>notify</emphasis> element in the same
          manner as for jobs.</para>

          <warning>
            <para>The <emphasis>invoke</emphasis> element is not supported as
            of this writing!</para>
          </warning>

          <para>A graph node that is a <emphasis>dax</emphasis> instead of a
          job would just use yet another submit file and pre-script generator
          to create a DAGMan invocation. The <emphasis>argument</emphasis>
          string pertains to the command line of the to-be-generated DAGMan
          invocation.</para>
        </section>

        <section>
          <title>Inner ADAG Nodes</title>

          <para>While completeness would argue to have a recursive nesting of
          <emphasis>adag</emphasis> elements, such recursive nestings are
          currently not supported, not even in the schema. If you need to nest
          workflows, please use the <emphasis>dax</emphasis> or
          <emphasis>dag</emphasis> element to achieve the same goal.</para>
        </section>
      </section>

      <section>
        <title>The Dependency Section</title>

        <para>This section describes the dependencies between the jobs.</para>

        <programlisting>  &lt;!-- part 3: list of control-flow dependencies --&gt;
  &lt;child ref="ID000002"&gt;
    &lt;parent ref="ID000001" edge-label="edge1" /&gt;
  &lt;/child&gt;
  &lt;child ref="ID000003"&gt;
    &lt;parent ref="ID000001" edge-label="edge2" /&gt;
  &lt;/child&gt;
  &lt;child ref="ID000004"&gt;
    &lt;parent ref="ID000002" edge-label="edge3" /&gt;
    &lt;parent ref="ID000003" edge-label="edge4" /&gt;
  &lt;/child&gt;</programlisting>

        <para>Each <emphasis>child</emphasis> element contains one or more
        <emphasis>parent</emphasis> element. Either element refers to a
        <emphasis>job</emphasis>, <emphasis>dag</emphasis> or
        <emphasis>dax</emphasis> element id attribute using the
        <emphasis>ref</emphasis> attribute. In this version, we relaxed the
        <code>xs:IDREF</code> constraint in favor of a restriction on the
        <code>xs:NMTOKEN</code> type to permit a larger set of
        identifiers.</para>

        <para>The <emphasis>parent</emphasis> element has an optional
        <emphasis>edge-label</emphasis> attribute.</para>

        <warning>
          <para>The <emphasis>edge-label</emphasis> attribute is currently
          unused.</para>
        </warning>

        <para>Its goal is to annotate edges when drawing workflow
        graphs.</para>
      </section>

      <section>
        <title>Closing</title>

        <para>As any XML element, the root element needs to be closed.</para>

        <programlisting>&lt;/adag&gt;</programlisting>
      </section>
    </section>

    <section>
      <title>DAX XML Schema Example</title>

      <para>The following code example shows the XML instance document
      representing the diamond workflow from <link
      linkend="images/DiamondWorkflow.png">the concepts figure</link>.</para>

      <programlisting>&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;!-- generated: 2010-11-19T20:00:29Z --&gt;
&lt;adag xmlns="http://pegasus.isi.edu/schema/DAX" 
      xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
      xsi:schemaLocation="http://pegasus.isi.edu/schema/DAX http://pegasus.isi.edu/schema/dax-3.2.xsd"
      version="3.2" 
      name="diamond" 
      index="0" 
      count="1"&gt;
  &lt;!-- part 1.1: included replica catalog --&gt;
  &lt;file name="f.a"&gt;
    &lt;pfn url="file:///home/voeckler/f.a" site="local" /&gt;
  &lt;/file&gt;
  &lt;!-- part 1.2: included transformation catalog --&gt;
  &lt;executable namespace="diamond" name="preprocess" version="2.0" 
              arch="x86_64" os="linux" osversion="2.6.18"&gt;
    &lt;pfn url="file:///opt/pegasus/default/bin/keg" site="local" /&gt;
  &lt;/executable&gt;
  &lt;executable namespace="diamond" name="findrange" version="2.0" 
              arch="x86_64" os="linux" osversion="2.6.18"&gt;
    &lt;pfn url="file:///opt/pegasus/default/bin/keg" site="local" /&gt;
  &lt;/executable&gt;
  &lt;executable namespace="diamond" name="analyze" version="2.0" 
              arch="x86_64" os="linux" osversion="2.6.18"&gt;
    &lt;pfn url="file:///opt/pegasus/default/bin/keg" site="local" /&gt;
  &lt;/executable&gt;
  &lt;!-- part 2: definition of all jobs (at least one) --&gt;
  &lt;job namespace="diamond" name="preprocess" version="2.0" id="ID000001"&gt;
    &lt;argument&gt;-a preprocess -T60 -i &lt;file name="f.a" /&gt; -o &lt;file name="f.b1" /&gt; &lt;file name="f.b2" /&gt;&lt;/argument&gt;
    &lt;uses name="f.b2" link="output" register="false" transfer="false" /&gt;
    &lt;uses name="f.b1" link="output" register="false" transfer="false" /&gt;
    &lt;uses name="f.a" link="input" /&gt;
  &lt;/job&gt;
  &lt;job namespace="diamond" name="findrange" version="2.0" id="ID000002"&gt;
    &lt;argument&gt;-a findrange -T60 -i &lt;file name="f.b1" /&gt; -o &lt;file name="f.c1" /&gt;&lt;/argument&gt;
    &lt;uses name="f.b1" link="input" register="false" transfer="false" /&gt;
    &lt;uses name="f.c1" link="output" register="false" transfer="false" /&gt;
  &lt;/job&gt;
  &lt;job namespace="diamond" name="findrange" version="2.0" id="ID000003"&gt;
    &lt;argument&gt;-a findrange -T60 -i &lt;file name="f.b2" /&gt; -o &lt;file name="f.c2" /&gt;&lt;/argument&gt;
    &lt;uses name="f.c2" link="output" register="false" transfer="false" /&gt;
    &lt;uses name="f.b2" link="input" register="false" transfer="false" /&gt;
  &lt;/job&gt;
  &lt;job namespace="diamond" name="analyze" version="2.0" id="ID000004"&gt;
    &lt;argument&gt;-a analyze -T60 -i &lt;file name="f.c1" /&gt; &lt;file name="f.c2" /&gt; -o &lt;file name="f.d" /&gt;&lt;/argument&gt;
    &lt;uses name="f.c2" link="input" register="false" transfer="false" /&gt;
    &lt;uses name="f.d" link="output" register="false" transfer="true" /&gt;
    &lt;uses name="f.c1" link="input" register="false" transfer="false" /&gt;
  &lt;/job&gt;
  &lt;!-- part 3: list of control-flow dependencies --&gt;
  &lt;child ref="ID000002"&gt;
    &lt;parent ref="ID000001" /&gt;
  &lt;/child&gt;
  &lt;child ref="ID000003"&gt;
    &lt;parent ref="ID000001" /&gt;
  &lt;/child&gt;
  &lt;child ref="ID000004"&gt;
    &lt;parent ref="ID000002" /&gt;
    &lt;parent ref="ID000003" /&gt;
  &lt;/child&gt;
&lt;/adag&gt;</programlisting>

      <para>The above workflow defines the black diamond from the abstract
      workflow section of the <link linkend="about">Introduction</link>
      chapter. It will require minimal configuration, because the catalog
      sections include all necessary declarations.</para>

      <para>The file element defines the location of the required input file
      in terms of the local machine. Please note that</para>

      <itemizedlist>
        <listitem>
          <para>The <emphasis>file</emphasis> element declares the required
          input file "f.a" in terms of the local machine. Please note that if
          you plan the workflow for a remote site, the has to be some way for
          the file to be staged from the local site to the remote site. While
          Pegasus will augment the workflow with such ancillary jobs, the site
          catalog as well as local and remote site have to be set up
          properlyl. For a locally run workflow you don't need to do
          anything.</para>
        </listitem>

        <listitem>
          <para>The <emphasis>executable</emphasis> elements declare the same
          executable keg that is to be run for each the logical transformation
          in terms of the local site. To declare it for a remote site, you
          would have to adjust the <emphasis>site</emphasis> attribute's value
          to the proper remote site handle. This section also shows that the
          same executable may come in different guises as
          transformation.</para>
        </listitem>

        <listitem>
          <para>The <emphasis>job</emphasis> elements define the workflow's
          logical constituents, the way to invoke the <literal>keg</literal>
          command, where to put filenames on the commandline, and what files
          are consumed or produced. In addition to the direction of files,
          further attributes determine whether to register the file with a
          replica catalog and whether to transfer it to the output site in
          case of a product. We are only interested in the final data product
          "f.d" in this workflow, and not any intermediary files. Typically,
          you would also want to register the data products in the replica
          catalog, especially in larger scenarios.</para>
        </listitem>

        <listitem>
          <para>The <emphasis>child</emphasis> elements define the control
          flow between the jobs.</para>
        </listitem>
      </itemizedlist>
    </section>
  </section>

  <section>
    <title>DAX generator API</title>

    <para>The DAX generating APIs support Java, Perl and Python. This section
    will show in each language the necessary code, using Pegasus-provided
    libraries, to generate the diamond DAX example above. There may be minor
    differences in details, e.g. to show-case certain features, but
    effectively all generate the same basic diamond.</para>

    <section id="api-java">
      <title>The Java DAX generator API</title>

      <para>The Java DAX API provided with the Pegasus distribution allows
      easy creation of complex and huge workflows. This API is used by several
      applications to generate their abstract DAX. SCEC, which is Southern
      California Earthquake Center, uses this API in their CyberShake workflow
      generator to generate huge DAX containing 10&rsquor;s of thousands of
      tasks with 100&rsquor;s of thousands of input and output files. The
      <ulink url="http://pegasus.isi.edu/wms/docs/3.0/javadoc/index.html">Java
      API</ulink> is well documented using <ulink
      url="http://pegasus.isi.edu/wms/docs/3.0/javadoc/edu/isi/pegasus/planner/dax/ADAG.html">Javadoc
      for ADAGs</ulink> .</para>

      <para>The steps involved in creating a DAX using the API are</para>

      <orderedlist>
        <listitem>
          <para>Create a new <emphasis>ADAG</emphasis> object</para>
        </listitem>

        <listitem>
          <para>Create <emphasis>File</emphasis> objects as necessary. You can
          augment the files with physical information, if you want to include
          them into your DAX. Otherwise, the physical information is
          determined from the replica catalog.</para>
        </listitem>

        <listitem>
          <para>(Optional) Create <emphasis>Executable</emphasis> objects, if
          you want to include your transformation catalog into your DAX.
          Otherwise, the translation of a job/task into executable location
          happens with the transformation catalog.</para>
        </listitem>

        <listitem>
          <para>Create a new <emphasis>Job</emphasis> object.</para>
        </listitem>

        <listitem>
          <para>Add arguments, files, profiles and other information to the
          <emphasis>Job</emphasis> object</para>
        </listitem>

        <listitem>
          <para>Add the job object to the <emphasis>ADAG</emphasis>
          object</para>
        </listitem>

        <listitem>
          <para>Repeat step 4-6 as necessary.</para>
        </listitem>

        <listitem>
          <para>Add all dependencies to the <emphasis>ADAG</emphasis>
          object.</para>
        </listitem>

        <listitem>
          <para>Call the <emphasis>writeToFile()</emphasis> method on the
          <emphasis>ADAG</emphasis> object to render the XML DAX file.</para>
        </listitem>
      </orderedlist>

      <para>An example Java code that generates the diamond dax show above is
      listed below. This same code can be found in the Pegasus distribution in
      the <filename
      class="directory">examples/grid-blackdiamond-java</filename> directory
      as <filename>BlackDiamonDAX.java</filename>:</para>

      <programlisting>/**
 *  Copyright 2007-2008 University Of Southern California
 *
 *  Licensed under the Apache License, Version 2.0 (the "License");
 *  you may not use this file except in compliance with the License.
 *  You may obtain a copy of the License at
 *
 *  http://www.apache.org/licenses/LICENSE-2.0
 *
 *  Unless required by applicable law or agreed to in writing,
 *  software distributed under the License is distributed on an "AS IS" BASIS,
 *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 *  See the License for the specific language governing permissions and
 *  limitations under the License.
 */

import edu.isi.pegasus.planner.dax.*;

public class BlackDiamondDAX {

    /**
     * Create an example DIAMOND DAX
     * @param args
     */
    public static void main(String[] args) {
        if (args.length != 3) {
            System.out.println("Usage: java ADAG &lt;site_handle&gt; &lt;pegasus_location&gt; &lt;filename.dax&gt;");
            System.exit(1);
        }

        try {
            Diamond(args[0], args[1]).writeToFile(args[2]);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

    }

    private static ADAG Diamond(String site_handle, String pegasus_location) throws Exception {

        java.io.File cwdFile = new java.io.File (".");
        String cwd = cwdFile.getCanonicalPath(); 

        ADAG dax = new ADAG("blackdiamond");

        File fa = new File("f.a");
        fa.addPhysicalFile("file://" + cwd + "/f.a", "local");
        dax.addFile(fa);

        File fb1 = new File("f.b1");
        File fb2 = new File("f.b2");
        File fc1 = new File("f.c1");
        File fc2 = new File("f.c2");
        File fd = new File("f.d");
        fd.setRegister(true);

        Executable preprocess = new Executable("pegasus", "preprocess", "4.0");
        preprocess.setArchitecture(Executable.ARCH.X86).setOS(Executable.OS.LINUX);
        preprocess.setInstalled(true);
        preprocess.addPhysicalFile("file://" + pegasus_location + "/bin/keg", site_handle);

        Executable findrange = new Executable("pegasus", "findrange", "4.0");
        findrange.setArchitecture(Executable.ARCH.X86).setOS(Executable.OS.LINUX);
        findrange.setInstalled(true);
        findrange.addPhysicalFile("file://" + pegasus_location + "/bin/keg", site_handle);

        Executable analyze = new Executable("pegasus", "analyze", "4.0");
        analyze.setArchitecture(Executable.ARCH.X86).setOS(Executable.OS.LINUX);
        analyze.setInstalled(true);
        analyze.addPhysicalFile("file://" + pegasus_location + "/bin/keg", site_handle);

        dax.addExecutable(preprocess).addExecutable(findrange).addExecutable(analyze);

        // Add a preprocess job
        Job j1 = new Job("j1", "pegasus", "preprocess", "4.0");
        j1.addArgument("-a preprocess -T 60 -i ").addArgument(fa);
        j1.addArgument("-o ").addArgument(fb1);
        j1.addArgument(" ").addArgument(fb2);
        j1.uses(fa, File.LINK.INPUT);
        j1.uses(fb1, File.LINK.OUTPUT);
        j1.uses(fb2, File.LINK.OUTPUT);
        dax.addJob(j1);

        // Add left Findrange job
        Job j2 = new Job("j2", "pegasus", "findrange", "4.0");
        j2.addArgument("-a findrange -T 60 -i ").addArgument(fb1);
        j2.addArgument("-o ").addArgument(fc1);
        j2.uses(fb1, File.LINK.INPUT);
        j2.uses(fc1, File.LINK.OUTPUT);
        dax.addJob(j2);

        // Add right Findrange job
        Job j3 = new Job("j3", "pegasus", "findrange", "4.0");
        j3.addArgument("-a findrange -T 60 -i ").addArgument(fb2);
        j3.addArgument("-o ").addArgument(fc2);
        j3.uses(fb2, File.LINK.INPUT);
        j3.uses(fc2, File.LINK.OUTPUT);
        dax.addJob(j3);

        // Add analyze job
        Job j4 = new Job("j4", "pegasus", "analyze", "4.0");
        j4.addArgument("-a analyze -T 60 -i ").addArgument(fc1);
        j4.addArgument(" ").addArgument(fc2);
        j4.addArgument("-o ").addArgument(fd);
        j4.uses(fc1, File.LINK.INPUT);
        j4.uses(fc2, File.LINK.INPUT);
        j4.uses(fd, File.LINK.OUTPUT);
        dax.addJob(j4);

        dax.addDependency("j1", "j2");
        dax.addDependency("j1", "j3");
        dax.addDependency("j2", "j4");
        dax.addDependency("j3", "j4");
        return dax;
    }
}</programlisting>

      <para>Of course, you will have to set up some catalogs and properties to
      run this example. The details are catpured in the examples directory
      <filename
      class="directory">examples/grid-blackdiamond-java</filename>.</para>
    </section>

    <section id="api-python">
      <title>The Python DAX generator API</title>

      <para>I am no python master. There is <ulink
      url="http://pegasus.isi.edu/wms/docs/3.0/python/">auto-generated python
      documentation</ulink> explaining this API.</para>

      <programlisting>#!/usr/bin/env python

from Pegasus.DAX3 import *
import sys
import os

if len(sys.argv) != 2:
        print "Usage: %s PEGASUS_HOME" % (sys.argv[0])
        sys.exit(1)

# Create a abstract dag
diamond = ADAG("diamond")

# Add input file to the DAX-level replica catalog
a = File("f.a")
a.addPFN(PFN("file://" + os.getcwd() + "/f.a", "local"))
diamond.addFile(a)
        
# Add executables to the DAX-level replica catalog
# In this case the binary is keg, which is shipped with Pegasus, so we use
# the remote PEGASUS_HOME to build the path.
e_preprocess = Executable(namespace="diamond", name="preprocess", version="4.0", os="linux", arch="x86_64")
e_preprocess.addPFN(PFN("file://" + sys.argv[1] + "/bin/keg", "TestCluster"))
diamond.addExecutable(e_preprocess)
        
e_findrange = Executable(namespace="diamond", name="findrange", version="4.0", os="linux", arch="x86_64")
e_findrange.addPFN(PFN("file://" + sys.argv[1] + "/bin/keg", "TestCluster"))
diamond.addExecutable(e_findrange)
        
e_analyze = Executable(namespace="diamond", name="analyze", version="4.0", os="linux", arch="x86_64")
e_analyze.addPFN(PFN("file://" + sys.argv[1] + "/bin/keg", "TestCluster"))
diamond.addExecutable(e_analyze)

# Add a preprocess job
preprocess = Job(namespace="diamond", name="preprocess", version="4.0")
b1 = File("f.b1")
b2 = File("f.b2")
preprocess.addArguments("-a preprocess","-T60","-i",a,"-o",b1,b2)
preprocess.uses(a, link=Link.INPUT)
preprocess.uses(b1, link=Link.OUTPUT)
preprocess.uses(b2, link=Link.OUTPUT)
diamond.addJob(preprocess)

# Add left Findrange job
frl = Job(namespace="diamond", name="findrange", version="4.0")
c1 = File("f.c1")
frl.addArguments("-a findrange","-T60","-i",b1,"-o",c1)
frl.uses(b1, link=Link.INPUT)
frl.uses(c1, link=Link.OUTPUT)
diamond.addJob(frl)

# Add right Findrange job
frr = Job(namespace="diamond", name="findrange", version="4.0")
c2 = File("f.c2")
frr.addArguments("-a findrange","-T60","-i",b2,"-o",c2)
frr.uses(b2, link=Link.INPUT)
frr.uses(c2, link=Link.OUTPUT)
diamond.addJob(frr)

# Add Analyze job
analyze = Job(namespace="diamond", name="analyze", version="4.0")
d = File("f.d")
analyze.addArguments("-a analyze","-T60","-i",c1,c2,"-o",d)
analyze.uses(c1, link=Link.INPUT)
analyze.uses(c2, link=Link.INPUT)
analyze.uses(d, link=Link.OUTPUT, register=True)
diamond.addJob(analyze)

# Add control-flow dependencies
diamond.addDependency(parent=preprocess, child=frl)
diamond.addDependency(parent=preprocess, child=frr)
diamond.addDependency(parent=frl, child=analyze)
diamond.addDependency(parent=frr, child=analyze)

# Write the DAX to stdout
diamond.writeXML(sys.stdout)</programlisting>
    </section>

    <section id="api-perl">
      <title>The Perl DAX generator</title>

      <para>The Perl API example below can be found in file
      <filename>blackdiamond.pl</filename> in directory <filename
      class="directory">examples/grid-blackdiamond-perl</filename>. It
      requires that you set the environment variable
      <envar>PEGASUS_HOME</envar> to the installation directory of Pegasus,
      and include into <envar>PERL5LIB</envar> the path to the directory
      <filename class="directory">lib/perl</filename> of the Pegasus
      installation. The actual code is longer, and will not require these
      settings, only the example below does. The Perl API is documented using
      <ulink url="http://pegasus.isi.edu/wms/docs/3.0/perl/">perldoc</ulink>.
      For each of the modules you can invoke
      <application>perldoc</application>, if your <envar>PERL5LIB</envar>
      variable is set.</para>

      <para>The steps to generate a DAX from Perl are similar to the Java
      steps. However, since most methods to the classes are deeply within the
      Perl class modules, the convenience module
      <code>Perl::DAX::Factory</code> makes most constructors accessible
      without you needing to type your fingers raw:</para>

      <orderedlist>
        <listitem>
          <para>Create a new <emphasis>ADAG</emphasis> object.</para>
        </listitem>

        <listitem>
          <para>Create <emphasis>Job</emphasis> objects as necessary.</para>
        </listitem>

        <listitem>
          <para>As example, the required input file "f.a" is declared as
          <emphasis>File</emphasis> object and linked to the
          <emphasis>ADAG</emphasis> object.</para>
        </listitem>

        <listitem>
          <para>The first job arguments and files are filled into the job, and
          the job is added to the <emphasis>ADAG</emphasis> object.</para>
        </listitem>

        <listitem>
          <para>Repeat step 4 for the remaining jobs.</para>
        </listitem>

        <listitem>
          <para>Add dependencies for all jobs. You have the option of
          assigning label text to edges, though these are not used
          (yet).</para>
        </listitem>

        <listitem>
          <para>To generate the DAX file, invoke the
          <emphasis>toXML()</emphasis> method on the <emphasis>ADAG</emphasis>
          object. The first argument is an opened file handle or
          <code>IO::Handle</code> descriptor scalar to write to, the second
          the default indentation for the root element, and the third the XML
          namespace to use for elements and attributes. The latter is
          typically unused unless you want to include your output into another
          XML document.</para>
        </listitem>
      </orderedlist>

      <programlisting>#!/usr/bin/env perl
#
use 5.006;
use strict;
use IO::Handle; 
use Cwd;
##!! PEGASUS_HOME set to install dir of Pegasus
##!! PERL5LIB includes $PEGASUS_HOME/lib/perl
use Pegasus::DAX::Factory qw(:all); 

use constant NS =&gt; 'diamond'; 

my $adag = newADAG( name =&gt; NS ); 
my $job1 = newJob( namespace =&gt; NS, name =&gt; 'preprocess', version =&gt; '2.0' );
my $job2 = newJob( namespace =&gt; NS, name =&gt; 'findrange', version =&gt; '2.0' );
my $job3 = newJob( namespace =&gt; NS, name =&gt; 'findrange', version =&gt; '2.0' );
my $job4 = newJob( namespace =&gt; NS, name =&gt; 'analyze', version =&gt; '2.0' );

my $file = newFile( name =&gt; 'f.a' );
$file-&gt;addPFN( newPFN( url =&gt; 'file://' . Cwd::abs_path(getcwd()) . '/f.a', 
                       site =&gt; 'local' ) ); 
$adag-&gt;addFile($file); 

if ( exists $ENV{'PEGASUS_HOME'} ) {
    use File::Spec;
    use POSIX (); 
    my $keg = File::Spec-&gt;catfile( $ENV{'PEGASUS_HOME'}, 'bin', 'keg' ); 
    my @os = POSIX::uname(); 
    $os[2] =~ s/^(\d+(\.\d+(\.\d+)?)?).*/$1/;
    if ( -x $keg ) { 
        my $app1 = newExecutable( namespace =&gt; NS, name =&gt; 'preprocess', version =&gt; '2.0',
                                  arch =&gt; $os[4], os =&gt; lc($^O), osversion =&gt; $os[2] ); 
        $app1-&gt;addPFN( newPFN( url =&gt; "file://$keg", site =&gt; 'local' ) );
        $adag-&gt;addExecutable($app1); 
    }
}

my %hash = ( link =&gt; LINK_OUT, register =&gt; 'false', transfer =&gt; 'true' ); 
my $fna = newFilename( name =&gt; $file-&gt;name, link =&gt; LINK_IN );
my $fnb1 = newFilename( name =&gt; 'f.b1', %hash );
my $fnb2 = newFilename( name =&gt; 'f.b2', %hash ); 
$job1-&gt;addArgument( '-a', $job1-&gt;name, '-T60', '-i', $fna,
                    '-o', $fnb1, $fnb2 ); 
$adag-&gt;addJob($job1); 

my $fnc1 = newFilename( name =&gt; 'f.c1', %hash );
$fnb1-&gt;link( LINK_IN ); 
$job2-&gt;addArgument( '-a', $job2-&gt;name, '-T60', '-i', $fnb1, 
                    '-o', $fnc1 ); 
$adag-&gt;addJob($job2);

my $fnc2 = newFilename( name =&gt; 'f.c2', %hash );
$fnb2-&gt;link( LINK_IN ); 
$job3-&gt;addArgument( '-a', $job3-&gt;name, '-T60', '-i', $fnb2, 
                    '-o', $fnc2 ); 
$adag-&gt;addJob($job3);

# yeah, you can create multiple children for the same parent
# string labels are distinguished from jobs, no problem
$adag-&gt;addDependency( $job1, $job2, 'edge1', $job3, 'edge2' ); 

my $fnd = newFilename( name =&gt; 'f.d', %hash ); 
$fnc1-&gt;link( LINK_IN );
$fnc2-&gt;link( LINK_IN ); 
$job4-&gt;separator('');                 # just to show the difference wrt default
$job4-&gt;addArgument( '-a ', $job4-&gt;name, ' -T60 -i ', $fnc1, ' ', $fnc2, 
                    ' -o ', $fnd );
$adag-&gt;addJob($job4);
# this is a convenience function -- easier than overloading addDependency?
$adag-&gt;addInverse( $job4, $job2, 'edge3', $job3, 'edge4' );

my $xmlns = shift; 
$adag-&gt;toXML( \*STDOUT, '', $xmlns );</programlisting>
    </section>
  </section>

  <section>
    <title>DAX generator without using a Pegasus DAX API</title>

    <para>If you are using some other scripting or programming environment,
    you can directly write out the DAX format using the provided schema using
    any language. For instance, LIGO, the Laser Interferometer Gravitational
    Wave Observatory, generate their DAX files as XML using their own Python
    code, not using our provided API.</para>

    <para>If you write your own XML, you <emphasis>must</emphasis> ensure that
    the generated XML is well formed and valid with respect to the DAX schema.
    You can use the <command>pegasus-dax-validator</command> to verify the
    validity of your generated file. Typically, you generate a smallish test
    file to, validate that your generator creates valid XML using the
    validator, and then ramp it up to produce the full workflow(s) you want to
    run. At this point the <command>pegasus-dax-validator</command> is a very
    simple program that will only take exactly one argument, the name of the
    file to check.The following snippet checks a black-diamond file that uses
    an improper <emphasis>osversion</emphasis> attribute in its
    <emphasis>executable</emphasis> element:</para>

    <screen><prompt>$</prompt> <command>pegasus-dax-validator <replaceable>blackdiamond.dax</replaceable></command>
ERROR: cvc-pattern-valid: Value '2.6.18-194.26.1.el5' is not facet-valid
 with respect to pattern '[0-9]+(\.[0-9]+(\.[0-9]+)?)?' for type 'VersionPattern'.
ERROR: cvc-attribute.3: The value '2.6.18-194.26.1.el5' of attribute 'osversion'
 on element 'executable' is not valid with respect to its type, 'VersionPattern'.

0 warnings, 2 errors, and 0 fatal errors detected.</screen>

    <para>We are working on improving this program, e.g. provide output with
    regards to the line number where the issue occurred. However, it will
    return with a non-zero exit code whenever errors were detected.</para>
  </section>
  </section>
  <section>
  <title>Command Reference</title>

  <para>This chapter contains reference material for all the command-line
  tools distributed with Pegasus.</para>

  <section>
    <title>pegasus-version</title>

    <para>pegasus-version is a simple command-line tool that reports the
    version number of the Pegasus distribution being used.</para>
  </section>

  <section>
    <title>pegasus-plan</title>

    <para>pegasus-plan generates a concrete, executable workflow from an
    abstract workflow description (DAX).</para>
  </section>

  <section>
    <title>pegasus-run</title>

    <para>pegasus-run executes a workflow that has been planned using
    pegasus-plan.</para>
  </section>

  <section>
    <title>pegasus-remove</title>

    <para>pegasus-remove is used to abort a running workflow.</para>
  </section>

  <section>
    <title>pegasus-status</title>

    <para>pegasus-status reports on the status of a workflow.</para>
  </section>

  <section>
    <title>pegasus-analyzer</title>

    <para>pegasus-analyzer is used to debug failed workflows.</para>
  </section>

  <section>
    <title>pegasus-statistics</title>

    <para>pegasus-statistics reports statistics about a workflow.</para>
  </section>

  <section>
    <title>pegasus-plots</title>

    <para>pegasus-plots generates charts and graphs that illustrate the
    statistics and execution of a workflow.</para>
  </section>

  <section>
    <title>pegasus-transfer</title>

    <para>pegasus-transfer is a wrapper for several file transfer
    clients.</para>
  </section>

  <section>
    <title>pegasus-sc-client</title>

    <para>pegasus-sc-client is used to generate and modify site
    catalogs.</para>
  </section>

  <section>
    <title>pegasus-tc-client</title>

    <para>pegasus-tc-client is used to generate and modify transformation
    catalogs.</para>
  </section>

  <section>
    <title>pegasus-s3</title>

    <para>pegasus-s3 is a client for the Amazon S3 object storage service and
    any other storage services that conform to the Amazon S3 API, such as
    Eucalyptus Walrus.</para>

    <section>
      <title>URL Format</title>

      <para>All URLs for objects stored in S3 should be specified in the
      following format:</para>

      <programlisting>s3[s]://USER@SITE[/BUCKET[/KEY]]</programlisting>

      <para>The protocol part can be s3:// or s3s://. If s3s:// is used, then
      pegasus-s3 will force the connection to use SSL and override the setting
      in the configuration file. If s3:// is used, then whether the connection
      uses SSL or not is determined by the value of the 'endpoint' variable in
      the configuration for the site.</para>

      <para>The <emphasis>USER@SITE</emphasis> part is required, but the
      <emphasis>BUCKET</emphasis> and <emphasis>KEY</emphasis> parts may be
      optional depending on the context.</para>

      <para>The <emphasis>USER@SITE</emphasis> portion is referred to as the
      'identity', and the <emphasis>SITE</emphasis> portion is referred to as
      the site. Both the identity and the site are looked up in the
      configuration file (see pegasus-s3 Configuration) to determine the
      parameters to use when establishing a connection to the service. The
      site portion is used to find the host and port, whether to use SSL, and
      other things. The identity portion is used to determine which
      authentication tokens to use. This format is designed to enable users to
      easily use multiple services with multiple authentication tokens. Note
      that neither the <emphasis>USER</emphasis> nor the
      <emphasis>SITE</emphasis> portion of the URL have any meaning outside of
      pegasus-s3. They do not refer to real usernames or hostnames, but are
      rather handles used to look up configuration values in the configuration
      file.</para>

      <para>The <emphasis>BUCKET</emphasis> portion of the URL is the part
      between the 3rd and 4th slashes. Buckets are part of a global namespace
      that is shared with other users of the storage service. As such, they
      should be unique.</para>

      <para>The <emphasis>KEY</emphasis> portion of the URL is anything after
      the 4th slash. Keys can include slashes, but S3-like storage services do
      not have the concept of a directory like regular file systems. Instead,
      keys are treated like opaque identifiers for individual objects. So, for
      example, the keys 'a/b' and 'a/c' have a common prefix, but cannot be
      said to be in the same 'directory'.</para>

      <para>Some example URLs are:</para>

      <programlisting>s3://ewa@amazon
s3://juve@skynet/gideon.isi.edu
s3://juve@magellan/pegasus-images/centos-5.5-x86_64-20101101.part.1
s3s://ewa@amazon/pegasus-images/data.tar.gz</programlisting>
    </section>

    <section>
      <title>Subcommands</title>

      <para>pegasus-s3 has several subcommands for different storage service
      operations.</para>

      <section>
        <title>help</title>

        <para><emphasis role="bold">pegasus-s3 help</emphasis></para>

        <para>The <emphasis role="bold">help</emphasis> subcommand lists all
        available subcommands.</para>
      </section>

      <section>
        <title>ls</title>

        <para><emphasis role="bold">pegasus-s3 ls [options]
        URL...</emphasis></para>

        <para>The <emphasis role="bold">ls</emphasis> subcommand lists the
        contents of a URL. If the URL does not contain a bucket, then all the
        buckets owned by the user are listed. If the URL contains a bucket,
        but no key, then all the keys in the bucket are listed. If the URL
        contains a bucket and a key, then all keys in the bucket that begin
        with the specified key are listed.</para>
      </section>

      <section>
        <title>mkdir</title>

        <para><emphasis role="bold">pegasus-s3 mkdir [options]
        URL...</emphasis></para>

        <para>The <emphasis role="bold">mkdir</emphasis> subcommand creates
        one or more buckets.</para>
      </section>

      <section>
        <title>rmdir</title>

        <para><emphasis role="bold">pegasus-s3 rmdir [options]
        URL...</emphasis></para>

        <para>The <emphasis role="bold">rmdir</emphasis> subcommand deletes
        one or more buckets from the storage service. In order to delete a
        bucket, the bucket must be empty.</para>
      </section>

      <section>
        <title>rm</title>

        <para><emphasis role="bold">pegasus-s3 rm [options]
        URL...</emphasis></para>

        <para>The <emphasis role="bold">rm</emphasis> subcommand deletes one
        or more keys from the storage service.</para>
      </section>

      <section>
        <title>put</title>

        <para><emphasis role="bold">pegasus-s3 put [options] FILE
        URL</emphasis></para>

        <para>The <emphasis role="bold">put</emphasis> subcommand stores the
        file specified by <emphasis>FILE</emphasis> in the storage service
        under the bucket and key specified by <emphasis>URL</emphasis>. If the
        URL contains a bucket, but not a key, then the file name is used as
        the key.</para>

        <para>If a transient failure occurs, then the upload will be retried
        several times before pegasus-s3 gives up and fails.</para>

        <para>The put subcommand can do both chunked and parallel uploads if
        the service supports multipart uploads (see multipart_uploads in the
        configuration). Currently only Amazon S3 supports multipart
        uploads.</para>

        <para>This subcommand will check the size of the file to make sure it
        can be stored before attempting to store it.</para>

        <para>Chunked uploads are useful to reduce the probability of an
        upload failing. If an upload is chunked, then pegasus-s3 issues
        separate PUT requests for each chunk of the file. Specifying smaller
        chunks (using --chunksize) will reduce the chances of an upload
        failing due to a transient error. Chunksizes can range from 5 MB to
        1GB (chunk sizes smaller than 5 MB produced incomplete uploads on
        Amazon S3). The maximum number of chunks for any single file is
        10,000, so if a large file is being uploaded with a small chunksize,
        then the chunksize will be increased to fit within the 10,000 chunk
        limit. By default, the file will be split into 10 MB chunks if the
        storage service supports multipart uploads. Chunked uploads can be
        disabled by specifying a chunksize of 0. If the upload is chunked,
        then each chunk is retried independently under transient failures. If
        any chunk fails permanently, then the upload is aborted.</para>

        <para>Parallel uploads can increase performance for services that
        support multipart uploads. In a parallel upload the file is split into
        N chunks and each chunk is uploaded concurrently by one of M threads
        in first-come, first-served fashion. If the chunksize is set to 0,
        then parallel uploads are disabled. If M &gt; N, then the actual
        number of threads used will be reduced to N. The number of threads can
        be specified using the --parallel argument. If --parallel is 0 or 1,
        then only a single thread is used. The default value is 0. There is no
        maximum number of threads, but it is likely that the link will be
        saturated by ~4 threads. Very high-bandwidth, long-delay links may get
        better results with up to ~8 threads.</para>

        <note>
          <para>Under certain circumstances, when a multipart upload fails it
          could leave behind data on the server. When a failure occurs the put
          subcommand will attempt to abort the upload. If the upload cannot be
          aborted, then a partial upload may remain on the server. To check
          for partial uploads run the <emphasis role="bold">lsup</emphasis>
          subcommand. If you see an upload that failed in the output of lsup,
          then run the <emphasis role="bold">rmup</emphasis> subcommand to
          remove it.</para>
        </note>
      </section>

      <section>
        <title>get</title>

        <para><emphasis role="bold">pegasus-s3 get [options] URL
        [FILE]</emphasis></para>

        <para>The <emphasis role="bold">get</emphasis> subcommand retrives an
        object from the storage service identified by <emphasis>URL</emphasis>
        and stores it in the file specified by <emphasis>FILE</emphasis>. If
        FILE is not specified, then the key is used as the file name (Note: if
        the key has slashes, then the file name will be a relative
        subdirectory, but pegasus-s3 will not create the subdirectory if it
        does not exist).</para>

        <para>If a transient failure occurs, then the download will be retried
        several times before pegasus-s3 gives up and fails.</para>

        <para>The get subcommand can do both chunked and parallel downloads if
        the service supports ranged downloads (see ranged_downloads in the
        configuration). Currently only Amazon S3 has good support for ranged
        downloads. Eucalyptus Walrus supports ranged downloads, but the
        current release, 1.6, is inconsistent with the Amazon interface and
        has a bug that causes ranged downloads to hang in some cases. It is
        recommended that ranged downloads not be used with Eucalyptus until
        these issues are resolved.</para>

        <para>Chunked downloads can be used to reduce the probability of a
        download failing. When a download is chunked, pegasus-s3 issues
        separate GET requests for each chunk of the file. Specifying smaller
        chunks (uisng --chunksize) will reduce the chances that a download
        will fail to do a transient error. Chunk sizes can range from 1 MB to
        1 GB. By default, a download will be split into 10 MB chunks if the
        site supports ranged downloads. Chunked downloads can be disabled by
        specifying a chunksize of 0. If a download is chunked, then each chunk
        is retried independently under transient failures. If any chunk fails
        permanently, then the download is aborted.</para>

        <para>Parallel downloads can increase performance for services that
        support ranged downloads. In a parallel download, the file to be
        retrieved is split into N chunks and each chunk is downloaded
        concurrently by one of M threads in a first-come, first-served
        fashion. If the chunksize is 0, then parallel downloads are disabled.
        If M &gt; N, then the actual number of threads used will be reduced to
        N. The number of threads can be specified using the --parallel
        argument. If --parallel is 0 or 1, then only a single thread is used.
        The default value is 0. There is no maximum number of threads, but it
        is likely that the link will be saturated by ~4 threads. Very
        high-bandwidth, long-delay links may get better results with up to ~8
        threads.</para>
      </section>

      <section>
        <title>lsup</title>

        <para><emphasis role="bold">pegasus-s3 lsup [options]
        URL</emphasis></para>

        <para>The <emphasis role="bold">lsup</emphasis> subcommand lists
        active uploads. The URL specified should point to a bucket. This
        command is only valid if the site supports multipart uploads. The
        output of this command is a list of keys and upload IDs.</para>

        <para>This subcommand is used with <emphasis
        role="bold">rmup</emphasis> to help recover from failures of multipart
        uploads.</para>
      </section>

      <section>
        <title>rmup</title>

        <para><emphasis role="bold">pegasus-s3 rmup [options] URL
        UPLOAD</emphasis></para>

        <para>The <emphasis role="bold">rmup</emphasis> subcommand cancels and
        active upload. The <emphasis>URL</emphasis> specified should point to
        a bucket, and <emphasis>UPLOAD</emphasis> is the long, complicated
        upload ID shown by the <emphasis role="bold">lsup</emphasis>
        subcommand.</para>

        <para>This subcommand is used with <emphasis
        role="bold">lsup</emphasis> to recover from failures of multipart
        uploads.</para>
      </section>
    </section>

    <section>
      <title>pegasus-s3 Configuration</title>

      <para>Each user should specify a configuration file that pegasus-s3 will
      use to look up connection parameters and authentication tokens.</para>

      <section>
        <title>Configuration file search path</title>

        <para>This client will look in the following locations, in order, to
        locate the user's configuration file:</para>

        <orderedlist>
          <listitem>
             The -C/--conf argument 
          </listitem>

          <listitem>
             The S3CFG environment variable 
          </listitem>

          <listitem>
             ~/.s3cfg 
          </listitem>
        </orderedlist>

        <para>If it does not find the configuration file in one of these
        locations it will fail with an error.</para>
      </section>

      <section>
        <title>Configuration file format</title>

        <para>The configuration file is in INI format and contains two types
        of entries.</para>

        <para>The first type of entry is a <emphasis role="bold">site
        entry</emphasis>, which specifies the configuration for a storage
        service. This entry specifies the service endpoint that pegasus-s3
        should connect to for the site, and some optional features that the
        site may support. Here is an example of a site entry for Amazon
        S3:</para>

        <programlisting>[amazon]
endpoint = http://s3.amazonaws.com/</programlisting>

        <para>The other type of entry is an <emphasis role="bold">identity
        entry</emphasis>, which specifies the authentication information for a
        user at a particular site. Here is an example of an identity
        entry:</para>

        <programlisting>[pegasus@amazon]
access_key = 90c4143642cb097c88fe2ec66ce4ad4e
secret_key = a0e3840e5baee6abb08be68e81674dca</programlisting>

        <para>It is important to note that user names and site names used are
        only logical--they do not correspond to actual hostnames or usernames,
        but are simply used as a convenient way to refer to the services and
        identities used by the client.</para>

        <para>The configuration file should be saved with limited permissions.
        Only the owner of the file should be able to read from it and write to
        it (i.e. it should have permissions of 0600 or 0400). If the file has
        more liberal permissions, then pegasus-s3 will fail with an error
        message. The purpose of this is to prevent the authentication tokens
        stored in the configuration file from being accessed by other
        users.</para>
      </section>

      <section>
        <title>Configuration variables</title>

        <table>
          <tgroup cols="3">
            <thead>
              <row>
                <entry>Variable</entry>

                <entry>Scope</entry>

                <entry>Description</entry>
              </row>
            </thead>

            <tbody>
              <row>
                <entry>endpoint</entry>

                <entry>site</entry>

                <entry>The URL of the web service endpoint. If the URL begins
                with 'https', then SSL will be used.</entry>
              </row>

              <row>
                <entry>max_object_size</entry>

                <entry>site</entry>

                <entry>The maximum size of an object in GB (default:
                5GB)</entry>
              </row>

              <row>
                <entry>multipart_uploads</entry>

                <entry>site</entry>

                <entry>Does the service support multipart uploads (True/False,
                default: False)</entry>
              </row>

              <row>
                <entry>ranged_downloads</entry>

                <entry>site</entry>

                <entry>Does the service support ranged downloads? (True/False,
                default: False)</entry>
              </row>

              <row>
                <entry>access_key</entry>

                <entry>identity</entry>

                <entry>The access key for the identity</entry>
              </row>

              <row>
                <entry>secret_key</entry>

                <entry>identity</entry>

                <entry>The secret key for the identity</entry>
              </row>
            </tbody>
          </tgroup>
        </table>
      </section>

      <section>
        <title>Example configuration</title>

        <para>This is an example configuration that specifies a single site
        (amazon) and a single identity (pegasus@amazon). For this site the
        maximum object size is 5TB, and the site supports both multipart
        uploads and ranged downloads, so both uploads and downloads can be
        done in parallel.</para>

        <programlisting>[amazon]
endpoint = https://s3.amazonaws.com/
max_object_size = 5120
multipart_uploads = True
ranged_downloads = True

[pegasus@amazon]
access_key = 90c4143642cb097c88fe2ec66ce4ad4e
secret_key = a0e3840e5baee6abb08be68e81674dca

[magellan]
# NERSC Magellan is a Eucalyptus site. It doesn't support multipart uploads,
# or ranged downloads (the defaults), and the maximum object size is 5GB
# (also the default)
endpoint = https://128.55.69.235:8773/services/Walrus

[juve@magellan]
access_key = quwefahsdpfwlkewqjsdoijldsdf
secret_key = asdfa9wejalsdjfljasldjfasdfa

[voekler@magellan]
# Each site can have multiple associated identities
access_key = asdkfaweasdfbaeiwhkjfbaqwhei
secret_key = asdhfuinakwjelfuhalsdflahsdl</programlisting>
      </section>
    </section>
  </section>

  <section>
    <title>pegasus-exitcode</title>

    <para>pegasus-exitcode is a utility that examines the STDOUT of a job to
    determine if the job failed, and renames the STDOUT and STDERR files of a
    job to preserve them in case the job is retried.</para>

    <para>Pegasus uses pegasus-exitcode as the DAGMan postscript for all jobs
    submitted via Globus GRAM. This tool exists as a workaround to a known
    problem with Globus where the exitcodes of GRAM jobs are not returned.
    This is a problem because Pegasus uses the exitcode of a job to determine
    if the job failed or not.</para>

    <para>In order to get around the exitcode problem, Pegasus wraps all GRAM
    jobs with Kickstart, which records the exitcode of the job in an XML
    invocation record, which it writes to the job's STDOUT. The STDOUT is
    transferred from the execution host back to the submit host when the job
    terminates. After the job terminates, DAGMan runs the job's postscript,
    which Pegasus sets to be pegasus-exitcode. pegasus-exitcode looks at the
    invocation record generated by kickstart to see if the job succeeded or
    failed. If the invocation record indicates a failure, then
    pegasus-exitcode returns a non-zero result, which indicates to DAGMan that
    the job has failed. If the invocation record indicates that the job
    succeeded, then pegasus-exitcode returns 0, which tells DAGMan that the
    job succeeeded.</para>

    <para>pegasus-exitcode performs several checks to determine whether a job
    failed or not. These checks include:</para>

    <orderedlist>
      <listitem>
        <para>Is STDOUT empty? If it is empty, then the job failed.</para>
      </listitem>

      <listitem>
        <para>Are there any &lt;status&gt; tags with a non-zero value? If
        there are, then the job failed. Note that, if this is a clustered job,
        there could be multiple &lt;status&gt; tags, one for each task. If any
        of them are non-zero, then the job failed.</para>
      </listitem>

      <listitem>
        <para>Is there at least one &lt;status&gt; tag with a zero value?
        There must be at least one successful invocation or the job has
        failed.</para>
      </listitem>
    </orderedlist>

    <para>In addition, pegasus-exitcode allows the caller to specify the
    exitcode returned by Condor using the --return argument. This can be
    passed to pegasus-exitcode in a DAGMan post script by using the $RETURN
    variable. If this value is non-zero, then pegasus-exitcode returns a
    non-zero result before performing any other checks. For GRAM jobs, the
    value of $RETURN will always be 0 regardless of whether the job failed or
    not.</para>

    <para>Also, pegasus-exitcode allows the caller to specify the number of
    successful tasks it should see using the --tasks argument. If
    pegasus-exitcode does not see N successful tasks, where N is set by
    --tasks, then it will return a non-zero result. The default value is 1.
    This can be used to detect failures in clustered jobs where, for any
    number of reasons, invocation records do not get generated for all the
    tasks in the clustered job.</para>

    <para>In addition to checking the success/failure of a job,
    pegasus-exitcode also renames the STDOUT and STDERR files of the job so
    that if the job is retried, the STDOUT and STDERR of the previous run are
    not lost. It does this by appending a sequence number to the end of the
    files. For example, if the STDOUT file is called "job.out", then the first
    time the job is run pegasus-exitcode will rename the file "job.out.000".
    If the job is run again, then pegasus-exitcode sees that "job.out.000"
    already exists and renames the file "job.out.001". It will continue to
    rename the file by incrementing the sequence number every time the job is
    executed.</para>
  </section>

  <section>
    <title>kickstart</title>

    <para>kickstart is a job wrapper that collects data about a job's
    execution environment, performance, and output.</para>
  </section>
  </section>
</chapter>
