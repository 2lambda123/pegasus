<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN"
"http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<chapter id="execution_environments">
  <title>Execution Environments</title>

  <para>Pegasus supports a number of execution environments. An execution
  environment is a setup where jobs from a workflow are running.</para>

  <section id="localhost">
    <title>General execution environment: Localhost</title>

    <para>In this configuration , Pegasus schedules the jobs to run locally on
    the submit host. Running locally is a good approach for smaller workflows,
    testing workflows, and for demonstations such as the <link
    linkend="tutorial_vm">Pegasus tutorial using Virtual
    Machines</link>.Pegasus supports two methods of local execution: local
    Condor pool, and shell planner. The former is preferred as the latter does
    not support all Pegasus' features (such as notifications).</para>

    <para>Running on a local Condor pool is achieved by executing the workflow
    on site local ( <emphasis role="bold">--sites local </emphasis>option to
    pegasus-plan ). The site "local" is a reserved site in Pegasus and results
    in the jobs to run on the submit host in condor universe local. The site
    catalog can be left very simple in this case:</para>

    <programlisting>&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;sitecatalog xmlns="http://pegasus.isi.edu/schema/sitecatalog"
             xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
             xsi:schemaLocation="http://pegasus.isi.edu/schema/sitecatalog
             http://pegasus.isi.edu/schema/sc-3.0.xsd" version="3.0"&gt;
    &lt;site  handle="local" arch="x86_64" os="LINUX"&gt;
        &lt;head-fs&gt;
            &lt;scratch&gt;
                &lt;shared&gt;
                    &lt;file-server protocol="file" url="file://" mount-point="/tmp/wf/work"/&gt;
                    &lt;internal-mount-point mount-point="/tmp/wf/work"/&gt;
                &lt;/shared&gt;
            &lt;/scratch&gt;
            &lt;storage&gt;
                &lt;shared&gt;
                    &lt;file-server protocol="file" url="file://" mount-point="/tmp/wf/storage"/&gt;
                    &lt;internal-mount-point mount-point="/tmp/wf/storage"/&gt;
                &lt;/shared&gt;
            &lt;/storage&gt;
        &lt;/head-fs&gt;
    &lt;/site&gt;
&lt;/sitecatalog&gt;
</programlisting>

    <para>The simplest execution environment does not involve Condor. Pegasus
    is capable of planning small workflows for local execution using a shell
    planner. Please refer to the <filename
    class="directory">share/pegasus/examples</filename> directory in your
    Pegasus installation, the shell planner's <link
    linkend="local_shell_examples">documentation section</link>, or the
    tutorials, for details.</para>
  </section>

  <section id="condor_pool">
    <title>General execution environment: Condor Pool</title>

    <para>A Condor pool is a set of machines that use Condor for resource
    management. A Condor pool can be a cluster of dedicated machines or a set
    of distributively owned machines. Pegasus can generate concrete workflows
    that can be executed on a Condor pool.</para>

    <figure>
      <title>The distributed resources appear to be part of a Condor
      pool.</title>

      <mediaobject>
        <imageobject>
          <imagedata contentdepth="100%" fileref="images/condor_layout.png"
                     scalefit="1" width="100%" />
        </imageobject>
      </mediaobject>
    </figure>

    <para>The workflow is submitted using DAGMan from one of the job
    submission machines in the Condor pool. It is the responsibility of the
    Central Manager of the pool to match the task in the workflow submitted by
    DAGMan to the execution machines in the pool. This matching process can be
    guided by including Condor specific attributes in the submit files of the
    tasks. If the user wants to execute the workflow on the execution machines
    (worker nodes) in a Condor pool, there should be a resource defined in the
    site catalog which represents these execution machines. The universe
    attribute of the resource should be vanilla. There can be multiple
    resources associated with a single Condor pool, where each resource
    identifies a subset of machine (worker nodes) in the pool.</para>

    <para>When running on a Condor pool, the user has to decide how Pegasus
    should transfer data. Please see the <link
    linkend="data_staging_configuration">Data Staging Configuration</link> for
    the options. The easiest is to use <emphasis
    role="bold">condorio</emphasis> as that mode does not require any extra
    setup - Condor will do the transfers using the existing Condor daemons.
    For an example of this mode see the example workflow in
    <filename>share/pegasus/examples/condor-blackdiamond-condorio/</filename>
    . In condorio mode, the site catalog for the execution site is very simple
    as storage is provided by Condor:</para>

    <programlisting>&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;sitecatalog xmlns="http://pegasus.isi.edu/schema/sitecatalog"
             xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
             xsi:schemaLocation="http://pegasus.isi.edu/schema/sitecatalog
             http://pegasus.isi.edu/schema/sc-3.0.xsd" version="3.0"&gt;
    &lt;site  handle="local" arch="x86_64" os="LINUX"&gt;
        &lt;head-fs&gt;
            &lt;scratch&gt;
                &lt;shared&gt;
                    &lt;file-server protocol="file" url="file://" mount-point="/tmp/wf/work"/&gt;
                    &lt;internal-mount-point mount-point="/tmp/wf/work"/&gt;
                &lt;/shared&gt;
            &lt;/scratch&gt;
            &lt;storage&gt;
                &lt;shared&gt;
                    &lt;file-server protocol="file" url="file://" mount-point="/tmp/wf/storage"/&gt;
                    &lt;internal-mount-point mount-point="/tmp/wf/storage"/&gt;
                &lt;/shared&gt;
            &lt;/storage&gt;
        &lt;/head-fs&gt;
    &lt;/site&gt;
    &lt;site  handle="condorpool" arch="x86_86" os="LINUX"&gt;
        &lt;head-fs&gt;
            &lt;scratch /&gt;
            &lt;storage /&gt;
        &lt;/head-fs&gt;
        &lt;profile namespace="pegasus" key="style" &gt;condor&lt;/profile&gt;
        &lt;profile namespace="condor" key="universe" &gt;vanilla&lt;/profhile&gt;
    &lt;/site&gt;
&lt;/sitecatalog&gt;</programlisting>

    <section id="glideins">
      <title>Glideins</title>

      <para>In this section we describe how machines from different
      administrative domains and supercomputing centers can be dynamically
      added to a Condor pool for certain timeframe. These machines join the
      Condor pool temporarily and can be used to execute jobs in a non
      preemptive manner. This functionality is achieved using a Condor feature
      called <emphasis role="bold">glideins</emphasis> (see <ulink
      url="http://cs.wisc.edu/condor/glidein">http://cs.wisc.edu/condor/glidein</ulink>)
      . The startd daemon is the Condor daemon which provides the compute
      slots and runs the jobs. In the glidein case, the submit machine is
      usually a static machine and the glideins are told configued to report
      to that submit machine. The glideins can be submitted to any type of
      resource: a GRAM enabled cluster, a campus cluster, a cloud environment
      such as Amazon AWS, or even another Condor cluster.</para>

      <tip>
        <para>As glideins are usually coming from different compute resource,
        and/or the glideins are running in an administrative domain different
        from the submit node, there is usually no shared filesystem available.
        Thus the most common <link linkend="data_staging_configuration">data
        staging modes</link> are <emphasis role="bold">condorio</emphasis> and
        <emphasis role="bold">nonsharedfs</emphasis> .</para>
      </tip>

      <para>There are many useful tools which submits and manages glideins for
      you:</para>

      <itemizedlist>
        <listitem>
          <para><ulink
          url="http://www.uscms.org/SoftwareComputing/Grid/WMS/glideinWMS/">GlideinWMS</ulink>
          is a tool and host environment used mostly on the <ulink
          url="http://www.opensciencegrid.org/">Open Science
          Grid</ulink>.</para>
        </listitem>

        <listitem>
          <para><ulink
          url="http://pegasus.isi.edu/projects/corralwms">CorralWMS</ulink> is
          a personal frontend for GlideinWMS. CorralWMS was developed by the
          Pegasus team and works very well for high throughput
          workflows.</para>
        </listitem>

        <listitem>
          <para><ulink
          url="http://research.cs.wisc.edu/condor/manual/v7.6/condor_glidein.html">condor_glidein</ulink>
          is a simple glidein tool for Globus GRAM clusters. condor_glidein is
          shipped with Condor.</para>
        </listitem>

        <listitem>
          <para>Glideins can also be created by hand or scripts. This is a
          useful solution for example for cluster which have no external job
          submit mechanisms or do not allow outside networking.</para>
        </listitem>
      </itemizedlist>
    </section>
  </section>

  <section id="cloud">
    <title>General execution environment: Clouds</title>

    <para><figure id="concepts-fig-cloud-layout">
        <title>Cloud Sample Site Layout</title>

        <mediaobject>
          <imageobject>
            <imagedata align="center" contentdepth="100%"
                       fileref="images/fg-pwms-prefio.3.png" scalefit="1"
                       valign="middle" width="100%" />
          </imageobject>
        </mediaobject>
      </figure></para>

    <para>The pevious figure shows a sample layout for sky computing (as in:
    multiple clouds) as supported by Pegasus. At this point, it is up to the
    user to provision the remote resources with a proper VM image that
    includes a Condor startd and proper Condor configuration to report back to
    a Condor collector that the Condor schedd has access to.</para>

    <para>In this discussion, the <emphasis>submit host</emphasis> (SH) is
    located logically external to the cloud provider(s). The SH is the point
    where a user submits Pegasus workflows for execution. This site typically
    runs a Condor collector to gather resource announcements, or is part of a
    larger Condor pool that collects these announcement. Condor makes the
    remote resources available to the submit host&amp;rsquor;s Condor
    installation.</para>

    <para>The <link linkend="concepts-fig-cloud-layout">figure above</link>
    shows the way Pegasus WMS is deployed in cloud computing resources,
    ignoring how these resources were provisioned. The provisioning request
    shows multiple resources per provisioning request.</para>

    <para>The provisioning broker -- Nimbus, Eucalyptus or EC2 -- at the
    remote site is responsible to allocate and set up the resources. For a
    multi-node request, the worker nodes often require access to a form of
    shared data storage. Concretely, either a POSIX-compliant shared file
    system (e.g. NFS, PVFS) is available to the nodes, or can be brought up
    for the lifetime of the application workflow. The task steps of the
    application workflow facilitate shared file systems to exchange
    intermediary results between tasks on the same cloud site. Pegasus also
    supports an S3 data mode for the application workflow data staging.</para>

    <para>The initial stage-in and final stage-out of application data into
    and out of the node set is part of any Pegasus-planned workflow. Several
    configuration options exist in Pegasus to deal with the dynamics of push
    and pull of data, and when to stage data. In many use-cases, some form of
    external access to or from the shared file system that is visible to the
    application workflow is required to facilitate successful data staging.
    However, Pegasus is prepared to deal with a set of boundary cases.</para>

    <para>The data server in the figure is shown at the submit host. This is
    not a strict requirement. The data server for consumed data and data
    products may both be different and external to the submit host.</para>

    <para>Once resources begin appearing in the pool managed by the submit
    machine&amp;rsquor;s Condor collector, the application workflow can be
    submitted to Condor. A Condor DAGMan will manage the application workflow
    execution. Pegasus run-time tools obtain timing-, performance and
    provenance information as the application workflow is executed. At this
    point, it is the user's responsibility to de-provision the allocated
    resources.</para>

    <para>In the figure, the cloud resources on the right side are assumed to
    have uninhibited outside connectivity. This enables the Condor I/O to
    communicate with the resources. The right side includes a setup where the
    worker nodes use all private IP, but have out-going connectivity and a NAT
    router to talk to the internet. The <emphasis>Condor connection
    broker</emphasis> (CCB) facilitates this setup almost effortlessly.</para>

    <para>The left side shows a more difficult setup where the connectivity is
    fully firewalled without any connectivity except to in-site nodes. In this
    case, a proxy server process, the <emphasis>generic connection
    broker</emphasis> (GCB), needs to be set up in the DMZ of the cloud site
    to facilitate Condor I/O between the submit host and worker nodes.</para>

    <para>If the cloud supports data storage servers, Pegasus is starting to
    support workflows that require staging in two steps: Consumed data is
    first staged to a data server in the remote site's DMZ, and then a second
    staging task moves the data from the data server to the worker node where
    the job runs. For staging out, data needs to be first staged from the
    job's worker node to the site's data server, and possibly from there to
    another data server external to the site. Pegasus is capable to plan both
    steps: Normal staging to the site's data server, and the worker-node
    staging from and to the site's data server as part of the job. We are
    working on expanding the current code to support a more generic set by
    Pegasus 3.1.</para>
  </section>

  <section id="globus_gram">
    <title>General execution environment: Globus GRAM enabled Cluster</title>

    <para><figure id="concepts-fig-site-layout">
        <title>Grid Sample Site Layout</title>

        <mediaobject>
          <imageobject>
            <imagedata align="center" contentdepth="100%"
                       fileref="images/concepts-site-layout.jpg" scalefit="1"
                       valign="middle" width="100%" />
          </imageobject>
        </mediaobject>
      </figure></para>

    <para>A generic grid environment shown in the figure <link
    linkend="concepts-fig-site-layout">above</link>. We will work from the
    left to the right top, then the right bottom.</para>

    <para>On the left side, you have a submit machine where Pegasus runs,
    Condor schedules jobs, and workflows are executed. We call it the
    <emphasis>submit host</emphasis> (SH), though its functionality can be
    assumed by a virtual machine image. In order to properly communicate over
    secured channels, it is important that the submit machine has a proper
    notion of time, i.e. runs an NTP daemon to keep accurate time. To be able
    to connect to remote clusters and receive connections from the remote
    clusters, the submit host has a public IP address to facilitate this
    communication.</para>

    <para>In order to send a job request to the remote cluster, Condor wraps
    the job into Globus calls via Condor-G. Globus uses GRAM to manage jobs on
    remote sites. In terms of a software stack, Pegasus wraps the job into
    Condor. Condor wraps the job into Globus. Globus transports the job to the
    remote site, and unwraps the Globus component, sending it to the remote
    site's <emphasis>resource manager</emphasis> (RM).</para>

    <para>To be able to communicate using the Globus security infrastructure
    (GSI), the submit machine needs to have the certificate authority (CA)
    certificates configured, requires a host certificate in certain
    circumstances, and the user a user certificate that is enabled on the
    remote site. On the remote end, the remote gatekeeper node requires a host
    certificate, all signing CA certificate chains and policy files, and a
    goot time source.</para>

    <para>In a grid environment, there are one or more clusters accessible via
    grid middleware like the <ulink url="http://www.globus.org/">Globus
    Toolkit</ulink>. In case of Globus, there is the Globus gatekeeper
    listening on TCP port 2119 of the remote cluster. The port is opened to a
    single machine called <emphasis>head node</emphasis> (HN).The head-node is
    typically located in a de-militarized zone (DMZ) of the firewall setup, as
    it requires limited outside connectivity and a public IP address so that
    it can be contacted. Additionally, once the gatekeeper accepted a job, it
    passes it on to a jobmanager. Often, these jobmanagers require a limited
    port range, in the example TCP ports 40000-41000, to call back to the
    submit machine.</para>

    <para>For the user to be able to run jobs on the remote site, the user
    must have some form of an account on the remtoe site. The user's grid
    identity is passed from the submit host. An entity called <emphasis>grid
    mapfile</emphasis> on the gatekeeper maps the user's grid identity into a
    remote account. While most sites do not permit account sharing, it is
    possible to map multiple user certificates to the same account.</para>

    <para>The gatekeeper is the interface through which jobs are submitted to
    the remote cluster's resource manager. A resource manager is a scheduling
    system like PBS, Maui, LSF, FBSNG or Condor that queues tasks and
    allocates worker nodes. The <emphasis>worker nodes</emphasis> (WN) in the
    remote cluster might not have outside connectivity and often use all
    private IP addresses. The Globus toolkit requires a shared filesystem to
    properly stage files between the head node and worker nodes.</para>

    <note>
      <para>The shared filesystem requirement is imposed by Globus. Pegasus is
      capable of supporting advanced site layouts that do not require a shared
      filesystem. Please contact us for details, should you require such a
      setup.</para>
    </note>

    <para>To stage data between external sites for the job, it is recommended
    to enable a GridFTP server. If a shared networked filesystem is involved,
    the GridFTP server should be located as close to the file-server as
    possible. The GridFTP server requires TCP port 2811 for the control
    channel, and a limited port range for data channels, here as an example
    the TPC ports from 40000 to 41000. The GridFTP server requires a host
    certificate, the signing CA chain and policy files, a stable time source,
    and a gridmap file that maps between a user's grid identify and the user's
    account on the remote site.</para>

    <para>The GridFTP server is often installed on the head node, the same as
    the gatekeeper, so that they can share the grid mapfile, CA certificate
    chains and other setups. However, for performance purposes it is
    recommended that the GridFTP server has its own machine.</para>
  </section>

  <section id="glite">
    <title>General execution environment: Glite</title>

    <para>This section describes the various changes required in the site
    catalog for Pegasus to generate an executable workflow that uses gLite
    blahp to directly submit to PBS on the local machine. This mode of
    submission should only be used when the condor on the submit host can
    directly talk to scheduler running on the cluster. It is recommended that
    the cluster that gLite talks to is designated as a separate compute site
    in the Pegasus site catalog. To tag a site as a gLite site the following
    two profiles need to be specified for the site in the site catalog</para>

    <orderedlist>
      <listitem>
        <para><emphasis role="bold">pegasus</emphasis> profile <emphasis
        role="bold">style</emphasis> with value set to <emphasis
        role="bold">glite</emphasis>.</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">condor</emphasis> profile <emphasis
        role="bold">grid_resource</emphasis> with value set to <emphasis
        role="bold">pbs|lsf</emphasis></para>
      </listitem>
    </orderedlist>

    <para>An example site catalog entry for a glite site looks as follows in
    the site catalog</para>

    <programlisting> &lt;site  handle="isi_viz_glite" arch="x86_64" os="LINUX"&gt;
       &lt;grid  type="gt2" contact="smarty.isi.edu/jobmanager-pbs" scheduler="PBS" jobtype="auxillary"/&gt;
       &lt;grid  type="gt2" contact="smarty.isi.edu/jobmanager-pbs" scheduler="PBS" jobtype="compute"/&gt;
       &lt;head-fs&gt;
             &lt;scratch&gt;
                   &lt;shared&gt;
                         &lt;file-server protocol="gsiftp" url="gsiftp://viz-login.isi.edu" \
                                                       mount-point="/nfs/scratch01"\&gt;
                         &lt;internal-mount-point mount-point="/nfs/scratch01"/&gt;
                   &lt;/shared&gt;
             &lt;/scratch&gt;
             &lt;storage&gt;
                   &lt;shared&gt;
                         &lt;file-server protocol="gsiftp" url="gsiftp://viz-login.isi.edu" \
                                                mount-point="/exports/storage01"\&gt;
                         &lt;internal-mount-point mount-point="/exports/storage01"/&gt;
                    &lt;/shared&gt;
             &lt;/storage&gt;
       &lt;/head-fs&gt;
       &lt;replica-catalog  type="LRC" url="rlsn://smarty.isi.edu" /&gt;
               
       <emphasis role="bold">&lt;!-- following profiles reqd for glite grid style--&gt;
       &lt;profile namespace="pegasus" key="style"&gt;glite&lt;/profile&gt; 
       &lt;profile namespace="condor" key="grid_resource"&gt;pbs&lt;/profile&gt;</emphasis>
 &lt;/site&gt;
</programlisting>

    <section>
      <title>Changes to Jobs</title>

      <para>As part of applying the style to the job, this style adds the
      following classads expressions to the job description.</para>

      <orderedlist>
        <listitem>
          <para>+remote_queue - value picked up from globus profile
          queue</para>
        </listitem>

        <listitem>
          <para>+remote_cerequirements - See below</para>
        </listitem>
      </orderedlist>

      <section>
        <title>Remote CE Requirements</title>

        <para>The remote CE requirements are constructed from the following
        profiles associated with the job. The profiles for a job are derived
        from various sources</para>

        <orderedlist>
          <listitem>
            <para>transformation catalog</para>
          </listitem>

          <listitem>
            <para>site catalog</para>
          </listitem>

          <listitem>
            <para>DAX</para>
          </listitem>

          <listitem>
            <para>user properties</para>
          </listitem>
        </orderedlist>

        <para>The following globus profiles if associated with the job are
        picked up and translated to corresponding glite key</para>

        <orderedlist>
          <listitem>
            <para>hostcount -&gt; PROCS</para>
          </listitem>

          <listitem>
            <para>count -&gt; NODES</para>
          </listitem>

          <listitem>
            <para>maxwalltime -&gt; WALLTIME</para>
          </listitem>
        </orderedlist>

        <para>The following condor profiles if associated with the job are
        picked up and translated to corresponding glite key</para>

        <orderedlist>
          <listitem>
            <para>priority -&gt; PRIORITY</para>
          </listitem>
        </orderedlist>

        <para>All the env profiles are translated to MYENV</para>

        <para>The remote_cerequirements expression is constructed on the basis
        of the profiles associated with job . An example
        +remote_cerequirements classad expression in the submit file is listed
        below</para>

        <programlisting><emphasis role="bold">+remote_cerequirements = "PROCS==18 &amp;&amp; NODES==1 &amp;&amp; PRIORITY==10 &amp;&amp; WALLTIME==3600 \
   &amp;&amp; PASSENV==1 &amp;&amp; JOBNAME==\"TEST JOB\" &amp;&amp; MYENV ==\"JAVA_HOME=/bin/java,APP_HOME=/bin/app\""</emphasis></programlisting>
      </section>

      <section>
        <title>Specifying directory for the jobs</title>

        <para>gLite blahp does not follow the remote_initialdir or initialdir
        classad directives. Hence, all the jobs that have this style applied
        don't have a remote directory specified in the submit directory.
        Instead, Pegasus relies on kickstart to change to the working
        directory when the job is launched on the remote node.</para>
      </section>
    </section>
  </section>

  <section id="campus_cluster">
    <title>Campus Cluster</title>

    <para>TODO</para>
  </section>

  <section id="xsede">
    <title>XSEDE</title>

    <para>Two alternatives: GRAM or glideins.</para>

    <para></para>
  </section>

  <section id="open_science_grid">
    <title>Open Science Grid / glideinWMS</title>

    <section>
      <para><ulink
      url="http://www.uscms.org/SoftwareComputing/Grid/WMS/glideinWMS/">glideinWMS</ulink>
      is a glidein system widely used on Open Science Grid. Pegasus has a
      convenience style named glideinWMS to make running workflows on top of
      glideinWMS easy. This style is similar to the Condor style, but provides
      some more defaults set up to work well with glideinWMS. All you have to
      do is specify the style profile in the site catalog:</para>

      <programlisting> &lt;site  handle="isi_viz" arch="x86" os="LINUX" osrelease="" osversion="" glibc=""&gt;
       &lt;grid  type="gt2" contact="smarty.isi.edu/jobmanager-pbs" scheduler="PBS" jobtype="auxillary"/&gt;
       &lt;grid  type="gt2" contact="smarty.isi.edu/jobmanager-pbs" scheduler="PBS" jobtype="compute"/&gt;
       &lt;head-fs&gt;
              &lt;scratch&gt;
                    &lt;shared&gt;
                         &lt;file-server protocol="gsiftp" url="gsiftp://viz-login.isi.edu" \
                                                                 mount-point="/nfs/scratch01"/&gt;
                         &lt;internal-mount-point mount-point="/nfs/scratch01"/&gt;
                    &lt;/shared&gt;
              &lt;/scratch&gt;
              &lt;storage&gt;
                     &lt;shared&gt;
                          &lt;file-server protocol="gsiftp" url="gsiftp://viz-login.isi.edu" \
                                                                      mount-point="/exports/storage01"/&gt;
                          &lt;internal-mount-point mount-point="/exports/storage01"/&gt;
                     &lt;/shared&gt;
              &lt;/storage&gt;
       &lt;/head-fs&gt;
       &lt;replica-catalog  type="LRC" url="rlsn://smarty.isi.edu"/&gt;
       <emphasis role="bold">&lt;profile namespace="pegasus" key="style"&gt;glideinwms&lt;/profile&gt;</emphasis>
 &lt;/site&gt;</programlisting>

      <para>Planned jobs will have universe, requirements and rank set:</para>

      <programlisting>universe = vanilla
rank = DaemonStartTime
requirements = ((GLIDEIN_Entry_Name == "isi_iz") || (TARGET.Pegasus_Site == "isi_viz")) \
               &amp;&amp; (IS_MONITOR_VM == False) &amp;&amp; (Arch != "") &amp;&amp; (OpSys != "") \
               &amp;&amp; (Disk != -42) &amp;&amp; (Memory &gt; 1) &amp;&amp; (FileSystemDomain != "")
</programlisting>
    </section>
  </section>

  <section id="futuregrid_nimbus">
    <title>FutureGrid Cloud / Nimbus</title>
  </section>

  <section id="amazon_aws">
    <title>Amazon AWS Cloud</title>

    <section>
      <para>In order to use Amazon to execute workflows you need to a) set up
      an execution environment in EC2, and b) configure Pegasus to plan
      workflows for that environment.</para>

      <para>There are many different ways to set up the execution environment
      in Amazon. The easiest way is to use a submit machine outside the cloud,
      and to provision several worker nodes and a file server node in the
      cloud as shown here:</para>

      <para>
        <figure id="ec2">
          <title>Amazon EC2</title>

          <mediaobject>
            <imageobject>
              <imagedata align="center" contentdepth="100%"
                         fileref="images/ec2.png" scalefit="1" valign="middle"
                         width="100%" />
            </imageobject>
          </mediaobject>
        </figure>
      </para>

      <para>The submit machine runs Pegasus and a Condor master (collector,
      schedd, negotiator),the workers run a Condor startd, and the file server
      node exports an NFS file system. The workers' startd is configured to
      connect to the master running outside the cloud. The worker also mounts
      the NFS file system. More information on setting up Condor for this
      environment can be found at <ulink
      url="http://www.isi.edu/~gideon/condor-ec2/">http://www.isi.edu/~gideon/condor-ec2/</ulink>.</para>

      <para>To set up Pegasus to plan workflows for Amazon you need to create
      an 'ec2' entry your site catalog. The site configuration is similar to
      what you would create for running on a local Condor pool (See previous
      section).</para>

      <programlisting> 
 &lt;site  handle="ec2" arch="x86" os="LINUX" osrelease="" osversion="" glibc=""&gt;
       &lt;!-- These entries are required, but not used --&gt;

       &lt;grid  type="gt2" contact="example.com/jobmanager-pbs" scheduler="PBS" jobtype="auxillary"/&gt;
       &lt;grid  type="gt2" contact="example.com/jobmanager-fork" scheduler="PBS" jobtype="compute"/&gt;
       &lt;head-fs&gt;
             &lt;scratch&gt;
                   &lt;shared&gt;
                         &lt;file-server protocol="gsiftp" url="gridftp://example.com" \
                                                       mount-point="/nfs"\&gt;
                         &lt;internal-mount-point mount-point="/nfs"/&gt;
                   &lt;/shared&gt;
             &lt;/scratch&gt;
             &lt;storage&gt;
                   &lt;shared&gt;
                         &lt;file-server protocol="gsiftp" url="gridftp://example.com" \
                                                mount-point=""\&gt;
                         &lt;internal-mount-point mount-point=""/&gt;
                    &lt;/shared&gt;
             &lt;/storage&gt;
       &lt;/head-fs&gt;
       &lt;replica-catalog  type="LRC" url="rlsn://smarty.isi.edu" /&gt;
               
       <emphasis role="bold">&lt;!-- following profiles reqd for ec2 --&gt;
       
       &lt;profile namespace="env" key="PEGASUS_HOME"&gt;/usr/local/pegasus/default&lt;/profile&gt;
       
       &lt;!-- The directory where a user wants to run the jobs on the
            nodes retrived from ec2 --&gt;
       &lt;profile namespace="env" key="wntmp"&gt;/mnt&lt;/profile&gt;

       &lt;!-- Use condor style vanilla universe jobs --&gt;
       &lt;profile namespace="pegasus" key="style"&gt;condor&lt;/profile&gt;
       &lt;profile namespace="condor" key="universe"&gt;vanilla&lt;/profile&gt;

       &lt;!-- This ensures that Condor sends stdout and stderr back to the submit host --&gt;	
       &lt;profile namespace="condor" key="should_transfer_files"&gt;YES&lt;/profile&gt;
       &lt;profile namespace="condor" key="transfer_output"&gt;true&lt;/profile&gt;
       &lt;profile namespace="condor" key="transfer_error"&gt;true&lt;/profile&gt;
       &lt;profile namespace="condor" key="WhenToTransferOutput"&gt;ON_EXIT&lt;/profile&gt;

       &lt;!-- This ensures that the jobs generated by Pegasus will run on the remote workers --&gt;
       &lt;profile namespace="condor" key="requirements"&gt;(Arch==Arch)&amp;amp;&amp;amp;(Disk!=0)&amp;amp;&amp;amp;(Memory!=0)&amp;amp;&amp;amp;(OpSys==OpSys)&amp;amp;&amp;amp;(FileSystemDomain!="")&lt;/profile&gt;

       </emphasis>
 &lt;/site&gt;
</programlisting>
    </section>

    <section>
      <title>Using S3 for intermediate files</title>

      <para>This section will show you how to use S3 to store intermediate
      files. In this mode, Pegasus transfers workflow inputs from the input
      site to S3 if they are not already in S3. When a job runs, the inputs
      for that job are fetched from S3 to the worker node, the job is
      executed, then the output files are transferred from the worker node
      back to S3. This is similar to how second-level staging works (SLS),
      and, in fact, the current S3 implementation is an extension of
      SLS.</para>

      <para>In order to use S3 for your workflows, you need to first create a
      config file for the S3 transfer client, s3cmd. This config file needs to
      be installed on both the submit host, and the worker nodes. Save the
      file as 's3cmd.cfg'. This file will contain your Amazon security tokens,
      so you will want to set the permissions so that other users cannot read
      it (e.g. chmod 600 s3cmd.cfg). In the listing below, replace
      'YOUR_AMAZON_ACCESS_KEY_HERE' and 'YOUR_AMAZON_SECRET_KEY_HERE' with
      your Amazon access key and secret key.</para>

      <programlisting>[default]
access_key = YOUR_AMAZON_ACCESS_KEY_HERE
secret_key = YOUR_AMAZON_SECRET_KEY_HERE
acl_public = False
bucket_location = US
debug_syncmatch = False
default_mime_type = binary/octet-stream
delete_removed = False
dry_run = False
encrypt = False
force = False
gpg_command = /usr/bin/gpg
gpg_decrypt = %(gpg_command)s -d --verbose --no-use-agent --batch --yes --passphrase-fd %(passphrase_fd)s -o %(output_file)s %(input_file)s
gpg_encrypt = %(gpg_command)s -c --verbose --no-use-agent --batch --yes --passphrase-fd %(passphrase_fd)s -o %(output_file)s %(input_file)s
gpg_passphrase = 
guess_mime_type = False
host_base = s3.amazonaws.com
host_bucket = %(bucket)s.s3.amazonaws.com
human_readable_sizes = False
preserve_attrs = True
proxy_host = 
proxy_port = 0
recv_chunk = 4096
send_chunk = 4096
simpledb_host = sdb.amazonaws.com
use_https = False
verbosity = WARNING</programlisting>

      <para>Next, you need to modify your site catalog to tell s3cmd the path
      to your config file. You can do this by specifying an environment
      variable called S3CFG using an 'env' profile. This needs to be added to
      both the '<emphasis role="bold">local</emphasis>' site, and any
      execution sites where you want to use S3. This environment variable will
      be used by s3cmd in the S3 transfer jobs to locate the config
      file.</para>

      <programlisting>&lt;profile namespace="env" key="S3CFG"&gt;/local/path/to/s3cmd.cfg&lt;/profile&gt;</programlisting>

      <para>Next, you need to update your pegasus.properties file to tell
      Pegasus to turn on S3 mode. This will tell Pegasus to use S3 instead of
      rgular shared filesystem.</para>

      <programlisting>pegasus.data.configuration = S3</programlisting>

      <para><note>
          <para>The current S3 support does not work mixing storage solutions.
          Once S3 mode is turned on, Pegasus shared file system mode is turned
          off. For example, you currently can not use S3 and iRods storage at
          the same time.</para>
        </note></para>

      <para>Finally, if you want to use an existing bucket to store your data
      in S3 add this to your site catalog entry (replacing the existing
      workdirectory):</para>

      <programlisting>&lt;scratch&gt;
  &lt;shared&gt;
    &lt;file-server protocol="s3" url="s3://user@amazon" mount-point="/<emphasis
          role="bold">existing-bucket</emphasis>"/&gt;
    &lt;internal-mount-point mount-point="/<emphasis role="bold">existing-bucket</emphasis>"/&gt;
  &lt;/shared&gt;
&lt;/scratch&gt;
</programlisting>

      <para>Otherwise, you can have Pegasus create a new bucket for each
      workflow by adding this to your site catalog entry (again, replacing any
      existing workdirectory):</para>

      <programlisting>&lt;scratch&gt;
  &lt;shared&gt;
    &lt;file-server protocol="s3" url="s3://user@amazon" mount-point="/"/&gt;
    &lt;internal-mount-point mount-point="/"/&gt;
  &lt;/shared&gt;
&lt;/scratch&gt;</programlisting>
    </section>
  </section>
</chapter>
