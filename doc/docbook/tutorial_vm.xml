<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN"
"http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<chapter id="tutorial_vm">
  <title>Pegasus Tutorial Using Self-contained Virtual Machine</title>

  <para>These are the student notes for the Pegasus WMS tutorial on the
  Virtual Machine that can be downloaded from the Pegasus Website. They are
  designed to be used in conjunction with instructor presentation and
  support.</para>

  <para>You will see two styles of machine text here:</para>

  <programlisting><command>Text like this is input that you should type.</command><computeroutput>

Text like this is the output you should get.</computeroutput></programlisting>

  <para>For example:</para>

  <programlisting><command>$ date</command><computeroutput>
Wed Jun 24 14:47:59 PST 2011
</computeroutput></programlisting>

  <section>
    <title>Downloading and Running the VM using Virtual Box</title>

    <para>You will need to install Virtual Box to run the virtual machine on
    your computer. If you already have one of the tools installed, use that.
    Otherwise download the binary versions and install them from the<ulink
    url="http://www.virtualbox.org/wiki/Downloads"> Virtual Box
    Website</ulink> .</para>

    <para>The instructors have tested the image with Virtual Box 3.2.10</para>

    <section>
      <title>Download the VM for Virtual Box use</title>

      <para>Download the corresponding disk image.</para>

      <itemizedlist>
        <listitem>
          <para><ulink
          url="http://pegasus.isi.edu/wms/download/3.1/Pegasus-3.1.0-Debian-6-x86.vbox.zip"
          xml:base="">Virtual Box Pegasus Image</ulink></para>

          <para>It is around <emphasis role="bold">1.2 GB</emphasis> in size.
          We recommend using a command line tool like <emphasis
          role="bold">wget</emphasis> to download the image. Downloading the
          image using the browser sometimes corrupts the image.</para>

          <programlisting>$ <emphasis role="bold">wget http://pegasus.isi.edu/wms/download/4.0/Pegasus-4.0.0-Debian-6-x86.vbox.zip
</emphasis>
http://pegasus.isi.edu/wms/download/4.0/Pegasus-4.0.0-Debian-6-x86.vbox.zip
           =&gt; `Pegasus-4.0.0-Debian-6-x86.vbox.zip'
Resolving pegasus.isi.edu... 128.9.64.219
Connecting to pegasus.isi.edu|128.9.64.219|:80... connected.
HTTP request sent, awaiting response... 200 OK
Length: 1,336,554,492 (1.2G) [application/x-zip]
</programlisting>

          <para>The Image is zipped. You will need to unzip it.</para>

          <para>If you have unzip you can do this directly</para>

          <programlisting><emphasis role="bold">$ unzip Pegasus-4.0.0-Debian-6-x86.vbox.zip</emphasis></programlisting>

          <para>After unzipping a folder named<emphasis role="bold">
          Pegasus-4.0.0-Debian-6-x86.vbox</emphasis> will be created that has
          the vmdk files for the VM.</para>
        </listitem>
      </itemizedlist>
    </section>

    <section>
      <title>Running the VM with Virtual Box</title>

      <para>Launch Virtual Box on your machine. Follow the steps to add the
      vmdk file to Virtual Box and create a virtual machine inside the Virtual
      Box</para>

      <orderedlist>
        <listitem>
          <para>In the Menu, click File and select Virtual Media Manager (
          File &gt; Virtual Media Manager )</para>
        </listitem>

        <listitem>
          <para>The Virtual Media Manager Windows opens up.</para>
        </listitem>

        <listitem>
          <para>Click on "Add" button to add the<emphasis role="bold">
          Pegasus-4.0.0-Debian-6-x86.vbox/Debian-6-x86.vmdk</emphasis> file
          that you just downloaded and unzipped.</para>
        </listitem>

        <listitem>
          <para>You will now see the Debian-6-x86.vmdk in the list of hard
          disks with Actual size listed as around 3.0 GB</para>
        </listitem>

        <listitem>
          <para>Close the Window for the Virtual Media Manager</para>
        </listitem>
      </orderedlist>

      <para>We will now create a Virtual Machine in the Virtual Box.</para>

      <orderedlist>
        <listitem>
          <para>In the Menu, click Machine and select New ( Machine &gt; New
          )</para>
        </listitem>

        <listitem>
          <para>It will open the New Virtual Machine Wizard. Click
          Continue</para>
        </listitem>

        <listitem>
          <para>In the VM Name and OS Type Window specify the name as<emphasis
          role="bold"> PegasusVM-4.0.0</emphasis>.</para>
        </listitem>

        <listitem>
          <para>Select the <emphasis role="bold">Operating System as
          Linux</emphasis>and <emphasis role="bold">Version as
          Debian</emphasis>. Click Continue.</para>
        </listitem>

        <listitem>
          <para>Set the base memory to <emphasis role="bold">384 MB</emphasis>
          . It defaults to 512 MB. Click Continue</para>
        </listitem>

        <listitem>
          <para>We now select the Virtual Hard Disk to use with the machine.
          Select the option box for Use Existing Hard Disk. Select <emphasis
          role="bold">Debian-6-x86.vmdk</emphasis> from the list . Click
          Continue</para>
        </listitem>

        <listitem>
          <para>Click Done.</para>
        </listitem>

        <listitem>
          <para>Now in the Virtual Box , start the PegasusVM-4.0.0
          machine.</para>
        </listitem>
      </orderedlist>
    </section>
  </section>

  <section>
    <title>Mapping and Executing Workflows using Pegasus</title>

    <para>In this chapter you will be introduced to planning and executing a
    workflow through Pegasus WMS locally. You will then plan and execute a
    larger Montage workflow on the GRID.</para>

    <para>When the virtual machine starts , it will automatically log you in
    as user <emphasis role="bold">tutorial</emphasis> . The password for this
    account is <emphasis role="bold">pegasus</emphasis>.</para>

    <para>After logging on, start a terminal. There is a shortcut on the
    desktop for the terminal.</para>

    <programlisting><command>$ </command><emphasis role="bold">tutorial@pegasus-vm:$</emphasis><emphasis
        role="bold"> pwd</emphasis>

/home/tutorial
</programlisting>

    <para>In general, to run workflows on the Grid you will need to obtain
    Grid Credentials. The VM already has a user certificate installed for the
    pegasus user. To generate the proxy ( grid credentials ) run the <emphasis
    role="bold">grid-proxy-init</emphasis> command.</para>

    <programlisting><command>$ grid-proxy-init
</command><computeroutput>
Your identity: /O=edu/OU=ISI/OU=isi.edu/CN=Tutorial User
Creating proxy ...................................................... Done
Your proxy is valid until: Sat Feb 25 00:13:29 2012

</computeroutput></programlisting>

    <para>All the exercises in this Chapter will be run from the
    $HOME/pegasus-wms/ directory. All the files that are required reside in
    this directory</para>

    <programlisting><command>$ cd $HOME/pegasus-wms</command></programlisting>

    <para>Files for the exercise are stored in subdirectories:</para>

    <programlisting><command>$ ls</command><computeroutput><computeroutput>

config  dax</computeroutput></computeroutput></programlisting>

    <para>You may also see some other files here.</para>

    <section>
      <title>Creating a DIAMOND DAX</title>

      <para>We generate a 4 node diamond dax. There is a small piece of java
      code that uses the DAX API to generate the DAX. Open the file
      $HOME/pegasus-wms/dax/CreateDAX.java in a file editor:</para>

      <programlisting><command>$ vi dax/CreateDAX.java</command></programlisting>

      <para>There is a function Diamond( String site_handle, String
      pegasus_location ) that constructs the DAX. Towards the end of the
      function there is some commented out code.</para>

      <programlisting>// Add analyze job
//To be uncommented for exercise 2.1
    
        Job j4 = new Job("j4", "pegasus", "analyze", "4.0");
        j4.addArgument("-a analyze -T 60 -i ").addArgument(fc1);
        j4.addArgument(" ").addArgument(fc2);
        j4.addArgument("-o ").addArgument(fd);
        j4.uses(fc1, File.LINK.INPUT);
        j4.uses(fc2, File.LINK.INPUT);
        j4.uses(fd, File.LINK.OUTPUT);
        
        //add job to the DAX
        dax.addJob(j4);

        //analyze job is a child to the findrange jobs
        dax.addDependency("j2", "j4");
        dax.addDependency("j3", "j4");
    
//End of commented out code for Exercise 2.1
</programlisting>

      <para>The above snippet of code, adds a job with the ID0000004 to the
      DAX. It illustrates how to specify</para>

      <orderedlist>
        <listitem>
          <para>the arguments for the job</para>
        </listitem>

        <listitem>
          <para>the logical files used by the job</para>
        </listitem>

        <listitem>
          <para>the dependencies to other jobs</para>
        </listitem>

        <listitem>
          <para>adding the job to the dax</para>
        </listitem>
      </orderedlist>

      <para>After uncommenting the code, compile and run the CreateDAX
      program.</para>

      <programlisting><emphasis role="bold">$ cd dax

$ javac -classpath `pegasus-config --classpath` CreateDAX.java

$ java -classpath .:`pegasus-config --classpath` CreateDAX local /opt/pegasus/default ./diamond.dax</emphasis></programlisting>

      <para>Let us view the generated diamond.dax.</para>

      <programlisting><emphasis role="bold">$ cat diamond.dax</emphasis></programlisting>

      <para>Inside the DAX, you should see three sections.</para>

      <orderedlist>
        <listitem>
          <para>list of input file locations</para>
        </listitem>

        <listitem>
          <para>list of executable locations</para>
        </listitem>

        <listitem>
          <para>definition of all jobs - each job in the workflow. 4 jobs in
          total.</para>
        </listitem>

        <listitem>
          <para>list of control-flow dependencies - this section specifies a
          partial order in which jobs are to executed.</para>
        </listitem>
      </orderedlist>
    </section>

    <section>
      <title>Replica Catalog</title>

      <para>First lets change to the tutorial base directory.<programlisting><command>$ cd $HOME/pegasus-wms</command></programlisting></para>

      <para>In this exercise you will insert entries into the Replica Catalog.
      The replica catalog that we will use today is a simple file based
      catalog. We also support and recommend the following for production
      runs</para>

      <itemizedlist>
        <listitem>
          <para>Globus RLS</para>
        </listitem>

        <listitem>
          <para>JDBC implementation</para>
        </listitem>
      </itemizedlist>

      <para>A Replica Catalog maintains the LFN to PFN mapping for the input
      files of your workflow. Pegasus queries it to determine the locations of
      the raw input data files required by the workflow. Additionally, all the
      materialized data is registered into Replica Catalog for data reuse
      later on.</para>

      <section>
        <title>Pre Populated Replica Catalog</title>

        <para>The instructors have provided a File based Replica Catalog
        configured for the tutorial exercises. The file is inside the config
        directory.</para>

        <itemizedlist>
          <listitem>
            <para>Let us see what the file looks like. <programlisting><command>$ cat config/rc.data</command><computeroutput>

f.a
     gsiftp://pegasus-vm/scratch/tutorial/inputdata/diamond/f.a
          pool="local"
big_region_20120223_151925_9953.hdr
     gsiftp://pegasus-vm/scratch/tutorial/inputdata/0.2degree/big_region.hdr
          pool="local"
cimages_20120223_151925_9953.tbl
     gsiftp://pegasus-vm/scratch/tutorial/inputdata/0.2degree/cimages.tbl
          pool="local"
pimages_20120223_151925_9953.tbl
     gsiftp://pegasus-vm/scratch/tutorial/inputdata/0.2degree/pimages.tbl
          pool="local"
region_20120223_151925_9953.hdr
     gsiftp://pegasus-vm/scratch/tutorial/inputdata/0.2degree/region.hdr
          pool="local"
rimages_20120223_151925_9953.tbl
     gsiftp://pegasus-vm/scratch/tutorial/inputdata/0.2degree/rimages.tbl
          pool="local"
statfile_20120223_151925_9953.tbl
     gsiftp://pegasus-vm/scratch/tutorial/inputdata/0.2degree/statfile.tbl
          pool="local"
2mass-atlas-990502s-j1350080.fits
     gsiftp://pegasus-vm/scratch/tutorial/inputdata/0.2degree/2mass-atlas-990502s-j1350080.fits
          pool="local"
2mass-atlas-990502s-j1350092.fits
     gsiftp://pegasus-vm/scratch/tutorial/inputdata/0.2degree/2mass-atlas-990502s-j1350092.fits
          pool="local"
2mass-atlas-990502s-j1420186.fits
     gsiftp://pegasus-vm/scratch/tutorial/inputdata/0.2degree/2mass-atlas-990502s-j1420186.fits
          pool="local"
2mass-atlas-990502s-j1420198.fits
     gsiftp://pegasus-vm/scratch/tutorial/inputdata/0.2degree/2mass-atlas-990502s-j1420198.fits
          pool="local"
2mass-atlas-990502s-j1430080.fits
     gsiftp://pegasus-vm/scratch/tutorial/inputdata/0.2degree/2mass-atlas-990502s-j1430080.fits
          pool="local"
2mass-atlas-990502s-j1430092.fits
     gsiftp://pegasus-vm/scratch/tutorial/inputdata/0.2degree/2mass-atlas-990502s-j1430092.fits
          pool="local"
2mass-atlas-990502s-j1440186.fits
     gsiftp://pegasus-vm/scratch/tutorial/inputdata/0.2degree/2mass-atlas-990502s-j1440186.fits
          pool="local"
2mass-atlas-990502s-j1440198.fits
     gsiftp://pegasus-vm/scratch/tutorial/inputdata/0.2degree/2mass-atlas-990502s-j1440198.fits
          pool="local"

</computeroutput></programlisting></para>
          </listitem>
        </itemizedlist>
      </section>

      <section>
        <title>pegasus-rc-client ( Optional Exercise )</title>

        <para>You can use the <emphasis role="bold">
        pegasus-rc-client</emphasis> command to insert , query and delete from
        the replica catalog.</para>

        <para>Before executing any of the pegasus-rc-client exercises lets us
        remove the pre populated replica catalog.</para>

        <programlisting><emphasis role="bold">$ rm $HOME/pegasus-wms/config/rc.data</emphasis></programlisting>

        <para>To execute the diamond dax created in <emphasis
        role="bold">exercise 2.1</emphasis>, we will need to register input
        file f.a in the replica catalog. The file f.a resides at
        /scratch/tutorial/inputdata/diamond/f.a . Let us insert a single entry
        into the replica catalog.</para>

        <programlisting><command>$  pegasus-rc-client  insert f.a \
          gsiftp://pegasus-vm/scratch/tutorial/inputdata/diamond/f.a pool=local</command></programlisting>

        <para>Let us know verify if f.a has been registered successfully by
        querying the replica catalog using pegasus-rc-client</para>

        <programlisting><emphasis role="bold">$ pegasus-rc-client  lookup f.a</emphasis>

f.a gsiftp://pegasus-vm/scratch/tutorial/inputdata/diamond/f.a pool="local"</programlisting>

        <para>The <emphasis role="bold">pegasus-rc-client</emphasis> also
        allows for bulk insertion of entries. We will be inserting the entries
        for montage workflow using the bulk mode. The input data to be used
        for the montage workflow resides in the
        /scratch/tutorial/inputdata/0.2degree directory. We are going to
        insert entries into the replica catalog that point to the files in
        this directory.</para>

        <para>The instructors have provided:</para>

        <itemizedlist>
          <listitem>
            <para>A file replicas.in, the input data file for the
            pegasus-rc-client that contains the mappings that need to be
            populated in the Replica Catalog. The file is inside the config
            directory</para>
          </listitem>

          <listitem>
            <para>Let us see what the file looks like.</para>

            <programlisting><command>$ cat config/rc.in</command><computeroutput>

big_region_20120223_151925_9953.hdr
     gsiftp://pegasus-vm/scratch/tutorial/inputdata/0.2degree/big_region.hdr
          pool="local"
cimages_20120223_151925_9953.tbl
     gsiftp://pegasus-vm/scratch/tutorial/inputdata/0.2degree/cimages.tbl
          pool="local"
pimages_20120223_151925_9953.tbl
     gsiftp://pegasus-vm/scratch/tutorial/inputdata/0.2degree/pimages.tbl
          pool="local"
region_20120223_151925_9953.hdr
     gsiftp://pegasus-vm/scratch/tutorial/inputdata/0.2degree/region.hdr
          pool="local"
rimages_20120223_151925_9953.tbl
     gsiftp://pegasus-vm/scratch/tutorial/inputdata/0.2degree/rimages.tbl
          pool="local"
statfile_20120223_151925_9953.tbl
     gsiftp://pegasus-vm/scratch/tutorial/inputdata/0.2degree/statfile.tbl
          pool="local"
2mass-atlas-990502s-j1350080.fits
     gsiftp://pegasus-vm/scratch/tutorial/inputdata/0.2degree/2mass-atlas-990502s-j1350080.fits
          pool="local"
2mass-atlas-990502s-j1350092.fits
     gsiftp://pegasus-vm/scratch/tutorial/inputdata/0.2degree/2mass-atlas-990502s-j1350092.fits
          pool="local"
2mass-atlas-990502s-j1420186.fits
     gsiftp://pegasus-vm/scratch/tutorial/inputdata/0.2degree/2mass-atlas-990502s-j1420186.fits
          pool="local"
2mass-atlas-990502s-j1420198.fits
     gsiftp://pegasus-vm/scratch/tutorial/inputdata/0.2degree/2mass-atlas-990502s-j1420198.fits
          pool="local"
2mass-atlas-990502s-j1430080.fits
     gsiftp://pegasus-vm/scratch/tutorial/inputdata/0.2degree/2mass-atlas-990502s-j1430080.fits
          pool="local"
2mass-atlas-990502s-j1430092.fits
     gsiftp://pegasus-vm/scratch/tutorial/inputdata/0.2degree/2mass-atlas-990502s-j1430092.fits
          pool="local"
2mass-atlas-990502s-j1440186.fits
     gsiftp://pegasus-vm/scratch/tutorial/inputdata/0.2degree/2mass-atlas-990502s-j1440186.fits
          pool="local"
2mass-atlas-990502s-j1440198.fits
     gsiftp://pegasus-vm/scratch/tutorial/inputdata/0.2degree/2mass-atlas-990502s-j1440198.fits
          pool="local"</computeroutput></programlisting>
          </listitem>

          <listitem>
            <para>Now we are ready to run rc-client and populate the data.
            Since each of you have an individual file replica catalog, all the
            10 entries should be successfully registered.</para>

            <programlisting><command>$ pegasus-rc-client --insert config/rc.in</command><computeroutput><computeroutput>

#Successfully worked on : 14 lines</computeroutput>
#Worked on total number of : 14 lines.</computeroutput></programlisting>
          </listitem>

          <listitem>
            <para>Now the entries have been successfully inserted into the
            Replica Catalog. We should query the replica catalog for a
            particular lfn.</para>

            <programlisting><command>$ pegasus-rc-client lookup pimages_20120223_151925_9953.tbl</command><computeroutput><computeroutput>

pimages_20120223_151925_9953.tbl
     gsiftp://pegasus-vm/scratch/tutorial/inputdata/0.2degree/pimages.tbl 
           pool="local"</computeroutput></computeroutput></programlisting>
          </listitem>
        </itemizedlist>
      </section>
    </section>

    <section>
      <title>The Site Catalog</title>

      <para>The site catalog contains information about the layout of your
      grid where you want to run your workflows. For each site following
      information is maintained</para>

      <itemizedlist>
        <listitem>
          <para>grid gateways</para>
        </listitem>

        <listitem>
          <para>head node filesystem</para>
        </listitem>

        <listitem>
          <para>worker node filesystem</para>
        </listitem>

        <listitem>
          <para>scratch and shared file systems on the head nodes and worker
          nodes</para>
        </listitem>

        <listitem>
          <para>replica catalog URL for the site</para>
        </listitem>

        <listitem>
          <para>site wide information like environment variables to be set
          when a job is run.</para>
        </listitem>
      </itemizedlist>

      <section>
        <title>Pre Populated Site Catalog</title>

        <para>The instructors have provided a pre-populated site catalog for
        use in the tutorial in $HOME/pegasus-wms/config directory.</para>

        <para>Lets see the site catalog for the Pegasus VM. It refers to two
        sites <emphasis role="bold">local</emphasis> and <emphasis
        role="bold">cluster</emphasis> .</para>

        <programlisting><command>$ cat $HOME/pegasus-wms/config/sites.xml3</command><computeroutput>

&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;sitecatalog xmlns="http://pegasus.isi.edu/schema/sitecatalog" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://pegasus.isi.edu/schema/sitecatalog http://pegasus.isi.edu/schema/sc-3.0.xsd" version="3.0"&gt;
     &lt;site handle="cluster" arch="x86" os="LINUX" osrelease="" osversion="" glibc=""&gt;
          &lt;grid type="gt2" contact="pegasus-vm/jobmanager-fork" scheduler="Fork" jobtype="auxillary"/&gt;
          &lt;grid type="gt2" contact="pegasus-vm/jobmanager-condor" scheduler="Condor" jobtype="compute"/&gt;
          &lt;head-fs&gt;
               &lt;scratch&gt;
                    &lt;shared&gt;
                         &lt;file-server protocol="gsiftp" url="gsiftp://pegasus-vm" mount-point="/home/tutorial/cluster-scratch"/&gt;
                         &lt;internal-mount-point mount-point="/home/tutorial/cluster-scratch"/&gt;
                    &lt;/shared&gt;
               &lt;/scratch&gt;
               &lt;storage&gt;
                    &lt;shared&gt;
                         &lt;file-server protocol="gsiftp" url="gsiftp://pegasus-vm" mount-point="/home/tutorial/cluster-storage"/&gt;
                         &lt;internal-mount-point mount-point="/home/tutorial/cluster-storage"/&gt;
                    &lt;/shared&gt;
               &lt;/storage&gt;
          &lt;/head-fs&gt;
          &lt;replica-catalog type="LRC" url="rlsn://localhost"/&gt;
          &lt;profile namespace="env" key="GLOBUS_LOCATION" &gt;/opt/globus/default&lt;/profile&gt;
          &lt;profile namespace="env" key="MONTAGE_BIN" &gt;.&lt;/profile&gt;
          &lt;profile namespace="env" key="JAVA_HOME" &gt;/usr&lt;/profile&gt;
          &lt;profile namespace="env" key="LD_LIBRARY_PATH" &gt;/opt/globus/default/lib&lt;/profile&gt;
          &lt;profile namespace="env" key="PEGASUS_HOME" &gt;/opt/pegasus/default&lt;/profile&gt;
          &lt;profile namespace="pegasus" key="clusters.num" &gt;1&lt;/profile&gt;
          &lt;profile namespace="pegasus" key="stagein.clusters" &gt;1&lt;/profile&gt;
     &lt;/site&gt;
     &lt;site handle="condor-pool" arch="x86" os="LINUX"&gt;
          &lt;head-fs&gt;
                  &lt;scratch /&gt;
                  &lt;storage /&gt;
          &lt;/head-fs&gt;
          &lt;profile namespace="pegasus" key="style"&gt;condor&lt;/profile&gt;
          &lt;profile namespace="condor" key="universe"&gt;vanilla&lt;/profile&gt;
          &lt;profile namespace="env" key="MONTAGE_BIN"&gt;.&lt;/profile&gt;
     &lt;/site&gt;
     &lt;site handle="local" arch="x86" os="LINUX" osrelease="" osversion="" glibc=""&gt;
          &lt;grid type="gt2" contact="localhost/jobmanager-fork" scheduler="Fork" jobtype="auxillary"/&gt;
          &lt;grid type="gt2" contact="localhost/jobmanager-fork" scheduler="Fork" jobtype="compute"/&gt;
          &lt;head-fs&gt;
               &lt;scratch&gt;
                    &lt;shared&gt;
                         &lt;file-server protocol="gsiftp" url="file://" mount-point="/home/tutorial/local-scratch"/&gt;
                         &lt;internal-mount-point mount-point="/home/tutorial/local-scratch"/&gt;
                    &lt;/shared&gt;
               &lt;/scratch&gt;
               &lt;storage&gt;
                    &lt;shared&gt;
                         &lt;file-server protocol="gsiftp" url="file://" mount-point="/home/tutorial/local-storage"/&gt;
                         &lt;internal-mount-point mount-point="/home/tutorial/local-storage"/&gt;
                    &lt;/shared&gt;
               &lt;/storage&gt;
          &lt;/head-fs&gt;
          &lt;replica-catalog type="LRC" url="rlsn://localhost"/&gt;
          &lt;profile namespace="env" key="GLOBUS_LOCATION" &gt;/opt/globus/default&lt;/profile&gt;
          &lt;profile namespace="env" key="JAVA_HOME" &gt;/usr&lt;/profile&gt;
          &lt;profile namespace="env" key="LD_LIBRARY_PATH" &gt;/opt/globus/default/lib&lt;/profile&gt;
          &lt;profile namespace="env" key="PEGASUS_HOME" &gt;/opt/pegasus/default&lt;/profile&gt;
     &lt;/site&gt;
&lt;/sitecatalog&gt;
 
</computeroutput></programlisting>
      </section>

      <section>
        <title>Generating a Site Catalog for OSG</title>

        <para>The client pegasus-sc-client can be used to generate a site
        catalog and transformation catalog for the Open Science Grid.</para>

        <programlisting><command>$ </command><emphasis role="bold"><computeroutput>pegasus-sc-client --vo engage --sc ./engage-osg-sc.xml \
  --source OSGMM --grid OSG -vvvv
</computeroutput></emphasis><computeroutput>
2012.02.24 12:19:04.815 PST: [INFO]  Adding site GridUNESP_CENTRAL 
2012.02.24 12:19:04.847 PST: [INFO]  Adding site GRASE-CSE-MAGIC 
2012.02.24 12:19:04.848 PST: [INFO]  Adding site Purdue-Rossmann 
2012.02.24 12:19:04.848 PST: [INFO]  Adding site UConn-OSG 
2012.02.24 12:19:04.849 PST: [INFO]  Adding site USCMS-FNAL-WC1 
2012.02.24 12:19:04.849 PST: [INFO]  Adding site FNAL_FERMIGRID 
2012.02.24 12:19:04.850 PST: [INFO]  Adding site LIGO_UWM_NEMO 
2012.02.24 12:19:04.850 PST: [INFO]  Adding site AGLT2 
2012.02.24 12:19:04.851 PST: [INFO]  Adding site SPRACE 
2012.02.24 12:19:04.851 PST: [INFO]  Adding site UCR-HEP 
2012.02.24 12:19:04.853 PST: [INFO]  Adding site UMissHEP 
2012.02.24 12:19:04.853 PST: [INFO]  Adding site Purdue-Steele 
2012.02.24 12:19:04.853 PST: [INFO]  Adding site MWT2 
2012.02.24 12:19:04.854 PST: [INFO]  Adding site RENCI-Blueridge 
2012.02.24 12:19:04.854 PST: [INFO]  Adding site CIT_CMS_T2 
2012.02.24 12:19:04.855 PST: [INFO]  Adding site Firefly 
2012.02.24 12:19:04.855 PST: [INFO]  Adding site Purdue-RCAC 
2012.02.24 12:19:04.857 PST: [INFO]  Adding site CIT_CMS_T2__1 
2012.02.24 12:19:04.857 PST: [INFO]  Adding site FNAL_GPGRID_1 
2012.02.24 12:19:04.857 PST: [INFO]  Site LOCAL . Creating default entry 
2012.02.24 12:19:04.863 PST: [INFO]  Loaded 21 sites  
2012.02.24 12:19:04.864 PST:   Writing out site catalog to /home/tutorial/pegasus-wms/./engage-osg-sc.xml 
2012.02.24 12:19:05.141 PST:   Number of SRM Properties retrieved 10 
2012.02.24 12:19:05.154 PST:   Writing out properties to /home/tutorial/pegasus-wms/./pegasus.2927071066946861892.properties 
2012.02.24 12:19:05.155 PST: [INFO]  Time taken to execute is 0.869 seconds 
2012.02.24 12:19:05.155 PST: [INFO] event.pegasus.planner planner.version 4.0.0  - FINISHED
</computeroutput></programlisting>
      </section>
    </section>

    <section>
      <title>Transformation Catalog</title>

      <para>The transformation catalog maintains information about where the
      application code resides on the grid. It also provides additional
      information about the transformation as to what system they are compiled
      for, what profiles or environment variables need to be set when the
      transformation is invoked etc.</para>

      <section>
        <title>Pre Populated Transformation Catalog</title>

        <para>The instructors have provided a ready transformation catalog
        (tc.data.text) in the $HOME/pegasus-wms/config directory</para>

        <para>In our case, it contains the locations where the Diamond or
        Montage code is installed in the Pegasus VM. Let us see the
        Transformation Catalog</para>

        <para>For each transformation the following information is
        captured</para>

        <orderedlist>
          <listitem>
            <para>tr - A transformation identifier. (Normally a
            Namespace::Name:Version.. The Namespace and Version are
            optional.)</para>
          </listitem>

          <listitem>
            <para>pfn - URL or file path for the location of the executable.
            The pfn is a file path if the transformation is of type INSTALLED
            and generally a url (file:/// or http:// or gridftp://) if of type
            STAGEABLE</para>
          </listitem>

          <listitem>
            <para>site - The site identifier for the site where the
            transformation is available</para>
          </listitem>

          <listitem>
            <para>type - The type of transformation. Whether it is Iinstalled
            ("INSTALLED") on the remote site or is availabe to stage
            ("STAGEABLE").</para>
          </listitem>

          <listitem>
            <para>arch os, osrelease, osversion - The
            arch/os/osrelease/osversion of the transformation. osrelease and
            osversion are optional.</para>

            <para>ARCH can have one of the following values x86, x86_64,
            sparcv7, sparcv9, ppc, aix. The default value for arch is
            x86</para>

            <para>OS can have one of the following values linux,sunos,macosx.
            The default value for OS if none specified is linux</para>
          </listitem>

          <listitem>
            <para>Profiles - One or many profiles can be attached to a
            transformation for all sites or to a transformation on a
            particular site.</para>
          </listitem>
        </orderedlist>

        <programlisting><emphasis role="bold">$ cat $HOME/pegasus-wms/config/tc.data.text</emphasis>

tr mDiff {
     site local {
          profile env "MONTAGE_HOME" "." 
          pfn "gsiftp://pegasus-vm/scratch/tutorial/software/montage/3.3/x86/bin/mDiff"
          arch "x86"
          os "LINUX"
          type "STAGEABLE"
     }
}

tr mFitplane {
     site local {
          pfn "gsiftp://pegasus-vm/scratch/tutorial/software/montage/3.3/x86/bin/mFitplane"
          arch "x86"
          os "LINUX"
          type "STAGEABLE"
     }
}

tr condor::dagman {
     site local {
          pfn "/usr/bin/condor_dagman"
          arch "x86"
          os "LINUX"
          type "INSTALLED"
     }
}

tr diamond::analyze:2.0 {
     site local {
          pfn "/opt/pegasus/default/bin/pegasus-keg"
          arch "x86"
          os "LINUX"
          type "INSTALLED"
     }
}

tr diamond::findrange:2.0 {
     site local {
          pfn "/opt/pegasus/default/bin/pegasus-keg"
          arch "x86"
          os "LINUX"
          type "INSTALLED"
     }
}

tr diamond::preprocess:2.0 {
     site local {
          pfn "/opt/pegasus/default/bin/pegasus-keg"
          arch "x86"
          os "LINUX"
          type "INSTALLED"
     }
}

tr mAdd:3.3 {
     site local {
          pfn "gsiftp://pegasus-vm/scratch/tutorial/software/montage/3.3/x86/bin/mAdd"
          arch "x86"
          os "LINUX"
          type "STAGEABLE"
     }
}

tr mBackground:3.3 {
     site local {
          pfn "gsiftp://pegasus-vm/scratch/tutorial/software/montage/3.3/x86/bin/mBackground"
          arch "x86"
          os "LINUX"
          type "STAGEABLE"
     }
}

tr mBgModel:3.3 {
     site local {
          pfn "gsiftp://pegasus-vm/scratch/tutorial/software/montage/3.3/x86/bin/mBgModel"
          arch "x86"
          os "LINUX"
          type "STAGEABLE"
     }
}

tr mConcatFit:3.3 {
     site local {
          pfn "gsiftp://pegasus-vm/scratch/tutorial/software/montage/3.3/x86/bin/mConcatFit"
          arch "x86"
          os "LINUX"
          type "STAGEABLE"
     }
}

tr mDiffFit:3.3 {
     site local {
          profile env "MONTAGE_HOME" "." 
          pfn "gsiftp://pegasus-vm/scratch/tutorial/software/montage/3.3/x86/bin/mDiffFit"
          arch "x86"
          os "LINUX"
          type "STAGEABLE"
     }
}

tr mImgtbl:3.3 {
     site local {
          pfn "gsiftp://pegasus-vm/scratch/tutorial/software/montage/3.3/x86/bin/mImgtbl"
          arch "x86"
          os "LINUX"
          type "STAGEABLE"
     }
}

tr mJPEG:3.3 {
     site local {
          pfn "gsiftp://pegasus-vm/scratch/tutorial/software/montage/3.3/x86/bin/mJPEG"
          arch "x86"
          os "LINUX"
          type "STAGEABLE"
     }
}

tr mProjectPP:3.3 {
     site local {
          profile condor "priority" "25" 
          pfn "gsiftp://pegasus-vm/scratch/tutorial/software/montage/3.3/x86/bin/mProjectPP"
          arch "x86"
          os "LINUX"
          type "STAGEABLE"
     }
}

tr mShrink:3.3 {
     site local {
          pfn "gsiftp://pegasus-vm/scratch/tutorial/software/montage/3.3/x86/bin/mShrink"
          arch "x86"
          os "LINUX"
          type "STAGEABLE"
     }
}

</programlisting>
      </section>

      <section>
        <title>pegasus-tc-client ( Optional )</title>

        <para>We will use the <emphasis
        role="bold">pegasus-tc-client</emphasis> to add the entry for the
        transformation dummy into the transformation catalog.</para>

        <itemizedlist>
          <listitem>
            <para></para>

            <programlisting><emphasis role="bold">$ pegasus-tc-client  -a -l diamond::dummy:2.0 \
      -p /opt/pegasus/default/bin/pegasus-keg -r local -t INSTALLED -s x86::LINUX
<emphasis>
2011.08.04 12:03:12.555 PDT:   Added tc entry sucessfully
</emphasis></emphasis></programlisting>

            <para>Let us try and query for the entry we inserted.</para>

            <programlisting><emphasis role="bold">$ pegasus-tc-client  -q -P -l diamond::dummy:2.0
</emphasis>
tr diamond::dummy:2.0 {
        site local {
                pfn "/opt/pegasus/default/bin/pegasus-keg"
                arch "x86"
                os "LINUX"
                type "INSTALLED"
        }
}</programlisting>

            <para>Let us try and query the transformation catalog for all the
            entries in it. Let us see what our transformation catalog looks
            like</para>

            <programlisting><command>$ pegasus-tc-client  -q -B</command><computeroutput>

tr condor::dagman {
     site local {
          pfn "/usr/bin/condor_dagman"
          arch "x86"
          os "LINUX"
          type "INSTALLED"
     }
}

tr diamond::analyze:2.0 {
     site local {
          pfn "/opt/pegasus/default/bin/pegasus-keg"
          arch "x86"
          os "LINUX"
          type "INSTALLED"
     }
}

tr diamond::dummy:2.0 {
     site local {
          pfn "/opt/pegasus/default/bin/pegasus-keg"
          arch "x86"
          os "LINUX"
          type "INSTALLED"
     }
}

tr diamond::findrange:2.0 {
     site local {
          pfn "/opt/pegasus/default/bin/pegasus-keg"
          arch "x86"
          os "LINUX"
          type "INSTALLED"
     }
}

tr diamond::preprocess:2.0 {
     site local {
          pfn "/opt/pegasus/default/bin/pegasus-keg"
          arch "x86"
          os "LINUX"
          type "INSTALLED"
     }
}

tr mAdd:3.3 {
     site local {
          pfn "gsiftp://pegasus-vm/scratch/tutorial/software/montage/3.3/x86/bin/mAdd"
          arch "x86"
          os "LINUX"
          type "STAGEABLE"
     }
}

tr mBackground:3.3 {
     site local {
          pfn "gsiftp://pegasus-vm/scratch/tutorial/software/montage/3.3/x86/bin/mBackground"
          arch "x86"
          os "LINUX"
          type "STAGEABLE"
     }
}

tr mBgModel:3.3 {
     site local {
          pfn "gsiftp://pegasus-vm/scratch/tutorial/software/montage/3.3/x86/bin/mBgModel"
          arch "x86"
          os "LINUX"
          type "STAGEABLE"
     }
}

tr mConcatFit:3.3 {
     site local {
          pfn "gsiftp://pegasus-vm/scratch/tutorial/software/montage/3.3/x86/bin/mConcatFit"
          arch "x86"
          os "LINUX"
          type "STAGEABLE"
     }
}

tr mDiff {
     site local {
          profile env "MONTAGE_HOME" "." 
          pfn "gsiftp://pegasus-vm/scratch/tutorial/software/montage/3.3/x86/bin/mDiff"
          arch "x86"
          os "LINUX"
          type "STAGEABLE"
     }
}

tr mDiffFit:3.3 {
     site local {
          profile env "MONTAGE_HOME" "." 
          pfn "gsiftp://pegasus-vm/scratch/tutorial/software/montage/3.3/x86/bin/mDiffFit"
          arch "x86"
          os "LINUX"
          type "STAGEABLE"
     }
}

tr mFitplane {
     site local {
          pfn "gsiftp://pegasus-vm/scratch/tutorial/software/montage/3.3/x86/bin/mFitplane"
          arch "x86"
          os "LINUX"
          type "STAGEABLE"
     }
}

tr mImgtbl:3.3 {
     site local {
          pfn "gsiftp://pegasus-vm/scratch/tutorial/software/montage/3.3/x86/bin/mImgtbl"
          arch "x86"
          os "LINUX"
          type "STAGEABLE"
     }
}

tr mJPEG:3.3 {
     site local {
          pfn "gsiftp://pegasus-vm/scratch/tutorial/software/montage/3.3/x86/bin/mJPEG"
          arch "x86"
          os "LINUX"
          type "STAGEABLE"
     }
}

tr mProjectPP:3.3 {
     site local {
          profile condor "priority" "25" 
          pfn "gsiftp://pegasus-vm/scratch/tutorial/software/montage/3.3/x86/bin/mProjectPP"
          arch "x86"
          os "LINUX"
          type "STAGEABLE"
     }
}

tr mShrink:3.3 {
     site local {
          pfn "gsiftp://pegasus-vm/scratch/tutorial/software/montage/3.3/x86/bin/mShrink"
          arch "x86"
          os "LINUX"
          type "STAGEABLE"
     }
}</computeroutput></programlisting>
          </listitem>
        </itemizedlist>
      </section>
    </section>

    <section>
      <title>Properties</title>

      <para>Pegasus Workflow Planner is configured via the use of java
      properties. The instructors have provided a ready properties file at
      $HOME/.pegasusrc .</para>

      <programlisting><command>$ cat $HOME/.pegasusrc</command><computeroutput><computeroutput>
 
##########################
# PEGASUS USER PROPERTIES 
##########################

## SELECT THE REPLICAT CATALOG MODE AND URL
pegasus.catalog.replica = File
pegasus.catalog.replica.file = ${user.home}/pegasus-wms/config/rc.data


## SELECT THE SITE CATALOG MODE AND FILE
pegasus.catalog.site = XML3
pegasus.catalog.site.file = ${user.home}/pegasus-wms/config/sites.xml3


## SELECT THE TRANSFORMATION CATALOG MODE AND FILE
pegasus.catalog.transformation = Text
pegasus.catalog.transformation.file = ${user.home}/pegasus-wms/config/tc.data.text

## USE DAGMAN RETRY FEATURE FOR FAILURES
dagman.retry=2

## CHECK JOB EXIT CODES FOR FAILURE
dagman.post.scope=all

## STAGE ALL OUR EXECUTABLES OR USE INSTALLED ONES 
pegasus.catalog.transformation.mapper = All

## WORK AND STORAGE DIR  
pegasus.dir.storage = storage
pegasus.dir.exec = exec

#JOB CATEGORIES
dagman.projection.maxjobs 2</computeroutput></computeroutput></programlisting>
    </section>

    <section>
      <title>Planning and Running Workflows Locally</title>

      <para>In this exercise we are going to run pegasus-plan to generate a
      executable workflow from the abstract workflow (diamond.dax). The
      Executable workflow contains condor submit files that are submitted
      locally using pegasus-run</para>

      <para>The instructors have provided: <itemizedlist>
          <listitem>
            <para>A dax (diamond.dax) in the $HOME/pegasus-wms/dax
            directory.</para>
          </listitem>
        </itemizedlist></para>

      <para>You will need to write some things yourself, by following the
      instructions below: <itemizedlist>
          <listitem>
            <para>Run pegasus-plan to generate the condor submit files out of
            the dax.</para>
          </listitem>

          <listitem>
            <para>Run pegasus-run to submit the workflow locally.</para>
          </listitem>
        </itemizedlist></para>

      <para>Instructions:</para>

      <itemizedlist>
        <listitem>
          <para>Let us run pegasus-plan on the diamond dax. <programlisting><command>$ cd ~/pegasus-wms

$ pegasus-plan --dax `pwd`/dax/diamond.dax --force \
               --dir dags -s local -o local --nocleanup -v</command></programlisting>
          The above command says that we need to plan the diamond dax locally.
          The condor submit files are to be generated in a directory structure
          whose base is dags. We also are requesting that no cleanup jobs be
          added as we require the intermediate data to be saved. Here is the
          output of pegasus-plan. <programlisting><computeroutput>2012.02.24 12:21:07.605 PST: [INFO] event.pegasus.parse.dax dax.id /home/tutorial/pegasus-wms/dax/diamond.dax  - STARTED 
2012.02.24 12:21:07.658 PST: [INFO]  Generating Stampede Events for Abstract Workflow 
2012.02.24 12:21:07.693 PST: [INFO]  Generating Stampede Events for Abstract Workflow -DONE 
2012.02.24 12:21:07.694 PST: [INFO] event.pegasus.refinement dax.id blackdiamond_0  - STARTED 
2012.02.24 12:21:07.707 PST: [INFO] event.pegasus.siteselection dax.id blackdiamond_0  - STARTED 
2012.02.24 12:21:07.719 PST: [INFO] event.pegasus.siteselection dax.id blackdiamond_0  - FINISHED 
2012.02.24 12:21:07.739 PST: [INFO]  Grafting transfer nodes in the workflow 
2012.02.24 12:21:07.739 PST: [INFO] event.pegasus.generate.transfer-nodes dax.id blackdiamond_0  - STARTED 
2012.02.24 12:21:07.765 PST: [INFO] event.pegasus.generate.transfer-nodes dax.id blackdiamond_0  - FINISHED 
2012.02.24 12:21:07.766 PST: [INFO] event.pegasus.generate.workdir-nodes dax.id blackdiamond_0  - STARTED 
2012.02.24 12:21:07.771 PST: [INFO] event.pegasus.generate.workdir-nodes dax.id blackdiamond_0  - FINISHED 
2012.02.24 12:21:07.771 PST: [INFO] event.pegasus.refinement dax.id blackdiamond_0  - FINISHED 
2012.02.24 12:21:07.798 PST: [INFO]  Generating codes for the concrete workflow 
2012.02.24 12:21:08.026 PST: [INFO]  Generating codes for the concrete workflow -DONE 
2012.02.24 12:21:08.026 PST:   


I have concretized your abstract workflow. The workflow has been entered 
into the workflow database with a state of "planned". The next step is 
to start or execute your workflow. The invocation required is


pegasus-run  /home/tutorial/pegasus-wms/dags/tutorial/pegasus/blackdiamond/run0001

 
2012.02.24 12:21:08.026 PST:   Time taken to execute is 0.791 seconds 
2012.02.24 12:21:08.026 PST: [INFO] event.pegasus.parse.dax dax.id /home/tutorial/pegasus-wms/dax/diamond.dax  - FINISHED
</computeroutput></programlisting></para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Now run pegasus-run as mentioned in the
          output of pegasus-plan. Do not copy the command below it is just for
          illustration purpose.</emphasis><programlisting><emphasis
                role="bold">$ pegasus-run \
 /home/tutorial/pegasus-wms/dags/tutorial/pegasus/blackdiamond/run0001</emphasis>

-----------------------------------------------------------------------
File for submitting this DAG to Condor           : blackdiamond-0.dag.condor.sub
Log of DAGMan debugging messages                 : blackdiamond-0.dag.dagman.out
Log of Condor library output                     : blackdiamond-0.dag.lib.out
Log of Condor library error messages             : blackdiamond-0.dag.lib.err
Log of the life of condor_dagman itself          : blackdiamond-0.dag.dagman.log

Submitting job(s).
1 job(s) submitted to cluster 1.
-----------------------------------------------------------------------

Your Workflow has been started and runs in base directory given below

cd /home/tutorial/pegasus-wms/dags/tutorial/pegasus/blackdiamond/run0001

*** To monitor the workflow you can run ***

pegasus-status -l /home/tutorial/pegasus-wms/dags/tutorial/pegasus/blackdiamond/run0001

*** To remove your workflow run ***
pegasus-remove /home/tutorial/pegasus-wms/dags/tutorial/pegasus/blackdiamond/run0001
</programlisting></para>
        </listitem>
      </itemizedlist>
    </section>
  </section>

  <section>
    <title>Monitoring, Debugging and Statistics</title>

    <para>In this section, we are going to list ways to track your workflow,
    how to debug a failed workflow and how to generates statistics and plots
    for a workflow run.</para>

    <section>
      <title>Tracking the progress of the workflow and debugging the
      workflows.</title>

      <para>We will change into the directory, that was mentioned by the
      output of pegasus-run command.</para>

      <programlisting><command>$ cd /home/tutorial/pegasus-wms/dags/tutorial/pegasus/blackdiamond/run0001</command></programlisting>

      <para>In this directory you will see a whole lot of files. That should
      not scare you. Unless things go wrong, you need to look at just a very
      few number of files to track the progress of the workflow</para>

      <itemizedlist>
        <listitem>
          <para><emphasis role="bold">Run the command pegasus-status as
          mentioned by pegasus-run above to check the status of your jobs. Use
          the watch command to auto repeat the command every 2
          seconds.</emphasis><programlisting><command>$ watch pegasus-status /home/tutorial/pegasus-wms/dags/tutorial/pegasus/blackdiamond/run0001</command><computeroutput>

STAT  IN_STATE  JOB                                               
Run      03:04  blackdiamond-0                                    
Run      01:00   ┣━findrange_j2                                   
Run      00:55   ┗━findrange_j3                                   
Summary: 3 Condor jobs total (R:3)

UNREADY   READY     PRE  QUEUED    POST SUCCESS FAILURE %DONE
      3       0       0       3       0       3       0  33.3
Summary: 1 DAG total (Running:1)</computeroutput></programlisting><tip>
              <para>watch does not end with ESC nor (q)uit, but with
              Ctrl+C.</para>
            </tip> The above output shows that a couple of jobs are running
          under the main dagman process. Keep a lookout to track whether a
          workflow is running or not. If you do not see any of your job in the
          output for sometime (say 30 seconds), we know the workflow has
          finished. We need to wait, as there might be delay in Condor DAGMan
          releasing the next job into the queue after a job has finished
          successfully.</para>

          <para>If output of pegasus-status is empty, then either your
          workflow has</para>

          <orderedlist>
            <listitem>
              <para>successfully completed</para>
            </listitem>

            <listitem>
              <para>stopped midway due to non recoverable error.</para>
            </listitem>
          </orderedlist>

          <para>We can now run pegasus-analyzer to analyze the
          workflow.</para>
        </listitem>

        <listitem>
          <para>Using <emphasis role="bold">pegasus-analyzer</emphasis> to
          analyze the workflow</para>

          <programlisting><emphasis role="bold">$ pegasus-analyzer  -i /home/tutorial/pegasus-wms/dags/tutorial/pegasus/blackdiamond/run0001</emphasis>

pegasus-analyzer: initializing...

************************************Summary*************************************

 Total jobs         :      8 (100.00%)
 # jobs succeeded   :      8 (100.00%)
 # jobs failed      :      0 (0.00%)
 # jobs unsubmitted :      0 (0.00%)

**************************************Done**************************************</programlisting>
        </listitem>

        <listitem>
          <para>Another way to monitor the workflow is to check the <emphasis
          role="bold">jobstate.log</emphasis> file. This is the output file of
          the monitoring daemon that is parsing all the condor log files to
          determine the status of the jobs. It logs the events seen by Condor
          into a more readable form for us. <programlisting><command>$ more jobstate.log</command><computeroutput>

1290676248 INTERNAL *** MONITORD_STARTED ***
1290676247 INTERNAL *** DAGMAN_STARTED 339.0 ***
...</computeroutput></programlisting> In the starting of the jobstate.log,
          when the workflow has just started running you will see a lot of
          entries with status UN_READY. That designates that DAGMan has just
          parsed in the .dag file and has not started working on any job as
          yet. Initially all the jobs in the workflow are listed as UN_READY.
          After sometime you will see entries in jobstate.log, that shows a
          job is being executed etc. <programlisting><computeroutput>
1290676261 create_dir_blackdiamond_0_local SUBMIT 340.0 local - 1
1290676266 create_dir_blackdiamond_0_local EXECUTE 340.0 local - 1
1290676266 create_dir_blackdiamond_0_local JOB_TERMINATED 340.0 local - 1
1290676266 create_dir_blackdiamond_0_local JOB_SUCCESS 0 local - 1
1290676266 create_dir_blackdiamond_0_local POST_SCRIPT_STARTED 340.0 local - 1
1290676271 create_dir_blackdiamond_0_local POST_SCRIPT_TERMINATED 340.0 local - 1
1290676271 create_dir_blackdiamond_0_local POST_SCRIPT_SUCCESS 0 local - 1</computeroutput></programlisting></para>

          <para>The above shows the being submitted and then executed on the
          grid. In addition it lists that job is being run on the grid site
          local (which is your submit machine). The various states of the job
          while it goes through submission to execution to post processing are
          in UPPERCASE.</para>
        </listitem>

        <listitem>
          <para>Successfully Completed : Let us again look at the
          jobstate.log. This time we need to look at the last few lines of
          jobstate.log <programlisting><command>$ tail jobstate.log</command><computeroutput>

1290676542 register_local_2_0 SUBMIT 347.0 local - 8
1290676547 register_local_2_0 EXECUTE 347.0 local - 8
1290676547 register_local_2_0 JOB_TERMINATED 347.0 local - 8
1290676547 register_local_2_0 JOB_SUCCESS 0 local - 8
1290676547 register_local_2_0 POST_SCRIPT_STARTED 347.0 local - 8
1290676552 register_local_2_0 POST_SCRIPT_TERMINATED 347.0 local - 8
1290676552 register_local_2_0 POST_SCRIPT_SUCCESS 0 local - 8
1290676552 INTERNAL *** DAGMAN_FINISHED 0 ***
1290676554 INTERNAL *** MONITORD_FINISHED 0 ***

</computeroutput></programlisting>Looking at the last two lines we see that
          DAGMan finished, and pegasus-monitord finished successfully with a
          status 0. This means workflow ran successfully. Congratulations you
          ran your workflow on the local site successfully. The workflow
          generates a final output file f.d that resides in the directory
          <emphasis
          role="bold">/home/tutorial/local-storage/storage/f.d</emphasis>
          .</para>

          <para>To view the file, you can do the following <programlisting><command>$ cat /home/tutorial/local-storage/storage/f.d
</command>
--- start f.c1 ----
  --- start f.b1 ----
    --- start f.a ----
      Input File for the Diamond Workflow.--- final f.a ----
    Timestamp Today: 20120227T163628.742-08:00 (1330389388.742;60.000)
    Applicationname: preprocess [v5025] @ 10.0.2.15 (VPN)
    Current Workdir: /home/tutorial/local-scratch/exec/tutorial/pegasus/blackdiamond/run0001
    Systemenvironm.: i686-Linux 2.6.32-5-686
    Processor Info.: 1 x Intel(R) Core(TM) i5 CPU         750  @ 2.67GHz @ 2635.360
    Load Averages  : 0.669 0.195 0.068
    Memory Usage MB: 1010 total, 536 free, 0 shared, 152 buffered
    Swap Usage   MB: 397 total, 397 free
    Filesystem Info: /media/cdrom0            udf,iso9660  7668MB total,  5460MB avail
    Filesystem Info: /media/floppy0           auto  7668MB total,  5460MB avail
    Output Filename: f.b1
    Input Filenames: f.a
  --- final f.b1 ----
  Timestamp Today: 20120227T163744.890-08:00 (1330389464.890;60.000)
  Applicationname: findrange [v5025] @ 10.0.2.15 (VPN)
  Current Workdir: /home/tutorial/local-scratch/exec/tutorial/pegasus/blackdiamond/run0001
  Systemenvironm.: i686-Linux 2.6.32-5-686
  Processor Info.: 1 x Intel(R) Core(TM) i5 CPU         750  @ 2.67GHz @ 2635.360
  Load Averages  : 1.490 0.516 0.186
  Memory Usage MB: 1010 total, 535 free, 0 shared, 152 buffered
  Swap Usage   MB: 397 total, 397 free
  Filesystem Info: /media/cdrom0            udf,iso9660  7668MB total,  5460MB avail
  Filesystem Info: /media/floppy0           auto  7668MB total,  5460MB avail
  Output Filename: f.c1
  Input Filenames: f.b1
--- final f.c1 ----
--- start f.c2 ----
  --- start f.b2 ----
    --- start f.a ----
      Input File for the Diamond Workflow.--- final f.a ----
    Timestamp Today: 20120227T163628.742-08:00 (1330389388.742;60.001)
    Applicationname: preprocess [v5025] @ 10.0.2.15 (VPN)
    Current Workdir: /home/tutorial/local-scratch/exec/tutorial/pegasus/blackdiamond/run0001
    Systemenvironm.: i686-Linux 2.6.32-5-686
    Processor Info.: 1 x Intel(R) Core(TM) i5 CPU         750  @ 2.67GHz @ 2635.360
    Load Averages  : 0.669 0.195 0.068
    Memory Usage MB: 1010 total, 536 free, 0 shared, 152 buffered
    Swap Usage   MB: 397 total, 397 free
    Filesystem Info: /media/cdrom0            udf,iso9660  7668MB total,  5460MB avail
    Filesystem Info: /media/floppy0           auto  7668MB total,  5460MB avail
    Output Filename: f.b2
    Input Filenames: f.a
  --- final f.b2 ----
  Timestamp Today: 20120227T163750.168-08:00 (1330389470.168;60.000)
  Applicationname: findrange [v5025] @ 10.0.2.15 (VPN)
  Current Workdir: /home/tutorial/local-scratch/exec/tutorial/pegasus/blackdiamond/run0001
  Systemenvironm.: i686-Linux 2.6.32-5-686
  Processor Info.: 1 x Intel(R) Core(TM) i5 CPU         750  @ 2.67GHz @ 2635.360
  Load Averages  : 1.451 0.523 0.189
  Memory Usage MB: 1010 total, 536 free, 0 shared, 152 buffered
  Swap Usage   MB: 397 total, 397 free
  Filesystem Info: /media/cdrom0            udf,iso9660  7668MB total,  5459MB avail
  Filesystem Info: /media/floppy0           auto  7668MB total,  5459MB avail
  Output Filename: f.c2
  Input Filenames: f.b2
--- final f.c2 ----
Timestamp Today: 20120227T163906.044-08:00 (1330389546.044;60.000)
Applicationname: analyze [v5025] @ 10.0.2.15 (VPN)
Current Workdir: /home/tutorial/local-scratch/exec/tutorial/pegasus/blackdiamond/run0001
Systemenvironm.: i686-Linux 2.6.32-5-686
Processor Info.: 1 x Intel(R) Core(TM) i5 CPU         750  @ 2.67GHz @ 2635.360
Load Averages  : 1.094 0.608 0.241
Memory Usage MB: 1010 total, 536 free, 0 shared, 152 buffered
Swap Usage   MB: 397 total, 397 free
Filesystem Info: /media/cdrom0            udf,iso9660  7668MB total,  5459MB avail
Filesystem Info: /media/floppy0           auto  7668MB total,  5459MB avail
Output Filename: f.d
Input Filenames: f.c1 f.c2</programlisting></para>
        </listitem>

        <listitem>
          <para>Unsuccessfully Completed (Workflow execution stopped midway) :
          Let us again look at the jobstate.log. Again we need to look at the
          last few lines of jobstate.log <programlisting><command>$ tail jobstate.log</command><computeroutput>

1290677127 stage_in_local_local_0 EXECUTE 352.0 local - 4
1290677127 stage_in_local_local_0 JOB_TERMINATED 352.0 local - 4
1290677127 stage_in_local_local_0 JOB_FAILURE 1 local - 4
1290677127 stage_in_local_local_0 POST_SCRIPT_STARTED 352.0 local - 4
1290677132 stage_in_local_local_0 POST_SCRIPT_TERMINATED 352.0 local - 4
1290677132 stage_in_local_local_0 POST_SCRIPT_FAILURE 1 local - 4
1290677132 INTERNAL *** DAGMAN_FINISHED 1 ***
1290677134 INTERNAL *** MONITORD_FINISHED 0 ***</computeroutput></programlisting>Looking
          at the last two lines we see that DAGMan finished, and
          pegasus-monitord finished unsuccessfully with a status 1. We can
          easily determine which job failed. It is stage_in_local_local_0 in
          this case. To determine the reason for failure we need to look at
          it's kickstart output file which is JOBNAME.out.NNN. where NNN is
          000 - NNN</para>
        </listitem>
      </itemizedlist>
    </section>

    <section>
      <title>Debugging a failed workflow using pegasus-analyzer</title>

      <para>In this section, we will run the diamond workflow but remove the
      input file so that the workflow fails during execution. This is to
      highlight how to use pegasus-analyzer to debug a failed workflow.</para>

      <para>First of all lets rename the input file f.a</para>

      <programlisting><emphasis role="bold"> $ mv /scratch/tutorial/inputdata/diamond/f.a /scratch/tutorial/inputdata/diamond/f.a.old

 $ cd $HOME/pegasus-wms</emphasis></programlisting>

      <para>We will now repeat exercise <emphasis role="bold">2.4 and
      2.5</emphasis> and submit the workflow again.</para>

      <programlisting><emphasis><emphasis role="bold">Plan and Submit the diamond workflow</emphasis> .</emphasis> Pass --submit to pegasus-plan to submit in case of successful planning

$  pegasus-plan --dax `pwd`/dax/diamond.dax --force \
        --dir dags -s local -o local --nocleanup --submit -v

<emphasis role="bold">
Use pegasus-status to track the workflow and wait it to fail</emphasis>

$ watch pegasus-status  /home/tutorial/pegasus-wms/dags/tutorial/pegasus/blackdiamond/run0002

STAT  IN_STATE  JOB                                               
Run      00:18  blackdiamond-0                                    
Summary: 1 Condor job total (R:1)

UNREADY   READY     PRE  QUEUED    POST SUCCESS FAILURE %DONE
      7       0       0       2       0       0       0   0.0
Summary: 1 DAG total (Running:1)

<emphasis role="bold">
The --long option to pegasus-status of a running workflow gives more detail
<emphasis>[pegasus@pegasus pegasus-wms]$ pegasus-status -l /home/tutorial/pegasus-wms/dags/tutorial/pegasus/blackdiamond/run0002
<emphasis>
STAT  IN_STATE  JOB                                               
Run      00:38  blackdiamond-0                                    
Run      00:08   ┗━stage_in_local_local_0                         
Summary: 2 Condor jobs total (R:2)

UNRDY READY   PRE  IN_Q  POST  DONE  FAIL %DONE STATE   DAGNAME                                 
    6     0     0     2     0     1     0  11.1 Running *blackdiamond-0.dag                     
Summary: 1 DAG total (Running:1)</emphasis>

</emphasis>
We can also use --long option to pegasus-status to see the FINAL status of the workflow</emphasis>

$ pegasus-status -l /home/tutorial/pegasus-wms/dags/tutorial/pegasus/blackdiamond/run0002

(no matching jobs found in Condor Q)
UNRDY READY   PRE  IN_Q  POST  DONE  FAIL %DONE STATE   DAGNAME                                 
    6     0     0     0     0     1     2  11.1 Failure *blackdiamond-0.dag                     
Summary: 1 DAG total (Failure:1)

</programlisting>

      <para>We will now run pegasus-analyzer on the failed workflow submit
      directory to see what job failed.</para>

      <programlisting><emphasis role="bold">$ pegasus-analyzer -i $HOME/pegasus-wms/dags/tutorial/pegasus/blackdiamond/run0002<emphasis>
</emphasis></emphasis>
pegasus-analyzer: initializing...

************************************Summary*************************************

 Total jobs         :      8 (100.00%)
 # jobs succeeded   :      1 (12.50%)
 # jobs failed      :      1 (12.50%)
 # jobs unsubmitted :      6 (75.00%)

******************************Failed jobs' details******************************

=============================stage_in_local_local_0=============================

 last state: POST_SCRIPT_FAILED
       site: local
submit file: stage_in_local_local_0.sub
output file: stage_in_local_local_0.out.002
 error file: stage_in_local_local_0.err.002

-------------------------------Task #1 - Summary--------------------------------

site        : local
hostname    : pegasus-vm.local
executable  : /opt/pegasus/4.0.0/bin/pegasus-transfer
arguments   : -
exitcode    : 1
working dir : /home/tutorial/pegasus-wms/dags/tutorial/pegasus/blackdiamond/run0002

------------------Task #1 - pegasus::transfer - None - stdout-------------------

2012-02-24 12:37:46,439    INFO:  Reading URL pairs from stdin
2012-02-24 12:37:46,440    INFO:  PATH=/opt/globus/default/bin:/opt/pegasus/default/bin:/usr/bin:/bin
2012-02-24 12:37:46,440    INFO:  LD_LIBRARY_PATH=/opt/globus/default/lib
2012-02-24 12:37:46,444    INFO:    wget               Version: 1.12    Path: /usr/bin/wget
2012-02-24 12:37:46,447    INFO:    globus-version     Version: 5.0.2   Path: /opt/globus/default/bin/globus-version
2012-02-24 12:37:46,454    INFO:    globus-url-copy    Version: 5.7     Path: /opt/globus/default/bin/globus-url-copy
2012-02-24 12:37:46,456    INFO:  Command'srm-copy'not found in the current environment
2012-02-24 12:37:46,457    INFO:  Command'iget'not found in the current environment
2012-02-24 12:37:46,459    INFO:    pegasus-s3         Version: N/A     Path: /opt/pegasus/default/bin/pegasus-s3
2012-02-24 12:37:46,460    INFO:  Sorting the tranfers based on transfer type and source/destination
2012-02-24 12:37:46,460    INFO:  ----------------------------------------------------------------------
2012-02-24 12:37:46,460    INFO:  Starting transfers - attempt 1
2012-02-24 12:37:46,460    INFO:  /bin/cp -f -L"/scratch/tutorial/inputdata/diamond/f.a""/home/tutorial/local-scratch/exec/tutorial/pegasus/blackdiamond/run0002/f.a"
2012-02-24 12:37:46,461   ERROR:  Command'/bin/cp -f -L"/scratch/tutorial/inputdata/diamond/f.a""/home/tutorial/local-scratch/exec/tutorial/pegasus/blackdiamond/run0002/f.a"'failed with error code 1
2012-02-24 12:37:56,512    INFO:  ----------------------------------------------------------------------
2012-02-24 12:37:56,512    INFO:  Starting transfers - attempt 2
2012-02-24 12:37:56,512    INFO:  /bin/cp -f -L"/scratch/tutorial/inputdata/diamond/f.a""/home/tutorial/local-scratch/exec/tutorial/pegasus/blackdiamond/run0002/f.a"
2012-02-24 12:37:56,517   ERROR:  Command'/bin/cp -f -L"/scratch/tutorial/inputdata/diamond/f.a""/home/tutorial/local-scratch/exec/tutorial/pegasus/blackdiamond/run0002/f.a"'failed with error code 1
2012-02-24 12:37:56,517    INFO:  Stats: no local files in the transfer set
2012-02-24 12:37:56,517 CRITICAL:  Some transfers failed! See above, and possibly stderr.

-------------Task #1 - pegasus::transfer - None - Kickstart stderr--------------

/bin/cp: cannot stat `/scratch/tutorial/inputdata/diamond/f.a': No such file or directory
/bin/cp: cannot stat `/scratch/tutorial/inputdata/diamond/f.a': No such file or directory</programlisting>

      <para>The above tells us that the stage-in job for the workflow failed,
      and points us to the stdout of the job. By default, all jobs in Pegasus
      are launched via kickstart that captures runtime provenance of the job
      and helps in debugging. Hence, the stdout of the job is the kickstart
      stdout which is in XML.</para>

      <para>. the duration of the job the start time for the job the node on
      which the job ran the stdout/stderr of the job the arguments with which
      it launched the job the environment that was set for the job before it
      was launched. the machine information about the node that the job ran on
      Amongst the above information, the dagman.out file gives a coarser
      grained estimate of the job duration and start time</para>
    </section>

    <section>
      <title>Kickstart and Condor DAGMan format and log files</title>

      <para>This section explains how to read kickstart output and DAGMan
      Condor log files.</para>

      <section>
        <title>Kickstart</title>

        <para>Kickstart is a light weight C executable that is shipped with
        the pegasus worker package. All jobs are launced via Kickstart on the
        remote end, unless explicitly disabled at the time of running
        pegasus-plan.</para>

        <para>Kickstart does not work with</para>

        <orderedlist>
          <listitem>
            <para>Condor Standard Universe Jobs</para>
          </listitem>

          <listitem>
            <para>MPI jobs</para>
          </listitem>
        </orderedlist>

        <para>Pegasus automatically disables kickstart for the above
        jobs.</para>

        <para>Kickstart captures useful runtime provenance information about
        the job launched by it on the remote note, and puts in an XML record
        that it writes to it's stdout. The stdout appears in the workflow
        submit directory as &lt;job&gt;.out.00n . Some useful information
        captured by kickstart and logged are as follows</para>

        <orderedlist>
          <listitem>
            <para>the exitcode with which the job it launched exited</para>
          </listitem>

          <listitem>
            <para>the duration of the job</para>
          </listitem>

          <listitem>
            <para>the start time for the job</para>
          </listitem>

          <listitem>
            <para>the node on which the job ran</para>
          </listitem>

          <listitem>
            <para>the directory in which the job ran</para>
          </listitem>

          <listitem>
            <para>the stdout/stderr of the job</para>
          </listitem>

          <listitem>
            <para>the arguments with which it launched the job</para>
          </listitem>

          <listitem>
            <para>the environment that was set for the job before it was
            launched.</para>
          </listitem>

          <listitem>
            <para>the machine information about the node that the job ran
            on</para>
          </listitem>
        </orderedlist>

        <section>
          <title>Reading a Kickstart Output File</title>

          <para>Lets look at the stdout of our failed job.</para>

          <programlisting><emphasis role="bold">$ </emphasis><emphasis
              role="bold">cat /home/tutorial/pegasus-wms/dags/tutorial/pegasus/blackdiamond/run0002/stage_in_local_local_0.out.002 </emphasis>

 &lt;?xml version="1.0" encoding="ISO-8859-1"?&gt;
 &lt;invocation xmlns="http://pegasus.isi.edu/schema/invocation" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xsi:schemaLocation="http://pegasus.isi.edu/schema/invocation http://pegasus.isi.edu/schema/iv-2.1.xsd" version="2.1"
  start="2010-11-29T19:10:23.862-08:00" duration="0.076" <emphasis role="bold">transformation="pegasus::pegasus-transfer"</emphasis> 
  derivation="pegasus::pegasus-transfer:1.0" <emphasis role="bold">resource="local"</emphasis> wf-label="blackdiamond" 
  wf-stamp="2010-11-29T18:57:59-08:00" interface="eth0" <emphasis role="bold">hostaddr="﻿10.0.2.15" hostname="pegasus-vm.local" </emphasis>
  pid="5428" uid="501" user="pegasus" gid="501" group="pegasus" umask="0022"&gt;
 
 <emphasis role="bold">&lt;mainjob start="2010-11-29T19:10:23.876-08:00" duration="0.063" pid="5429"&gt;
 </emphasis>   &lt;usage utime="0.040" stime="0.023" minflt="2758" majflt="0" nswap="0" nsignals="0" nvcsw="5" nivcsw="20"/&gt;
    <emphasis role="bold">&lt;status raw="256"&gt;&lt;regular exitcode="1"/&gt;&lt;/status&gt;</emphasis>
    &lt;statcall error="0"&gt;
      &lt;file name="/opt/pegasus/default/bin/pegasus-transfer"&gt;23212F7573722F62696E2F656E762070&lt;/file&gt;
      &lt;statinfo mode="0100775" size="25314" inode="2022205" nlink="1" blksize="4096" blocks="64" 
               mtime="2010-11-23T13:14:52-08:00" 
             atime="2010-11-29T19:10:07-08:00" ctime="2010-11-25T00:01:52-08:00" uid="501" user="pegasus" 
               gid="501" group="pegasus"/&gt;
    &lt;/statcall&gt;
    &lt;argument-vector/&gt;
  &lt;/mainjob&gt;
  <emphasis role="bold">&lt;cwd&gt;/home/tutorial/pegasus-wms/dags/tutorial/pegasus/blackdiamond/run0002&lt;/cwd&gt;</emphasis>
  &lt;usage utime="0.002" stime="0.013" minflt="475" majflt="0" nswap="0" nsignals="0" nvcsw="1" nivcsw="5"/&gt;
  

   ﻿<emphasis role="bold">&lt;machine page-size="4096"&gt;
    &lt;stamp&gt;2010-12-23T10:56:43.817-08:00&lt;/stamp&gt;
    &lt;uname system="linux" nodename="pegasus-vm" release="2.6.32-5-686" machine="i686"&gt;
      #1 SMP Fri Dec 10 16:12:40 UTC 2010&lt;/uname&gt;
   &lt;linux&gt;
    &lt;ram total="527044608" free="242290688" shared="0" buffer="41041920"/&gt;
    &lt;swap total="417325056" free="417325056"/&gt;
    &lt;boot idle="1597.500"&gt;2010-12-23T10:29:16.599-08:00&lt;/boot&gt;
    &lt;cpu count="1" speed="2797" vendor="GenuineIntel"&gt;Intel(R) Xeon(R) CPU E5462 @ 2.80GHz&lt;/cpu&gt;
    &lt;load min1="0.05" min5="0.02" min15="0.00"/&gt;
    &lt;proc total="88" running="1" sleeping="87" vmsize="344793088" rss="123768832"/&gt;
    &lt;task total="101" running="1" sleeping="100"/&gt;
   &lt;/linux&gt;
  &lt;/machine&gt;</emphasis>
 <emphasis role="bold">
</emphasis>
  &lt;statcall error="0" id="stdin"&gt;
    &lt;descriptor number="0"/&gt;
    &lt;statinfo mode="0100664" size="142" inode="2250032" nlink="1" blksize="4096" blocks="16" 
     mtime="2010-11-29T19:09:20-08:00"   atime="2010-11-29T19:10:07-08:00" ctime="2010-11-29T19:09:20-08:00" 
     uid="501" user="pegasus" gid="501" group="pegasus"/&gt;
  &lt;/statcall&gt;

  <emphasis role="bold">&lt;statcall error="0" id="stdout"&gt;
    &lt;temporary name="/opt/condor/local.pegasus/spool/local_univ_execute/dir_5427/gs.out.awOX6p" descriptor="3"/&gt;
    &lt;statinfo mode="0100600" size="762" inode="2054511" nlink="1" blksize="4096" blocks="16" 
          mtime="2010-11-29T19:10:23-08:00" atime="2010-11-29T19:10:23-08:00" ctime="2010-11-29T19:10:23-08:00" 
             uid="501" user="pegasus" gid="501" group="pegasus"/&gt;
    &lt;data&gt;2010-11-29 19:10:23,920    INFO:  Reading URL pairs from stdin
2010-11-29 19:10:23,921    INFO:  PATH=/usr/local/globus/default/bin:/opt/pegasus/default/bin:/usr/bin:/bin
2010-11-29 19:10:23,921    INFO:  LD_LIBRARY_PATH=/usr/local/globus/default/lib:/usr/java/jdk1.6.0_20/jre/lib/amd64/
2010-11-29 19:10:23,921    INFO:  Executing cp commands
/bin/cp: cannot stat `/scratch/tutorial/inputdata/diamond/f.a&amp;apos;: No such file or directory
2010-11-29 19:10:23,932 CRITICAL:  Command &amp;apos;/bin/cp -L &amp;quot;/scratch/tutorial/inputdata/diamond/f.a&amp;quot; 
    &amp;quot;/home/tutorial/local-scratch/exec/pegasus/pegasus/blackdiamond/run0002/f.a&amp;quot;&amp;apos; failed with error code 1
&lt;/data&gt;
  &lt;/statcall&gt;</emphasis>

  <emphasis role="bold">&lt;statcall error="0" id="stderr"&gt;
    &lt;temporary name="/opt/condor/local.pegasus/spool/local_univ_execute/dir_5427/gs.err.oz9MOG" descriptor="4"/&gt;
    &lt;statinfo mode="0100600" size="0" inode="2054512" nlink="1" blksize="4096" blocks="8" 
    mtime="2010-11-29T19:10:23-08:00"  atime="2010-11-29T19:10:23-08:00" ctime="2010-11-29T19:10:23-08:00" 
    uid="501" user="pegasus" gid="501" group="pegasus"/&gt;
  &lt;/statcall&gt;
</emphasis>
  &lt;statcall error="2" id="gridstart"&gt;
    &lt;!-- ignore above error --&gt;
    &lt;file name="condor_exec.exe"/&gt;
  &lt;/statcall&gt;
  &lt;statcall error="0" id="logfile"&gt;
    &lt;descriptor number="1"/&gt;
    &lt;statinfo mode="0100644" size="0" inode="2250072" nlink="1" blksize="4096" blocks="8" mtime="2010-11-29T19:10:23-08:00" 
    atime="2010-11-29T19:10:23-08:00" ctime="2010-11-29T19:10:23-08:00" uid="501" user="pegasus" gid="501" group="pegasus"/&gt;
  &lt;/statcall&gt;
  &lt;statcall error="0" id="channel"&gt;
    &lt;fifo name="/opt/condor/local.pegasus/spool/local_univ_execute/dir_5427/gs.app.qCOCwX" descriptor="5" count="0"
     rsize="0" wsize="0"/&gt;
    &lt;statinfo mode="010640" size="0" inode="2054524" nlink="1" blksize="4096" blocks="8" mtime="2010-11-29T19:10:23-08:00" 
     atime="2010-11-29T19:10:23-08:00" ctime="2010-11-29T19:10:23-08:00" uid="501" user="pegasus" gid="501" 
    group="pegasus"/&gt;
  &lt;/statcall&gt;
  &lt;environment&gt;
    &lt;env key="GLOBUS_LOCATION"&gt;/usr/local/globus/default&lt;/env&gt;
    &lt;env key="GRIDSTART_CHANNEL"&gt;/opt/condor/local.pegasus/spool/local_univ_execute/dir_5427/gs.app.qCOCwX&lt;/env&gt;
    &lt;env key="JAVA_HOME"&gt;/usr&lt;/env&gt;
    &lt;env key="LD_LIBRARY_PATH"&gt;/usr/java/jdk1.6.0_20/jre/lib/amd64/server:/usr/java/jdk1.6.0_20/jre/lib/amd64:&lt;/env&gt;
    &lt;env key="PEGASUS_HOME"&gt;/opt/pegasus/default&lt;/env&gt;
    &lt;env key="TEMP"&gt;/opt/condor/local.pegasus/spool/local_univ_execute/dir_5427&lt;/env&gt;
    &lt;env key="TMP"&gt;/opt/condor/local.pegasus/spool/local_univ_execute/dir_5427&lt;/env&gt;
    &lt;env key="TMPDIR"&gt;/opt/condor/local.pegasus/spool/local_univ_execute/dir_5427&lt;/env&gt;
    &lt;env key="_CONDOR_ANCESTOR_4843"&gt;4862:1291085504:2790807554&lt;/env&gt;
    &lt;env key="_CONDOR_ANCESTOR_4862"&gt;5427:1291086623:1798288782&lt;/env&gt;
    &lt;env key="_CONDOR_ANCESTOR_5427"&gt;5428:1291086623:2750667008&lt;/env&gt;
    &lt;env key="_CONDOR_HIGHPORT"&gt;41000&lt;/env&gt;
    &lt;env key="_CONDOR_JOB_AD"&gt;/opt/condor/local.pegasus/spool/local_univ_execute/dir_5427/.job.ad&lt;/env&gt;
    &lt;env key="_CONDOR_LOWPORT"&gt;40000&lt;/env&gt;
    &lt;env key="_CONDOR_MACHINE_AD"&gt;/opt/condor/local.pegasus/spool/local_univ_execute/dir_5427/.machine.ad&lt;/env&gt;
    &lt;env key="_CONDOR_SCRATCH_DIR"&gt;/opt/condor/local.pegasus/spool/local_univ_execute/dir_5427&lt;/env&gt;
    &lt;env key="_CONDOR_SLOT"&gt;1&lt;/env&gt;
  &lt;/environment&gt;
  &lt;resource&gt;
    &lt;soft id="RLIMIT_CPU"&gt;unlimited&lt;/soft&gt;
    &lt;hard id="RLIMIT_CPU"&gt;unlimited&lt;/hard&gt;
    &lt;soft id="RLIMIT_FSIZE"&gt;unlimited&lt;/soft&gt;
    &lt;hard id="RLIMIT_FSIZE"&gt;unlimited&lt;/hard&gt;
    &lt;soft id="RLIMIT_DATA"&gt;unlimited&lt;/soft&gt;
    &lt;hard id="RLIMIT_DATA"&gt;unlimited&lt;/hard&gt;
    &lt;soft id="RLIMIT_STACK"&gt;unlimited&lt;/soft&gt;
    &lt;hard id="RLIMIT_STACK"&gt;unlimited&lt;/hard&gt;
    &lt;soft id="RLIMIT_CORE"&gt;0&lt;/soft&gt;
    &lt;hard id="RLIMIT_CORE"&gt;0&lt;/hard&gt;
    &lt;soft id="RESOURCE_5"&gt;unlimited&lt;/soft&gt;
    &lt;hard id="RESOURCE_5"&gt;unlimited&lt;/hard&gt;
    &lt;soft id="RLIMIT_NPROC"&gt;unlimited&lt;/soft&gt;
    &lt;hard id="RLIMIT_NPROC"&gt;unlimited&lt;/hard&gt;
    &lt;soft id="RLIMIT_NOFILE"&gt;1024&lt;/soft&gt;
    &lt;hard id="RLIMIT_NOFILE"&gt;1024&lt;/hard&gt;
    &lt;soft id="RLIMIT_MEMLOCK"&gt;32768&lt;/soft&gt;
    &lt;hard id="RLIMIT_MEMLOCK"&gt;32768&lt;/hard&gt;
    &lt;soft id="RLIMIT_AS"&gt;unlimited&lt;/soft&gt;
    &lt;hard id="RLIMIT_AS"&gt;unlimited&lt;/hard&gt;
    &lt;soft id="RLIMIT_LOCKS"&gt;unlimited&lt;/soft&gt;
    &lt;hard id="RLIMIT_LOCKS"&gt;unlimited&lt;/hard&gt;
    &lt;soft id="RLIMIT_SIGPENDING"&gt;8192&lt;/soft&gt;
    &lt;hard id="RLIMIT_SIGPENDING"&gt;8192&lt;/hard&gt;
    &lt;soft id="RLIMIT_MSGQUEUE"&gt;819200&lt;/soft&gt;
    &lt;hard id="RLIMIT_MSGQUEUE"&gt;819200&lt;/hard&gt;
    &lt;soft id="RLIMIT_NICE"&gt;0&lt;/soft&gt;
    &lt;hard id="RLIMIT_NICE"&gt;0&lt;/hard&gt;
    &lt;soft id="RLIMIT_RTPRIO"&gt;0&lt;/soft&gt;
    &lt;hard id="RLIMIT_RTPRIO"&gt;0&lt;/hard&gt;
  &lt;/resource&gt;
&lt;/invocation&gt;

</programlisting>
        </section>
      </section>

      <section>
        <title>Condor DAGMan format and log files etc.</title>

        <para>In this exercise we will learn about the DAG file format and
        some of the log files generated when the DAG runs.</para>

        <itemizedlist>
          <listitem>
            <para>Now take a look at the DAG file...</para>

            <programlisting><command>$ cat $HOME/pegasus-wms/dags/tutorial/pegasus/blackdiamond/run0001/blackdiamond-0.dag</command><computeroutput>

######################################################################
# PEGASUS WMS GENERATED DAG FILE
# DAG blackdiamond
# Index = 0, Count = 1
######################################################################
MAXJOBS projection 2

JOB create_dir_blackdiamond_0_local create_dir_blackdiamond_0_local.sub
SCRIPT POST create_dir_blackdiamond_0_local /opt/pegasus/default/bin/pegasus-exitcode 
  /home/tutorial/pegasus-wms/dags/tutorial/pegasus/blackdiamond/run0001/create_dir_blackdiamond_0_local.out
RETRY create_dir_blackdiamond_0_local 2

JOB stage_in_local_local_0 stage_in_local_local_0.sub
SCRIPT POST stage_in_local_local_0 /opt/pegasus/default/bin/pegasus-exitcode  
 /home/tutorial/pegasus-wms/dags/tutorial/pegasus/blackdiamond/run0001/stage_in_local_local_0.out
RETRY stage_in_local_local_0 2

JOB preprocess_j1 preprocess_j1.sub
SCRIPT POST preprocess_j1 /opt/pegasus/default/bin/pegasus-exitcode   
/home/tutorial/pegasus-wms/dags/tutorial/pegasus/blackdiamond/run0001/preprocess_j1.out
RETRY preprocess_j1 2

JOB findrange_j2 findrange_j2.sub
SCRIPT POST findrange_j2 /opt/pegasus/default/bin/pegasus-exitcode   
/home/tutorial/pegasus-wms/dags/tutorial/pegasus/blackdiamond/run0001/findrange_j2.out
RETRY findrange_j2 2

JOB findrange_j3 findrange_j3.sub
SCRIPT POST findrange_j3 /opt/pegasus/default/bin/pegasus-exitcode   
/home/tutorial/pegasus-wms/dags/tutorial/pegasus/blackdiamond/run0001/findrange_j3.out
RETRY findrange_j3 2

JOB analyze_j4 analyze_j4.sub
SCRIPT POST analyze_j4 /opt/pegasus/default/bin/pegasus-exitcode   
/home/tutorial/pegasus-wms/dags/tutorial/pegasus/blackdiamond/run0001/analyze_j4.out
RETRY analyze_j4 2

JOB stage_out_local_local_2_0 stage_out_local_local_2_0.sub
SCRIPT POST stage_out_local_local_2_0 /opt/pegasus/default/bin/pegasus-exitcode   
/home/tutorial/pegasus-wms/dags/tutorial/pegasus/blackdiamond/run0001/stage_out_local_local_2_0.out
RETRY stage_out_local_local_2_0 2

JOB register_local_2_0 register_local_2_0.sub
SCRIPT POST register_local_2_0 /opt/pegasus/default/bin/pegasus-exitcode   
/home/tutorial/pegasus-wms/dags/tutorial/pegasus/blackdiamond/run0001/register_local_2_0.out
RETRY register_local_2_0 2

PARENT findrange_j2 CHILD analyze_j4
PARENT preprocess_j1 CHILD findrange_j2
PARENT preprocess_j1 CHILD findrange_j3
PARENT findrange_j3 CHILD analyze_j4
PARENT analyze_j4 CHILD stage_out_local_local_2_0
PARENT stage_in_local_local_0 CHILD preprocess_j1
PARENT stage_out_local_local_2_0 CHILD register_local_2_0
PARENT create_dir_blackdiamond_0_local CHILD analyze_j4
PARENT create_dir_blackdiamond_0_local CHILD findrange_j2
PARENT create_dir_blackdiamond_0_local CHILD preprocess_j1
PARENT create_dir_blackdiamond_0_local CHILD findrange_j3
PARENT create_dir_blackdiamond_0_local CHILD stage_in_local_local_0
######################################################################
# End of DAG
##################################################################</computeroutput></programlisting>
          </listitem>

          <listitem>
            <para>... and the dagman.out file.</para>

            <programlisting><emphasis role="bold"><command>$</command><computeroutput> cat $HOME/pegasus-wms/dags/tutorial/pegasus/blackdiamond/run0001/blackdiamond-0.dag.dagman.out </computeroutput></emphasis><computeroutput>

11/25 01:10:47 ******************************************************
11/25 01:10:47 ** condor_scheduniv_exec.339.0 (CONDOR_DAGMAN) STARTING UP
11/25 01:10:47 ** /opt/condor/7.4.2/bin/condor_dagman
11/25 01:10:47 ** SubsystemInfo: name=DAGMAN type=DAGMAN(10) class=DAEMON(1)
11/25 01:10:47 ** Configuration: subsystem:DAGMAN local:&lt;NONE&gt; class:DAEMON
11/25 01:10:47 ** $CondorVersion: 7.4.2 Mar 29 2010 BuildID: 227044 $
11/25 01:10:47 ** $CondorPlatform: X86_64-LINUX_RHEL5 $
11/25 01:10:47 ** PID = 7844
11/25 01:10:47 ** Log last touched time unavailable (No such file or directory)
11/25 01:10:47 ******************************************************
11/25 01:10:47 Using config source: /opt/condor/config/condor_config
11/25 01:10:47 Using local config sources: 
11/25 01:10:47    /opt/condor/config/condor_config.local
11/25 01:10:47 DaemonCore: Command Socket at &lt;172.16.80.129:40035&gt;
11/25 01:10:47 DAGMAN_DEBUG_CACHE_SIZE setting: 5242880
11/25 01:10:47 DAGMAN_DEBUG_CACHE_ENABLE setting: False
11/25 01:10:47 DAGMAN_SUBMIT_DELAY setting: 0
11/25 01:10:47 DAGMAN_MAX_SUBMIT_ATTEMPTS setting: 6
11/25 01:10:47 DAGMAN_STARTUP_CYCLE_DETECT setting: 0
11/25 01:10:47 DAGMAN_MAX_SUBMITS_PER_INTERVAL setting: 5
11/25 01:10:47 DAGMAN_USER_LOG_SCAN_INTERVAL setting: 5
11/25 01:10:47 allow_events (DAGMAN_IGNORE_DUPLICATE_JOB_EXECUTION, DAGMAN_ALLOW_EVENTS) setting: 114
11/25 01:10:47 DAGMAN_RETRY_SUBMIT_FIRST setting: 1
11/25 01:10:47 DAGMAN_RETRY_NODE_FIRST setting: 0
11/25 01:10:47 DAGMAN_MAX_JOBS_IDLE setting: 0
11/25 01:10:47 DAGMAN_MAX_JOBS_SUBMITTED setting: 0
11/25 01:10:47 DAGMAN_MUNGE_NODE_NAMES setting: 1
11/25 01:10:47 DAGMAN_PROHIBIT_MULTI_JOBS setting: 0
11/25 01:10:47 DAGMAN_SUBMIT_DEPTH_FIRST setting: 0
11/25 01:10:47 DAGMAN_ABORT_DUPLICATES setting: 1
11/25 01:10:47 DAGMAN_ABORT_ON_SCARY_SUBMIT setting: 1
11/25 01:10:47 DAGMAN_PENDING_REPORT_INTERVAL setting: 600
11/25 01:10:47 DAGMAN_AUTO_RESCUE setting: 1
11/25 01:10:47 DAGMAN_MAX_RESCUE_NUM setting: 100
11/25 01:10:47 DAGMAN_DEFAULT_NODE_LOG setting: null
11/25 01:10:47 ALL_DEBUG setting: 
11/25 01:10:47 DAGMAN_DEBUG setting: 
....
11/25 01:10:47 Default node log file is:
 &lt;/home/tutorial/pegasus-wms/dags/tutorial/pegasus/blackdiamond/run0001/blackdiamond-0.dag.nodes.log&gt;
11/25 01:10:47 DAG Lockfile will be written to blackdiamond-0.dag.lock
11/25 01:10:47 DAG Input file is blackdiamond-0.dag
11/25 01:10:47 Parsing 1 dagfiles
11/25 01:10:47 Parsing blackdiamond-0.dag ...
11/25 01:10:47 Dag contains 8 total jobs
11/25 01:10:47 Sleeping for 12 seconds to ensure ProcessId uniqueness
11/25 01:10:59 Bootstrapping...
11/25 01:10:59 Number of pre-completed nodes: 0
11/25 01:10:59 Registering condor_event_timer...
11/25 01:11:00 Sleeping for one second for log file consistency
11/25 01:11:01 Submitting Condor Node create_dir_blackdiamond_0_local job(s)...
11/25 01:11:01 submitting: condor_submit -a dag_node_name' '=' 'create_dir_blackdiamond_0_local -a 
+DAGManJobId' '=' '339 -a DAGManJobId' '=' '339 -a submit_event_notes' '=' 'DAG' 'Node:' '
create_dir_blackdiamond_0_local -a +DAGParentNodeNames' '=' '"" create_dir_blackdiamond_0_local.sub
11/25 01:11:01 From submit: Submitting job(s).
11/25 01:11:01 From submit: Logging submit event(s).
11/25 01:11:01 From submit: 1 job(s) submitted to cluster 340.
11/25 01:11:01  assigned Condor ID (340.0)
11/25 01:11:01 Just submitted 1 job this cycle...
11/25 01:11:01 Currently monitoring 1 Condor log file(s)
11/25 01:11:01 Event: ULOG_SUBMIT for Condor Node create_dir_blackdiamond_0_local (340.0)
11/25 01:11:01 Number of idle job procs: 1
11/25 01:11:01 Of 8 nodes total:
11/25 01:11:01  Done     Pre   Queued    Post   Ready   Un-Ready   Failed
11/25 01:11:01   ===     ===      ===     ===     ===        ===      ===
11/25 01:11:01     0       0        1       0       0          7        0
....
11/25 01:11:06 Currently monitoring 1 Condor log file(s)
11/25 01:11:06 Event: ULOG_EXECUTE for Condor Node create_dir_blackdiamond_0_local (340.0)
11/25 01:11:06 Number of idle job procs: 0
11/25 01:11:06 Event: ULOG_JOB_TERMINATED for Condor Node create_dir_blackdiamond_0_local (340.0)
11/25 01:11:06 Node create_dir_blackdiamond_0_local job proc (340.0) completed successfully.
11/25 01:11:06 Node create_dir_blackdiamond_0_local job completed
11/25 01:11:06 Running POST script of Node create_dir_blackdiamond_0_local...
11/25 01:11:06 Number of idle job procs: 0
11/25 01:11:06 Of 8 nodes total:
11/25 01:11:06  Done     Pre   Queued    Post   Ready   Un-Ready   Failed
11/25 01:11:06   ===     ===      ===     ===     ===        ===      ===
11/25 01:11:06     0       0        0       1       0          7        0
11/25 01:11:11 Currently monitoring 1 Condor log file(s)
11/25 01:11:11 Event: ULOG_POST_SCRIPT_TERMINATED for Condor Node create_dir_blackdiamond_0_local (340.0)
11/25 01:11:11 POST Script of Node create_dir_blackdiamond_0_local completed successfully.
11/25 01:11:11 Of 8 nodes total:
11/25 01:11:11  Done     Pre   Queued    Post   Ready   Un-Ready   Failed
11/25 01:11:11   ===     ===      ===     ===     ===        ===      ===
11/25 01:11:11     1       0        0       0       1          6        0
....
11/25 01:15:52 Event: ULOG_POST_SCRIPT_TERMINATED for Condor Node register_local_2_0 (347.0)
11/25 01:15:52 POST Script of Node register_local_2_0 completed successfully.
11/25 01:15:52 Of 8 nodes total:
11/25 01:15:52  Done     Pre   Queued    Post   Ready   Un-Ready   Failed
11/25 01:15:52   ===     ===      ===     ===     ===        ===      ===
11/25 01:15:52     8       0        0       0       0          0        0
11/25 01:15:52 All jobs Completed!
11/25 01:15:52 Note: 0 total job deferrals because of -MaxJobs limit (0)
11/25 01:15:52 Note: 0 total job deferrals because of -MaxIdle limit (0)
11/25 01:15:52 Note: 0 total job deferrals because of node category throttles
11/25 01:15:52 Note: 0 total PRE script deferrals because of -MaxPre limit (20)
11/25 01:15:52 Note: 0 total POST script deferrals because of -MaxPost limit (20)
11/25 01:15:52 **** condor_scheduniv_exec.339.0 (condor_DAGMAN) pid 7844 EXITING WITH STATUS 0
[p

</computeroutput></programlisting>
          </listitem>
        </itemizedlist>
      </section>
    </section>

    <section>
      <title>Removing a running workflow</title>

      <para>Sometimes you may want to halt the execution of the workflow or
      just permanently remove it. You can stop/halt a workflow by running the
      pegasus-remove command mentioned in the output of pegasus-run</para>

      <programlisting><command>$ pegasus-remove $HOME/pegasus-wms/dags/tutorial/pegasus/diamond/runXXXX</command><computeroutput>

Job 2788.0 marked for removal</computeroutput></programlisting>
    </section>

    <section>
      <title>Generating statistics and plots of a workflow run</title>

      <para>In this section, we will generate statistics and plots of the
      diamond workflow we ran using pegasus-statistics and
      pegasus-plots</para>

      <section>
        <title>Generating Statistics Using pegasus-statistics</title>

        <para>pegasus-statistics generates workflow execution statistics. To
        generate statistics run the command as shown below</para>

        <programlisting><emphasis role="bold">$ cd $HOME/pegasus-wms

$ pegasus-statistics -s all dags/tutorial/pegasus/blackdiamond/run0001/</emphasis>

**********************************************SUMMARY***********************************************
# legends

# Workflow summary:
#               Summary of the workflow execution. It shows total
#          tasks/jobs/sub workflows run, how many succeeded/failed etc.
#          In case of hierarchical workflow the calculation shows the 
#          statistics across all the sub workflows.It shows the following 
#          statistics about tasks, jobs and sub workflows.
#          * Succeeded - total count of succeeded tasks/jobs/sub workflows.
#          * Failed - total count of failed tasks/jobs/sub workflows.
#          * Incomplete - total count of tasks/jobs/sub workflows that are 
#            not in succeeded or failed state. This includes all the jobs 
#            that are not submitted, submitted but not completed etc. This  
#            is calculated as  difference between 'total' count and sum of 
#            'succeeded' and 'failed' count.
#          * Total - total count of tasks/jobs/sub workflows.
#          * Retries - total retry count of tasks/jobs/sub workflows.
#          * Total Run - total count of tasks/jobs/sub workflows executed 
#            during workflow run. This is the cumulative of retries, 
#            succeeded and failed count. 

# Workflow wall time:
#               The walltime from the start of the workflow execution
#          to the end as reported by the DAGMAN.In case of rescue dag the value
#          is the cumulative of all retries.

# Workflow cumulative job wall time:
#               The sum of the walltime of all jobs as reported by kickstart. 
#          In case of job retries the value is the cumulative of all retries.
#          For workflows having sub workflow jobs (i.e SUBDAG and SUBDAX jobs),
#          the walltime value includes jobs from the sub workflows as well.

# Cumulative job walltime as seen from submit side:
#               The sum of the walltime of all jobs as reported by DAGMan.
#          This is similar to the regular cumulative job walltime, but includes
#          job management overhead and delays. In case of job retries the value is
#          the cumulative of all retries. For workflows having sub workflow jobs 
#          (i.e SUBDAG and SUBDAX jobs), the walltime value includes jobs
#          from the sub workflows as well.

-------------------------------------------------------------------------------------------------------------------------------------------------
Type                Succeeded           Failed              Incomplete          Total                    Retries             Total Run (Retries Included)
Tasks               4                   0                   0                   4                   ||   0                   4                   
Jobs                8                   0                   0                   8                   ||   0                   8                   
Sub Workflows       0                   0                   0                   0                   ||   0                   0                   
-------------------------------------------------------------------------------------------------------------------------------------------------

Workflow wall time                               : 5 mins, 5 secs,      (total 305 seconds)

Workflow cumulative job wall time                : 4 mins, 0 secs,      (total 240 seconds)

Cumulative job walltime as seen from submit side : 4 mins, 0 secs,      (total 240 seconds)

Summary                           : /home/tutorial/pegasus-wms/dags/tutorial/pegasus/blackdiamond/run0001/statistics/summary.txt

Workflow execution statistics     : /home/tutorial/pegasus-wms/dags/tutorial/pegasus/blackdiamond/run0001/statistics/workflow.txt

Job instance statistics           : /home/tutorial/pegasus-wms/dags/tutorial/pegasus/blackdiamond/run0001/statistics/jobs.txt

Transformation statistics         : /home/tutorial/pegasus-wms/dags/tutorial/pegasus/blackdiamond/run0001/statistics/breakdown.txt

Time statistics                   : /home/tutorial/pegasus-wms/dags/tutorial/pegasus/blackdiamond/run0001/statistics/time.txt

****************************************************************************************************</programlisting>

        <para><emphasis role="bold">Workflow statistics
        table</emphasis></para>

        <para>Workflow statistics table contains information about the
        workflow run like total execution time, job's failed etc.</para>

        <table>
          <title>Table Workflow Statistics</title>

          <tgroup cols="2">
            <tbody>
              <row>
                <entry>Workflow runtime</entry>

                <entry>5 min. 5 sec.</entry>
              </row>

              <row>
                <entry>Cumulative workflow runtime</entry>

                <entry>4 min. 0 sec.</entry>
              </row>

              <row>
                <entry>Total jobs</entry>

                <entry>8</entry>
              </row>

              <row>
                <entry># jobs succeeded</entry>

                <entry>8</entry>
              </row>

              <row>
                <entry># jobs failed</entry>

                <entry>0</entry>
              </row>

              <row>
                <entry># jobs unsubmitted</entry>

                <entry>0</entry>
              </row>

              <row>
                <entry># jobs unknown</entry>

                <entry>0</entry>
              </row>
            </tbody>
          </tgroup>
        </table>

        <para><emphasis role="bold">Job statistics table</emphasis></para>

        <para>Job statistics table contains the following details about the
        jobs in the workflow. A sample table is shown below.</para>

        <itemizedlist>
          <listitem>
            <para>Job - the name of the job</para>
          </listitem>

          <listitem>
            <para>Try - number representing the job instance run count</para>
          </listitem>

          <listitem>
            <para>Site - the site where the job ran</para>
          </listitem>

          <listitem>
            <para>Kickstart(sec.) - the actual duration of the job in seconds
            on the remote compute node. In case of retries the value is the
            cumulative of all retries.</para>
          </listitem>

          <listitem>
            <para>Mult - multiplier factor specified by the user</para>
          </listitem>

          <listitem>
            <para>Kickstart-Mult - Kickstart time multiplied by the multiplier
            factor</para>
          </listitem>

          <listitem>
            <para>CPU-Time - remote cpu time computed as the stime +
            utime</para>
          </listitem>

          <listitem>
            <para>Post(sec.) - the postscript time as reported by DAGMan .In
            case of retries the value is the cumulative of all retries.</para>
          </listitem>

          <listitem>
            <para>CondorQTime(sec.) - the time between submission by DAGMan
            and the remote Grid submission. It is an estimate of the time
            spent in the condor q on the submit node. In case of retries the
            value is the cumulative of all retries.</para>
          </listitem>

          <listitem>
            <para>Resource(sec.) - the time between the remote Grid submission
            and start of remote execution . It is an estimate of the time job
            spent in the remote queue. In case of retries the value is the
            cumulative of all retries.</para>
          </listitem>

          <listitem>
            <para>Runtime(sec.) - the time spent on the resource as seen by
            Condor DAGMan . Is always &gt;= kickstart. In case of retries the
            value is the cumulative of all retries.</para>
          </listitem>

          <listitem>
            <para>Seqexec(sec.) - the time taken for the completion of a
            clustered job. In case of retries the value is the cumulative of
            all retries.</para>
          </listitem>

          <listitem>
            <para>Seqexec-Delay(sec.) - the time difference between the time
            for the completion of a clustered job and sum of all the
            individual tasks kickstart time. In case of retries the value is
            the cumulative of all retries.</para>
          </listitem>

          <listitem>
            <para>Exitcode - Exitcode for this job</para>
          </listitem>

          <listitem>
            <para>Hostname - Name of the host where the job ran, as reported
            by kickstart</para>
          </listitem>
        </itemizedlist>

        <table>
          <title>Table Job Statistics</title>

          <tgroup align="center" cols="15">
            <thead>
              <row>
                <entry align="center">Job</entry>

                <entry align="center">Try</entry>

                <entry align="center">Site</entry>

                <entry align="center">Kickstart</entry>

                <entry align="center">Mult</entry>

                <entry align="center">Kickstart-Mult</entry>

                <entry align="center">CPU-Time</entry>

                <entry align="center">Post</entry>

                <entry align="center">CondorQTime</entry>

                <entry align="center">Resource</entry>

                <entry align="center">Runtime</entry>

                <entry align="center">Seqexec</entry>

                <entry align="center">Seqexec-Delay</entry>

                <entry align="center">Exitcode</entry>

                <entry align="center">Hostname</entry>
              </row>
            </thead>

            <tbody>
              <row>
                <entry>analyze_j4</entry>

                <entry>1</entry>

                <entry>local</entry>

                <entry>60.03</entry>

                <entry>1</entry>

                <entry>60.03</entry>

                <entry>59.851</entry>

                <entry>5.0</entry>

                <entry>5.0</entry>

                <entry>-</entry>

                <entry>60.0</entry>

                <entry>-</entry>

                <entry>-</entry>

                <entry>0</entry>

                <entry>pegasus-vm.local</entry>
              </row>

              <row>
                <entry>create_dir_blackdiamond_0_local</entry>

                <entry>1</entry>

                <entry>local</entry>

                <entry>0.029</entry>

                <entry>1</entry>

                <entry>0.029</entry>

                <entry>0.028</entry>

                <entry>5.0</entry>

                <entry>5.0</entry>

                <entry>-</entry>

                <entry>0.0</entry>

                <entry>-</entry>

                <entry>-</entry>

                <entry>0</entry>

                <entry>pegasus-vm.local</entry>
              </row>

              <row>
                <entry>findrange_j2</entry>

                <entry>1</entry>

                <entry>local</entry>

                <entry>60.03</entry>

                <entry>1</entry>

                <entry>60.03</entry>

                <entry>32.398</entry>

                <entry>5.0</entry>

                <entry>5.0</entry>

                <entry>-</entry>

                <entry>60.0</entry>

                <entry>-</entry>

                <entry>-</entry>

                <entry>0</entry>

                <entry>pegasus-vm.local</entry>
              </row>

              <row>
                <entry>findrange_j3</entry>

                <entry>1</entry>

                <entry>local</entry>

                <entry>60.03</entry>

                <entry>1</entry>

                <entry>60.03</entry>

                <entry>32.426</entry>

                <entry>5.0</entry>

                <entry>10.0</entry>

                <entry>-</entry>

                <entry>60.0</entry>

                <entry>-</entry>

                <entry>-</entry>

                <entry>0</entry>

                <entry>pegasus-vm.local</entry>
              </row>

              <row>
                <entry>preprocess_j1</entry>

                <entry>1</entry>

                <entry>local</entry>

                <entry>60.03</entry>

                <entry>1</entry>

                <entry>60.03</entry>

                <entry>59.815</entry>

                <entry>5.0</entry>

                <entry>60.00</entry>

                <entry>-</entry>

                <entry>60.0</entry>

                <entry>-</entry>

                <entry>-</entry>

                <entry>0</entry>

                <entry>pegasus-vm.local</entry>
              </row>

              <row>
                <entry>register_local_2_0</entry>

                <entry>1</entry>

                <entry>local</entry>

                <entry>0.242</entry>

                <entry>1</entry>

                <entry>0.242</entry>

                <entry>0.208</entry>

                <entry>5.0</entry>

                <entry>5.0</entry>

                <entry>-</entry>

                <entry>0.0</entry>

                <entry>-</entry>

                <entry>-</entry>

                <entry>0</entry>

                <entry>pegasus-vm.local</entry>
              </row>

              <row>
                <entry>stage_in_local_local_0</entry>

                <entry>1</entry>

                <entry>local</entry>

                <entry>0.06</entry>

                <entry>1</entry>

                <entry>0.06</entry>

                <entry>0.056</entry>

                <entry>5.0</entry>

                <entry>5.0</entry>

                <entry>-</entry>

                <entry>0.0</entry>

                <entry>-</entry>

                <entry>-</entry>

                <entry>0</entry>

                <entry>pegasus-vm.local</entry>
              </row>

              <row>
                <entry>stage_out_local_local_2_0</entry>

                <entry>1</entry>

                <entry>local</entry>

                <entry>0.056</entry>

                <entry>1</entry>

                <entry>0.056</entry>

                <entry>0.052</entry>

                <entry>5.0</entry>

                <entry>5.0</entry>

                <entry>-</entry>

                <entry>0.0</entry>

                <entry>-</entry>

                <entry>-</entry>

                <entry>0</entry>

                <entry>pegasus-vm.local</entry>
              </row>
            </tbody>
          </tgroup>
        </table>

        <para><emphasis role="bold">Logical transformation statistics
        table</emphasis></para>

        <para>Logical transformation statistics table contains information
        about each type of transformation in the workflow.</para>

        <table>
          <title>Table: Logical Transformation Statistics</title>

          <tgroup align="center" cols="9">
            <thead>
              <row>
                <entry align="center">Transformation</entry>

                <entry align="center">Count</entry>

                <entry align="center">Succeeded</entry>

                <entry align="center">Failed</entry>

                <entry align="center">Mean</entry>

                <entry align="center">Variance</entry>

                <entry align="center">Min</entry>

                <entry>Max</entry>

                <entry>Total</entry>
              </row>
            </thead>

            <tbody>
              <row>
                <entry>diamond::analyze:4.0</entry>

                <entry>1</entry>

                <entry>1</entry>

                <entry>0</entry>

                <entry>60.1600</entry>

                <entry>0.0000</entry>

                <entry>60.1600</entry>

                <entry>60.1600</entry>

                <entry>60.1600</entry>
              </row>

              <row>
                <entry>diamond::findrange:4.0</entry>

                <entry>2</entry>

                <entry>2</entry>

                <entry>0</entry>

                <entry>60.3100</entry>

                <entry>0.0100</entry>

                <entry>60.2500</entry>

                <entry>60.3700</entry>

                <entry>120.6200</entry>
              </row>

              <row>
                <entry>diamond::preprocess:4.0</entry>

                <entry>1</entry>

                <entry>1</entry>

                <entry>0</entry>

                <entry>60.4800</entry>

                <entry>0.0000</entry>

                <entry>60.4800</entry>

                <entry>60.4800</entry>

                <entry>60.4800</entry>
              </row>
            </tbody>
          </tgroup>
        </table>
      </section>

      <section>
        <title>Generating plots using pegasus-plots</title>

        <para>pegasus-plots generates graphs and charts to visualize workflow
        execution. To generate graphs and charts run the command as shown
        below.</para>

        <programlisting><emphasis role="bold">$ cd $HOME/pegasus-wms 

$ pegasus-plots -p all dags/tutorial/pegasus/blackdiamond/run0001/</emphasis>

**********************************************SUMMARY***********************************************

Graphs and charts generated by pegasus-plots can be viewed by opening the generated html file in the web browser  : 
/home/tutorial/pegasus-wms/dags/tutorial/pegasus/blackdiamond/run0001/plots/index.html

****************************************************************************************************</programlisting>

        <para></para>

        <section>
          <title>Home Page</title>

          <para>/home/tutorial/pegasus-wms/dags/tutorial/pegasus/blackdiamond/run0001/plots/index.html</para>

          <figure>
            <title>Figure: Home Page</title>

            <mediaobject>
              <imageobject>
                <imagedata fileref="images/tutorial-blackdiamond-home.png" />
              </imageobject>
            </mediaobject>
          </figure>
        </section>

        <section>
          <title>Abstract Worfklow / DAX Image</title>

          <para>/home/tutorial/pegasus-wms/dags/tutorial/pegasus/blackdiamond/run0001/plots/dax_graph/</para>

          <figure>
            <title>Figure: Black Diamond DAX Image</title>

            <mediaobject>
              <imageobject>
                <imagedata fileref="images/tutorial-blackdiamond-dax.png" />
              </imageobject>
            </mediaobject>
          </figure>
        </section>

        <section>
          <title>Executable Workflow / DAG Image</title>

          <para>/home/tutorial/pegasus-wms/dags/tutorial/pegasus/blackdiamond/run0001/plots/dag_graph/</para>

          <figure>
            <title>Figure: Black Diamond DAG Image</title>

            <mediaobject>
              <imageobject>
                <imagedata fileref="images/tutorial-blackdiamond-dag.png" />
              </imageobject>
            </mediaobject>
          </figure>
        </section>

        <section>
          <title>Gantt Chart of Workflow Execution</title>

          <para>/home/tutorial/pegasus-wms/dags/tutorial/pegasus/blackdiamond/run0001/plots/gantt_chart/</para>

          <para><emphasis role="bold">X axis </emphasis>- time in seconds .
          Each tic is 60 seconds</para>

          <para><emphasis role="bold">Y axis</emphasis> - Job Number .</para>

          <figure>
            <title>Figure: Gantt Chart of Workflow Execution</title>

            <mediaobject>
              <imageobject>
                <imagedata fileref="images/tutorial-blackdiamond-gantt.png"
                           format="PNG" remap="" width="1000" />
              </imageobject>
            </mediaobject>
          </figure>
        </section>

        <section>
          <title>Host over time chart</title>

          <para>/home/tutorial/pegasus-wms/dags/tutorial/pegasus/blackdiamond/run0001/plots/host_chart/</para>

          <para><emphasis role="bold">X axis </emphasis>- time in seconds .
          Each tic is 60 seconds</para>

          <para><emphasis role="bold">Y axis</emphasis> - Job Number .</para>

          <figure>
            <title>Figure: Gantt Chart of Workflow Execution</title>

            <mediaobject>
              <imageobject>
                <imagedata fileref="images/tutorial-blackdiamond-host.png" />
              </imageobject>
            </mediaobject>
          </figure>
        </section>

        <section>
          <title>Invocation Beakdown chart</title>

          <para>/home/tutorial/pegasus-wms/dags/tutorial/pegasus/blackdiamond/run0001/plots/breakdown_chart/</para>

          <para><emphasis role="bold">X axis </emphasis>- time (seconds),
          count (seconds).</para>

          <para><emphasis role="bold">Y axis</emphasis> - Runtime
          (seconds).</para>

          <figure>
            <title>Figure: Invocation Breakdown Chart</title>

            <mediaobject>
              <imageobject>
                <imagedata fileref="images/tutorial-blackdiamond-invocation.png"
                           format="PNG" remap="" width="1000" />
              </imageobject>
            </mediaobject>
          </figure>
        </section>
      </section>
    </section>
  </section>

  <section>
    <title>Planning and Executing Workflow against a Remote Resource</title>

    <para>In this exercise we are going to run pegasus-plan to generate a
    executable workflow from the abstract workflow (montage.dax). The
    Executable workflow contains condor submit files that are submitted to
    remote grid resources using pegasus-run</para>

    <para>The instructors have provided:</para>

    <itemizedlist>
      <listitem>
        <para>A dax (montage.dax) in the $HOME/pegasus-wms/dax/
        directory.</para>
      </listitem>
    </itemizedlist>

    <para>You will need to write some things yourself, by following the
    instructions below: <itemizedlist>
        <listitem>
          <para>Run pegasus-plan to generate the condor submit files out of
          the dax.</para>
        </listitem>
      </itemizedlist></para>

    <para>Instructions:</para>

    <itemizedlist>
      <listitem>
        <para>Let us run pegasus-plan on the montage dax on the tg_ncsa
        cluster. If multiple sites are available you could provide the sites
        using a comma "," separated list like tg_ncsa,viz etc.<programlisting><command>$ cd $HOME/pegasus-wms

$ pegasus-plan --dir dags --sites cluster --output local --force \
               --nocleanup --dax `pwd`/dax/montage.dax --submit -v</command></programlisting>
        The above command says that we need to plan the montage dax on the
        <emphasis role="bold">cluster</emphasis> site. The cluster site in the
        VM is managed by SGE that is running in the VM. The jobs for this
        workflow will be submitted to <emphasis
        role="bold">jobmanager-condor</emphasis> in the VM. The output data
        needs to be transferred back to the local host. The condor submit
        files are to be generated in a directory structure whose base is dags.
        We also are requesting that no cleanup jobs be added as we require the
        intermediate data on the remote host. Here is the output of
        pegasus-plan. <programlisting><computeroutput>2012.02.24 12:49:47.783 PST: [INFO] event.pegasus.parse.dax dax.id /home/tutorial/pegasus-wms/dax/montage.dax  - STARTED 
2012.02.24 12:49:47.966 PST: [INFO]  Generating Stampede Events for Abstract Workflow 
2012.02.24 12:49:48.040 PST: [INFO]  Generating Stampede Events for Abstract Workflow -DONE 
2012.02.24 12:49:48.041 PST: [INFO] event.pegasus.refinement dax.id montage_0  - STARTED 
2012.02.24 12:49:48.058 PST: [INFO] event.pegasus.siteselection dax.id montage_0  - STARTED 
2012.02.24 12:49:48.091 PST: [INFO] event.pegasus.siteselection dax.id montage_0  - FINISHED 
2012.02.24 12:49:48.118 PST: [INFO]  Grafting transfer nodes in the workflow 
2012.02.24 12:49:48.118 PST: [INFO] event.pegasus.generate.transfer-nodes dax.id montage_0  - STARTED 
2012.02.24 12:49:48.170 PST: [INFO] event.pegasus.generate.transfer-nodes dax.id montage_0  - FINISHED 
2012.02.24 12:49:48.173 PST: [INFO] event.pegasus.generate.workdir-nodes dax.id montage_0  - STARTED 
2012.02.24 12:49:48.175 PST: [INFO] event.pegasus.generate.workdir-nodes dax.id montage_0  - FINISHED 
2012.02.24 12:49:48.175 PST: [INFO] event.pegasus.refinement dax.id montage_0  - FINISHED 
2012.02.24 12:49:48.206 PST: [INFO]  Generating codes for the concrete workflow 
2012.02.24 12:49:48.485 PST: [INFO]  Generating codes for the concrete workflow -DONE 
2012.02.24 12:49:48.677 PST:   Submitting job(s). 
2012.02.24 12:49:48.682 PST:   1 job(s) submitted to cluster 21. 
2012.02.24 12:49:48.687 PST:    
2012.02.24 12:49:48.693 PST:   ----------------------------------------------------------------------- 
2012.02.24 12:49:48.703 PST:   File for submitting this DAG to Condor           : montage-0.dag.condor.sub 
2012.02.24 12:49:48.711 PST:   Log of DAGMan debugging messages                 : montage-0.dag.dagman.out 
2012.02.24 12:49:48.720 PST:   Log of Condor library output                     : montage-0.dag.lib.out 
2012.02.24 12:49:48.727 PST:   Log of Condor library error messages             : montage-0.dag.lib.err 
2012.02.24 12:49:48.735 PST:   Log of the life of condor_dagman itself          : montage-0.dag.dagman.log 
2012.02.24 12:49:48.744 PST:    
2012.02.24 12:49:48.751 PST:   ----------------------------------------------------------------------- 
2012.02.24 12:49:48.759 PST:    
2012.02.24 12:49:48.767 PST:   Your Workflow has been started and runs in base directory given below 
2012.02.24 12:49:48.775 PST:    
2012.02.24 12:49:48.783 PST:   cd /home/tutorial/pegasus-wms/dags/tutorial/pegasus/montage/run0001 
2012.02.24 12:49:48.791 PST:    
2012.02.24 12:49:48.800 PST:   *** To monitor the workflow you can run *** 
2012.02.24 12:49:48.808 PST:    
2012.02.24 12:49:48.815 PST:   pegasus-status -l /home/tutorial/pegasus-wms/dags/tutorial/pegasus/montage/run0001 
2012.02.24 12:49:48.822 PST:    
2012.02.24 12:49:48.827 PST:   *** To remove your workflow run *** 
2012.02.24 12:49:48.835 PST:   pegasus-remove /home/tutorial/pegasus-wms/dags/tutorial/pegasus/montage/run0001 
2012.02.24 12:49:48.843 PST:    
2012.02.24 12:49:48.852 PST:   Time taken to execute is 1.466 seconds 
2012.02.24 12:49:48.852 PST: [INFO] event.pegasus.parse.dax dax.id /home/tutorial/pegasus-wms/dax/montage.dax  - FINISHED</computeroutput></programlisting></para>
      </listitem>

      <listitem>
        <para>If you get any errors above while running pegasus-plan you can
        add -vvvvv to enable maximum verbosity on pegasus-run.</para>
      </listitem>
    </itemizedlist>

    <para>The above command submits the workflow to Condor DAGMan/CondorG.
    After submitting it starts a monitoring daemon pegasus-monitord that
    parses the condor log files to update the status of the jobs and push it
    in a work database.</para>

    <para>Monitor the workflow using the commands provided in the output of
    the pegasus-run command and other commands explained earlier.</para>

    <para>The workflow generates a single output file montage.jpg that resides
    in the directory <emphasis
    role="bold">/home/tutorial/local-storage/storage/montage.jpg</emphasis> if
    it runs successfully</para>

    <para>The grid workflow will take time to execute on the VM. On the
    instructor's MAC Pro Desktop it took about<emphasis role="bold"> 30
    minutes </emphasis>to run.</para>
  </section>

  <section>
    <title>Advanced Exercises</title>

    <section>
      <title>Optimizing a workflow by clustering small jobs (To Be Done
      offline)</title>

      <para>Sometimes a workflow may have too many jobs whose execution time
      is a few seconds long. In such instances the overhead of scheduling each
      job on a grid is too large and the runtime of the entire workflow can be
      optimized by using Pegasus clustering techniques. One such technique is
      to cluster jobs horizontally on the same level into one or more
      sequential jobs.</para>

      <programlisting><command>$ cd $HOME/pegasus-wms

$ pegasus-plan --dir `pwd`/dags --sites cluster --output local --nocleanup --force \
               --cluster horizontal --dax `pwd`/dax/montage.dax -v</command></programlisting>

      <para>After clustering the executable workflow will contain 26 jobs
      compared to 44 in the non clustered mode.</para>
    </section>

    <section>
      <title>Data Reuse</title>

      <para>In the DAX you can specify what output data products you want to
      track in the replica catalog. This is done by setting the register flags
      with the output files for a job. For our tutorial, we only register the
      final output data products. So if you were able to execute the diamond
      or the montage workflow successfully, we can do data reuse. Let us run
      <emphasis role="bold">pegasus-plan </emphasis>on the diamond workflow
      again. However, this time we will remove the <emphasis
      role="bold">--force</emphasis> option.</para>

      <programlisting><emphasis role="bold">$ cd $HOME/pegasus-wms

$ pegasus-plan --dax `pwd`/dax/diamond.dax --dir `pwd`/dags -s local -o local --nocleanup -v</emphasis>

2011.07.29 13:22:13.022 PDT: [INFO] event.pegasus.parse.dax dax.id /home/tutorial/pegasus-wms/dax/diamond.dax  - STARTED 
2011.07.29 13:22:13.113 PDT: [INFO]  Generating Stampede Events for Abstract Workflow 
2011.07.29 13:22:13.196 PDT: [INFO]  Generating Stampede Events for Abstract Workflow -DONE 
2011.07.29 13:22:13.198 PDT: [INFO] event.pegasus.refinement dax.id blackdiamond_0  - STARTED 
2011.07.29 13:22:13.222 PDT: [INFO] event.pegasus.reduce dax.id blackdiamond_0  - STARTED 
2011.07.29 13:22:13.223 PDT: [INFO]  Nodes/Jobs Deleted from the Workflow during reduction  
2011.07.29 13:22:13.223 PDT: [INFO]       analyze_j4 
2011.07.29 13:22:13.223 PDT: [INFO]       findrange_j2 
2011.07.29 13:22:13.223 PDT: [INFO]       findrange_j3 
2011.07.29 13:22:13.224 PDT: [INFO]       preprocess_j1 
2011.07.29 13:22:13.224 PDT: [INFO]  Nodes/Jobs Deleted from the Workflow during reduction  - DONE 
2011.07.29 13:22:13.224 PDT: [INFO] event.pegasus.reduce dax.id blackdiamond_0  - FINISHED 
2011.07.29 13:22:13.224 PDT: [INFO] event.pegasus.siteselection dax.id blackdiamond_0  - STARTED 
2011.07.29 13:22:13.246 PDT: [INFO] event.pegasus.siteselection dax.id blackdiamond_0  - FINISHED 
2011.07.29 13:22:13.317 PDT: [INFO]  Grafting transfer nodes in the workflow 
2011.07.29 13:22:13.318 PDT: [INFO] event.pegasus.generate.transfer-nodes dax.id blackdiamond_0  - STARTED 
2011.07.29 13:22:13.419 PDT: [INFO]  Adding stage out jobs for jobs deleted from the workflow 
2011.07.29 13:22:13.421 PDT: [INFO]  The leaf file f.d is already at the output pool local 
2011.07.29 13:22:13.421 PDT: [INFO] event.pegasus.generate.transfer-nodes dax.id blackdiamond_0  - FINISHED 
2011.07.29 13:22:13.424 PDT: [INFO] event.pegasus.generate.workdir-nodes dax.id blackdiamond_0  - STARTED 
2011.07.29 13:22:13.426 PDT: [INFO] event.pegasus.generate.workdir-nodes dax.id blackdiamond_0  - FINISHED 
2011.07.29 13:22:13.426 PDT: [INFO] event.pegasus.generate.cleanup-wf dax.id blackdiamond_0  - STARTED 
2011.07.29 13:22:13.428 PDT: [INFO] event.pegasus.generate.cleanup-wf dax.id blackdiamond_0  - FINISHED 
2011.07.29 13:22:13.428 PDT: [INFO] event.pegasus.refinement dax.id blackdiamond_0  - FINISHED 
2011.07.29 13:22:13.518 PDT: [INFO]  Generating codes for the concrete workflow 
2011.07.29 13:22:13.927 PDT: [INFO]  Generating codes for the concrete workflow -DONE 
2011.07.29 13:22:13.927 PDT:   


The executable workflow generated contains only a single NOOP job.
It seems that the output files are already at the output site. 
To regenerate the output data from scratch specify --force option.



pegasus-run  /home/tutorial/pegasus-wms/dags/tutorial/pegasus/blackdiamond/run0003

 
2011.07.29 13:22:13.927 PDT:   Time taken to execute is 1.387 seconds 
2011.07.29 13:22:13.927 PDT: [INFO] event.pegasus.parse.dax dax.id /home/tutorial/pegasus-wms/dax/diamond.dax  - FINISHED 
</programlisting>

      <para>You can increase the debug level to see how pegasus deletes the
      jobs bottom up of the workflow. Pass -vvvv to pegasus-plan
      command.</para>
    </section>

    <section>
      <title>Hierarchal Workflows</title>

      <para>Pegasus 4.0 allows you to create workflows of workflows i.e your
      workflow can contain dax jobs that refer to the sub-workflows. In this
      exercise, we will execute a workflow super-diamond that will execute two
      diamond workflows.</para>

      <para>Let us look at superdiamond.dax in the dax directory</para>

      <programlisting><emphasis role="bold">$ cat $HOME/pegasus-wms/dax/superdiamond.dax</emphasis>

&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;!-- generated on: 2010-11-25T08:42:30-08:00 --&gt;
&lt;!-- generated by: pegasus [ ?? ] --&gt;
&lt;adag xmlns="http://pegasus.isi.edu/schema/DAX" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://pegasus.isi.edu/schema/DAX http://pegasus.isi.edu/schema/dax-3.2.xsd" versi
on="3.2" name="superdiamond" index="0" count="1"&gt;

&lt;!-- Section 1: Files - Acts as a Replica Catalog (can be empty) --&gt;

   &lt;file name="f.a"&gt;
      &lt;pfn url="file:///scratch/tutorial/inputdata/diamond/f.a" site="local"/&gt;
   &lt;/file&gt;
   
   &lt;file name="black-1.dax"&gt;
      &lt;pfn url="/home/tutorial/pegasus-wms/dax/black-1.dax" site="local"/&gt;
   &lt;/file&gt;

   &lt;file name="black-2.dax"&gt;
      &lt;pfn url="/home/tutorial/pegasus-wms/dax/black-2.dax" site="local"/&gt;
   &lt;/file&gt;


&lt;!-- Section 2: Executables - Acts as a Transformaton Catalog (can be empty) --&gt;


&lt;!-- Section 3: Transformations - Aggregates executables and Files (can be empty) --&gt;


&lt;!-- Section 4: Job's, DAX's or Dag's - Defines a JOB or DAX or DAG (Atleast 1 required) --&gt;

   <emphasis role="bold">&lt;dax id="d1" file="black-1.dax"  &gt;
    &lt;argument&gt;-s local --force -o local&lt;/argument&gt;
   &lt;/dax&gt;</emphasis>

   &lt;dax id="d2" file="black-2.dax"  &gt;
    &lt;argument&gt;-s local --force -o local&lt;/argument&gt;
   &lt;/dax&gt;



&lt;!-- Section 5: Dependencies - Parent Child relationships (can be empty) --&gt;

   &lt;child ref="d2"&gt;
      &lt;parent ref="d1"/&gt;
   &lt;/child&gt;

&lt;/adag&gt;
</programlisting>

      <para>Now let us submit this super diamond workflow</para>

      <programlisting><emphasis role="bold">$ pegasus-plan --dax `pwd`/dax/superdiamond.dax --force --submit \
               --dir dags -s local -o local --nocleanup -v</emphasis>

2011.07.29 13:23:35.646 PDT: [INFO] event.pegasus.parse.dax dax.id /home/tutorial/pegasus-wms/dax/superdiamond.dax  - STARTED 
2011.07.29 13:23:35.717 PDT: [INFO]  Generating Stampede Events for Abstract Workflow 
2011.07.29 13:23:35.772 PDT: [INFO]  Generating Stampede Events for Abstract Workflow -DONE 
2011.07.29 13:23:35.774 PDT: [INFO] event.pegasus.refinement dax.id superdiamond_0  - STARTED 
2011.07.29 13:23:35.789 PDT: [INFO] event.pegasus.siteselection dax.id superdiamond_0  - STARTED 
2011.07.29 13:23:35.798 PDT: [INFO] event.pegasus.siteselection dax.id superdiamond_0  - FINISHED 
2011.07.29 13:23:35.842 PDT: [INFO]  Grafting transfer nodes in the workflow 
2011.07.29 13:23:35.842 PDT: [INFO] event.pegasus.generate.transfer-nodes dax.id superdiamond_0  - STARTED 
2011.07.29 13:23:35.918 PDT: [INFO] event.pegasus.generate.transfer-nodes dax.id superdiamond_0  - FINISHED 
2011.07.29 13:23:35.922 PDT: [INFO] event.pegasus.generate.workdir-nodes dax.id superdiamond_0  - STARTED 
2011.07.29 13:23:35.929 PDT: [INFO] event.pegasus.generate.workdir-nodes dax.id superdiamond_0  - FINISHED 
2011.07.29 13:23:35.929 PDT: [INFO] event.pegasus.generate.cleanup-wf dax.id superdiamond_0  - STARTED 
2011.07.29 13:23:35.931 PDT: [INFO] event.pegasus.generate.cleanup-wf dax.id superdiamond_0  - FINISHED 
2011.07.29 13:23:35.932 PDT: [INFO] event.pegasus.refinement dax.id superdiamond_0  - FINISHED 
2011.07.29 13:23:35.995 PDT: [INFO]  Generating codes for the concrete workflow 
2011.07.29 13:23:36.130 PDT: [INFO] event.pegasus.parse.dax dax.id /home/tutorial/pegasus-wms/dax/black-1.dax  - STARTED 
2011.07.29 13:23:36.161 PDT: [INFO] event.pegasus.parse.dax dax.id /home/tutorial/pegasus-wms/dax/black-2.dax  - STARTED 
2011.07.29 13:23:36.488 PDT: [INFO]  Generating codes for the concrete workflow -DONE 
2011.07.29 13:23:36.489 PDT: [INFO]  Generating code for the cleanup workflow 
2011.07.29 13:23:36.626 PDT: [INFO]  Generating code for the cleanup workflow -DONE 
2011.07.29 13:23:36.829 PDT:   Submitting job(s). 
2011.07.29 13:23:36.839 PDT:   1 job(s) submitted to cluster 23. 
2011.07.29 13:23:36.845 PDT:    
2011.07.29 13:23:36.850 PDT:   ----------------------------------------------------------------------- 
2011.07.29 13:23:36.856 PDT:   File for submitting this DAG to Condor           : superdiamond-0.dag.condor.sub 
2011.07.29 13:23:36.862 PDT:   Log of DAGMan debugging messages                 : superdiamond-0.dag.dagman.out 
2011.07.29 13:23:36.867 PDT:   Log of Condor library output                     : superdiamond-0.dag.lib.out 
2011.07.29 13:23:36.874 PDT:   Log of Condor library error messages             : superdiamond-0.dag.lib.err 
2011.07.29 13:23:36.880 PDT:   Log of the life of condor_dagman itself          : superdiamond-0.dag.dagman.log 
2011.07.29 13:23:36.886 PDT:    
2011.07.29 13:23:36.892 PDT:   ----------------------------------------------------------------------- 
2011.07.29 13:23:36.898 PDT:    
2011.07.29 13:23:36.915 PDT:   Your Workflow has been started and runs in base directory given below 
2011.07.29 13:23:36.920 PDT:    
2011.07.29 13:23:36.926 PDT:   cd /home/tutorial/pegasus-wms/dags/tutorial/pegasus/superdiamond/run0001 
2011.07.29 13:23:36.933 PDT:    
2011.07.29 13:23:36.938 PDT:   *** To monitor the workflow you can run *** 
2011.07.29 13:23:36.944 PDT:    
2011.07.29 13:23:36.951 PDT:   pegasus-status -l /home/tutorial/pegasus-wms/dags/tutorial/pegasus/superdiamond/run0001 
2011.07.29 13:23:36.957 PDT:    
2011.07.29 13:23:36.962 PDT:   *** To remove your workflow run *** 
2011.07.29 13:23:36.968 PDT:   pegasus-remove /home/tutorial/pegasus-wms/dags/tutorial/pegasus/superdiamond/run0001 
2011.07.29 13:23:36.974 PDT:    
2011.07.29 13:23:36.980 PDT:   Time taken to execute is 1.821 seconds 
2011.07.29 13:23:36.980 PDT: [INFO] event.pegasus.parse.dax dax.id /home/tutorial/pegasus-wms/dax/black-1.dax  - FINISHED</programlisting>

      <para>You can track the workflow using the pegasus-status command</para>

      <programlisting><emphasis role="bold">$ watch pegasus-status -l /home/tutorial/pegasus-wms/dags/tutorial/pegasus/superdiamond/run0001</emphasis>
</programlisting>

      <para>After the workflow has completed you will see the black-1-f.d and
      black-2-f.d in the storage directory</para>

      <programlisting><emphasis role="bold">$ ls -lh /home/tutorial/local-storage/storage/black-*</emphasis>

-rw-r--r-- 1 pegasus pegasus 3.6K Nov 29 21:36 /home/tutorial/local-storage/storage/black-1-f.d
-rw-r--r-- 1 pegasus pegasus 3.6K Nov 29 21:41 /home/tutorial/local-storage/storage/black-2-f.d</programlisting>

      <section>
        <title>Directory Structure For the Hierarchal Workflows</title>

        <para>Pegasus ensures that each of the workflows have their own submit
        directory and execution directories.</para>

        <para>The table below lists the submit directories for all the
        workflows in this exercise</para>

        <table>
          <title>Table: Submit Directory Structure for Hierarchal
          Workflows</title>

          <tgroup cols="2">
            <tbody>
              <row>
                <entry>superdiamond ( the outer level workflow )</entry>

                <entry>/home/tutorial/pegasus-wms/dags/tutorial/pegasus/superdiamond/run0001</entry>
              </row>

              <row>
                <entry>black-1 ( the first sub workflow )</entry>

                <entry>/home/tutorial/pegasus-wms/dags/tutorial/pegasus/superdiamond/run0001/black-1_d1</entry>
              </row>

              <row>
                <entry>black-2 ( the second sub workflow )</entry>

                <entry>/home/tutorial/pegasus-wms/dags/tutorial/pegasus/superdiamond/run0001/black-2_d2</entry>
              </row>
            </tbody>
          </tgroup>
        </table>

        <para>The table below lists the execution directories ( one per
        workflow ) in this exercise</para>

        <table>
          <title>Table: Execution Directory Structure for Hierarchal
          Workflows</title>

          <tgroup cols="2">
            <tbody>
              <row>
                <entry>superdiamond ( the outer level workflow )</entry>

                <entry>/home/tutorial/local-scratch/exec/tutorial/pegasus/superdiamond/run0001</entry>
              </row>

              <row>
                <entry>black-1 ( the first sub workflow )</entry>

                <entry>/home/tutorial/local-scratch/exec/tutorial/pegasus/superdiamond/run0001/black-1_d1</entry>
              </row>

              <row>
                <entry>black-2 ( the second sub workflow )</entry>

                <entry>/home/tutorial/local-scratch/exec/tutorial/pegasus/superdiamond/run0001/black-2_d2</entry>
              </row>
            </tbody>
          </tgroup>
        </table>
      </section>
    </section>

    <section>
      <title>Running Workflow without a Shared File System</title>

      <para>Pegasus has the ability to run workflows on clusters which do not
      have a shared file system. This capability is easily exposed through a
      property 'pegasus.data.configuration'.</para>

      <para>When the compute nodes do not have a shared file system, all input
      files, executable have to be staged to the compute node to run a job
      successfully. This staging of files can be done in one of two ways which
      are as follows.</para>

      <para>The option is also useful when you want to run workflows on
      clusters which would not allow you install anything on the compute
      nodes.</para>

      <para>For additional information visit pegasus-lite section in <link
      linkend="running_workflows">running workflows</link> page.</para>

      <section>
        <title>CondorIO</title>

        <para>This setup applies to a condor pool where the worker nodes
        making up a condor pool don't share a filesystem. All data IO is
        achieved using Condor File IO. This is a special case of the non
        shared filesystem setup, where instead of using pegasus-transfer to
        transfer input and output data, Condor File IO is used.</para>

        <para>To run a workflow in this mode we set <emphasis
        role="bold">pegasus.data.configuration=condorio</emphasis>. In this
        mode moving of any input/executable files to the execute node is
        handle by Condor scheduler.</para>

        <programlisting><emphasis role="bold">$ cd $HOME/pegasus-wms</emphasis><emphasis
            role="bold">

$ pegasus-plan -Dpegasus.data.configuration=condorio --dir `pwd`/dags --sites condor-pool --output local --nocleanup --force \
               --dax `pwd`/dax/montage.dax --submit -v<emphasis>

2012.02.24 14:33:09.312 PST: [INFO] event.pegasus.parse.dax dax.id /home/tutorial/pegasus-wms/dax/montage.dax  - STARTED 
2012.02.24 14:33:09.484 PST: [INFO]  Generating Stampede Events for Abstract Workflow 
2012.02.24 14:33:09.547 PST: [INFO]  Generating Stampede Events for Abstract Workflow -DONE 
2012.02.24 14:33:09.550 PST: [INFO] event.pegasus.refinement dax.id montage_0  - STARTED 
2012.02.24 14:33:09.562 PST: [INFO] event.pegasus.siteselection dax.id montage_0  - STARTED 
2012.02.24 14:33:09.594 PST: [INFO] event.pegasus.siteselection dax.id montage_0  - FINISHED 
2012.02.24 14:33:09.633 PST: [INFO] event.pegasus.cluster dax.id montage_0  - STARTED 
2012.02.24 14:33:09.655 PST: [INFO] event.pegasus.cluster dax.id montage_0  - FINISHED 
2012.02.24 14:33:09.655 PST: [INFO]  Grafting transfer nodes in the workflow 
2012.02.24 14:33:09.655 PST: [INFO] event.pegasus.generate.transfer-nodes dax.id montage_0  - STARTED 
2012.02.24 14:33:09.707 PST: [INFO] event.pegasus.generate.transfer-nodes dax.id montage_0  - FINISHED 
2012.02.24 14:33:09.710 PST: [INFO] event.pegasus.generate.workdir-nodes dax.id montage_0  - STARTED 
2012.02.24 14:33:09.716 PST: [INFO] event.pegasus.generate.workdir-nodes dax.id montage_0  - FINISHED 
2012.02.24 14:33:09.717 PST: [INFO] event.pegasus.refinement dax.id montage_0  - FINISHED 
2012.02.24 14:33:09.742 PST: [INFO]  Generating codes for the concrete workflow 
2012.02.24 14:33:10.029 PST: [INFO]  Generating codes for the concrete workflow -DONE 
2012.02.24 14:33:10.216 PST:   Submitting job(s). 
2012.02.24 14:33:10.221 PST:   1 job(s) submitted to cluster 132. 
2012.02.24 14:33:10.228 PST:    
2012.02.24 14:33:10.235 PST:   ----------------------------------------------------------------------- 
2012.02.24 14:33:10.243 PST:   File for submitting this DAG to Condor           : montage-0.dag.condor.sub 
2012.02.24 14:33:10.251 PST:   Log of DAGMan debugging messages                 : montage-0.dag.dagman.out 
2012.02.24 14:33:10.259 PST:   Log of Condor library output                     : montage-0.dag.lib.out 
2012.02.24 14:33:10.267 PST:   Log of Condor library error messages             : montage-0.dag.lib.err 
2012.02.24 14:33:10.276 PST:   Log of the life of condor_dagman itself          : montage-0.dag.dagman.log 
2012.02.24 14:33:10.283 PST:    
2012.02.24 14:33:10.291 PST:   ----------------------------------------------------------------------- 
2012.02.24 14:33:10.299 PST:    
2012.02.24 14:33:10.307 PST:   Your Workflow has been started and runs in base directory given below 
2012.02.24 14:33:10.315 PST:    
2012.02.24 14:33:10.323 PST:   cd /home/tutorial/pegasus-wms/dags/tutorial/pegasus/montage/run0003 
2012.02.24 14:33:10.331 PST:    
2012.02.24 14:33:10.340 PST:   *** To monitor the workflow you can run *** 
2012.02.24 14:33:10.348 PST:    
2012.02.24 14:33:10.355 PST:   pegasus-status -l /home/tutorial/pegasus-wms/dags/tutorial/pegasus/montage/run0003 
2012.02.24 14:33:10.363 PST:    
2012.02.24 14:33:10.371 PST:   *** To remove your workflow run *** 
2012.02.24 14:33:10.379 PST:   pegasus-remove /home/tutorial/pegasus-wms/dags/tutorial/pegasus/montage/run0003 
2012.02.24 14:33:10.387 PST:    
2012.02.24 14:33:10.396 PST:   Time taken to execute is 1.422 seconds 
2012.02.24 14:33:10.396 PST: [INFO] event.pegasus.parse.dax dax.id /home/tutorial/pegasus-wms/dax/montage.dax  - FINISHED</emphasis></emphasis></programlisting>

        <para>You can track the workflow using the pegasus-status
        command</para>

        <programlisting><emphasis role="bold">$ watch pegasus-status -l /home/tutorial/pegasus-wms/dags/tutorial/pegasus/montage/run0003</emphasis></programlisting>

        <para>The workflow generates a single output file montage.jpg that
        resides in the directory <emphasis
        role="bold">/home/tutorial/local-storage/storage/montage.jpg</emphasis>,
        if it runs successfully.</para>
      </section>

      <section>
        <title>Non Shared FS mode</title>

        <para>Pegasus can also run workflows on local file-systems of worker
        nodes with the the worker nodes not sharing a filesystem. The data
        transfers take place between the worker node and a staging/data
        coordination site. The staging site server can be a file server on the
        head node of a cluster or a separate machine.</para>

        <para>To run a workflow in this mode we set <emphasis
        role="bold">pegasus.data.configuration=nonsharedfs</emphasis>. You
        will also need to specify the <emphasis
        role="bold">--staging-site</emphasis> option for pegasus-plan.</para>

        <programlisting><emphasis role="bold">$ cd $HOME/pegasus-wms</emphasis><emphasis
            role="bold">

$ pegasus-plan -Dpegasus.data.configuration=nonsharedfs --dir `pwd`/dags --sites condor-pool --staging-site local --output local --nocleanup --force \
               --dax `pwd`/dax/montage.dax --submit -v<emphasis>

2012.02.24 14:26:16.770 PST: [INFO] event.pegasus.parse.dax dax.id /home/tutorial/pegasus-wms/dax/montage.dax  - STARTED 
2012.02.24 14:26:16.933 PST: [INFO]  Generating Stampede Events for Abstract Workflow 
2012.02.24 14:26:17.001 PST: [INFO]  Generating Stampede Events for Abstract Workflow -DONE 
2012.02.24 14:26:17.002 PST: [INFO] event.pegasus.refinement dax.id montage_0  - STARTED 
2012.02.24 14:26:17.018 PST: [INFO] event.pegasus.siteselection dax.id montage_0  - STARTED 
2012.02.24 14:26:17.053 PST: [INFO] event.pegasus.siteselection dax.id montage_0  - FINISHED 
2012.02.24 14:26:17.093 PST: [INFO] event.pegasus.cluster dax.id montage_0  - STARTED 
2012.02.24 14:26:17.119 PST: [INFO] event.pegasus.cluster dax.id montage_0  - FINISHED 
2012.02.24 14:26:17.120 PST: [INFO]  Grafting transfer nodes in the workflow 
2012.02.24 14:26:17.121 PST: [INFO] event.pegasus.generate.transfer-nodes dax.id montage_0  - STARTED 
2012.02.24 14:26:17.181 PST: [INFO] event.pegasus.generate.transfer-nodes dax.id montage_0  - FINISHED 
2012.02.24 14:26:17.183 PST: [INFO] event.pegasus.generate.workdir-nodes dax.id montage_0  - STARTED 
2012.02.24 14:26:17.190 PST: [INFO] event.pegasus.generate.workdir-nodes dax.id montage_0  - FINISHED 
2012.02.24 14:26:17.190 PST: [INFO] event.pegasus.refinement dax.id montage_0  - FINISHED 
2012.02.24 14:26:17.215 PST: [INFO]  Generating codes for the concrete workflow 
2012.02.24 14:26:17.543 PST: [INFO]  Generating codes for the concrete workflow -DONE 
2012.02.24 14:26:17.713 PST:   Submitting job(s). 
2012.02.24 14:26:17.719 PST:   1 job(s) submitted to cluster 88. 
2012.02.24 14:26:17.725 PST:    
2012.02.24 14:26:17.733 PST:   ----------------------------------------------------------------------- 
2012.02.24 14:26:17.738 PST:   File for submitting this DAG to Condor           : montage-0.dag.condor.sub 
2012.02.24 14:26:17.744 PST:   Log of DAGMan debugging messages                 : montage-0.dag.dagman.out 
2012.02.24 14:26:17.755 PST:   Log of Condor library output                     : montage-0.dag.lib.out 
2012.02.24 14:26:17.763 PST:   Log of Condor library error messages             : montage-0.dag.lib.err 
2012.02.24 14:26:17.770 PST:   Log of the life of condor_dagman itself          : montage-0.dag.dagman.log 
2012.02.24 14:26:17.780 PST:    
2012.02.24 14:26:17.787 PST:   ----------------------------------------------------------------------- 
2012.02.24 14:26:17.795 PST:    
2012.02.24 14:26:17.802 PST:   Your Workflow has been started and runs in base directory given below 
2012.02.24 14:26:17.807 PST:    
2012.02.24 14:26:17.815 PST:   cd /home/tutorial/pegasus-wms/dags/tutorial/pegasus/montage/run0004 
2012.02.24 14:26:17.823 PST:    
2012.02.24 14:26:17.829 PST:   *** To monitor the workflow you can run *** 
2012.02.24 14:26:17.836 PST:    
2012.02.24 14:26:17.843 PST:   pegasus-status -l /home/tutorial/pegasus-wms/dags/tutorial/pegasus/montage/run0004 
2012.02.24 14:26:17.851 PST:    
2012.02.24 14:26:17.860 PST:   *** To remove your workflow run *** 
2012.02.24 14:26:17.868 PST:   pegasus-remove /home/tutorial/pegasus-wms/dags/tutorial/pegasus/montage/run0004 
2012.02.24 14:26:17.875 PST:    
2012.02.24 14:26:17.884 PST:   Time taken to execute is 1.455 seconds 
2012.02.24 14:26:17.885 PST: [INFO] event.pegasus.parse.dax dax.id /home/tutorial/pegasus-wms/dax/montage.dax  - FINISHED</emphasis></emphasis></programlisting>

        <para>You can track the workflow using the pegasus-status
        command.</para>

        <programlisting><emphasis role="bold">$ watch pegasus-status -l /home/tutorial/pegasus-wms/dags/tutorial/pegasus/montage/run0004</emphasis></programlisting>

        <para>The workflow generates a single output file montage.jpg that
        resides in the directory <emphasis
        role="bold">/home/tutorial/local-storage/storage/montage.jpg</emphasis>,
        if it runs successfully.</para>
      </section>
    </section>
  </section>
</chapter>
