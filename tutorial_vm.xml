<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN"
"http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<chapter id="tutorial_vm">
  <title>Pegasus Tutorial Using Self-contained Virtual Machine</title>

  <note>
    <para>These tutorial notes refer to Pegasus 2.4. The VM Tutorial for
    Pegasus 3.0 will be available in the first week of December 2010.</para>
  </note>

  <para>These are the student notes for the Pegasus WMS tutorial on the
  Virtual Machine that can be downloaded from the Pegasus Website. They are
  designed to be used in conjunction with instructor presentation and
  support.</para>

  <para>You will see two styles of machine text here:</para>

  <programlisting><command>Text like this is input that you should type.</command><computeroutput>

Text like this is the output you should get.</computeroutput></programlisting>

  <para>For example:</para>

  <programlisting><command>$ date</command><computeroutput>
Mon June 1 11:54:58 BST 2008</computeroutput></programlisting>

  <section>
    <title>Downloading and Running the VM</title>

    <para>You will need to install VMPlayer or Virtual Box to run the virtual
    machine on your computer. If you already have one of the tools installed,
    use that. Otherwise download and install one from:</para>

    <itemizedlist>
      <listitem>
        <para><ulink
        url="http://www.virtualbox.org/">http://www.virtualbox.org/</ulink></para>
      </listitem>

      <listitem>
        <para><ulink
        url="http://www.vmware.com/products/player/">http://www.vmware.com/products/player/</ulink></para>
      </listitem>
    </itemizedlist>

    <para>Download the corresponding disk image:</para>

    <itemizedlist>
      <listitem>
        <para><ulink
        url="http://pegasus.isi.edu/tutorial/virtual/PegasusVM.tar.bz2">VMWare
        Pegasus Image</ulink></para>
      </listitem>
    </itemizedlist>

    <para>When you start the virtual machine , it will ask for a password for
    a user called <emphasis role="bold">pegasus</emphasis>. The password is
    same as the username i.e. pegasus.</para>

    <para>After logging on, start a terminal</para>

    <programlisting><command>$ </command><emphasis role="bold">[pegasus@pegasus ~]$ pwd</emphasis>

/home/pegasus
</programlisting>

    <para>In general, to run workflows on the Grid you will need to obtain
    Grid Credentials. The VM already has a user certificate installed for the
    pegasus user. To generate the proxy ( grid credentials) run the <emphasis
    role="bold">grid-proxy-init</emphasis> command.</para>

    <programlisting><command>$ [pegasus@pegasus ~]$ grid-proxy-init 

</command><computeroutput>Your identity: /O=Grid/OU=GlobusTest/OU=simpleCA-seqware/CN=Pegasus User
Creating proxy ............................................. Done
Your proxy is valid until: Tue Jul 20 02:19:53 2010

</computeroutput></programlisting>

    <para>Check your proxy using grid-proxy-info.</para>

    <programlisting><command>$ </command><emphasis role="bold">[pegasus@pegasus ~]$ grid-proxy-info</emphasis><computeroutput><computeroutput>

subject  : /O=Grid/OU=GlobusTest/OU=simpleCA-seqware/CN=Pegasus User/CN=294698404
issuer   : /O=Grid/OU=GlobusTest/OU=simpleCA-seqware/CN=Pegasus User
identity : /O=Grid/OU=GlobusTest/OU=simpleCA-seqware/CN=Pegasus User
type     : RFC 3820 compliant impersonation proxy
strength : 512 bits
path     : /tmp/x509up_u501
timeleft : 11:55:49

</computeroutput></computeroutput></programlisting>
  </section>

  <section>
    <title>Mapping and Executing Workflows using Pegasus</title>

    <para>In this chapter you will be introduced to planning and executing a
    workflow through Pegasus WMS locally. You will then plan and execute a
    larger Montage workflow on the GRID.</para>

    <para>All the exercises in this Chapter will be run from the
    $HOME/pegasus-wms/ directory. All the files that are required reside in
    this directory</para>

    <programlisting><command>$ cd $HOME/pegasus-wms</command></programlisting>

    <para>Files for the exercise are stored in subdirectories:</para>

    <programlisting><command>$ ls</command><computeroutput><computeroutput>

config dags dax</computeroutput></computeroutput></programlisting>

    <para>You may also see some other files here.</para>

    <section>
      <title>Creating a DIAMOND DAX</title>

      <para>We generate a 4 node diamond dax. There is a small piece of java
      code that uses the DAX API to generate the DAX. Open the file
      $HOME/pegasus-wms/dax/CreateDAX.java in a file editor:</para>

      <programlisting><command>$ emacs -nw dax/CreateDAX.java</command></programlisting>

      <para>There is a function constructDAX( String daxFile ) that constructs
      the DAX. Towards the end of the function there is some commented out
      code.</para>

      <programlisting>$ //create fourth job //analyze
    //To be uncommented for exercise 2.1
    /*
    String id4="ID0000004";
    job=new Job (NAMESPACE,ANALYZE,VERSION,id4);
    //add the arguments to the job
    job.addArgument(new PseudoText("-a analyze "));
    job.addArgument(new PseudoText("-T60 "));
    job.addArgument(new PseudoText("-i "));
    job.addArgument(new Filename(FC1));
    job.addArgument(new PseudoText(" "));
    job.addArgument(new Filename(FC2));
    job.addArgument(new PseudoText(" -o "));
    job.addArgument(new Filename(FD));

    //add the files used by the job
    job.addUses(new Filename(FC1,LFN.INPUT));
    job.addUses(new Filename(FC2,LFN.INPUT));
    job.addUses(new Filename(FD,LFN.OUTPUT));

    //add the job to the dax
    dax.addJob(job);
    
    //the job with id4 is a child to both jobs id2 and id3
    dax.addChild(id4,id2);
    dax.addChild(id4,id3);
    */
    //End of commented out code for Exercise 2.1

</programlisting>

      <para>The above snippet of code, adds a job with the ID0000004 to the
      DAX. It illustrates how to specify</para>

      <orderedlist>
        <listitem>
          <para>the arguments for the job</para>
        </listitem>

        <listitem>
          <para>the logical files used by the job</para>
        </listitem>

        <listitem>
          <para>the dependencies to other jobs</para>
        </listitem>

        <listitem>
          <para>adding the job to the dax</para>
        </listitem>
      </orderedlist>

      <para>After uncommenting the code, compile and run the CreateDAX
      program.</para>

      <programlisting><emphasis role="bold">$ cd dax

$ javac CreateDAX.java

$ java -classpath .:$CLASSPATH CreateDAX ./diamond.dax
</emphasis></programlisting>

      <para>Let us view the generated diamond.dax.</para>

      <programlisting><emphasis role="bold">$ cat diamond.dax</emphasis></programlisting>

      <para>Inside the DAX, you should see three sections.</para>

      <orderedlist>
        <listitem>
          <para>list of all the files used in the workflow</para>
        </listitem>

        <listitem>
          <para>definition of all jobs - each job in the workflow. 4 jobs in
          total.</para>
        </listitem>

        <listitem>
          <para>list of control-flow dependencies - this section specifies a
          partial order in which jobs are to executed.</para>
        </listitem>
      </orderedlist>
    </section>

    <section>
      <title>Setting up the Replica Catalog</title>

      <para>First lets change to the tutorial base directory.<programlisting><command>$ cd $HOME/pegasus-wms</command></programlisting></para>

      <para>In this exercise you will insert entries into the Replica Catalog.
      The replica catalog that we will use today is a simple file based
      catalog. We also support and recommend GLOBUS RLS or a JDBC
      implementation for production runs.</para>

      <para>A Replica Catalog maintains the LFN to PFN mapping for the input
      files of your workflow. Pegasus queries it to determine the locations of
      the raw input data files required by the workflow. Additionally, all the
      materialized data is registered into Replica Catalog for data reuse
      later on.</para>

      <para>You can use the <emphasis role="bold"> rc-client</emphasis>
      command to insert , query and delete from the replica catalog.</para>

      <para>To execute the diamond dax created in <emphasis
      role="bold">exercise 2.1</emphasis>, we will need to register input file
      f.a in the replica catalog. The file f.a resides at
      /scratch/tutorial/inputdata/diamond/f.a . Let us insert a single entry
      into the replica catalog.</para>

      <programlisting><command>$  rc-client -Dpegasus.user.properties=config/properties insert f.a \
          gsiftp://pegasus/scratch/tutorial/inputdata/diamond/f.a pool=local
</command></programlisting>

      <para>Let us know verify if f.a has been registered successfully by
      querying the replica catalog using rc-client</para>

      <programlisting><emphasis role="bold">$ rc-client -Dpegasus.user.properties=config/properties lookup f.a</emphasis>

 f.a gsiftp://pegasus/scratch/tutorial/inputdata/diamond/f.a pool="local"
</programlisting>

      <para>The <emphasis role="bold">rc-client</emphasis> also allows for
      bulk insertion of entries. We will be inserting the entries for montage
      workflow using the bulk mode. The input data to be used for the montage
      workflow resides in the /scratch/tutorial/inputdata/0.2degree directory.
      We are going to insert entries into the replica catalog that point to
      the files in this directory.</para>

      <para>The instructors have provided:</para>

      <itemizedlist>
        <listitem>
          <para>A file replicas.in, the input data file for the rc-client that
          contains the mappings that need to be populated in the Replica
          Catalog. The file is inside the config directory</para>
        </listitem>
      </itemizedlist>

      <para>Instructions:</para>

      <itemizedlist>
        <listitem>
          <para>Let us see what the file looks like. <programlisting><command>$ cat config/rc.in</command><computeroutput>


statfile_20070529_153243_22618.tbl
     gsiftp://pegasus/scratch/tutorial/inputdata/0.2degree/statfile.tbl
          pool="local"
2mass-atlas-990502s-j1440198.fits
     gsiftp://pegasus/scratch/0.2degree/2mass-atlas-990502s-j1440198.fits
          pool="local"
2mass-atlas-990502s-j1440186.fits
     gsiftp://pegasus/scratch/0.2degree/2mass-atlas-990502s-j1440186.fits
          pool="local"
 2mass-atlas-990502s-j1430092.fits
     gsiftp://pegasus/scratch/0.2degree/2mass-atlas-990502s-j1430092.fits
          pool="local"
 2mass-atlas-990502s-j1420198.fits
     gsiftp://pegasus/scratch/0.2degree/2mass-atlas-990502s-j1420198.fits
          pool="local"
 2mass-atlas-990502s-j1420186.fits
     gsiftp://pegasus/scratch/0.2degree/2mass-atlas-990502s-j1420186.fits
          pool="local"
 cimages_20070529_153243_22618.tbl
     gsiftp://pegasus/scratch/0.2degree/cimages.tbl pool="local"
 pimages_20070529_153243_22618.tbl
     gsiftp://pegasus/scratch/0.2degree/pimages.tbl pool="local"
 region_20070529_153243_22618.hdr
     gsiftp://pegasus/scratch/0.2degree/region.hdr pool="local"
 2mass-atlas-990502s-j1430080.fits
     gsiftp://pegasus/scratch/0.2degree/2mass-atlas-990502s-j1430080.fits
          pool="local"</computeroutput></programlisting></para>
        </listitem>

        <listitem>
          <para>Now we are ready to run rc-client and populate the data. Since
          each of you have an individual file replica catalog, all the 10
          entries should be successfully registered. <programlisting><command>$ rc-client -Dpegasus.user.properties=config/properties --insert config/rc.in</command><computeroutput><computeroutput>

#Successfully worked on : 12 lines</computeroutput>
#Worked on total number of : 12 lines.</computeroutput></programlisting></para>
        </listitem>

        <listitem>
          <para>Now the entries have been successfully inserted into the
          Replica Catalog. We should query the replica catalog for a
          particular lfn. <programlisting><command>$ rc-client -Dpegasus.user.properties=config/properties \
                   lookup pimages_20080505_143233_14944.tbl</command><computeroutput><computeroutput>

pimages_20080505_143233_14944.tbl
         gsiftp://pegasus/scratch/tutorial/inputdata/0.2degree/pimages.tbl 
           pool="local"</computeroutput></computeroutput></programlisting></para>
        </listitem>
      </itemizedlist>

      <para>Congratulations!! You have the replica catalog setup correctly for
      use. This is the catalog which you will tinker with most, while running
      Pegasus.</para>
    </section>

    <section>
      <title>Setting up the Site Catalog and Transformation Catalog</title>

      <para>In this exercise you will setup your Site Catalog and the
      Transformation Catalog.</para>

      <para>The instructors have provided: <itemizedlist>
          <listitem>
            <para>A ready transformation catalog (tc.data) in the
            $HOME/pegasus-wms/config directory.</para>
          </listitem>

          <listitem>
            <para>A semi ready properties file in the $HOME/pegasus-wms/config
            directory.</para>
          </listitem>
        </itemizedlist></para>

      <itemizedlist>
        <listitem>
          <para>The site catalog contains information about the layout of your
          grid where you want to run your workflows. For each site information
          like workdirectory, jobmanagers to use, gridftp servers to use and
          other site wide information like environment variables to be set is
          maintained.</para>

          <para>Lets see the site catalog for the Pegasus VM. It refers to two
          sites <emphasis role="bold">local</emphasis> and <emphasis
          role="bold">cluster</emphasis> .</para>

          <para><programlisting><command>$ cat $HOME/pegasus-wms/config/sites.xml</command><computeroutput></computeroutput><computeroutput>

&lt;sitecatalog xmlns="http://pegasus.isi.edu/schema/sitecatalog" xsi:schemaLocation="http://pegasus.isi.edu/schema/sitecatalog
 http://pegasus.isi.edu/schema/sc-2.0.xsd" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" version="2.0"&gt;
  
  &lt;site handle="local" sysinfo="INTEL32::LINUX"&gt;
    &lt;profile namespace="env" key="PEGASUS_HOME" &gt;/opt/pegasus/default&lt;/profile&gt;
    &lt;profile namespace="env" key="GLOBUS_LOCATION" &gt;/usr/local/globus/default&lt;/profile&gt;
    &lt;profile namespace="env" key="LD_LIBRARY_PATH" &gt;/usr/local/globus/default/lib&lt;/profile&gt;
    &lt;profile namespace="env" key="JAVA_HOME" &gt;/usr/java/default&lt;/profile&gt;
    &lt;lrc url="rlsn://localhost" /&gt;
    &lt;gridftp  url="file://" storage="/home/pegasus/local-storage" major="5" minor="0" patch="1"&gt; 
    &lt;/gridftp&gt;
    &lt;jobmanager universe="transfer" url="localhost/jobmanager-fork" major="5" minor="0" patch="1" /&gt;
    &lt;jobmanager universe="vanilla" url="localhost/jobmanager-fork" major="5" minor="0" patch="1" /&gt;
    &lt;workdirectory &gt;/home/pegasus/local-scratch&lt;/workdirectory&gt;
  &lt;/site&gt;
  
  &lt;site handle="cluster" sysinfo="INTEL32::LINUX"&gt;
    &lt;profile namespace="env" key="PEGASUS_HOME" &gt;/opt/pegasus/default&lt;/profile&gt;
    &lt;profile namespace="env" key="GLOBUS_LOCATION" &gt;/usr/local/globus/default&lt;/profile&gt;
    &lt;profile namespace="env" key="LD_LIBRARY_PATH" &gt;/usr/local/globus/default/lib&lt;/profile&gt;
    &lt;profile namespace="env" key="JAVA_HOME" &gt;/usr/java/default&lt;/profile&gt;
    &lt;profile namespace="pegasus" key="bundle.stagein" &gt;1&lt;/profile&gt;
    &lt;profile namespace="pegasus" key="bundle" &gt;1&lt;/profile&gt;
    &lt;lrc url="rlsn://localhost" /&gt;
    &lt;gridftp  url="gsiftp://pegasus" storage="/home/pegasus/cluster-storage" major="5" minor="0" patch="1"&gt; 
    &lt;/gridftp&gt;
    &lt;jobmanager universe="transfer" url="pegasus/jobmanager-fork" major="5" minor="0" patch="1" /&gt;
    &lt;jobmanager universe="vanilla" url="pegasus/jobmanager-sge" major="5" minor="0" patch="1" /&gt;
    &lt;workdirectory &gt;/home/pegasus/cluster-scratch&lt;/workdirectory&gt;
  &lt;/site&gt;

&lt;/sitecatalog&gt;
</computeroutput></programlisting></para>
        </listitem>
      </itemizedlist>

      <itemizedlist>
        <listitem>
          <para>The transformation catalog maintains information about where
          the application code resides on the grid. In our case, it contains
          the locations where the Diamond or Montage code is installed in the
          Pegasus VM. We will use the <emphasis
          role="bold">tc-client</emphasis> to add the entry for the
          transformation analyze into the transformation catalog.</para>

          <programlisting><emphasis role="bold">$ tc-client -Dpegasus.user.properties=config/properties -a -l diamond::analyze:2.0 \
      -p /opt/pegasus/default/bin/keg -r local -t INSTALLED -s INTEL32::LINUX 
<emphasis>
 2008.04.30 15:11:59.313 PDT: [INFO] Added tc entry sucessfully
</emphasis></emphasis></programlisting>

          <para>Let us try and query for the entry we inserted</para>

          <programlisting><emphasis role="bold">$ tc-client -Dpegasus.user.properties=config/properties -q -P -l diamond::analyze:2.0
<emphasis>
 #RESID     LTX                 PFN                                       TYPE          SYSINFO

local    diamond::analyze:2.0    /opt/pegasus/default/bin/keg    INSTALLED    INTEL32::LINUX
</emphasis></emphasis></programlisting>

          <para>We can also query the transformation catalog for all the
          entries in it. Let us see what our transformation catalog looks
          like</para>

          <programlisting><command>$ tc-client -Dpegasus.user.properties=config/properties -q -B          </command><computeroutput>

local  bin/mDiff       
             gsiftp://pegasus/scratch/tutorial/software/montage/3.0/x86/bin/mDiff       
                                STATIC_BINARY   INTEL32::LINUX  ENV::MONTAGE_HOME="."
local   bin/mFitplane
             gsiftp://pegasus/scratch/tutorial/software/montage/3.0/x86/bin/mFitplane
                                STATIC_BINARY   INTEL32::LINUX  ENV::MONTAGE_HOME="."
local   mAdd:3.0  
             gsiftp://pegasus/scratch/tutorial/software/montage/3.0/x86/bin/mAdd
                                STATIC_BINARY   INTEL32::LINUX  ENV::MONTAGE_HOME="."
local   mBackground:3.0
             gsiftp://pegasus/scratch/tutorial/software/montage/3.0/x86/bin/mBackground
                                STATIC_BINARY   INTEL32::LINUX  ENV::MONTAGE_HOME="."
local   mBgModel:3.0
             gsiftp://pegasus/scratch/tutorial/software/montage/3.0/x86/bin/mBgModel
                                STATIC_BINARY   INTEL32::LINUX  ENV::MONTAGE_HOME="."
local   mConcatFit:3.0 
             gsiftp://pegasus/scratch/tutorial/software/montage/3.0/x86/bin/mConcatFit
                                STATIC_BINARY   INTEL32::LINUX  ENV::MONTAGE_HOME="."
local   mDiffFit:3.0
             gsiftp://pegasus/scratch/tutorial/software/montage/3.0/x86/bin/mDiffFit 
                                STATIC_BINARY   INTEL32::LINUX  ENV::MONTAGE_HOME="."
local   mImgtbl:3.0 
             gsiftp://pegasus/scratch/tutorial/software/montage/3.0/x86/bin/mImgtbl  
                                STATIC_BINARY   INTEL32::LINUX  ENV::MONTAGE_HOME="."
local   mJPEG:3.0       
             gsiftp://pegasus/scratch/tutorial/software/montage/3.0/x86/bin/mJPEG  
                                STATIC_BINARY   INTEL32::LINUX  ENV::MONTAGE_HOME="."
local   mProject:3.0 
             gsiftp://pegasus/scratch/tutorial/software/montage/3.0/x86/bin/mProjectPP 
                                STATIC_BINARY   INTEL32::LINUX  ENV::MONTAGE_HOME="."
local   mProjectPP:3.0
             gsiftp://pegasus/scratch/tutorial/software/montage/3.0/x86/bin/mProjectPP
                                STATIC_BINARY   INTEL32::LINUX  ENV::MONTAGE_HOME="."
local   mShrink:3.0
             gsiftp://pegasus/scratch/tutorial/software/montage/3.0/x86/bin/mShrink
                                STATIC_BINARY   INTEL32::LINUX  NULL</computeroutput></programlisting>
        </listitem>

        <listitem>
          <para>Open the properties file and check a few properties.</para>

          <programlisting><command>$ cat config/properties</command><computeroutput><computeroutput>
##########################
# PEGASUS USER PROPERTIES 
##########################

## SELECT THE REPLICAT CATALOG MODE AND URL
pegasus.catalog.replica = SimpleFile
pegasus.catalog.replica.file = ${user.home}/pegasus-wms/config/rc.data
#pegasus.catalog.replica.url=rlsn://smarty.isi.edu

## SELECT THE SITE CATALOG MODE AND FILE
pegasus.catalog.site = XML
pegasus.catalog.site.file = ${user.home}/pegasus-wms/config/sites.xml


## SELECT THE TRANSFORMATION CATALOG MODE AND FILE
pegasus.catalog.transformation = File
pegasus.catalog.transformation.file = ${user.home}/pegasus-wms/config/tc.data

## SET UP THE WORK AND INVOCATION DATABASE
#pegasus.catalog.work =  Database
#pegasus.catalog.provenance = InvocationSchema

## Database related properties
#pegasus.catalog..db.driver = MySQL
#pegasus.catalog.*.db.url = jdbc:mysql://smarty.isi.edu/osg2008
#pegasus.catalog.*.db.user = osg2008user
#pegasus.catalog.*.db.password =  Osg2008

## USE DAGMAN RETRY FEATURE FOR FAILURES
pegasus.dagman.retry=2

## STAGE ALL OUR EXECUTABLES OR USE INSTALLED ONES 
pegasus.catalog.transformation.mapper = All

## CHECK JOB EXIT CODES FOR FAILURE
pegasus.exitcode.scope=all

## OPTIMZE DATA &amp; EXECUTABLE TRANSFERS
pegasus.transfer.refiner=Bundle

#STAGE DATA AND EXECUTABLES USING GRIDFTP 3rd PARTY MODE
pegasus.transfer.*.thirdparty.sites=*

pegasus.transfer.*.impl = GUC


## WORK AND STORAGE DIR  
pegasus.dir.storage = storage
pegasus.dir.exec = exec

# JOB Priorities

pegasus.job.priority=10
pegasus.transfer.*.priority=100

#JOB CATEGORIES
pegasus.dagman.projection.maxjobs 2
</computeroutput></computeroutput></programlisting>
        </listitem>

        <listitem>
          <para>Also the client pegasus-get-sites can be used to generate a
          site catalog and transformation catalog for the Open Science
          Grid.</para>

          <programlisting><command>$ </command><emphasis role="bold"><computeroutput>[pegasus@pegasus pegasus-wms]$ pegasus-get-sites -Dpegasus.user.properties=`pwd`/config/properties --vo engage --sc ./engage-osg-sc.xml \
  --source OSGMM --grid OSG 
</computeroutput></emphasis><computeroutput>

2010.07.19 16:44:49.933 PDT: [INFO] event.pegasus.planner planner.version 2.4.2  - STARTED 
2010.07.19 16:44:52.481 PDT: [INFO]  Adding site BNL-ATLAS 
2010.07.19 16:44:52.561 PDT: [INFO]  Adding site BNL-ATLAS__1 
2010.07.19 16:44:52.562 PDT: [INFO]  Adding site CIT_CMS_T2 
2010.07.19 16:44:52.563 PDT: [INFO]  Adding site CIT_CMS_T2__1 
2010.07.19 16:44:52.563 PDT: [INFO]  Adding site Clemson-Palmetto 
2010.07.19 16:44:52.564 PDT: [INFO]  Adding site FNAL_FERMIGRID 
2010.07.19 16:44:52.565 PDT: [INFO]  Adding site Firefly 
2010.07.19 16:44:52.565 PDT: [INFO]  Adding site Firefly__1 
2010.07.19 16:44:52.566 PDT: [INFO]  Adding site GLOW 
2010.07.19 16:44:52.567 PDT: [INFO]  Adding site GridUNESP_CENTRAL 
2010.07.19 16:44:52.567 PDT: [INFO]  Adding site LIGO_UWM_NEMO 
2010.07.19 16:44:52.568 PDT: [INFO]  Adding site MIT_CMS 
2010.07.19 16:44:52.569 PDT: [INFO]  Adding site NYSGRID_CORNELL_NYS1 
2010.07.19 16:44:52.569 PDT: [INFO]  Adding site Nebraska 
2010.07.19 16:44:52.570 PDT: [INFO]  Adding site Nebraska__1 
2010.07.19 16:44:52.570 PDT: [INFO]  Adding site Purdue-RCAC 
2010.07.19 16:44:52.571 PDT: [INFO]  Adding site Purdue-Steele 
2010.07.19 16:44:52.572 PDT: [INFO]  Adding site RENCI-Engagement 
2010.07.19 16:44:52.573 PDT: [INFO]  Adding site SBGrid-Harvard-East 
2010.07.19 16:44:52.587 PDT: [INFO]  Adding site TTU-ANTAEUS 
2010.07.19 16:44:52.588 PDT: [INFO]  Adding site UCHC_CBG 
2010.07.19 16:44:52.601 PDT: [INFO]  Adding site UCR-HEP 
2010.07.19 16:44:52.602 PDT: [INFO]  Adding site UCSDT2 
2010.07.19 16:44:52.604 PDT: [INFO]  Adding site UConn-OSG 
2010.07.19 16:44:52.618 PDT: [INFO]  Adding site UFlorida-HPC 
2010.07.19 16:44:52.619 PDT: [INFO]  Adding site UFlorida-PG 
2010.07.19 16:44:52.619 PDT: [INFO]  Adding site USCMS-FNAL-WC1 
2010.07.19 16:44:52.620 PDT: [INFO]  Adding site WQCG-Harvard-OSG 
2010.07.19 16:44:52.620 PDT: [INFO]  Site LOCAL . Creating default entry 
2010.07.19 16:44:52.691 PDT: [INFO]  Loaded 30 sites  
2010.07.19 16:44:52.691 PDT: [INFO]  Writing out site catalog to /home/pegasus/pegasus-wms/./engage-osg-sc.xml 
2010.07.19 16:44:54.050 PDT: [INFO]  Number of SRM Properties retrieved 14 
2010.07.19 16:44:54.086 PDT: [INFO]  Writing out properties to /home/pegasus/pegasus-wms/./pegasus.5645415039834437744.properties 
2010.07.19 16:44:54.089 PDT: [INFO]  Time taken to execute is 4.15 seconds 
2010.07.19 16:44:54.089 PDT: [INFO] event.pegasus.planner planner.version 2.4.2

</computeroutput></programlisting>
        </listitem>
      </itemizedlist>
    </section>

    <section>
      <title>Planning workflow using pegasus-plan and running locally using
      pegasus-run.</title>

      <para>In this exercise we are going to run pegasus-plan to generate a
      executable workflow from the abstract workflow (diamond.dax). The
      Executable workflow generated, are condor submit files that are
      submitted locally using pegasus-run</para>

      <para>The instructors have provided: <itemizedlist>
          <listitem>
            <para>A dax (diamond.dax) in the $HOME/pegasus-wms/dax
            directory.</para>
          </listitem>
        </itemizedlist></para>

      <para>You will need to write some things yourself, by following the
      instructions below: <itemizedlist>
          <listitem>
            <para>Run pegasus-plan to generate the condor submit files out of
            the dax.</para>
          </listitem>

          <listitem>
            <para>Run pegasus-run to submit the workflow locally.</para>
          </listitem>
        </itemizedlist></para>

      <para>Instructions:</para>

      <itemizedlist>
        <listitem>
          <para>Let us run pegasus-plan on the diamond dax. <programlisting><command>$ cd ~/pegasus-wms
$ pegasus-plan -Dpegasus.user.properties=`pwd`/config/properties \
               --dax `pwd`/dax/diamond.dax --force\
               --dir dags -s local -o local --nocleanup</command></programlisting>
          The above command says that we need to plan the diamond dax locally.
          The condor submit files are to be generated in a directory structure
          whose base is dags. We also are requesting that no cleanup jobs be
          added as we require the intermediate data to be saved. Here is the
          output of pegasus-plan. <programlisting><computeroutput>2010.07.19 16:47:05.276 PDT: [INFO] event.pegasus.planner planner.version 2.4.2  - STARTED 
2010.07.19 16:47:05.858 PDT: [INFO] event.pegasus.parse.dax dax.id /home/pegasus/pegasus-wms/dax/diamond.dax  - STARTED 
2010.07.19 16:47:05.901 PDT: [INFO] event.pegasus.parse.dax dax.id /home/pegasus/pegasus-wms/dax/diamond.dax  - FINISHED 
2010.07.19 16:47:05.944 PDT: [INFO] event.pegasus.refinement dax.id diamond_0  - STARTED 
2010.07.19 16:47:05.957 PDT: [INFO] event.pegasus.siteselection dax.id diamond_0  - STARTED 
2010.07.19 16:47:05.977 PDT: [INFO] event.pegasus.siteselection dax.id diamond_0  - FINISHED 
2010.07.19 16:47:05.986 PDT: [INFO]  Grafting transfer nodes in the workflow 
2010.07.19 16:47:05.987 PDT: [INFO] event.pegasus.generate.transfer-nodes dax.id diamond_0  - STARTED 
2010.07.19 16:47:06.031 PDT: [INFO] event.pegasus.generate.transfer-nodes dax.id diamond_0  - FINISHED 
2010.07.19 16:47:06.032 PDT: [INFO] event.pegasus.generate.workdir-nodes dax.id diamond_0  - STARTED 
2010.07.19 16:47:06.034 PDT: [INFO] event.pegasus.generate.workdir-nodes dax.id diamond_0  - FINISHED 
2010.07.19 16:47:06.034 PDT: [INFO] event.pegasus.generate.cleanup-wf dax.id diamond_0  - STARTED 
2010.07.19 16:47:06.035 PDT: [INFO] event.pegasus.generate.cleanup-wf dax.id diamond_0  - FINISHED 
2010.07.19 16:47:06.035 PDT: [INFO] event.pegasus.refinement dax.id diamond_0  - FINISHED 
2010.07.19 16:47:06.202 PDT: [INFO]  Generating codes for the concrete workflow 
2010.07.19 16:47:06.316 PDT: [INFO]  Generating codes for the concrete workflow -DONE 
2010.07.19 16:47:06.316 PDT: [INFO]  Generating code for the cleanup workflow 
2010.07.19 16:47:06.369 PDT: [INFO]  Generating code for the cleanup workflow -DONE 
2010.07.19 16:47:06.373 PDT: [INFO]  


I have concretized your abstract workflow. The workflow has been entered 
into the workflow database with a state of "planned". The next step is 
to start or execute your workflow. The invocation required is


pegasus-run -Dpegasus.user.properties=/home/pegasus/pegasus-wms/dags/pegasus/pegasus/diamond/run0001/pegasus.4459134667464687814.properties \
 --nodatabase /home/pegasus/pegasus-wms/dags/pegasus/pegasus/diamond/run0001

 
2010.07.19 16:47:06.373 PDT: [INFO]  Time taken to execute is 1.065 seconds 
2010.07.19 16:47:06.374 PDT: [INFO] event.pegasus.planner planner.version 2.4.2  - FINISHED 
</computeroutput></programlisting></para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Now run pegasus-run as mentioned in the
          output of pegasus-plan. Do not copy the command below it is just for
          illustration purpose.</emphasis><programlisting><emphasis
                role="bold">[pegasus@pegasus pegasus-wms]$ pegasus-run \
     -Dpegasus.user.properties=/home/pegasus/pegasus-wms/dags/pegasus/pegasus/diamond/run0001/pegasus.4459134667464687814.properties \
    --nodatabase /home/pegasus/pegasus-wms/dags/pegasus/pegasus/diamond/run0001
</emphasis>
-----------------------------------------------------------------------
File for submitting this DAG to Condor           : diamond-0.dag.condor.sub
Log of DAGMan debugging messages                 : diamond-0.dag.dagman.out
Log of Condor library output                     : diamond-0.dag.lib.out
Log of Condor library error messages             : diamond-0.dag.lib.err
Log of the life of condor_dagman itself          : diamond-0.dag.dagman.log

-no_submit given, not submitting DAG to Condor.  You can do this with:
"condor_submit diamond-0.dag.condor.sub"
-----------------------------------------------------------------------
Submitting job(s).
Logging submit event(s).
1 job(s) submitted to cluster 72.

I have started your workflow, committed it to DAGMan, and updated its
state in the work database. A separate daemon was started to collect
information about the progress of the workflow. The job state will soon
be visible. Your workflow runs in base directory. 

cd /home/pegasus/pegasus-wms/dags/pegasus/pegasus/diamond/run0001

*** To monitor the workflow you can run ***

pegasus-status -w diamond-0 -t 20100719T164705-0700 
or
pegasus-status /home/pegasus/pegasus-wms/dags/pegasus/pegasus/diamond/run0001

*** To remove your workflow run ***

pegasus-remove -d 72.0
or
pegasus-remove /home/pegasus/pegasus-wms/dags/pegasus/pegasus/diamond/run0001

</programlisting></para>
        </listitem>
      </itemizedlist>
    </section>

    <section>
      <title>Tracking the progress of the workflow and debugging the
      workflows.</title>

      <para>In this exercise we are going to list ways to track your workflow,
      and give some debugging hints when something goes wrong.</para>

      <para>We will change into the directory, that was mentioned by the
      output of pegasus-run command.</para>

      <programlisting><command>$ cd /home/pegasus/pegasus-wms/dags/pegasus/pegasus/diamond/runXXXX</command></programlisting>

      <para>In this directory you will see a whole lot of files. That should
      not scare you. Unless things go wrong, you need to look at just a very
      few number of files to track the progress of the workflow</para>

      <itemizedlist>
        <listitem>
          <para><emphasis role="bold">Run the command pegasus-status as
          mentioned by pegasus-run above to check the status of your jobs. Use
          the watch command to auto repeat the command every 2
          seconds.</emphasis><programlisting><command>$ watch pegasus-status /home/pegasus/pegasus-wms/dags/pegasus/pegasus/diamond/runXXXX</command><computeroutput>

-- Submitter: pegasus : &lt;172.16.80.128:40195&gt; : pegasus
 ID      OWNER/NODENAME   SUBMITTED     RUN_TIME ST PRI SIZE CMD               
  84.0   pegasus         7/19 16:59   0+00:01:17 R  0   7.3  condor_dagman -f -
  87.0    |-preprocess_  7/19 17:00   0+00:00:31 R  10  0.1  kickstart -n diamo
</computeroutput></programlisting> The above output shows that a couple of
          jobs are running under the main dagman process. Keep a lookout to
          track whether a workflow is running or not. If you do not see any of
          your job in the output for sometime (say 30 seconds), we know the
          workflow has finished. We need to wait, as there might be delay in
          Condor DAGMan releasing the next job into the queue after a job has
          finished successfully.</para>

          <para>If output of pegasus-status is empty, then either your
          workflow has - successfully completed - stopped midway due to non
          recoverable error. We can now run pegasus-analyzer to analyze the
          workflow.</para>
        </listitem>

        <listitem>
          <para>Using <emphasis role="bold">pegasus-analyzer</emphasis> to
          analyze the workflow</para>

          <programlisting><emphasis role="bold">[pegasus@pegasus run0001]$ pegasus-analyzer -q -i /home/pegasus/pegasus-wms/dags/pegasus/pegasus/diamond/run0001</emphasis>

pegasus-analyzer: initializing...

************************************Summary*************************************

 Total jobs         :     11 (100.00%)
 # jobs succeeded   :     11 (100.00%)
 # jobs failed      :      0 (0.00%)
 # jobs unsubmitted :      0 (0.00%)

**************************************Done**************************************

pegasus-analyzer: end of status report

</programlisting>
        </listitem>

        <listitem>
          <para>Another way to monitor the workflow is to check the <emphasis
          role="bold">jobstate.log</emphasis> file. This is the output file of
          the monitoring daemon that is parsing all the condor log files to
          determine the status of the jobs. It logs the events seen by Condor
          into a more readable form for us. <programlisting><command>$ more jobstate.log</command><computeroutput>

1201557528 INTERNAL *** TAILSTATD_STARTED ***
1201557528 INTERNAL *** DAGMAN_STARTED *** 
1201557528 generate_ID000001_0 UN_READY - - - 
1201557528 findrange_ID000003_0 UN_READY - - - 
1201557528 findrange_ID000002_0 UN_READY - - - 
1201557528 analyze_ID000004_0 UN_READY - - -
[..]</computeroutput></programlisting> In the starting of the jobstate.log,
          when the workflow has just started running you will see a lot of
          entries with status UN_READY. That designates that DAGMan has just
          parsed in the .dag file and has not started working on any job as
          yet. Initially all the jobs in the workflow are listed as UN_READY.
          After sometime you will see entries in jobstate.log, that shows a
          job is being executed etc. <programlisting><computeroutput>1201557747 generate_ID000001_0 EXECUTE 19996.0 local - 
1201557747 generate_ID000001_0 GLOBUS_SUBMIT 19996.0 local - 
1201557812 generate_ID000001_0 JOB_TERMINATED 19996.0 local - 
1201557812 generate_ID000001_0 POST_SCRIPT_STARTED - local - 
1201557817 generate_ID000001_0 POST_SCRIPT_TERMINATED 19996.0 local - 
1201557817 generate_ID000001_0 POST_SCRIPT_SUCCESS - local -</computeroutput></programlisting></para>

          <para>The above shows the being submitted and then executed on the
          grid. In addition it lists that job is being run on the grid site
          local (which is your submit machine). The various states of the job
          while it goes through submission to execution to post processing are
          in UPPERCASE.</para>
        </listitem>

        <listitem>
          <para>Successfully Completed : Let us again look at the
          jobstate.log. This time we need to look at the last few lines of
          jobstate.log <programlisting><command>$ tail jobstate.log</command><computeroutput>

1201559232 analyze_ID000004 JOB_TERMINATED 20023.0 local - 
1201559232 analyze_ID000004 POST_SCRIPT_STARTED - local -
1201559238 analyze_ID000004 POST_SCRIPT_TERMINATED 20023.0 local -
1201559238 analyze_ID000004 POST_SCRIPT_SUCCESS - local -
1201559238 INTERNAL *** DAGMAN_FINISHED ***
1201559239 INTERNAL *** TAILSTATD_FINISHED 0 ***</computeroutput></programlisting>
          Looking at the last two lines we see that DAGMan finished, and
          tailstatd finished successfully with a status 0. This means workflow
          ran successfully. Congratulations you ran your workflow on the local
          site successfully. The workflow generates a final output file f.d
          that resides in the directory <emphasis
          role="bold">/home/pegasus/local-storage/storage/f.d</emphasis>
          .</para>

          <para>To view the file, you can do the following <programlisting><command>$ cat /home/pegasus/local-storage/storage/f.d
</command>--- start f.c1 ----
  --- start f.b1 ----
    --- start f.a ----
      Input File for the Diamond Workflow.--- final f.a ----
    Timestamp Today: 20100719T170023-07:00 (1279584023.790;60.001)
    Applicationname: preprocess @ 172.16.80.128 (VPN)
    Current Workdir: /home/pegasus/local-scratch/exec/pegasus/pegasus/diamond/run0001
    Systemenvironm.: x86_64-Linux 2.6.18-194.8.1.el5
    Processor Info.: 1 x Intel(R) Xeon(R) CPU           E5462  @ 2.80GHz @ 2794.173
    Output Filename: f.b1
    Input Filenames: f.a
  --- final f.b1 ----
  Timestamp Today: 20100719T170139-07:00 (1279584099.621;60.022)
  Applicationname: findrange @ 172.16.80.128 (VPN)
  Current Workdir: /home/pegasus/local-scratch/exec/pegasus/pegasus/diamond/run0001
  Systemenvironm.: x86_64-Linux 2.6.18-194.8.1.el5
  Processor Info.: 1 x Intel(R) Xeon(R) CPU           E5462  @ 2.80GHz @ 2794.173
  Output Filename: f.c1
  Input Filenames: f.b1
--- final f.c1 ----
--- start f.c2 ----
  --- start f.b2 ----
    --- start f.a ----
      Input File for the Diamond Workflow.--- final f.a ----
    Timestamp Today: 20100719T170023-07:00 (1279584023.790;60.001)
    Applicationname: preprocess @ 172.16.80.128 (VPN)
    Current Workdir: /home/pegasus/local-scratch/exec/pegasus/pegasus/diamond/run0001
    Systemenvironm.: x86_64-Linux 2.6.18-194.8.1.el5
    Processor Info.: 1 x Intel(R) Xeon(R) CPU           E5462  @ 2.80GHz @ 2794.173
    Output Filename: f.b2
    Input Filenames: f.a
  --- final f.b2 ----
  Timestamp Today: 20100719T170144-07:00 (1279584104.781;60.001)
  Applicationname: findrange @ 172.16.80.128 (VPN)
  Current Workdir: /home/pegasus/local-scratch/exec/pegasus/pegasus/diamond/run0001
  Systemenvironm.: x86_64-Linux 2.6.18-194.8.1.el5
  Processor Info.: 1 x Intel(R) Xeon(R) CPU           E5462  @ 2.80GHz @ 2794.173
  Output Filename: f.c2
  Input Filenames: f.b2
--- final f.c2 ----
Timestamp Today: 20100719T170300-07:00 (1279584180.331;60.001)
Applicationname: analyze @ 172.16.80.128 (VPN)
Current Workdir: /home/pegasus/local-scratch/exec/pegasus/pegasus/diamond/run0001
Systemenvironm.: x86_64-Linux 2.6.18-194.8.1.el5
Processor Info.: 1 x Intel(R) Xeon(R) CPU           E5462  @ 2.80GHz @ 2794.173
Output Filename: f.d
Input Filenames: f.c1 f.c2
</programlisting></para>
        </listitem>

        <listitem>
          <para>Unsuccessfully Completed (Workflow execution stopped midway) :
          Let us again look at the jobstate.log. Again we need to look at the
          last few lines of jobstate.log <programlisting><command>$ tail jobstate.log</command><computeroutput>

1180840233 analyze_ID000004_0 JOB_TERMINATED 2787.0 local -
1180840233 analyze_ID000004_0 POST_SCRIPT_STARTED - local - 
1180840238 analyze_ID000004_0 POST_SCRIPT_TERMINATED 2787.0 local - 
1180840238 analyze_ID000004_0 POST_SCRIPT_FAILURE 1 local -
1180840373 INTERNAL *** DAGMAN_FINISHED *** 
1180840373 INTERNAL *** TAILSTATD_FINISHED 1 ***</computeroutput></programlisting>Looking
          at the last two lines we see that DAGMan finished, and tailstatd
          finished unsuccessfully with a status 1. We can easily determine
          which job failed. It is inter_tx_mDiffFit_ID000007_0 in this case.
          To determine the reason for failure we need to look at it's
          kickstart output file which is JOBNAME.out.NNN. where NNN is 000 -
          NNN</para>
        </listitem>
      </itemizedlist>
    </section>

    <section>
      <title>Debugging a failed workflow using pegasus-analyzer</title>

      <para>In this section, we will run the diamond workflow but remove the
      input file so that the workflow fails during execution. This is to
      highlight how to use pegasus-analyzer to debug a failed workflow.</para>

      <para>First of all lets rename the input file f.a</para>

      <programlisting><emphasis role="bold"> mv /scratch/tutorial/inputdata/diamond/f.a /scratch/tutorial/inputdata/diamond/f.a.old
</emphasis></programlisting>

      <para>We will now repeat exercise <emphasis role="bold">2.4 and
      2.5</emphasis> and submit the workflow again.</para>

      <programlisting><emphasis role="bold">Plan and Submit the diamond workflow</emphasis> . Pass --submit to pegasus-plan to submit in case of successful planning

[pegasus@pegasus pegasus-wms]$  pegasus-plan -Dpegasus.user.properties=`pwd`/config/properties --dax `pwd`/dax/diamond.dax --force \
        --dir dags -s local -o local --nocleanup --submit

<emphasis role="bold">
Use pegasus-status to track the workflow and wait it to fail</emphasis>

[pegasus@pegasus pegasus-wms]$ pegasus-status  /home/pegasus/pegasus-wms/dags/pegasus/pegasus/diamond/run0002


-- Submitter: pegasus : &lt;172.16.80.128:40195&gt; : pegasus
 ID      OWNER/NODENAME   SUBMITTED     RUN_TIME ST PRI SIZE CMD               
  96.0   pegasus         7/19 17:40   0+00:01:06 R  0   7.3  condor_dagman -f -

<emphasis role="bold">
We can also use --long option to pegasus-status to see the FINAL status of the workflow</emphasis>

[pegasus@pegasus pegasus-wms]$ pegasus-status --long /home/pegasus/pegasus-wms/dags/pegasus/pegasus/diamond/run0002
diamond-0.dag FAILED (status 1)
07/19 17:42:07  Done     Pre   Queued    Post   Ready   Un-Ready   Failed
07/19 17:42:07   ===     ===      ===     ===     ===        ===      ===
07/19 17:42:07     1       0        0       0       0          9        1

WORKFLOW STATUS : 1/11 ( 9% ) FAILED (rescue needs to be submitted)

</programlisting>

      <para>We will now run pegasus-analyzer on the failed workflow submit
      directory to see what job failed.</para>

      <programlisting><emphasis role="bold">[pegasus@pegasus pegasus-wms]$ pegasus-analyzer -q -i /home/pegasus/pegasus-wms/dags/pegasus/pegasus/diamond/run0002
</emphasis>
pegasus-analyzer: initializing...

************************************Summary*************************************

 Total jobs         :     11 (100.00%)
 # jobs succeeded   :      1 (9.09%)
 # jobs failed      :      1 (9.09%)
 # jobs unsubmitted :      9 (81.82%)

******************************Failed jobs' details******************************

================================stage_in_local_0================================

 last state: POST_SCRIPT_FAILURE
       site: local
submit file: /home/pegasus/pegasus-wms/dags/pegasus/pegasus/diamond/run0002/stage_in_local_0.sub
output file: /home/pegasus/pegasus-wms/dags/pegasus/pegasus/diamond/run0002/stage_in_local_0.out.002
 error file: /home/pegasus/pegasus-wms/dags/pegasus/pegasus/diamond/run0002/stage_in_local_0.err.002

**************************************Done**************************************

pegasus-analyzer: end of status report

</programlisting>

      <para>The above tells us that the stage-in job for the workflow failed,
      and points us to the stdout of the job. By default, all jobs in Pegasus
      are launched via kickstart that captures runtime provenance of the job
      and helps in debugging. Hence, the stdout of the job is the kickstart
      stdout which is in XML.</para>

      <para>. the duration of the job the start time for the job the node on
      which the job ran the stdout/stderr of the job the arguments with which
      it launched the job the environment that was set for the job before it
      was launched. the machine information about the node that the job ran on
      Amongst the above information, the dagman.out file gives a coarser
      grained estimate of the job duration and start time</para>
    </section>

    <section>
      <title>Kickstart</title>

      <para>Kickstart is a light weight C executable that is shipped with the
      pegasus worker package. All jobs are launced via Kickstart on the remote
      end, unless explicitly disabled at the time of running
      pegasus-plan.</para>

      <para>Kickstart does not work with</para>

      <orderedlist>
        <listitem>
          <para>Condor Standard Universe Jobs</para>
        </listitem>

        <listitem>
          <para>MPI jobs</para>
        </listitem>
      </orderedlist>

      <para>Pegasus automatically disables kickstart for the above
      jobs.</para>

      <para>Kickstart captures useful runtime provenance information about the
      job launched by it on the remote note, and puts in an XML record that it
      writes to it's stdout. The stdout appears in the workflow submit
      directory as &lt;job&gt;.out.00n . Some useful information captured by
      kickstart and logged are as follows</para>

      <orderedlist>
        <listitem>
          <para>the exitcode with which the job it launched exited</para>
        </listitem>

        <listitem>
          <para>the duration of the job</para>
        </listitem>

        <listitem>
          <para>the start time for the job</para>
        </listitem>

        <listitem>
          <para>the node on which the job ran</para>
        </listitem>

        <listitem>
          <para>the directory in which the job ran</para>
        </listitem>

        <listitem>
          <para>the stdout/stderr of the job</para>
        </listitem>

        <listitem>
          <para>the arguments with which it launched the job</para>
        </listitem>

        <listitem>
          <para>the environment that was set for the job before it was
          launched.</para>
        </listitem>

        <listitem>
          <para>the machine information about the node that the job ran
          on</para>
        </listitem>
      </orderedlist>

      <section>
        <title>Reading a Kickstart Output File</title>

        <para>Lets look at the stdout of our failed job.</para>

        <programlisting><emphasis role="bold">[pegasus@pegasus pegasus-wms]$ cat  /home/pegasus/pegasus-wms/dags/pegasus/pegasus/diamond/run0002/stage_in_local_0.out.002</emphasis>

&lt;?xml version="1.0" encoding="ISO-8859-1"?&gt;
&lt;invocation xmlns="http://pegasus.isi.edu/schema/invocation" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" \
xsi:schemaLocation="http://pegasus.isi.edu/schema/invocation http://pegasus.isi.edu/schema/iv-2.0.xsd" version="2.0" \
start="2010-07-19T17:41:57.857-07:00" duration="0.091" <emphasis role="bold">transformation="globus::guc"</emphasis> derivation="globus::guc" <emphasis
            role="bold">resource="local"</emphasis> \
wf-label="diamond" wf-stamp="2010-07-19T14:36:28-07:00" <emphasis role="bold">hostaddr="172.16.80.128"</emphasis> <emphasis
            role="bold">hostname="pegasus"</emphasis> pid="12792" uid="501" \
user="pegasus" gid="501" group="pegasus" umask="0022"&gt;
  <emphasis role="bold">&lt;mainjob start="2010-07-19T17:41:57.870-07:00" duration="0.078" pid="12793"&gt;</emphasis>
    &lt;usage utime="0.012" stime="0.012" minflt="1115" majflt="0" nswap="0" nsignals="0" nvcsw="2" nivcsw="24"/&gt;
    <emphasis role="bold">&lt;status raw="256"&gt;&lt;regular exitcode="1"/&gt;&lt;/status&gt;</emphasis>
    &lt;statcall error="0"&gt;
      &lt;!-- deferred flag: 0 --&gt;
      &lt;file name="/usr/local/globus/default/bin/globus-url-copy"&gt;7F454C46020101000000000000000000&lt;/file&gt;
      &lt;statinfo mode="0100755" size="94699" inode="1109806" nlink="1" blksize="4096" blocks="208" mtime="2008-10-02T14:38:57-07:00" atime="2010-07-19T17:41:42-07:00" ctime="2010-05-24T15:17:51-07:00" uid="30101" gid="30101"/&gt;
    &lt;/statcall&gt;
    <emphasis role="bold">&lt;argument-vector&gt;
      &lt;arg nr="1"&gt;-p&lt;/arg&gt;
      &lt;arg nr="2"&gt;1&lt;/arg&gt;
      &lt;arg nr="3"&gt;-cd&lt;/arg&gt;
      &lt;arg nr="4"&gt;-vb&lt;/arg&gt;
      &lt;arg nr="5"&gt;-f&lt;/arg&gt;
      &lt;arg nr="6"&gt;stage_in_local_0.in&lt;/arg&gt;
    &lt;/argument-vector&gt;</emphasis>
  &lt;/mainjob&gt;
  <emphasis role="bold">&lt;cwd&gt;/home/pegasus/pegasus-wms/dags/pegasus/pegasus/diamond/run0002&lt;/cwd&gt;</emphasis>
  &lt;usage utime="0.002" stime="0.014" minflt="472" majflt="0" nswap="0" nsignals="0" nvcsw="1" nivcsw="6"/&gt;
  <emphasis role="bold">&lt;machine page-size="4096" provider="LINUX"&gt;
    &lt;stamp&gt;2010-07-19T17:41:57.857-07:00&lt;/stamp&gt;
    &lt;uname system="linux" nodename="pegasus" release="2.6.18-194.8.1.el5" machine="x86_64"&gt;#1 SMP Thu Jul 1 19:04:48 EDT 2010&lt;/uname&gt;
    &lt;ram total="1051533312" free="43859968" shared="0" buffer="102002688"/&gt;
    &lt;swap total="2113921024" free="2113921024"/&gt;
    &lt;boot idle="21305.050"&gt;2010-07-19T11:37:35.937-07:00&lt;/boot&gt;
    &lt;cpu count="1" speed="2794" vendor="GenuineIntel"&gt;Intel(R) Xeon(R) CPU E5462 @ 2.80GHz&lt;/cpu&gt;
    &lt;load min1="0.00" min5="0.00" min15="0.00"/&gt;
    &lt;proc total="113" running="1" sleeping="111" zombie="1" vmsize="7912247296" rss="401776640"/&gt;
    &lt;task total="140" running="1" sleeping="138" zombie="1"/&gt;
  &lt;/machine&gt;</emphasis>
  &lt;statcall error="0" id="stdin"&gt;
    &lt;!-- deferred flag: 0 --&gt;
    &lt;file name="/dev/null"/&gt;
    &lt;statinfo mode="020666" size="0" inode="1777" nlink="1" blksize="4096" blocks="0" mtime="2010-07-19T11:38:11-07:00" atime="2010-07-19T11:38:11-07:00" ctime="2010-07-19T11:38:11-07:00" uid="0" user="root" gid="0" group="root"/&gt;
  &lt;/statcall&gt;
  <emphasis role="bold">&lt;statcall error="0" id="stdout"&gt;
    &lt;temporary name="/opt/condor/local.pegasus/spool/local_univ_execute/dir_12791/gs.out.3qsk2X" descriptor="3"/&gt;
    &lt;statinfo mode="0100600" size="149" inode="2021925" nlink="1" blksize="4096" blocks="16" mtime="2010-07-19T17:41:57-07:00" atime="2010-07-19T17:41:57-07:00" ctime="2010-07-19T17:41:57-07:00" uid="501" user="pegasus" gid="501" group="pegasus"/&gt;
    &lt;data&gt;Source: gsiftp://pegasus/scratch/tutorial/inputdata/diamond/
Dest:   file:///home/pegasus/local-scratch/exec/pegasus/pegasus/diamond/run0002/
  f.a

&lt;/data&gt;
  &lt;/statcall&gt;
  &lt;statcall error="0" id="stderr"&gt;
    &lt;temporary name="/opt/condor/local.pegasus/spool/local_univ_execute/dir_12791/gs.err.kooIKM" descriptor="4"/&gt;
    &lt;statinfo mode="0100600" size="338" inode="2021926" nlink="1" blksize="4096" blocks="16" mtime="2010-07-19T17:41:57-07:00" atime="2010-07-19T17:41:57-07:00" ctime="2010-07-19T17:41:57-07:00" uid="501" user="pegasus" gid="501" group="pegasus"/&gt;
    &lt;data&gt;
error: globus_ftp_client: the server responded with an error
500 500-Command failed. : globus_l_gfs_file_open failed.
500-globus_xio: Unable to open file /scratch/tutorial/inputdata/diamond/f.a
500-globus_xio: System error in open: No such file or directory
500-globus_xio: A system call failed: No such file or directory
500 End.

&lt;/data&gt;</emphasis>
  &lt;/statcall&gt;
  &lt;statcall error="2" id="gridstart"&gt;
    &lt;!-- deferred flag: 0 --&gt;
    &lt;file name="condor_exec.exe"/&gt;
  &lt;/statcall&gt;
  &lt;statcall error="0" id="logfile"&gt;
    &lt;descriptor number="1"/&gt;
    &lt;statinfo mode="0100644" size="0" inode="2054616" nlink="1" blksize="4096" blocks="8" mtime="2010-07-19T17:41:57-07:00" atime="2010-07-19T17:41:57-07:00" ctime="2010-07-19T17:41:57-07:00" uid="501" user="pegasus" gid="501" group="pegasus"/&gt;
  &lt;/statcall&gt;
  &lt;statcall error="0" id="channel"&gt;
    &lt;fifo name="/opt/condor/local.pegasus/spool/local_univ_execute/dir_12791/gs.app.zYO6sB" descriptor="5" count="0" rsize="0" wsize="0"/&gt;
    &lt;statinfo mode="010640" size="0" inode="2021927" nlink="1" blksize="4096" blocks="8" mtime="2010-07-19T17:41:57-07:00" atime="2010-07-19T17:41:57-07:00" ctime="2010-07-19T17:41:57-07:00" uid="501" user="pegasus" gid="501" group="pegasus"/&gt;
  &lt;/statcall&gt;
  &lt;environment&gt;
    &lt;env key="GLOBUS_LOCATION"&gt;/usr/local/globus/default&lt;/env&gt;
    &lt;env key="GRIDSTART_CHANNEL"&gt;/opt/condor/local.pegasus/spool/local_univ_execute/dir_12791/gs.app.zYO6sB&lt;/env&gt;
    &lt;env key="JAVA_HOME"&gt;/usr/java/default&lt;/env&gt;
    &lt;env key="LD_LIBRARY_PATH"&gt;/usr/local/globus/default/lib&lt;/env&gt;
    &lt;env key="PEGASUS_HOME"&gt;/opt/pegasus/default&lt;/env&gt;
    &lt;env key="TEMP"&gt;/opt/condor/local.pegasus/spool/local_univ_execute/dir_12791&lt;/env&gt;
    &lt;env key="TMP"&gt;/opt/condor/local.pegasus/spool/local_univ_execute/dir_12791&lt;/env&gt;
    &lt;env key="TMPDIR"&gt;/opt/condor/local.pegasus/spool/local_univ_execute/dir_12791&lt;/env&gt;
    &lt;env key="_CONDOR_ANCESTOR_12791"&gt;12792:1279586517:2377531392&lt;/env&gt;
    &lt;env key="_CONDOR_ANCESTOR_3849"&gt;4088:1279564727:2726859010&lt;/env&gt;
    &lt;env key="_CONDOR_ANCESTOR_4088"&gt;12791:1279586517:3701537309&lt;/env&gt;
    &lt;env key="_CONDOR_HIGHPORT"&gt;41000&lt;/env&gt;
    &lt;env key="_CONDOR_JOB_AD"&gt;/opt/condor/local.pegasus/spool/local_univ_execute/dir_12791/.job.ad&lt;/env&gt;
    &lt;env key="_CONDOR_LOWPORT"&gt;40000&lt;/env&gt;
    &lt;env key="_CONDOR_MACHINE_AD"&gt;/opt/condor/local.pegasus/spool/local_univ_execute/dir_12791/.machine.ad&lt;/env&gt;
    &lt;env key="_CONDOR_SCRATCH_DIR"&gt;/opt/condor/local.pegasus/spool/local_univ_execute/dir_12791&lt;/env&gt;
    &lt;env key="_CONDOR_SLOT"&gt;1&lt;/env&gt;
  &lt;/environment&gt;
  &lt;resource&gt;
    &lt;soft id="RLIMIT_CPU"&gt;unlimited&lt;/soft&gt;
    &lt;hard id="RLIMIT_CPU"&gt;unlimited&lt;/hard&gt;
    &lt;soft id="RLIMIT_FSIZE"&gt;unlimited&lt;/soft&gt;
    &lt;hard id="RLIMIT_FSIZE"&gt;unlimited&lt;/hard&gt;
    &lt;soft id="RLIMIT_DATA"&gt;unlimited&lt;/soft&gt;
    &lt;hard id="RLIMIT_DATA"&gt;unlimited&lt;/hard&gt;
    &lt;soft id="RLIMIT_STACK"&gt;unlimited&lt;/soft&gt;
    &lt;hard id="RLIMIT_STACK"&gt;unlimited&lt;/hard&gt;
    &lt;soft id="RLIMIT_CORE"&gt;0&lt;/soft&gt;
    &lt;hard id="RLIMIT_CORE"&gt;0&lt;/hard&gt;
    &lt;soft id="RESOURCE_5"&gt;unlimited&lt;/soft&gt;
    &lt;hard id="RESOURCE_5"&gt;unlimited&lt;/hard&gt;
    &lt;soft id="RLIMIT_NPROC"&gt;8192&lt;/soft&gt;
    &lt;hard id="RLIMIT_NPROC"&gt;8192&lt;/hard&gt;
    &lt;soft id="RLIMIT_NOFILE"&gt;1024&lt;/soft&gt;
    &lt;hard id="RLIMIT_NOFILE"&gt;1024&lt;/hard&gt;
    &lt;soft id="RLIMIT_MEMLOCK"&gt;32768&lt;/soft&gt;
    &lt;hard id="RLIMIT_MEMLOCK"&gt;32768&lt;/hard&gt;
    &lt;soft id="RLIMIT_AS"&gt;unlimited&lt;/soft&gt;
    &lt;hard id="RLIMIT_AS"&gt;unlimited&lt;/hard&gt;
    &lt;soft id="RLIMIT_LOCKS"&gt;unlimited&lt;/soft&gt;
    &lt;hard id="RLIMIT_LOCKS"&gt;unlimited&lt;/hard&gt;
    &lt;soft id="RLIMIT_SIGPENDING"&gt;8192&lt;/soft&gt;
    &lt;hard id="RLIMIT_SIGPENDING"&gt;8192&lt;/hard&gt;
    &lt;soft id="RLIMIT_MSGQUEUE"&gt;819200&lt;/soft&gt;
    &lt;hard id="RLIMIT_MSGQUEUE"&gt;819200&lt;/hard&gt;
    &lt;soft id="RLIMIT_NICE"&gt;0&lt;/soft&gt;
    &lt;hard id="RLIMIT_NICE"&gt;0&lt;/hard&gt;
    &lt;soft id="RLIMIT_RTPRIO"&gt;0&lt;/soft&gt;
    &lt;hard id="RLIMIT_RTPRIO"&gt;0&lt;/hard&gt;
  &lt;/resource&gt;
&lt;/invocation&gt;
[pegasus@pegasus pegasus-wms]$ 

</programlisting>
      </section>
    </section>

    <section>
      <title>Condor DAGMan format and log files etc.</title>

      <para>In this exercise we will learn about the DAG file format and some
      of the log files generated when the DAG runs.</para>

      <itemizedlist>
        <listitem>
          <para>Now take a look at the DAG file...</para>

          <programlisting><command>$ cat $HOME/pegasus-wms/dags/pegasus/pegasus/diamond/run0001/diamond-0.dag</command><computeroutput>

######################################################################
# PEGASUS GENERATED SUBMIT FILE 
# DAG diamond 
# Index = 0, Count = 1
######################################################################
JOB generate_ID000001 generate_ID000001.sub
RETRY generate_ID000001 2

JOB findrange_ID000002 findrange_ID000002.sub
RETRY findrange_ID000002 2

JOB findrange_ID000003 findrange_ID000003.sub
RETRY findrange_ID000003 2

JOB analyze_ID000004 analyze_ID000004.sub
RETRY analyze_ID000004 2

JOB diamond_0_pegasus_concat diamond_0_pegasus_concat.sub 

JOB diamond_0_local_cdir diamond_0_local_cdir.sub 
SCRIPT POST diamond_0_local_cdir /nfs/software/pegasus/default/bin/exitpost 
      -Dpegasus.user.properties=/nfs/home/trainXX/pegasus-wms/dags/trainXX/pegasus
/diamond/run0001/pegasus.31433.properties
/home/trainXX/pegasus-wms/dags/trainXX/pegasus/diamond/run0001/diamond_0_local_cdir.out
          
RETRY diamond_0_local_cdir 2 

PARENT generate_ID000001 CHILD findrange_ID000002 
PARENT generate_ID000001 CHILD findrange_ID000003
PARENT findrange_ID000002 CHILD analyze_ID000004
PARENT findrange_ID000003 CHILD analyze_ID000004
PARENT diamond_0_pegasus_concat CHILD generate_ID000001
PARENT diamond_0_local_cdir CHILD diamond_0_pegasus_concat
######################################################################
# End of DAG
######################################################################</computeroutput></programlisting>
        </listitem>

        <listitem>
          <para>... and the dagman.out file.</para>

          <programlisting><command>$ cat $HOME/pegasus-wms/dags/pegasus/pegasus/diamond/run0001/diamond-0.dag.dagman.out</command><computeroutput>

07/19 16:59:37 ******************************************************
07/19 16:59:37 ** condor_scheduniv_exec.84.0 (CONDOR_DAGMAN) STARTING UP
07/19 16:59:37 ** /opt/condor/7.4.2/bin/condor_dagman
07/19 16:59:37 ** SubsystemInfo: name=DAGMAN type=DAGMAN(10) class=DAEMON(1)
07/19 16:59:37 ** Configuration: subsystem:DAGMAN local:&lt;NONE&gt; class:DAEMON
07/19 16:59:37 ** $CondorVersion: 7.4.2 Mar 29 2010 BuildID: 227044 $
07/19 16:59:37 ** $CondorPlatform: X86_64-LINUX_RHEL5 $
07/19 16:59:37 ** PID = 11922
07/19 16:59:37 ** Log last touched time unavailable (No such file or directory)
07/19 16:59:37 ******************************************************
07/19 16:59:37 Using config source: /opt/condor/config/condor_config
07/19 16:59:37 Using local config sources: 
07/19 16:59:37    /opt/condor/config/condor_config.local
07/19 16:59:37 DaemonCore: Command Socket at &lt;172.16.80.128:40962&gt;
07/19 16:59:37 DAGMAN_DEBUG_CACHE_SIZE setting: 5242880
07/19 16:59:37 DAGMAN_DEBUG_CACHE_ENABLE setting: False
07/19 16:59:37 DAGMAN_SUBMIT_DELAY setting: 0
07/19 16:59:37 DAGMAN_MAX_SUBMIT_ATTEMPTS setting: 6
....
07/19 16:59:51 From submit: 1 job(s) submitted to cluster 85.
07/19 16:59:51  assigned Condor ID (85.0)
07/19 16:59:51 Just submitted 1 job this cycle...
07/19 16:59:51 Currently monitoring 1 Condor log file(s)
07/19 16:59:51 Event: ULOG_SUBMIT for Condor Node create_dir_diamond_0_local (85.0)
07/19 16:59:51 Number of idle job procs: 1
07/19 16:59:51 Of 11 nodes total:
07/19 16:59:51  Done     Pre   Queued    Post   Ready   Un-Ready   Failed
07/19 16:59:51   ===     ===      ===     ===     ===        ===      ===
07/19 16:59:51     0       0        1       0       0         10        0
07/19 16:59:56 Currently monitoring 1 Condor log file(s)
07/19 16:59:56 Event: ULOG_EXECUTE for Condor Node create_dir_diamond_0_local (85.0)
07/19 16:59:56 Number of idle job procs: 0
07/19 16:59:56 Event: ULOG_JOB_TERMINATED for Condor Node create_dir_diamond_0_local (85.0)
07/19 16:59:56 Node create_dir_diamond_0_local job proc (85.0) completed successfully.
07/19 16:59:56 Node create_dir_diamond_0_local job completed
07/19 16:59:56 Running POST script of Node create_dir_diamond_0_local...
07/19 16:59:56 Number of idle job procs: 0
07/19 16:59:56 Of 11 nodes total:
07/19 16:59:56  Done     Pre   Queued    Post   Ready   Un-Ready   Failed
07/19 16:59:56   ===     ===      ===     ===     ===        ===      ===
07/19 16:59:56     0       0        0       1       0         10        0
07/19 17:00:01 Currently monitoring 1 Condor log file(s)
07/19 17:00:01 Event: ULOG_POST_SCRIPT_TERMINATED for Condor Node create_dir_diamond_0_local (85.0)
07/19 17:00:01 POST Script of Node create_dir_diamond_0_local completed successfully.
07/19 17:00:01 Of 11 nodes total:
07/19 17:00:01  Done     Pre   Queued    Post   Ready   Un-Ready   Failed
07/19 17:00:01   ===     ===      ===     ===     ===        ===      ===
07/19 17:00:01     1       0        0       0       1          9        0
7/19 17:01:39  assigned Condor ID (90.0)
07/19 17:01:39 Just submitted 3 jobs this cycle...
07/19 17:01:39 Currently monitoring 1 Condor log file(s)
07/19 17:01:39 Event: ULOG_SUBMIT for Condor Node findrange_ID0000002 (88.0)
07/19 17:01:39 Number of idle job procs: 1
07/19 17:01:39 Event: ULOG_SUBMIT for Condor Node findrange_ID0000003 (89.0)
07/19 17:01:39 Number of idle job procs: 2
07/19 17:01:39 Event: ULOG_EXECUTE for Condor Node findrange_ID0000002 (88.0)
07/19 17:01:39 Number of idle job procs: 1
07/19 17:01:39 Event: ULOG_SUBMIT for Condor Node stage_out_local_0_0 (90.0)
.......
07/19 17:04:33 Number of idle job procs: 0
07/19 17:04:33 Of 11 nodes total:
07/19 17:04:33  Done     Pre   Queued    Post   Ready   Un-Ready   Failed
07/19 17:04:33   ===     ===      ===     ===     ===        ===      ===
07/19 17:04:33    10       0        0       1       0          0        0
07/19 17:04:38 Currently monitoring 1 Condor log file(s)
07/19 17:04:38 Event: ULOG_POST_SCRIPT_TERMINATED for Condor Node register_local_2_0 (95.0)
07/19 17:04:38 POST Script of Node register_local_2_0 completed successfully.
07/19 17:04:38 Of 11 nodes total:
07/19 17:04:38  Done     Pre   Queued    Post   Ready   Un-Ready   Failed
07/19 17:04:38   ===     ===      ===     ===     ===        ===      ===
07/19 17:04:38    11       0        0       0       0          0        0
07/19 17:04:38 All jobs Completed!
07/19 17:04:38 Note: 0 total job deferrals because of -MaxJobs limit (0)
07/19 17:04:38 Note: 0 total job deferrals because of -MaxIdle limit (0)
07/19 17:04:38 Note: 0 total job deferrals because of node category throttles
07/19 17:04:38 Note: 0 total PRE script deferrals because of -MaxPre limit (20)
07/19 17:04:38 Note: 0 total POST script deferrals because of -MaxPost limit (20)
07/19 17:04:38 **** condor_scheduniv_exec.84.0 (condor_DAGMAN) pid 11922 EXITING WITH STATUS 0



</computeroutput></programlisting>
        </listitem>
      </itemizedlist>
    </section>

    <section>
      <title>Removing a running workflow</title>

      <para>Sometimes you may want to halt the execution of the workflow or
      just permanently remove it. You can stop/halt a workflow by running the
      pegasus-remove command mentioned in the output of pegasus-run</para>

      <programlisting><command>$ pegasus-remove $HOME/pegasus-wms/dags/pegasus/pegasus/diamond/runXXXX</command><computeroutput>

Job 2788.0 marked for removal</computeroutput></programlisting>
    </section>

    <section>
      <title>Planning workflow using pegasus-plan and Running pegasus-run to
      submit the workflow to a grid resource.</title>

      <para>In this exercise we are going to run pegasus-plan to generate a
      executable workflow from the abstract workflow (montage.dax). The
      Executable workflow generated, are condor submit files that are
      submitted to remote grid resources using pegasus-run</para>

      <para>The instructors have provided:</para>

      <itemizedlist>
        <listitem>
          <para>A dax (montage.dax) in the $HOME/pegasus-wms/dax/
          directory.</para>
        </listitem>
      </itemizedlist>

      <para>You will need to write some things yourself, by following the
      instructions below: <itemizedlist>
          <listitem>
            <para>Run pegasus-plan to generate the condor submit files out of
            the dax.</para>
          </listitem>
        </itemizedlist></para>

      <para>Instructions:</para>

      <itemizedlist>
        <listitem>
          <para>Let us run pegasus-plan on the montage dax on the tg_ncsa
          cluster. If multiple sites are available you could provide the sites
          using a comma "," separated list like tg_ncsa,viz
          etc.<programlisting><command>$ cd $HOME/pegasus-wms
$ pegasus-plan -Dpegasus.user.properties=`pwd`/config/properties \
               --dir dags --sites cluster --output local --force \
               --nocleanup --dax `pwd`/dax/montage.dax --submit</command></programlisting>
          The above command says that we need to plan the montage dax on the
          <emphasis role="bold">cluster</emphasis> site. The cluster site in
          the VM is managed by SGE that is running in the VM. The jobs for
          this workflow will be submitted to <emphasis
          role="bold">jobmanager-sge</emphasis> in the VM. The output data
          needs to be transferred back to the local host. The condor submit
          files are to be generated in a directory structure whose base is
          dags. We also are requesting that no cleanup jobs be added as we
          require the intermediate data on the remote host. Here is the output
          of pegasus-plan. <programlisting><computeroutput>2010.07.20 13:51:34.808 PDT: [INFO] event.pegasus.planner planner.version 2.4.2  - STARTED 
2010.07.20 13:51:35.324 PDT: [INFO] event.pegasus.parse.dax dax.id /home/pegasus/pegasus-wms/dax/montage.dax  - STARTED 
2010.07.20 13:51:35.493 PDT: [INFO] event.pegasus.parse.dax dax.id /home/pegasus/pegasus-wms/dax/montage.dax  - FINISHED 
2010.07.20 13:51:35.525 PDT: [INFO] event.pegasus.refinement dax.id montage_0  - STARTED 
2010.07.20 13:51:35.535 PDT: [INFO] event.pegasus.siteselection dax.id montage_0  - STARTED 
2010.07.20 13:51:35.590 PDT: [INFO] event.pegasus.siteselection dax.id montage_0  - FINISHED 
2010.07.20 13:51:35.598 PDT: [INFO]  Grafting transfer nodes in the workflow 
2010.07.20 13:51:35.599 PDT: [INFO] event.pegasus.generate.transfer-nodes dax.id montage_0  - STARTED 
2010.07.20 13:51:35.662 PDT: [INFO] event.pegasus.generate.transfer-nodes dax.id montage_0  - FINISHED 
2010.07.20 13:51:35.663 PDT: [INFO] event.pegasus.generate.workdir-nodes dax.id montage_0  - STARTED 
2010.07.20 13:51:35.665 PDT: [INFO] event.pegasus.generate.workdir-nodes dax.id montage_0  - FINISHED 
2010.07.20 13:51:35.665 PDT: [INFO] event.pegasus.generate.cleanup-wf dax.id montage_0  - STARTED 
2010.07.20 13:51:35.666 PDT: [INFO] event.pegasus.generate.cleanup-wf dax.id montage_0  - FINISHED 
2010.07.20 13:51:35.666 PDT: [INFO] event.pegasus.refinement dax.id montage_0  - FINISHED 
2010.07.20 13:51:35.732 PDT: [INFO]  Generating codes for the concrete workflow 
2010.07.20 13:51:35.932 PDT: [INFO]  Generating codes for the concrete workflow -DONE 
2010.07.20 13:51:35.932 PDT: [INFO]  Generating code for the cleanup workflow 
2010.07.20 13:51:35.985 PDT: [INFO]  Generating code for the cleanup workflow -DONE 
2010.07.20 13:51:36.143 PDT: [INFO]   
2010.07.20 13:51:36.150 PDT: [INFO]  ----------------------------------------------------------------------- 
2010.07.20 13:51:36.156 PDT: [INFO]  File for submitting this DAG to Condor           : montage-0.dag.condor.sub 
2010.07.20 13:51:36.163 PDT: [INFO]  Log of DAGMan debugging messages                 : montage-0.dag.dagman.out 
2010.07.20 13:51:36.169 PDT: [INFO]  Log of Condor library output                     : montage-0.dag.lib.out 
2010.07.20 13:51:36.176 PDT: [INFO]  Log of Condor library error messages             : montage-0.dag.lib.err 
2010.07.20 13:51:36.183 PDT: [INFO]  Log of the life of condor_dagman itself          : montage-0.dag.dagman.log 
2010.07.20 13:51:36.193 PDT: [INFO]   
2010.07.20 13:51:36.201 PDT: [INFO]  -no_submit given, not submitting DAG to Condor.  You can do this with: 
2010.07.20 13:51:36.225 PDT: [INFO]  "condor_submit montage-0.dag.condor.sub" 
2010.07.20 13:51:36.231 PDT: [INFO]  ----------------------------------------------------------------------- 
2010.07.20 13:51:36.237 PDT: [INFO]  Submitting job(s). 
2010.07.20 13:51:36.247 PDT: [INFO]  Logging submit event(s). 
2010.07.20 13:51:36.253 PDT: [INFO]  1 job(s) submitted to cluster 118. 
2010.07.20 13:51:36.355 PDT: [INFO]   
2010.07.20 13:51:36.363 PDT: [INFO]  I have started your workflow, committed it to DAGMan, and updated its 
2010.07.20 13:51:36.369 PDT: [INFO]  state in the work database. A separate daemon was started to collect 
2010.07.20 13:51:36.374 PDT: [INFO]  information about the progress of the workflow. The job state will soon 
2010.07.20 13:51:36.380 PDT: [INFO]  be visible. Your workflow runs in base directory.  
2010.07.20 13:51:36.386 PDT: [INFO]   
2010.07.20 13:51:36.393 PDT: [INFO]  cd /home/pegasus/pegasus-wms/dags/pegasus/pegasus/montage/run0001 
2010.07.20 13:51:36.399 PDT: [INFO]   
2010.07.20 13:51:36.405 PDT: [INFO]  *** To monitor the workflow you can run *** 
2010.07.20 13:51:36.415 PDT: [INFO]   
2010.07.20 13:51:36.421 PDT: [INFO]  pegasus-status -w montage-0 -t 20100720T135134-0700  
2010.07.20 13:51:36.427 PDT: [INFO]  or 
2010.07.20 13:51:36.433 PDT: [INFO]  pegasus-status /home/pegasus/pegasus-wms/dags/pegasus/pegasus/montage/run0001 
2010.07.20 13:51:36.440 PDT: [INFO]   
2010.07.20 13:51:36.446 PDT: [INFO]  *** To remove your workflow run *** 
2010.07.20 13:51:36.452 PDT: [INFO]   
2010.07.20 13:51:36.458 PDT: [INFO]  pegasus-remove -d 118.0 
2010.07.20 13:51:36.465 PDT: [INFO]  or 
2010.07.20 13:51:36.471 PDT: [INFO]  pegasus-remove /home/pegasus/pegasus-wms/dags/pegasus/pegasus/montage/run0001 
2010.07.20 13:51:36.478 PDT: [INFO]   
2010.07.20 13:51:36.484 PDT: [INFO]  Time taken to execute is 1.666 seconds 
2010.07.20 13:51:36.484 PDT: [INFO] event.pegasus.planner planner.version 2.4.2  - FINISHED 
</computeroutput></programlisting></para>
        </listitem>

        <listitem>
          <para>If you get any errors above while running pegasus-plan you can
          add -vvvvv to enable maximum verbosity on pegasus-run.</para>
        </listitem>
      </itemizedlist>

      <para>The above command submits the workflow to Condor DAGMan/CondorG.
      After submitting it starts a monitoring daemon tailstatd that parses the
      condor log files to update the status of the jobs and push it in a work
      database.</para>

      <para>Monitor the workflow using the commands provided in the output of
      the pegasus-run command and other commands explained earlier.</para>

      <para>The workflow generates a single output file montage.jpg that
      resides in the directory <emphasis
      role="bold">/home/pegasus/local-storage/storage/montage.jpg</emphasis>
      if it runs successfully</para>

      <para>The grid workflow will take time to execute on the VM. On the
      instructor's MAC Pro Desktop it took about<emphasis role="bold"> 30
      minutes </emphasis>to run.</para>
    </section>
  </section>

  <section>
    <title>Advanced Exercises</title>

    <section>
      <title>Optimizing a workflow by clustering small jobs (To Be Done
      offline)</title>

      <para>Sometimes a workflow may have too many jobs whose execution time
      is a few seconds long. In such instances the overhead of scheduling each
      job on a grid is too large and the runtime of the entire workflow can be
      optimized by using Pegasus clustering techniques. One such technique is
      to cluster jobs horizontally on the same level into one or more
      sequential jobs.</para>

      <programlisting><command>$ cd $HOME/pegasus-wms
$ pegasus-plan -Dpegasus.user.properties=`pwd`/config/properties \
            --dir `pwd`/dags --sites cluster --output local --nocleanup --force\
            --cluster horizontal --dax `pwd`/dax/montage.dax </command></programlisting>

      <para>After clustering the executable workflow will contain 29 jobs
      compared to 46 in the non clustered mode.</para>
    </section>

    <section>
      <title>Data Reuse</title>

      <para>In the DAX you can specify what output data products you want to
      track in the replica catalog. This is done by setting the register flags
      with the output files for a job. For our tutorial, we only register the
      final output data products. So if you were able to execute the diamond
      or the montage workflow successfully, we can do data reuse. Let us run
      <emphasis role="bold">pegasus-plan </emphasis>on the diamond workflow
      again. However, this time we will remove the <emphasis
      role="bold">--force</emphasis> option.</para>

      <programlisting><emphasis role="bold">$ cd $HOME/pegasus-wms
$ pegasus-plan -Dpegasus.user.properties=`pwd`/config/properties --dax `pwd`/dax/diamond.dax --dir `pwd`/dags -s local \
                   -o local --nocleanup
</emphasis>
2008.04.30 17:12:09.851 PDT: [INFO] Parsing the DAX
2008.04.30 17:12:10.299 PDT: [INFO] Parsing the DAX (completed)
2008.04.30 17:12:10.344 PDT: [INFO] Parsing the site catalog 
2008.04.30 17:12:10.480 PDT: [INFO] Parsing the site catalog (completed)
2008.04.30 17:12:10.545 PDT: [INFO] Doing site selection
2008.04.30 17:12:10.568 PDT: [INFO] Doing site selection (completed)
2008.04.30 17:12:10.569 PDT: [INFO] Grafting transfer nodes in the workflow
2008.04.30 17:12:10.618 PDT: [INFO] The leaf file f.d is already at the output pool local
2008.04.30 17:12:10.618 PDT: [INFO] Grafting transfer nodes in the workflow (completed)
2008.04.30 17:12:10.627 PDT: [INFO] Grafting the remote workdirectory creation jobs in the workflow
2008.04.30 17:12:10.632 PDT: [INFO] Grafting the remote workdirectory creation jobs in the workflow (completed)
2008.04.30 17:12:10.632 PDT: [INFO] Generating the cleanup workflow
2008.04.30 17:12:10.636 PDT: [INFO] Generating the cleanup workflow (completed)
2008.04.30 17:12:10.662 PDT: [INFO] 


The executable workflow generated contains no nodes.
It seems that the output files are already at the output site. 
To regenerate the output data from scratch specify --force option.



2008.04.30 17:12:10.663 PDT: [INFO] Time taken to execute is 1.081 seconds
</programlisting>

      <para>You can increase the debug level to see how pegasus deletes the
      jobs bottom up of the workflow. Pass -vvvvv to pegasus-plan
      command.</para>
    </section>
  </section>
</chapter>