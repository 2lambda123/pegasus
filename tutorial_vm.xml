<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN"
"http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<chapter id="tutorial_vm">
  <title>Pegasus Tutorial Using Self-contained Virtual Machine</title>

  <para>These are the student notes for the Pegasus WMS tutorial on the
  Virtual Machine that can be downloaded from the Pegasus Website. They are
  designed to be used in conjunction with instructor presentation and
  support.</para>

  <para>You will see two styles of machine text here:</para>

  <programlisting><command>Text like this is input that you should type.</command><computeroutput>

Text like this is the output you should get.</computeroutput></programlisting>

  <para>For example:</para>

  <programlisting><command>$ date</command><computeroutput>
Wed Nov 24 14:47:59 PST 2010
</computeroutput></programlisting>

  <section>
    <title>Downloading and Running the VM using Virtual Box</title>

    <para>You will need to install Virtual Box to run the virtual machine on
    your computer. If you already have one of the tools installed, use that.
    Otherwise download the binary versions and install them from the<ulink
    url="http://www.virtualbox.org/wiki/Downloads"> Virtual Box
    Website</ulink> . </para>

    <para>The instructors have tested the image with Virtual Box 3.2.10</para>

    <section>
      <title>Download the VM for Virtual Box use</title>

      <para>Download the corresponding disk image.</para>

      <itemizedlist>
        <listitem>
          <para><ulink
          url="http://pegasus.isi.edu/wms/download/3.0/Pegasus3.0-VM-box.tar.bz2">Virtual
          Box Pegasus Image</ulink></para>

          <para>It is around <emphasis role="bold">2 GB</emphasis> in size. We
          recommend using a command line tool like <emphasis
          role="bold">wget</emphasis> to download the image. Downloading the
          image using the browser sometimes corrupts the image.</para>

          <programlisting>$ <emphasis role="bold">wget http://pegasus.isi.edu/wms/download/3.0/Pegasus3.0-VM-box.tar.bz2

--</emphasis>14:52:11--  http://pegasus.isi.edu/tutorial/virtual/Pegasus3.0-VM-box.tar.bz2
           =&gt; `Pegasus3.0-VM-box.tar.bz2'
Resolving pegasus.isi.edu... 128.9.64.219
Connecting to pegasus.isi.edu|128.9.64.219|:80... connected.
HTTP request sent, awaiting response... 200 OK
Length: 1,487,950,091 (1.4G) [application/x-bzip2]
...</programlisting>

          <para>The Image is bzipped . You will need to unzip it.</para>

          <para>If you have gnu tar you can do this directly</para>

          <programlisting><emphasis role="bold">$ gtar jxvf Pegasus3.0-VM-box.tar.bz2</emphasis></programlisting>

          <para>Else you need to do the following</para>

          <programlisting><emphasis role="bold">$ bunzip2 Pegasus3.0-VM-box.tar.bz2

$ tar xvf Pegasus3.0-VM-box.tar</emphasis></programlisting>
        </listitem>
      </itemizedlist>
    </section>

    <section>
      <title>Running the VM with Virtual Box</title>

      <para>Launch Virtual Box on your machine. Follow the steps to add the
      vmdk file to Virtual Box and create a virtual machine inside the Virtual
      Box </para>

      <orderedlist>
        <listitem>
          <para>In the Menu, click File and select Virtual Media Manager (
          File &gt; Virtual Media Manager )</para>
        </listitem>

        <listitem>
          <para>The Virtual Media Manager Windows opens up.</para>
        </listitem>

        <listitem>
          <para>Click on "Add" button to add the <emphasis
          role="bold">PegasusVM3.0-vbox.vmdk</emphasis> file that you just
          downloaded and unzipped.</para>
        </listitem>

        <listitem>
          <para>You will now see the <emphasis
          role="bold">PegasusVM3.0-vbox.vmdk</emphasis> in the list of hard
          disks with Actual size listed as around 5.10 GB</para>
        </listitem>

        <listitem>
          <para>Close the Window for the Virtual Media Manager</para>
        </listitem>
      </orderedlist>

      <para>We will now create a Virtual Machine in the Virtual Box.</para>

      <orderedlist>
        <listitem>
          <para>In the Menu, click Machine and select New ( Machine &gt; New
          )</para>
        </listitem>

        <listitem>
          <para>It will open the New Virtual Machine Wizard. Click
          Continue</para>
        </listitem>

        <listitem>
          <para>In the VM Name and OS Type Window specify the name as<emphasis
          role="bold"> PegasusVM-3.0</emphasis>. Select the <emphasis
          role="bold">Operating System as Linux </emphasis>and <emphasis
          role="bold">Version as Red Hat ( 64 bit )</emphasis>. Click
          Continue.</para>
        </listitem>

        <listitem>
          <para>You will now see the <emphasis
          role="bold">PegasusVM3.0-vbox.vmdk</emphasis> in the list of hard
          disks with Actual size listed as around 5.10 GB</para>
        </listitem>

        <listitem>
          <para>Set the base memory to <emphasis role="bold">1024
          MB</emphasis> . It defaults to 512 MB. Click Continue</para>
        </listitem>

        <listitem>
          <para>We now select the Virtual Hard Disk to use with the machine.
          Select the option box for Use Existing Hard Disk. Select <emphasis
          role="bold">PegasusVM3.0-vbox.vmdk</emphasis> from the list . Click
          Continue</para>
        </listitem>

        <listitem>
          <para>Click Done</para>
        </listitem>

        <listitem>
          <para>Now in the Virtual Box , start the PegasusVM-3.0
          machine.</para>
        </listitem>
      </orderedlist>
    </section>
  </section>

  <section>
    <title>Downloading and Running the VM using VMWare</title>

    <para>You will need to install VMWare to run the virtual machine on your
    computer. If you already have one of the tools installed, use that.
    Otherwise download and install one from:</para>

    <itemizedlist>
      <listitem>
        <para><emphasis role="bold">Windows</emphasis> - Download VMPlayer (
        free download ) from the <ulink
        url="http://www.vmware.com/products/player/">VMWare
        website</ulink></para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">MacOSX</emphasis> - Download VMware Fusion
        ( not free ). A 30 day trial version can be downloaded from <ulink
        url="https://www.vmware.com/tryvmware/?p=vmware-fusion31&amp;lp=1">VMWare
        website</ulink></para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">Linux</emphasis> - Download VMware
        Workstation ( not free ). A 30 day trial version can be downloaded
        from <ulink
        url="https://www.vmware.com/tryvmware/?p=vmware-workstation&amp;lp=1">VMWare
        website</ulink></para>
      </listitem>
    </itemizedlist>

    <section>
      <title>Download the VM for VMWare use</title>

      <para>Download the corresponding disk image.</para>

      <itemizedlist>
        <listitem>
          <para><ulink
          url="http://pegasus.isi.edu/wms/download/3.0/Pegasus3.0-VM.tar.bz2">VMWare
          Pegasus Image</ulink></para>

          <para>It is around <emphasis role="bold">2 GB</emphasis> in size. We
          recommend using a command line tool like <emphasis
          role="bold">wget</emphasis> to download the image. Downloading the
          image using the browser sometimes corrupts the image.</para>

          <programlisting>$ <emphasis role="bold">wget http://pegasus.isi.edu/tutorial/virtual/Pegasus3.0-VM.tar.bz2

--</emphasis>14:52:11--  http://pegasus.isi.edu/tutorial/virtual/Pegasus3.0-VM.tar.bz2
           =&gt; `Pegasus3.0-VM.tar.bz2'
Resolving pegasus.isi.edu... 128.9.64.219
Connecting to pegasus.isi.edu|128.9.64.219|:80... connected.
HTTP request sent, awaiting response... 200 OK
Length: 1,487,950,091 (1.4G) [application/x-bzip2]
...</programlisting>

          <para>The Image is bzipped. You will need to unzip it.</para>

          <para>If you have gnu tar you can do this directly</para>

          <programlisting><emphasis role="bold">$ gtar jxvf Pegasus3.0-VM.tar.bz2</emphasis></programlisting>

          <para>Else you need to do the following</para>

          <programlisting><emphasis role="bold">$ bunzip2 Pegasus3.0-VM.tar.bz2

$ tar xvf Pegasus3.0-VM.tar</emphasis></programlisting>
        </listitem>
      </itemizedlist>
    </section>

    <section>
      <title>Running the VM with VMWare</title>

      <para>To start the virtual machine double click on the vmware suffixed
      file/package Pegasus3.0-VM.vmwarevm . After double clicking, VMware will
      start up asking</para>

      <orderedlist>
        <listitem>
          <para>if you moved it</para>
        </listitem>

        <listitem>
          <para>if you want to cancel.</para>
        </listitem>

        <listitem>
          <para>if you copied</para>
        </listitem>
      </orderedlist>

      <para>Select the option<emphasis role="bold"> 3</emphasis> i.e you
      copied it.</para>
    </section>
  </section>

  <section>
    <title>Mapping and Executing Workflows using Pegasus</title>

    <para>In this chapter you will be introduced to planning and executing a
    workflow through Pegasus WMS locally. You will then plan and execute a
    larger Montage workflow on the GRID.</para>

    <para>When the virtual machine ( Pegasus3.0-VM.vmwarevm ) starts , it will
    ask for a password for a user called <emphasis
    role="bold">pegasus</emphasis>. The password is same as the username i.e.
    pegasus.</para>

    <para>After logging on, start a terminal. There is a shortcut on the
    desktop for the terminal.</para>

    <programlisting><command>$ </command><emphasis role="bold">[pegasus@pegasus ~]$ pwd</emphasis>

/home/pegasus
</programlisting>

    <para>In general, to run workflows on the Grid you will need to obtain
    Grid Credentials. The VM already has a user certificate installed for the
    pegasus user. To generate the proxy ( grid credentials ) run the <emphasis
    role="bold">grid-proxy-init</emphasis> command.</para>

    <programlisting><command>$ [pegasus@pegasus ~]$ grid-proxy-init 

</command><computeroutput>Your identity: /O=Grid/OU=GlobusTest/OU=simpleCA-seqware/CN=Pegasus User
Creating proxy ................................... Done
Your proxy is valid until: Thu Nov 25 02:48:18 2010

</computeroutput></programlisting>

    <para>All the exercises in this Chapter will be run from the
    $HOME/pegasus-wms/ directory. All the files that are required reside in
    this directory</para>

    <programlisting><command>$ cd $HOME/pegasus-wms</command></programlisting>

    <para>Files for the exercise are stored in subdirectories:</para>

    <programlisting><command>$ ls</command><computeroutput><computeroutput>

config  dax</computeroutput></computeroutput></programlisting>

    <para>You may also see some other files here.</para>

    <section>
      <title>Creating a DIAMOND DAX</title>

      <para>We generate a 4 node diamond dax. There is a small piece of java
      code that uses the DAX API to generate the DAX. Open the file
      $HOME/pegasus-wms/dax/CreateDAX.java in a file editor:</para>

      <programlisting><command>$ emacs -nw dax/CreateDAX.java</command></programlisting>

      <para>There is a function Diamond( String site_handle, String
      pegasus_location ) that constructs the DAX. Towards the end of the
      function there is some commented out code.</para>

      <programlisting> // Add analyze job
//To be uncommented for exercise 2.1
    
        Job j4 = new Job("j4", "pegasus", "analyze", "4.0");
        j4.addArgument("-a analyze -T 60 -i ").addArgument(fc1);
        j4.addArgument(" ").addArgument(fc2);
        j4.addArgument("-o ").addArgument(fd);
        j4.uses(fc1, File.LINK.INPUT);
        j4.uses(fc2, File.LINK.INPUT);
        j4.uses(fd, File.LINK.OUTPUT);
        
        //add job to the DAX
        dax.addJob(j4);

        //analyze job is a child to the findrange jobs
        dax.addDependency("j2", "j4");
        dax.addDependency("j3", "j4");
    
//End of commented out code for Exercise 2.1
</programlisting>

      <para>The above snippet of code, adds a job with the ID0000004 to the
      DAX. It illustrates how to specify</para>

      <orderedlist>
        <listitem>
          <para>the arguments for the job</para>
        </listitem>

        <listitem>
          <para>the logical files used by the job</para>
        </listitem>

        <listitem>
          <para>the dependencies to other jobs</para>
        </listitem>

        <listitem>
          <para>adding the job to the dax</para>
        </listitem>
      </orderedlist>

      <para>After uncommenting the code, compile and run the CreateDAX
      program.</para>

      <programlisting><emphasis role="bold">$ cd dax

$ javac -classpath .:/opt/pegasus/default/lib/pegasus.jar CreateDAX.java

$  java -classpath .:/opt/pegasus/default/lib/pegasus.jar CreateDAX local /opt/pegasus/default ./diamond.dax
</emphasis></programlisting>

      <para>Let us view the generated diamond.dax.</para>

      <programlisting><emphasis role="bold">$ cat diamond.dax</emphasis></programlisting>

      <para>Inside the DAX, you should see three sections.</para>

      <orderedlist>
        <listitem>
          <para>list of input file locations</para>
        </listitem>

        <listitem>
          <para>list of executable locations</para>
        </listitem>

        <listitem>
          <para>definition of all jobs - each job in the workflow. 4 jobs in
          total.</para>
        </listitem>

        <listitem>
          <para>list of control-flow dependencies - this section specifies a
          partial order in which jobs are to executed.</para>
        </listitem>
      </orderedlist>
    </section>

    <section>
      <title>Replica Catalog</title>

      <para>First lets change to the tutorial base directory.<programlisting><command>$ cd $HOME/pegasus-wms</command></programlisting></para>

      <para>In this exercise you will insert entries into the Replica Catalog.
      The replica catalog that we will use today is a simple file based
      catalog. We also support and recommend the following for production
      runs</para>

      <itemizedlist>
        <listitem>
          <para>Globus RLS</para>
        </listitem>

        <listitem>
          <para>JDBC implementation</para>
        </listitem>
      </itemizedlist>

      <para>A Replica Catalog maintains the LFN to PFN mapping for the input
      files of your workflow. Pegasus queries it to determine the locations of
      the raw input data files required by the workflow. Additionally, all the
      materialized data is registered into Replica Catalog for data reuse
      later on.</para>

      <section>
        <title>Pre Populated Replica Catalog</title>

        <para>The instructors have provided a File based Replica Catalog
        configured for the tutorial exercises. The file is inside the config
        directory.</para>

        <itemizedlist>
          <listitem>
            <para>Let us see what the file looks like. <programlisting><command>$ cat config/rc.data</command><computeroutput>


statfile_20070529_153243_22618.tbl
     gsiftp://pegasus/scratch/tutorial/inputdata/0.2degree/statfile.tbl
          pool="local"
2mass-atlas-990502s-j1440198.fits
     gsiftp://pegasus/scratch/0.2degree/2mass-atlas-990502s-j1440198.fits
          pool="local"
2mass-atlas-990502s-j1440186.fits
     gsiftp://pegasus/scratch/0.2degree/2mass-atlas-990502s-j1440186.fits
          pool="local"
 2mass-atlas-990502s-j1430092.fits
     gsiftp://pegasus/scratch/0.2degree/2mass-atlas-990502s-j1430092.fits
          pool="local"
 2mass-atlas-990502s-j1420198.fits
     gsiftp://pegasus/scratch/0.2degree/2mass-atlas-990502s-j1420198.fits
          pool="local"
 2mass-atlas-990502s-j1420186.fits
     gsiftp://pegasus/scratch/0.2degree/2mass-atlas-990502s-j1420186.fits
          pool="local"
 cimages_20070529_153243_22618.tbl
     gsiftp://pegasus/scratch/0.2degree/cimages.tbl pool="local"
 pimages_20070529_153243_22618.tbl
     gsiftp://pegasus/scratch/0.2degree/pimages.tbl pool="local"
 region_20070529_153243_22618.hdr
     gsiftp://pegasus/scratch/0.2degree/region.hdr pool="local"
 2mass-atlas-990502s-j1430080.fits
     gsiftp://pegasus/scratch/0.2degree/2mass-atlas-990502s-j1430080.fits
          pool="local"

...</computeroutput></programlisting></para>
          </listitem>
        </itemizedlist>
      </section>

      <section>
        <title>pegasus-rc-client ( Optional Exercise )</title>

        <para>You can use the <emphasis role="bold">
        pegasus-rc-client</emphasis> command to insert , query and delete from
        the replica catalog.</para>

        <para>Before executing any of the pegasus-rc-client exercises lets us
        remove the pre populated replica catalog.</para>

        <programlisting><emphasis role="bold">$ rm $HOME/pegasus-wms/config/rc.data</emphasis></programlisting>

        <para>To execute the diamond dax created in <emphasis
        role="bold">exercise 2.1</emphasis>, we will need to register input
        file f.a in the replica catalog. The file f.a resides at
        /scratch/tutorial/inputdata/diamond/f.a . Let us insert a single entry
        into the replica catalog.</para>

        <programlisting><command>$  pegasus-rc-client  insert f.a \
          gsiftp://pegasus/scratch/tutorial/inputdata/diamond/f.a pool=local
</command></programlisting>

        <para>Let us know verify if f.a has been registered successfully by
        querying the replica catalog using pegasus-rc-client</para>

        <programlisting><emphasis role="bold">$ pegasus-rc-client  lookup f.a</emphasis>

 f.a gsiftp://pegasus/scratch/tutorial/inputdata/diamond/f.a pool="local"
</programlisting>

        <para>The <emphasis role="bold">pegasus-rc-client</emphasis> also
        allows for bulk insertion of entries. We will be inserting the entries
        for montage workflow using the bulk mode. The input data to be used
        for the montage workflow resides in the
        /scratch/tutorial/inputdata/0.2degree directory. We are going to
        insert entries into the replica catalog that point to the files in
        this directory.</para>

        <para>The instructors have provided:</para>

        <itemizedlist>
          <listitem>
            <para>A file replicas.in, the input data file for the
            pegasus-rc-client that contains the mappings that need to be
            populated in the Replica Catalog. The file is inside the config
            directory</para>
          </listitem>

          <listitem>
            <para>Let us see what the file looks like.</para>

            <programlisting><command>$ cat config/rc.in</command><computeroutput>


statfile_20070529_153243_22618.tbl
     gsiftp://pegasus/scratch/tutorial/inputdata/0.2degree/statfile.tbl
          pool="local"
2mass-atlas-990502s-j1440198.fits
     gsiftp://pegasus/scratch/0.2degree/2mass-atlas-990502s-j1440198.fits
          pool="local"
2mass-atlas-990502s-j1440186.fits
     gsiftp://pegasus/scratch/0.2degree/2mass-atlas-990502s-j1440186.fits
          pool="local"
 2mass-atlas-990502s-j1430092.fits
     gsiftp://pegasus/scratch/0.2degree/2mass-atlas-990502s-j1430092.fits
          pool="local"
 2mass-atlas-990502s-j1420198.fits
     gsiftp://pegasus/scratch/0.2degree/2mass-atlas-990502s-j1420198.fits
          pool="local"
 2mass-atlas-990502s-j1420186.fits
     gsiftp://pegasus/scratch/0.2degree/2mass-atlas-990502s-j1420186.fits
          pool="local"
 cimages_20070529_153243_22618.tbl
     gsiftp://pegasus/scratch/0.2degree/cimages.tbl pool="local"
 pimages_20070529_153243_22618.tbl
     gsiftp://pegasus/scratch/0.2degree/pimages.tbl pool="local"
 region_20070529_153243_22618.hdr
     gsiftp://pegasus/scratch/0.2degree/region.hdr pool="local"
 2mass-atlas-990502s-j1430080.fits
     gsiftp://pegasus/scratch/0.2degree/2mass-atlas-990502s-j1430080.fits
          pool="local"</computeroutput></programlisting>
          </listitem>

          <listitem>
            <para>Now we are ready to run rc-client and populate the data.
            Since each of you have an individual file replica catalog, all the
            10 entries should be successfully registered.</para>

            <programlisting><command>$ pegasus-rc-client  --insert config/rc.in</command><computeroutput><computeroutput>

#Successfully worked on : 12 lines</computeroutput>
#Worked on total number of : 12 lines.</computeroutput></programlisting>
          </listitem>

          <listitem>
            <para>Now the entries have been successfully inserted into the
            Replica Catalog. We should query the replica catalog for a
            particular lfn.</para>

            <programlisting><command>$ pegasus-rc-client lookup pimages_20080505_143233_14944.tbl</command><computeroutput><computeroutput>

pimages_20080505_143233_14944.tbl
         gsiftp://pegasus/scratch/tutorial/inputdata/0.2degree/pimages.tbl 
           pool="local"</computeroutput></computeroutput></programlisting>
          </listitem>
        </itemizedlist>
      </section>
    </section>

    <section>
      <title>The Site Catalog</title>

      <para>The site catalog contains information about the layout of your
      grid where you want to run your workflows. For each site following
      information is maintained</para>

      <itemizedlist>
        <listitem>
          <para>grid gateways</para>
        </listitem>

        <listitem>
          <para>head node filesystem</para>
        </listitem>

        <listitem>
          <para>worker node filesystem</para>
        </listitem>

        <listitem>
          <para>scratch and shared file systems on the head nodes and worker
          nodes</para>
        </listitem>

        <listitem>
          <para>replica catalog URL for the site</para>
        </listitem>

        <listitem>
          <para>site wide information like environment variables to be set
          when a job is run.</para>
        </listitem>
      </itemizedlist>

      <section>
        <title>Pre Populated Site Catalog</title>

        <para>The instructors have provided a pre-populated site catalog for
        use in the tutorial in $HOME/pegasus-wms/config directory.</para>

        <para>Lets see the site catalog for the Pegasus VM. It refers to two
        sites <emphasis role="bold">local</emphasis> and <emphasis
        role="bold">cluster</emphasis> .</para>

        <programlisting><command>$ cat $HOME/pegasus-wms/config/sites.xml3</command><computeroutput>

&lt;sitecatalog xmlns="http://pegasus.isi.edu/schema/sitecatalog" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
  xsi:schemaLocation="http://pegasus.isi.edu/schema/sitecatalog http://pegasus.isi.edu/schema/sc-3.0.xsd" version="3.0"&gt;

&lt;site  handle="cluster" arch="x86" os="LINUX" osrelease="" osversion="" glibc=""&gt;
  &lt;grid  type="gt2" contact="pegasus/jobmanager-fork" scheduler="Fork" jobtype="auxillary"/&gt;
  &lt;grid  type="gt2" contact="pegasus/jobmanager-sge" scheduler="SGE" jobtype="compute"/&gt;
  &lt;head-fs&gt;
   &lt;scratch&gt;
    &lt;shared&gt;
     &lt;file-server protocol="gsiftp" url="gsiftp://pegasus" mount-point="/home/pegasus/cluster-scratch"/&gt;
     &lt;internal-mount-point mount-point="/home/pegasus/cluster-scratch"/&gt;
    &lt;/shared&gt;
   &lt;/scratch&gt;
   &lt;storage&gt;
    &lt;shared&gt;
     &lt;file-server protocol="gsiftp" url="gsiftp://pegasus" mount-point="/home/pegasus/cluster-storage"/&gt;
     &lt;internal-mount-point mount-point="/home/pegasus/cluster-storage"/&gt;
    &lt;/shared&gt;
   &lt;/storage&gt;
  &lt;/head-fs&gt;
  &lt;replica-catalog  type="LRC" url="rlsn://localhost"/&gt;
  &lt;profile namespace="env" key="GLOBUS_LOCATION" &gt;/usr/local/globus/gt-5.0.2&lt;/profile&gt;
  &lt;profile namespace="env" key="JAVA_HOME" &gt;/usr/java/default&lt;/profile&gt;
  &lt;profile namespace="env" key="LD_LIBRARY_PATH" &gt;/usr/local/globus/gt-5.0.2/lib&lt;/profile&gt;
  &lt;profile namespace="env" key="PEGASUS_HOME" &gt;/opt/pegasus/default&lt;/profile&gt;
  &lt;profile namespace="pegasus" key="bundle" &gt;1&lt;/profile&gt;
  &lt;profile namespace="pegasus" key="stagein.clusters" &gt;1&lt;/profile&gt;
 &lt;/site&gt;

 &lt;site  handle="local" arch="x86" os="LINUX" osrelease="" osversion="" glibc=""&gt;
  &lt;grid  type="gt2" contact="localhost/jobmanager-fork" scheduler="Fork" jobtype="auxillary"/&gt;
  &lt;grid  type="gt2" contact="localhost/jobmanager-fork" scheduler="Fork" jobtype="compute"/&gt;
  &lt;head-fs&gt;
   &lt;scratch&gt;
    &lt;shared&gt;
     &lt;file-server protocol="gsiftp" url="file://" mount-point="/home/pegasus/local-scratch"/&gt;
     &lt;internal-mount-point mount-point="/home/pegasus/local-scratch"/&gt;
    &lt;/shared&gt;
   &lt;/scratch&gt;
   &lt;storage&gt;
    &lt;shared&gt;
     &lt;file-server protocol="gsiftp" url="file://" mount-point="/home/pegasus/local-storage"/&gt;
     &lt;internal-mount-point mount-point="/home/pegasus/local-storage"/&gt;
    &lt;/shared&gt;
   &lt;/storage&gt;
  &lt;/head-fs&gt;
  &lt;replica-catalog  type="LRC" url="rlsn://localhost"/&gt;
  &lt;profile namespace="env" key="GLOBUS_LOCATION" &gt;/usr/local/globus/gt-5.0.2&lt;/profile&gt;
  &lt;profile namespace="env" key="JAVA_HOME" &gt;/usr/java/default&lt;/profile&gt;
  &lt;profile namespace="env" key="LD_LIBRARY_PATH" &gt;/usr/local/globus/gt-5.0.2/lib&lt;/profile&gt;
  &lt;profile namespace="env" key="PEGASUS_HOME" &gt;/opt/pegasus/default&lt;/profile&gt;
 &lt;/site&gt;

&lt;/sitecatalog&gt;
 
</computeroutput></programlisting>
      </section>

      <section>
        <title>Generating a Site Catalog for OSG</title>

        <para>The client pegasus-sc-client can be used to generate a site
        catalog and transformation catalog for the Open Science Grid.</para>

        <programlisting><command>$ </command><emphasis role="bold"><computeroutput>[pegasus@pegasus pegasus-wms]$ pegasus-sc-client --vo engage --sc ./engage-osg-sc.xml \
  --source OSGMM --grid OSG -vvvv 
</computeroutput></emphasis><computeroutput>
2010.11.24 18:00:46.410 PST: [INFO]  Skipping site CIT_CMS_T2 
2010.11.24 18:00:46.416 PST: [INFO]  Adding site RENCI-Engagement 
2010.11.24 18:00:46.475 PST: [INFO]  Adding site Nebraska 
2010.11.24 18:00:46.476 PST: [INFO]  Adding site Prairiefire 
2010.11.24 18:00:46.476 PST: [INFO]  Adding site BNL-ATLAS 
2010.11.24 18:00:46.477 PST: [INFO]  Adding site BNL-ATLAS__1 
2010.11.24 18:00:46.478 PST: [INFO]  Adding site UFlorida-PG 
2010.11.24 18:00:46.478 PST: [INFO]  Skipping site CIT_CMS_T2__1 
2010.11.24 18:00:46.478 PST: [INFO]  Adding site RENCI-Blueridge 
2010.11.24 18:00:46.479 PST: [INFO]  Adding site Nebraska__1 
2010.11.24 18:00:46.480 PST: [INFO]  Adding site UMissHEP 
2010.11.24 18:00:46.480 PST: [INFO]  Adding site UCR-HEP 
2010.11.24 18:00:46.481 PST: [INFO]  Adding site LIGO_UWM_NEMO 
2010.11.24 18:00:46.482 PST: [INFO]  Adding site FNAL_FERMIGRID 
2010.11.24 18:00:46.482 PST: [INFO]  Adding site USCMS-FNAL-WC1 
2010.11.24 18:00:46.483 PST: [INFO]  Adding site UConn-OSG 
2010.11.24 18:00:46.484 PST: [INFO]  Adding site UFlorida-HPC 
2010.11.24 18:00:46.484 PST: [INFO]  Adding site GridUNESP_CENTRAL 
2010.11.24 18:00:46.493 PST: [INFO]  Adding site NWICG_NotreDame 
2010.11.24 18:00:46.494 PST: [INFO]  Site LOCAL . Creating default entry 
2010.11.24 18:00:46.527 PST: [INFO]  Loaded 19 sites  
2010.11.24 18:00:46.527 PST:   Writing out site catalog to /home/pegasus/pegasus-wms/./engage-osg-sc.xml 
2010.11.24 18:00:46.959 PST:   Number of SRM Properties retrieved 14 
2010.11.24 18:00:46.970 PST:   Writing out properties to /home/pegasus/pegasus-wms/./pegasus.6475454308491531036.properties 
2010.11.24 18:00:46.972 PST: [INFO]  Time taken to execute is 1.101 seconds 
2010.11.24 18:00:46.972 PST: [INFO] event.pegasus.planner planner.version 3.0.0  - FINISHED 


</computeroutput></programlisting>
      </section>
    </section>

    <section>
      <title>Transformation Catalog</title>

      <para>The transformation catalog maintains information about where the
      application code resides on the grid. It also provides additional
      information about the transformation as to what system they are compiled
      for, what profiles or environment variables need to be set when the
      transformation is invoked etc.</para>

      <section>
        <title>Pre Populated Transformation Catalog</title>

        <para>The instructors have provided a ready transformation catalog
        (tc.data.text) in the $HOME/pegasus-wms/config directory</para>

        <para>In our case, it contains the locations where the Diamond or
        Montage code is installed in the Pegasus VM. Let us see the
        Transformation Catalog</para>

        <para>For each transformation the following information is
        captured</para>

        <orderedlist>
          <listitem>
            <para>tr - A transformation identifier. (Normally a
            Namespace::Name:Version.. The Namespace and Version are
            optional.)</para>
          </listitem>

          <listitem>
            <para>pfn - URL or file path for the location of the executable.
            The pfn is a file path if the transformation is of type INSTALLED
            and generally a url (file:/// or http:// or gridftp://) if of type
            STAGEABLE</para>
          </listitem>

          <listitem>
            <para>site - The site identifier for the site where the
            transformation is available</para>
          </listitem>

          <listitem>
            <para>type - The type of transformation. Whether it is Iinstalled
            ("INSTALLED") on the remote site or is availabe to stage
            ("STAGEABLE").</para>
          </listitem>

          <listitem>
            <para>arch os, osrelease, osversion - The
            arch/os/osrelease/osversion of the transformation. osrelease and
            osversion are optional.</para>

            <para>ARCH can have one of the following values x86, x86_64,
            sparcv7, sparcv9, ppc, aix. The default value for arch is
            x86</para>

            <para>OS can have one of the following values linux,sunos,macosx.
            The default value for OS if none specified is linux</para>
          </listitem>

          <listitem>
            <para>Profiles - One or many profiles can be attached to a
            transformation for all sites or to a transformation on a
            particular site.</para>
          </listitem>
        </orderedlist>

        <programlisting><emphasis role="bold">$ cat $HOME/pegasus-wms/config/tc.data.text</emphasis>


# multiple line text-based transformation catalog: 2010-11-24T20:46:41.710-08:00
tr bin/mDiff {
        site local {
                profile env "MONTAGE_HOME" "." 
                pfn "gsiftp://pegasus/scratch/tutorial/software/montage/3.0/x86/
bin/mDiff"
                arch "x86"
                os "LINUX"
                type "STAGEABLE"
        }
}

tr bin/mFitplane {
        site local {
                pfn "gsiftp://pegasus/scratch/tutorial/software/montage/3.0/x86/
bin/mFitplane"
                arch "x86"
                os "LINUX"
                type "STAGEABLE"
        }
}

tr condor::dagman {
        site local {
                pfn "/opt/condor/default/bin/condor_dagman"
                arch "x86"
                os "LINUX"
                type "INSTALLED"
        }
}

tr diamond::findrange:2.0 {
        site local {
                pfn "/opt/pegasus/default/bin/keg"
                arch "x86"
                os "LINUX"
                type "INSTALLED"
        }
}

tr diamond::preprocess:2.0 {
        site local {
                pfn "/opt/pegasus/default/bin/keg"
                arch "x86"
                os "LINUX"
                type "INSTALLED"
        }
}

tr mAdd:3.0 {
        site local {
                pfn "gsiftp://pegasus/scratch/tutorial/software/montage/3.0/x86/
bin/mAdd"
                arch "x86"
                os "LINUX"
                type "STAGEABLE"
        }
}

tr mBackground:3.0 {
        site local {
                pfn "gsiftp://pegasus/scratch/tutorial/software/montage/3.0/x86/
bin/mBackground"
                arch "x86"
                os "LINUX"
                type "STAGEABLE"
        }
}

tr mBgModel:3.0 {
        site local {
                pfn "gsiftp://pegasus/scratch/tutorial/software/montage/3.0/x86/
bin/mBgModel"
                arch "x86"
                os "LINUX"
                type "STAGEABLE"
        }
}

tr mConcatFit:3.0 {
        site local {
                pfn "gsiftp://pegasus/scratch/tutorial/software/montage/3.0/x86/
bin/mConcatFit"
                arch "x86"
                os "LINUX"
                type "STAGEABLE"
        }
}

tr mDiffFit:3.0 {
        site local {
                profile env "MONTAGE_HOME" "." 
                pfn "gsiftp://pegasus/scratch/tutorial/software/montage/3.0/x86/
bin/mDiffFit"
                arch "x86"
                os "LINUX"
                type "STAGEABLE"
        }
}

tr mImgtbl:3.0 {
        site local {
                pfn "gsiftp://pegasus/scratch/tutorial/software/montage/3.0/x86/
bin/mImgtbl"
                arch "x86"
                os "LINUX"
                type "STAGEABLE"
        }
}

tr mJPEG:3.0 {
        site local {
                pfn "gsiftp://pegasus/scratch/tutorial/software/montage/3.0/x86/
bin/mJPEG"
                arch "x86"
                os "LINUX"
                type "STAGEABLE"
        }
}

tr mProjectPP:3.0 {
        site local {
                profile condor "priority" "25" 
                pfn "gsiftp://pegasus/scratch/tutorial/software/montage/3.0/x86/
bin/mProjectPP"
                arch "x86"
                os "LINUX"
                type "STAGEABLE"
        }
}

tr mShrink:3.0 {
        site local {
                pfn "gsiftp://pegasus/scratch/tutorial/software/montage/3.0/x86/
bin/mShrink"
                arch "x86"
                os "LINUX"
                type "STAGEABLE"
        }
}
</programlisting>
      </section>

      <section>
        <title>pegasus-tc-client ( Optional )</title>

        <para>We will use the <emphasis
        role="bold">pegasus-tc-client</emphasis> to add the entry for the
        transformation dummy into the transformation catalog.</para>

        <itemizedlist>
          <listitem>
            <para></para>

            <programlisting><emphasis role="bold">$ pegasus-tc-client  -a -l diamond::dummy:2.0 \
      -p /opt/pegasus/default/bin/keg -r local -t INSTALLED -s INTEL32::LINUX 
<emphasis>
 2008.04.30 15:11:59.313 PDT: [INFO] Added tc entry sucessfully
</emphasis></emphasis></programlisting>

            <para>Let us try and query for the entry we inserted.</para>

            <programlisting><emphasis role="bold">$ pegasus-tc-client  -q -P -l diamond::dummy:2.0
</emphasis>
#RESID     LTX                     PFN                                           TYPE          SYSINFO

local    diamond::analyze:2.0    /cluster-software/pegasus/current/bin/keg    INSTALLED    INTEL32::LINUX

</programlisting>

            <para>Let us try and query the transformation catalog for all the
            entries in it. Let us see what our transformation catalog looks
            like</para>

            <programlisting><command>$ pegasus-tc-client  -q -B          </command><computeroutput>

local   mDiff       
             gsiftp://pegasus/scratch/tutorial/software/montage/3.0/x86/bin/mDiff       
                                STATIC_BINARY   INTEL32::LINUX  ENV::MONTAGE_HOME="."
local   mFitplane
             gsiftp://pegasus/scratch/tutorial/software/montage/3.0/x86/bin/mFitplane
                                STATIC_BINARY   INTEL32::LINUX  ENV::MONTAGE_HOME="."
local   mAdd:3.0  
             gsiftp://pegasus/scratch/tutorial/software/montage/3.0/x86/bin/mAdd
                                STATIC_BINARY   INTEL32::LINUX  ENV::MONTAGE_HOME="."
local   mBackground:3.0
             gsiftp://pegasus/scratch/tutorial/software/montage/3.0/x86/bin/mBackground
                                STATIC_BINARY   INTEL32::LINUX  ENV::MONTAGE_HOME="."
local   mBgModel:3.0
             gsiftp://pegasus/scratch/tutorial/software/montage/3.0/x86/bin/mBgModel
                                STATIC_BINARY   INTEL32::LINUX  ENV::MONTAGE_HOME="."
local   mConcatFit:3.0 
             gsiftp://pegasus/scratch/tutorial/software/montage/3.0/x86/bin/mConcatFit
                                STATIC_BINARY   INTEL32::LINUX  ENV::MONTAGE_HOME="."
local   mDiffFit:3.0
             gsiftp://pegasus/scratch/tutorial/software/montage/3.0/x86/bin/mDiffFit 
                                STATIC_BINARY   INTEL32::LINUX  ENV::MONTAGE_HOME="."
local   mImgtbl:3.0 
             gsiftp://pegasus/scratch/tutorial/software/montage/3.0/x86/bin/mImgtbl  
                                STATIC_BINARY   INTEL32::LINUX  ENV::MONTAGE_HOME="."
local   mJPEG:3.0       
             gsiftp://pegasus/scratch/tutorial/software/montage/3.0/x86/bin/mJPEG  
                                STATIC_BINARY   INTEL32::LINUX  ENV::MONTAGE_HOME="."
local   mProject:3.0 
             gsiftp://pegasus/scratch/tutorial/software/montage/3.0/x86/bin/mProjectPP 
                                STATIC_BINARY   INTEL32::LINUX  ENV::MONTAGE_HOME="."
local   mProjectPP:3.0
             gsiftp://pegasus/scratch/tutorial/software/montage/3.0/x86/bin/mProjectPP
                                STATIC_BINARY   INTEL32::LINUX  ENV::MONTAGE_HOME="."
local   mShrink:3.0
             gsiftp://pegasus/scratch/tutorial/software/montage/3.0/x86/bin/mShrink
                                STATIC_BINARY   INTEL32::LINUX  NULL</computeroutput></programlisting>
          </listitem>
        </itemizedlist>
      </section>
    </section>

    <section>
      <title>Properties</title>

      <para>Pegasus Workflow Planner is configured via the use of java
      properties. The instructors have provided a ready properties file at
      $HOME/.pegasusrc .</para>

      <programlisting><command>$ cat $HOME/.pegasusrc</command><computeroutput><computeroutput>
 
##########################
# PEGASUS USER PROPERTIES 
##########################

## SELECT THE REPLICAT CATALOG MODE AND URL
pegasus.catalog.replica = File
pegasus.catalog.replica.file = ${user.home}/pegasus-wms/config/rc.data


## SELECT THE SITE CATALOG MODE AND FILE
pegasus.catalog.site = XML3
pegasus.catalog.site.file = ${user.home}/pegasus-wms/config/sites.xml3


## SELECT THE TRANSFORMATION CATALOG MODE AND FILE
pegasus.catalog.transformation = Text
pegasus.catalog.transformation.file = ${user.home}/pegasus-wms/config/tc.data.text

## USE DAGMAN RETRY FEATURE FOR FAILURES
dagman.retry=2

## CHECK JOB EXIT CODES FOR FAILURE
dagman.post.scope=all

## STAGE ALL OUR EXECUTABLES OR USE INSTALLED ONES 
pegasus.catalog.transformation.mapper = All

## WORK AND STORAGE DIR  
pegasus.dir.storage = storage
pegasus.dir.exec = exec

#JOB CATEGORIES
dagman.projection.maxjobs 2
[pegasus@pegasus pegasus-wms</computeroutput></computeroutput></programlisting>
    </section>

    <section>
      <title>Planning and Running Workflows Locally</title>

      <para>In this exercise we are going to run pegasus-plan to generate a
      executable workflow from the abstract workflow (diamond.dax). The
      Executable workflow generated, are condor submit files that are
      submitted locally using pegasus-run</para>

      <para>The instructors have provided: <itemizedlist>
          <listitem>
            <para>A dax (diamond.dax) in the $HOME/pegasus-wms/dax
            directory.</para>
          </listitem>
        </itemizedlist></para>

      <para>You will need to write some things yourself, by following the
      instructions below: <itemizedlist>
          <listitem>
            <para>Run pegasus-plan to generate the condor submit files out of
            the dax.</para>
          </listitem>

          <listitem>
            <para>Run pegasus-run to submit the workflow locally.</para>
          </listitem>
        </itemizedlist></para>

      <para>Instructions:</para>

      <itemizedlist>
        <listitem>
          <para>Let us run pegasus-plan on the diamond dax. <programlisting><command>$ cd ~/pegasus-wms
$ pegasus-plan --dax `pwd`/dax/diamond.dax --force\
               --dir dags -s local -o local --nocleanup -v</command></programlisting>
          The above command says that we need to plan the diamond dax locally.
          The condor submit files are to be generated in a directory structure
          whose base is dags. We also are requesting that no cleanup jobs be
          added as we require the intermediate data to be saved. Here is the
          output of pegasus-plan. <programlisting><computeroutput>2010.07.19 16:47:05.276 PDT: [INFO] event.pegasus.planner planner.version 2.4.2  - STARTED 
2010.07.19 16:47:05.858 PDT: [INFO] event.pegasus.parse.dax dax.id /home/pegasus/pegasus-wms/dax/diamond.dax  
2010.07.19 16:47:05.901 PDT: [INFO] event.pegasus.parse.dax dax.id /home/pegasus/pegasus-wms/dax/diamond.dax   
2010.07.19 16:47:05.944 PDT: [INFO] event.pegasus.refinement dax.id diamond_0  - STARTED 
2010.07.19 16:47:05.957 PDT: [INFO] event.pegasus.siteselection dax.id diamond_0  - STARTED 
2010.07.19 16:47:05.977 PDT: [INFO] event.pegasus.siteselection dax.id diamond_0  - FINISHED 
2010.07.19 16:47:05.986 PDT: [INFO]  Grafting transfer nodes in the workflow 
2010.07.19 16:47:05.987 PDT: [INFO] event.pegasus.generate.transfer-nodes dax.id diamond_0  - STARTED 
2010.07.19 16:47:06.031 PDT: [INFO] event.pegasus.generate.transfer-nodes dax.id diamond_0  - FINISHED 
2010.07.19 16:47:06.032 PDT: [INFO] event.pegasus.generate.workdir-nodes dax.id diamond_0  - STARTED 
2010.07.19 16:47:06.034 PDT: [INFO] event.pegasus.generate.workdir-nodes dax.id diamond_0  - FINISHED 
2010.07.19 16:47:06.034 PDT: [INFO] event.pegasus.generate.cleanup-wf dax.id diamond_0  - STARTED 
2010.07.19 16:47:06.035 PDT: [INFO] event.pegasus.generate.cleanup-wf dax.id diamond_0  - FINISHED 
2010.07.19 16:47:06.035 PDT: [INFO] event.pegasus.refinement dax.id diamond_0  - FINISHED 
2010.07.19 16:47:06.202 PDT: [INFO]  Generating codes for the concrete workflow 
2010.07.19 16:47:06.316 PDT: [INFO]  Generating codes for the concrete workflow -DONE 
2010.07.19 16:47:06.316 PDT: [INFO]  Generating code for the cleanup workflow 
2010.07.19 16:47:06.369 PDT: [INFO]  Generating code for the cleanup workflow -DONE 
2010.07.19 16:47:06.373 PDT: [INFO]  


I have concretized your abstract workflow. The workflow has been entered 
into the workflow database with a state of "planned". The next step is 
to start or execute your workflow. The invocation required is


pegasus-run 
-Dpegasus.user.properties=$HOME/.../blackdiamond/run0001/pegasus.4459134667464687814.properties 
 --nodatabase /home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0001

 
2010.07.19 16:47:06.373 PDT: [INFO]  Time taken to execute is 1.065 seconds 
2010.07.19 16:47:06.374 PDT: [INFO] event.pegasus.planner planner.version 2.4.2  - FINISHED 
</computeroutput></programlisting></para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Now run pegasus-run as mentioned in the
          output of pegasus-plan. Do not copy the command below it is just for
          illustration purpose.</emphasis><programlisting><emphasis
                role="bold"></emphasis><emphasis role="bold">[pegasus@pegasus pegasus-wms]$ pegasus-run \
 -Dpegasus.user.properties=$HOME/.../blackdiamond/run0001/pegasus.350356687577055673.properties \
       $HOME/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0001</emphasis>

-----------------------------------------------------------------------
File for submitting this DAG to Condor           : blackdiamond-0.dag.condor.sub
Log of DAGMan debugging messages                 : blackdiamond-0.dag.dagman.out
Log of Condor library output                     : blackdiamond-0.dag.lib.out
Log of Condor library error messages             : blackdiamond-0.dag.lib.err
Log of the life of condor_dagman itself          : blackdiamond-0.dag.dagman.log

-no_submit given, not submitting DAG to Condor.  You can do this with:
"condor_submit blackdiamond-0.dag.condor.sub"
-----------------------------------------------------------------------
Submitting job(s).
Logging submit event(s).
1 job(s) submitted to cluster 320.

Your Workflow has been started and runs in base directory given below

cd /home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0001

*** To monitor the workflow you can run ***

pegasus-status -l /home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0001

*** To remove your workflow run ***
pegasus-remove -d 320.0
or
pegasus-remove /home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0001

[pegasus@pegasus pegasus-wms]$ 

</programlisting></para>
        </listitem>
      </itemizedlist>
    </section>
  </section>

  <section>
    <title>Monitoring, Debugging and Statistics</title>

    <para>In this section, we are going to list ways to track your workflow,
    how to debug a failed workflow and how to generates statistics and plots
    for a workflow run.</para>

    <section>
      <title>Tracking the progress of the workflow and debugging the
      workflows.</title>

      <para>We will change into the directory, that was mentioned by the
      output of pegasus-run command.</para>

      <programlisting><command>$ cd /home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0001</command></programlisting>

      <para>In this directory you will see a whole lot of files. That should
      not scare you. Unless things go wrong, you need to look at just a very
      few number of files to track the progress of the workflow</para>

      <itemizedlist>
        <listitem>
          <para><emphasis role="bold">Run the command pegasus-status as
          mentioned by pegasus-run above to check the status of your jobs. Use
          the watch command to auto repeat the command every 2
          seconds.</emphasis><programlisting><command>$ watch pegasus-status /home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0001</command><computeroutput>

-- Submitter: pegasus : &lt;172.16.80.128:40195&gt; : pegasus
 ID      OWNER/NODENAME   SUBMITTED     RUN_TIME ST PRI SIZE CMD               
  84.0   pegasus         7/19 16:59   0+00:01:17 R  0   7.3  condor_dagman -f -
  87.0    |-preprocess_  7/19 17:00   0+00:00:31 R  10  0.1  kickstart -n diamo
</computeroutput></programlisting><tip>
              <para>watch does not end with ESC nor (q)uit, but with
              Ctrl+C.</para>
            </tip> The above output shows that a couple of jobs are running
          under the main dagman process. Keep a lookout to track whether a
          workflow is running or not. If you do not see any of your job in the
          output for sometime (say 30 seconds), we know the workflow has
          finished. We need to wait, as there might be delay in Condor DAGMan
          releasing the next job into the queue after a job has finished
          successfully.</para>

          <para>If output of pegasus-status is empty, then either your
          workflow has</para>

          <orderedlist>
            <listitem>
              <para>successfully completed</para>
            </listitem>

            <listitem>
              <para>stopped midway due to non recoverable error.</para>
            </listitem>
          </orderedlist>

          <para>We can now run pegasus-analyzer to analyze the
          workflow.</para>
        </listitem>

        <listitem>
          <para>Using <emphasis role="bold">pegasus-analyzer</emphasis> to
          analyze the workflow</para>

          <programlisting><emphasis role="bold">$ pegasus-analyzer -q -i /home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0001</emphasis>

pegasus-analyzer: initializing...

************************************Summary*************************************

 Total jobs         :      8 (100.00%)
 # jobs succeeded   :      8 (100.00%)
 # jobs failed      :      0 (0.00%)
 # jobs unsubmitted :      0 (0.00%)

**************************************Done**************************************

pegasus-analyzer: end of status report


</programlisting>
        </listitem>

        <listitem>
          <para>Another way to monitor the workflow is to check the <emphasis
          role="bold">jobstate.log</emphasis> file. This is the output file of
          the monitoring daemon that is parsing all the condor log files to
          determine the status of the jobs. It logs the events seen by Condor
          into a more readable form for us. <programlisting><command>$ more jobstate.log</command><computeroutput>

1290676248 INTERNAL *** MONITORD_STARTED ***
1290676247 INTERNAL *** DAGMAN_STARTED 339.0 ***
[..]</computeroutput></programlisting> In the starting of the jobstate.log,
          when the workflow has just started running you will see a lot of
          entries with status UN_READY. That designates that DAGMan has just
          parsed in the .dag file and has not started working on any job as
          yet. Initially all the jobs in the workflow are listed as UN_READY.
          After sometime you will see entries in jobstate.log, that shows a
          job is being executed etc. <programlisting><computeroutput>
1290676261 create_dir_blackdiamond_0_local SUBMIT 340.0 local - 1
1290676266 create_dir_blackdiamond_0_local EXECUTE 340.0 local - 1
1290676266 create_dir_blackdiamond_0_local JOB_TERMINATED 340.0 local - 1
1290676266 create_dir_blackdiamond_0_local JOB_SUCCESS 0 local - 1
1290676266 create_dir_blackdiamond_0_local POST_SCRIPT_STARTED 340.0 local - 1
1290676271 create_dir_blackdiamond_0_local POST_SCRIPT_TERMINATED 340.0 local - 1
1290676271 create_dir_blackdiamond_0_local POST_SCRIPT_SUCCESS 0 local - 1</computeroutput></programlisting></para>

          <para>The above shows the being submitted and then executed on the
          grid. In addition it lists that job is being run on the grid site
          local (which is your submit machine). The various states of the job
          while it goes through submission to execution to post processing are
          in UPPERCASE.</para>
        </listitem>

        <listitem>
          <para>Successfully Completed : Let us again look at the
          jobstate.log. This time we need to look at the last few lines of
          jobstate.log <programlisting><command>$ tail jobstate.log</command><computeroutput>

1290676542 register_local_2_0 SUBMIT 347.0 local - 8
1290676547 register_local_2_0 EXECUTE 347.0 local - 8
1290676547 register_local_2_0 JOB_TERMINATED 347.0 local - 8
1290676547 register_local_2_0 JOB_SUCCESS 0 local - 8
1290676547 register_local_2_0 POST_SCRIPT_STARTED 347.0 local - 8
1290676552 register_local_2_0 POST_SCRIPT_TERMINATED 347.0 local - 8
1290676552 register_local_2_0 POST_SCRIPT_SUCCESS 0 local - 8
1290676552 INTERNAL *** DAGMAN_FINISHED 0 ***
1290676554 INTERNAL *** MONITORD_FINISHED 0 ***
</computeroutput></programlisting> Looking at the last two lines we see that
          DAGMan finished, and pegasus-monitord finished successfully with a
          status 0. This means workflow ran successfully. Congratulations you
          ran your workflow on the local site successfully. The workflow
          generates a final output file f.d that resides in the directory
          <emphasis
          role="bold">/home/pegasus/local-storage/storage/f.d</emphasis>
          .</para>

          <para>To view the file, you can do the following <programlisting><command>$ cat /home/pegasus/local-storage/storage/f.d
</command>

--- start f.c1 ----
  --- start f.b1 ----
    --- start f.a ----
      Input File for the Diamond Workflow.--- final f.a ----
    Timestamp Today: 20101125T011133.275-08:00 (1290676293.275;60.001)
    Applicationname: preprocess @ 172.16.80.129 (VPN)
    Current Workdir: /home/pegasus/local-scratch/exec/pegasus/pegasus/blackdiamond/run0001
    Systemenvironm.: x86_64-Linux 2.6.18-194.8.1.el5
    Processor Info.: 1 x Intel(R) Xeon(R) CPU           E5462  @ 2.80GHz @ 2794.306
    Load Averages  : 0.691 0.238 0.113
    Memory Usage MB: 1002 total, 350 free, 0 shared, 33 buffered
    Swap Usage   MB: 2015 total, 2015 free
    Filesystem Info: /                        ext3    17GB total,     9GB avail
    Output Filename: f.b1
    Input Filenames: f.a
  --- final f.b1 ----
  Timestamp Today: 20101125T011249.441-08:00 (1290676369.441;60.024)
  Applicationname: findrange @ 172.16.80.129 (VPN)
  Current Workdir: /home/pegasus/local-scratch/exec/pegasus/pegasus/blackdiamond/run0001
  Systemenvironm.: x86_64-Linux 2.6.18-194.8.1.el5
  Processor Info.: 1 x Intel(R) Xeon(R) CPU           E5462  @ 2.80GHz @ 2794.306
  Load Averages  : 1.413 0.530 0.219
  Memory Usage MB: 1002 total, 349 free, 0 shared, 33 buffered
  Swap Usage   MB: 2015 total, 2015 free
  Filesystem Info: /                        ext3    17GB total,     9GB avail
  Output Filename: f.c1
  Input Filenames: f.b1
--- final f.c1 ----
--- start f.c2 ----
  --- start f.b2 ----
    --- start f.a ----
      Input File for the Diamond Workflow.--- final f.a ----
    Timestamp Today: 20101125T011133.275-08:00 (1290676293.275;60.001)
    Applicationname: preprocess @ 172.16.80.129 (VPN)
    Current Workdir: /home/pegasus/local-scratch/exec/pegasus/pegasus/blackdiamond/run0001
    Systemenvironm.: x86_64-Linux 2.6.18-194.8.1.el5
    Processor Info.: 1 x Intel(R) Xeon(R) CPU           E5462  @ 2.80GHz @ 2794.306
    Load Averages  : 0.691 0.238 0.113
    Memory Usage MB: 1002 total, 350 free, 0 shared, 33 buffered
    Swap Usage   MB: 2015 total, 2015 free
    Filesystem Info: /                        ext3    17GB total,     9GB avail
    Output Filename: f.b2
    Input Filenames: f.a
  --- final f.b2 ----
  Timestamp Today: 20101125T011254.555-08:00 (1290676374.555;60.001)
  Applicationname: findrange @ 172.16.80.129 (VPN)
  Current Workdir: /home/pegasus/local-scratch/exec/pegasus/pegasus/blackdiamond/run0001
  Systemenvironm.: x86_64-Linux 2.6.18-194.8.1.el5
  Processor Info.: 1 x Intel(R) Xeon(R) CPU           E5462  @ 2.80GHz @ 2794.306
  Load Averages  : 1.380 0.538 0.223
  Memory Usage MB: 1002 total, 349 free, 0 shared, 33 buffered
  Swap Usage   MB: 2015 total, 2015 free
  Filesystem Info: /                        ext3    17GB total,     9GB avail
  Output Filename: f.c2
  Input Filenames: f.b2
--- final f.c2 ----
Timestamp Today: 20101125T011410.669-08:00 (1290676450.669;60.001)
Applicationname: analyze @ 172.16.80.129 (VPN)
Current Workdir: /home/pegasus/local-scratch/exec/pegasus/pegasus/blackdiamond/run0001
Systemenvironm.: x86_64-Linux 2.6.18-194.8.1.el5
Processor Info.: 1 x Intel(R) Xeon(R) CPU           E5462  @ 2.80GHz @ 2794.306
Load Averages  : 1.025 0.597 0.266
Memory Usage MB: 1002 total, 349 free, 0 shared, 34 buffered
Swap Usage   MB: 2015 total, 2015 free
Filesystem Info: /                        ext3    17GB total,     9GB avail
Output Filename: f.d
Input Filenames: f.c1 f.c2
</programlisting></para>
        </listitem>

        <listitem>
          <para>Unsuccessfully Completed (Workflow execution stopped midway) :
          Let us again look at the jobstate.log. Again we need to look at the
          last few lines of jobstate.log <programlisting><command>$ tail jobstate.log</command><computeroutput>

1290677127 stage_in_local_local_0 EXECUTE 352.0 local - 4
1290677127 stage_in_local_local_0 JOB_TERMINATED 352.0 local - 4
1290677127 stage_in_local_local_0 JOB_FAILURE 1 local - 4
1290677127 stage_in_local_local_0 POST_SCRIPT_STARTED 352.0 local - 4
1290677132 stage_in_local_local_0 POST_SCRIPT_TERMINATED 352.0 local - 4
1290677132 stage_in_local_local_0 POST_SCRIPT_FAILURE 1 local - 4
1290677132 INTERNAL *** DAGMAN_FINISHED 1 ***
1290677134 INTERNAL *** MONITORD_FINISHED 0 ***

</computeroutput></programlisting>Looking at the last two lines we see that
          DAGMan finished, and pegasus-monitord finished unsuccessfully with a
          status 1. We can easily determine which job failed. It is
          stage_in_local_local_0 in this case. To determine the reason for
          failure we need to look at it's kickstart output file which is
          JOBNAME.out.NNN. where NNN is 000 - NNN</para>
        </listitem>
      </itemizedlist>
    </section>

    <section>
      <title>Debugging a failed workflow using pegasus-analyzer</title>

      <para>In this section, we will run the diamond workflow but remove the
      input file so that the workflow fails during execution. This is to
      highlight how to use pegasus-analyzer to debug a failed workflow.</para>

      <para>First of all lets rename the input file f.a</para>

      <programlisting><emphasis role="bold"> $ mv /scratch/tutorial/inputdata/diamond/f.a /scratch/tutorial/inputdata/diamond/f.a.old

 $ cd $HOME/pegasus-wms
 </emphasis></programlisting>

      <para>We will now repeat exercise <emphasis role="bold">2.4 and
      2.5</emphasis> and submit the workflow again.</para>

      <programlisting><emphasis><emphasis role="bold">Plan and Submit the diamond workflow</emphasis> .</emphasis> Pass --submit to pegasus-plan to submit in case of successful planning

$  pegasus-plan --dax `pwd`/dax/diamond.dax --force \
        --dir dags -s local -o local --nocleanup --submit -v

<emphasis role="bold">
Use pegasus-status to track the workflow and wait it to fail</emphasis>

$ watch pegasus-status  /home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0002


-- Submitter: pegasus : &lt;172.16.80.128:40195&gt; : pegasus
 ID      OWNER/NODENAME   SUBMITTED     RUN_TIME ST PRI SIZE CMD               
  96.0   pegasus         7/19 17:40   0+00:01:06 R  0   7.3  condor_dagman -f -

<emphasis role="bold">
The --long option to pegasus-status of a running workflow gives more detail
<emphasis>[pegasus@pegasus pegasus-wms]$ pegasus-status -l /home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0002
blackdiamond-0.dag is running.
11/25 01:25:06  Done     Pre   Queued    Post   Ready   Un-Ready   Failed
11/25 01:25:06   ===     ===      ===     ===     ===        ===      ===
11/25 01:25:06     1       0        1       0       0          6        0</emphasis><emphasis>

WORKFLOW STATUS : RUNNING | 1/8 ( 12% ) | (condor processing workflow)
</emphasis>


We can also use --long option to pegasus-status to see the FINAL status of the workflow</emphasis>

$ pegasus-status -l /home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0002
blackdiamond-0.dag FAILED (status 1)
11/25 01:25:32  Done     Pre   Queued    Post   Ready   Un-Ready   Failed
11/25 01:25:32   ===     ===      ===     ===     ===        ===      ===
11/25 01:25:32     1       0        0       0       0          6        1

WORKFLOW STATUS : FAILED | 1/8 ( 12% ) | (rescue needs to be submitted)


</programlisting>

      <para>We will now run pegasus-analyzer on the failed workflow submit
      directory to see what job failed.</para>

      <programlisting><emphasis role="bold">$ pegasus-analyzer  -i $HOME/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0002<emphasis>
pegasus-analyzer: initializing...
</emphasis></emphasis>
************************************Summary*************************************

 Total jobs         :      8 (100.00%)
 # jobs succeeded   :      1 (12.50%)
 # jobs failed      :      1 (12.50%)
 # jobs unsubmitted :      6 (75.00%)

******************************Failed jobs' details******************************

=============================stage_in_local_local_0=============================

 last state: POST_SCRIPT_FAILURE
       site: local
submit file: /home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0002/stage_in_local_local_0.sub
output file: /home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0002/stage_in_local_local_0.out.002
 error file: /home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0002/stage_in_local_local_0.err.002

**************************************Done**************************************

pegasus-analyzer: end of status report

[pegasus@pegasus pegasus-wms]$ pegasus-analyzer  -i /home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0002
pegasus-analyzer: initializing...

************************************Summary*************************************

 Total jobs         :      8 (100.00%)
 # jobs succeeded   :      1 (12.50%)
 # jobs failed      :      1 (12.50%)
 # jobs unsubmitted :      6 (75.00%)

******************************Failed jobs' details******************************

=============================stage_in_local_local_0=============================

 last state: POST_SCRIPT_FAILURE
       site: local
submit file: /home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0002/stage_in_local_local_0.sub
output file: /home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0002/stage_in_local_local_0.out.002
 error file: /home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0002/stage_in_local_local_0.err.002

-------------------------------Task #1 - Summary--------------------------------

site        : local
hostname    : pegasus
executable  : /opt/pegasus/default/bin/pegasus-transfer
arguments   : 
exitcode    : 1
working dir : /home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0002

--Task #1 - pegasus::pegasus-transfer - pegasus::pegasus-transfer:1.0 - stdout--

2010-11-25 01:25:22,320    INFO:  Reading URL pairs from stdin
2010-11-25 01:25:22,321    INFO:  PATH=/usr/local/globus/default/bin:/opt/pegasus/default/bin:/usr/bin:/bin
2010-11-25 01:25:22,321    INFO:  LD_LIBRARY_PATH=/usr/local/globus/default/lib:/usr/java/jdk1.6.0_20/jre/lib/amd64
2010-11-25 01:25:22,321    INFO:  Executing cp commands
/bin/cp: cannot stat `/scratch/tutorial/inputdata/diamond/f.a': No such file or directory
2010-11-25 01:25:22,331 CRITICAL:  Command'/bin/cp -L"/scratch/tutorial/inputdata/diamond/f.a"
                 "/home/pegasus/local-scratch/exec/pegasus/pegasus/blackdiamond/run0002/f.a"'failed with error code 1

**************************************Done**************************************

pegasus-analyzer: end of status report

[pegasus@pegasus pegasus-wms]$ 

</programlisting>

      <para>The above tells us that the stage-in job for the workflow failed,
      and points us to the stdout of the job. By default, all jobs in Pegasus
      are launched via kickstart that captures runtime provenance of the job
      and helps in debugging. Hence, the stdout of the job is the kickstart
      stdout which is in XML.</para>

      <para>. the duration of the job the start time for the job the node on
      which the job ran the stdout/stderr of the job the arguments with which
      it launched the job the environment that was set for the job before it
      was launched. the machine information about the node that the job ran on
      Amongst the above information, the dagman.out file gives a coarser
      grained estimate of the job duration and start time</para>
    </section>

    <section>
      <title>Kickstart and Condor DAGMan format and log files</title>

      <para>This section explains how to read kickstart output and DAGMan
      Condor log files.</para>

      <section>
        <title>Kickstart</title>

        <para>Kickstart is a light weight C executable that is shipped with
        the pegasus worker package. All jobs are launced via Kickstart on the
        remote end, unless explicitly disabled at the time of running
        pegasus-plan.</para>

        <para>Kickstart does not work with</para>

        <orderedlist>
          <listitem>
            <para>Condor Standard Universe Jobs</para>
          </listitem>

          <listitem>
            <para>MPI jobs</para>
          </listitem>
        </orderedlist>

        <para>Pegasus automatically disables kickstart for the above
        jobs.</para>

        <para>Kickstart captures useful runtime provenance information about
        the job launched by it on the remote note, and puts in an XML record
        that it writes to it's stdout. The stdout appears in the workflow
        submit directory as &lt;job&gt;.out.00n . Some useful information
        captured by kickstart and logged are as follows</para>

        <orderedlist>
          <listitem>
            <para>the exitcode with which the job it launched exited</para>
          </listitem>

          <listitem>
            <para>the duration of the job</para>
          </listitem>

          <listitem>
            <para>the start time for the job</para>
          </listitem>

          <listitem>
            <para>the node on which the job ran</para>
          </listitem>

          <listitem>
            <para>the directory in which the job ran</para>
          </listitem>

          <listitem>
            <para>the stdout/stderr of the job</para>
          </listitem>

          <listitem>
            <para>the arguments with which it launched the job</para>
          </listitem>

          <listitem>
            <para>the environment that was set for the job before it was
            launched.</para>
          </listitem>

          <listitem>
            <para>the machine information about the node that the job ran
            on</para>
          </listitem>
        </orderedlist>

        <section>
          <title>Reading a Kickstart Output File</title>

          <para>Lets look at the stdout of our failed job.</para>

          <programlisting><emphasis role="bold">$ </emphasis><emphasis
              role="bold">cat /home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0002/stage_in_local_local_0.out.002 </emphasis>

 &lt;?xml version="1.0" encoding="ISO-8859-1"?&gt;
 &lt;invocation xmlns="http://pegasus.isi.edu/schema/invocation" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xsi:schemaLocation="http://pegasus.isi.edu/schema/invocation http://pegasus.isi.edu/schema/iv-2.1.xsd" version="2.1"
  start="2010-11-29T19:10:23.862-08:00" duration="0.076" <emphasis role="bold">transformation="pegasus::pegasus-transfer"</emphasis> 
  derivation="pegasus::pegasus-transfer:1.0" <emphasis role="bold">resource="local"</emphasis> wf-label="blackdiamond" 
  wf-stamp="2010-11-29T18:57:59-08:00" interface="eth0" <emphasis role="bold">hostaddr="172.16.80.131" hostname="pegasus" </emphasis>
  pid="5428" uid="501" user="pegasus" gid="501" group="pegasus" umask="0022"&gt;
 
 <emphasis role="bold">&lt;mainjob start="2010-11-29T19:10:23.876-08:00" duration="0.063" pid="5429"&gt;
 </emphasis>   &lt;usage utime="0.040" stime="0.023" minflt="2758" majflt="0" nswap="0" nsignals="0" nvcsw="5" nivcsw="20"/&gt;
    <emphasis role="bold">&lt;status raw="256"&gt;&lt;regular exitcode="1"/&gt;&lt;/status&gt;</emphasis>
    &lt;statcall error="0"&gt;
      &lt;file name="/opt/pegasus/default/bin/pegasus-transfer"&gt;23212F7573722F62696E2F656E762070&lt;/file&gt;
      &lt;statinfo mode="0100775" size="25314" inode="2022205" nlink="1" blksize="4096" blocks="64" 
               mtime="2010-11-23T13:14:52-08:00" 
             atime="2010-11-29T19:10:07-08:00" ctime="2010-11-25T00:01:52-08:00" uid="501" user="pegasus" 
               gid="501" group="pegasus"/&gt;
    &lt;/statcall&gt;
    &lt;argument-vector/&gt;
  &lt;/mainjob&gt;
  <emphasis role="bold">&lt;cwd&gt;/home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0002&lt;/cwd&gt;</emphasis>
  &lt;usage utime="0.002" stime="0.013" minflt="475" majflt="0" nswap="0" nsignals="0" nvcsw="1" nivcsw="5"/&gt;
  
 <emphasis role="bold">&lt;machine page-size="4096"&gt;
    &lt;stamp&gt;2010-11-29T19:10:23.862-08:00&lt;/stamp&gt;
    &lt;uname system="linux" nodename="pegasus" release="2.6.18-194.8.1.el5" machine="x86_64"&gt;
                  #1 SMP Thu Jul 1 19:04:48 EDT 2010&lt;/uname&gt;
   &lt;linux&gt;
    &lt;ram total="1051533312" free="306233344" shared="0" buffer="44756992"/&gt;
    &lt;swap total="536862720" free="536862720"/&gt;
    &lt;boot idle="2026.550"&gt;2010-11-29T18:32:29.013-08:00&lt;/boot&gt;
    &lt;cpu count="1" speed="2794" vendor="GenuineIntel"&gt;Intel(R) Xeon(R) CPU E5462 @ 2.80GHz&lt;/cpu&gt;
    &lt;load min1="0.00" min5="0.15" min15="0.17"/&gt;
    &lt;proc total="114" running="2" sleeping="112" vmsize="8332234752" rss="381333504"/&gt;
    &lt;task total="141" running="2" sleeping="139"/&gt;
   &lt;/linux&gt;
  &lt;/machine&gt;</emphasis>

  &lt;statcall error="0" id="stdin"&gt;
    &lt;descriptor number="0"/&gt;
    &lt;statinfo mode="0100664" size="142" inode="2250032" nlink="1" blksize="4096" blocks="16" 
     mtime="2010-11-29T19:09:20-08:00"   atime="2010-11-29T19:10:07-08:00" ctime="2010-11-29T19:09:20-08:00" 
     uid="501" user="pegasus" gid="501" group="pegasus"/&gt;
  &lt;/statcall&gt;

  <emphasis role="bold">&lt;statcall error="0" id="stdout"&gt;
    &lt;temporary name="/opt/condor/local.pegasus/spool/local_univ_execute/dir_5427/gs.out.awOX6p" descriptor="3"/&gt;
    &lt;statinfo mode="0100600" size="762" inode="2054511" nlink="1" blksize="4096" blocks="16" 
          mtime="2010-11-29T19:10:23-08:00" atime="2010-11-29T19:10:23-08:00" ctime="2010-11-29T19:10:23-08:00" 
             uid="501" user="pegasus" gid="501" group="pegasus"/&gt;
    &lt;data&gt;2010-11-29 19:10:23,920    INFO:  Reading URL pairs from stdin
2010-11-29 19:10:23,921    INFO:  PATH=/usr/local/globus/default/bin:/opt/pegasus/default/bin:/usr/bin:/bin
2010-11-29 19:10:23,921    INFO:  LD_LIBRARY_PATH=/usr/local/globus/default/lib:/usr/java/jdk1.6.0_20/jre/lib/amd64/
2010-11-29 19:10:23,921    INFO:  Executing cp commands
/bin/cp: cannot stat `/scratch/tutorial/inputdata/diamond/f.a&amp;apos;: No such file or directory
2010-11-29 19:10:23,932 CRITICAL:  Command &amp;apos;/bin/cp -L &amp;quot;/scratch/tutorial/inputdata/diamond/f.a&amp;quot; 
    &amp;quot;/home/pegasus/local-scratch/exec/pegasus/pegasus/blackdiamond/run0002/f.a&amp;quot;&amp;apos; failed with error code 1
&lt;/data&gt;
  &lt;/statcall&gt;</emphasis>

  <emphasis role="bold">&lt;statcall error="0" id="stderr"&gt;
    &lt;temporary name="/opt/condor/local.pegasus/spool/local_univ_execute/dir_5427/gs.err.oz9MOG" descriptor="4"/&gt;
    &lt;statinfo mode="0100600" size="0" inode="2054512" nlink="1" blksize="4096" blocks="8" 
    mtime="2010-11-29T19:10:23-08:00"  atime="2010-11-29T19:10:23-08:00" ctime="2010-11-29T19:10:23-08:00" 
    uid="501" user="pegasus" gid="501" group="pegasus"/&gt;
  &lt;/statcall&gt;
</emphasis>
  &lt;statcall error="2" id="gridstart"&gt;
    &lt;!-- ignore above error --&gt;
    &lt;file name="condor_exec.exe"/&gt;
  &lt;/statcall&gt;
  &lt;statcall error="0" id="logfile"&gt;
    &lt;descriptor number="1"/&gt;
    &lt;statinfo mode="0100644" size="0" inode="2250072" nlink="1" blksize="4096" blocks="8" mtime="2010-11-29T19:10:23-08:00" 
    atime="2010-11-29T19:10:23-08:00" ctime="2010-11-29T19:10:23-08:00" uid="501" user="pegasus" gid="501" group="pegasus"/&gt;
  &lt;/statcall&gt;
  &lt;statcall error="0" id="channel"&gt;
    &lt;fifo name="/opt/condor/local.pegasus/spool/local_univ_execute/dir_5427/gs.app.qCOCwX" descriptor="5" count="0"
     rsize="0" wsize="0"/&gt;
    &lt;statinfo mode="010640" size="0" inode="2054524" nlink="1" blksize="4096" blocks="8" mtime="2010-11-29T19:10:23-08:00" 
     atime="2010-11-29T19:10:23-08:00" ctime="2010-11-29T19:10:23-08:00" uid="501" user="pegasus" gid="501" 
    group="pegasus"/&gt;
  &lt;/statcall&gt;
  &lt;environment&gt;
    &lt;env key="GLOBUS_LOCATION"&gt;/usr/local/globus/default&lt;/env&gt;
    &lt;env key="GRIDSTART_CHANNEL"&gt;/opt/condor/local.pegasus/spool/local_univ_execute/dir_5427/gs.app.qCOCwX&lt;/env&gt;
    &lt;env key="JAVA_HOME"&gt;/usr/java/default&lt;/env&gt;
    &lt;env key="LD_LIBRARY_PATH"&gt;/usr/java/jdk1.6.0_20/jre/lib/amd64/server:/usr/java/jdk1.6.0_20/jre/lib/amd64:&lt;/env&gt;
    &lt;env key="PEGASUS_HOME"&gt;/opt/pegasus/default&lt;/env&gt;
    &lt;env key="TEMP"&gt;/opt/condor/local.pegasus/spool/local_univ_execute/dir_5427&lt;/env&gt;
    &lt;env key="TMP"&gt;/opt/condor/local.pegasus/spool/local_univ_execute/dir_5427&lt;/env&gt;
    &lt;env key="TMPDIR"&gt;/opt/condor/local.pegasus/spool/local_univ_execute/dir_5427&lt;/env&gt;
    &lt;env key="_CONDOR_ANCESTOR_4843"&gt;4862:1291085504:2790807554&lt;/env&gt;
    &lt;env key="_CONDOR_ANCESTOR_4862"&gt;5427:1291086623:1798288782&lt;/env&gt;
    &lt;env key="_CONDOR_ANCESTOR_5427"&gt;5428:1291086623:2750667008&lt;/env&gt;
    &lt;env key="_CONDOR_HIGHPORT"&gt;41000&lt;/env&gt;
    &lt;env key="_CONDOR_JOB_AD"&gt;/opt/condor/local.pegasus/spool/local_univ_execute/dir_5427/.job.ad&lt;/env&gt;
    &lt;env key="_CONDOR_LOWPORT"&gt;40000&lt;/env&gt;
    &lt;env key="_CONDOR_MACHINE_AD"&gt;/opt/condor/local.pegasus/spool/local_univ_execute/dir_5427/.machine.ad&lt;/env&gt;
    &lt;env key="_CONDOR_SCRATCH_DIR"&gt;/opt/condor/local.pegasus/spool/local_univ_execute/dir_5427&lt;/env&gt;
    &lt;env key="_CONDOR_SLOT"&gt;1&lt;/env&gt;
  &lt;/environment&gt;
  &lt;resource&gt;
    &lt;soft id="RLIMIT_CPU"&gt;unlimited&lt;/soft&gt;
    &lt;hard id="RLIMIT_CPU"&gt;unlimited&lt;/hard&gt;
    &lt;soft id="RLIMIT_FSIZE"&gt;unlimited&lt;/soft&gt;
    &lt;hard id="RLIMIT_FSIZE"&gt;unlimited&lt;/hard&gt;
    &lt;soft id="RLIMIT_DATA"&gt;unlimited&lt;/soft&gt;
    &lt;hard id="RLIMIT_DATA"&gt;unlimited&lt;/hard&gt;
    &lt;soft id="RLIMIT_STACK"&gt;unlimited&lt;/soft&gt;
    &lt;hard id="RLIMIT_STACK"&gt;unlimited&lt;/hard&gt;
    &lt;soft id="RLIMIT_CORE"&gt;0&lt;/soft&gt;
    &lt;hard id="RLIMIT_CORE"&gt;0&lt;/hard&gt;
    &lt;soft id="RESOURCE_5"&gt;unlimited&lt;/soft&gt;
    &lt;hard id="RESOURCE_5"&gt;unlimited&lt;/hard&gt;
    &lt;soft id="RLIMIT_NPROC"&gt;unlimited&lt;/soft&gt;
    &lt;hard id="RLIMIT_NPROC"&gt;unlimited&lt;/hard&gt;
    &lt;soft id="RLIMIT_NOFILE"&gt;1024&lt;/soft&gt;
    &lt;hard id="RLIMIT_NOFILE"&gt;1024&lt;/hard&gt;
    &lt;soft id="RLIMIT_MEMLOCK"&gt;32768&lt;/soft&gt;
    &lt;hard id="RLIMIT_MEMLOCK"&gt;32768&lt;/hard&gt;
    &lt;soft id="RLIMIT_AS"&gt;unlimited&lt;/soft&gt;
    &lt;hard id="RLIMIT_AS"&gt;unlimited&lt;/hard&gt;
    &lt;soft id="RLIMIT_LOCKS"&gt;unlimited&lt;/soft&gt;
    &lt;hard id="RLIMIT_LOCKS"&gt;unlimited&lt;/hard&gt;
    &lt;soft id="RLIMIT_SIGPENDING"&gt;8192&lt;/soft&gt;
    &lt;hard id="RLIMIT_SIGPENDING"&gt;8192&lt;/hard&gt;
    &lt;soft id="RLIMIT_MSGQUEUE"&gt;819200&lt;/soft&gt;
    &lt;hard id="RLIMIT_MSGQUEUE"&gt;819200&lt;/hard&gt;
    &lt;soft id="RLIMIT_NICE"&gt;0&lt;/soft&gt;
    &lt;hard id="RLIMIT_NICE"&gt;0&lt;/hard&gt;
    &lt;soft id="RLIMIT_RTPRIO"&gt;0&lt;/soft&gt;
    &lt;hard id="RLIMIT_RTPRIO"&gt;0&lt;/hard&gt;
  &lt;/resource&gt;
&lt;/invocation&gt;

</programlisting>
        </section>
      </section>

      <section>
        <title>Condor DAGMan format and log files etc.</title>

        <para>In this exercise we will learn about the DAG file format and
        some of the log files generated when the DAG runs.</para>

        <itemizedlist>
          <listitem>
            <para>Now take a look at the DAG file...</para>

            <programlisting><command>$ cat $HOME/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0001/blackdiamond-0.dag</command><computeroutput>


######################################################################
# PEGASUS WMS GENERATED DAG FILE
# DAG blackdiamond
# Index = 0, Count = 1
######################################################################
MAXJOBS projection 2

JOB create_dir_blackdiamond_0_local create_dir_blackdiamond_0_local.sub
SCRIPT POST create_dir_blackdiamond_0_local /opt/pegasus/default/bin/pegasus-exitcode 
  /home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0001/create_dir_blackdiamond_0_local.out
RETRY create_dir_blackdiamond_0_local 2

JOB stage_in_local_local_0 stage_in_local_local_0.sub
SCRIPT POST stage_in_local_local_0 /opt/pegasus/default/bin/pegasus-exitcode  
 /home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0001/stage_in_local_local_0.out
RETRY stage_in_local_local_0 2

JOB preprocess_j1 preprocess_j1.sub
SCRIPT POST preprocess_j1 /opt/pegasus/default/bin/pegasus-exitcode   
/home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0001/preprocess_j1.out
RETRY preprocess_j1 2

JOB findrange_j2 findrange_j2.sub
SCRIPT POST findrange_j2 /opt/pegasus/default/bin/pegasus-exitcode   
/home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0001/findrange_j2.out
RETRY findrange_j2 2

JOB findrange_j3 findrange_j3.sub
SCRIPT POST findrange_j3 /opt/pegasus/default/bin/pegasus-exitcode   
/home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0001/findrange_j3.out
RETRY findrange_j3 2

JOB analyze_j4 analyze_j4.sub
SCRIPT POST analyze_j4 /opt/pegasus/default/bin/pegasus-exitcode   
/home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0001/analyze_j4.out
RETRY analyze_j4 2

JOB stage_out_local_local_2_0 stage_out_local_local_2_0.sub
SCRIPT POST stage_out_local_local_2_0 /opt/pegasus/default/bin/pegasus-exitcode   
/home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0001/stage_out_local_local_2_0.out
RETRY stage_out_local_local_2_0 2

JOB register_local_2_0 register_local_2_0.sub
SCRIPT POST register_local_2_0 /opt/pegasus/default/bin/pegasus-exitcode   
/home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0001/register_local_2_0.out
RETRY register_local_2_0 2

PARENT findrange_j2 CHILD analyze_j4
PARENT preprocess_j1 CHILD findrange_j2
PARENT preprocess_j1 CHILD findrange_j3
PARENT findrange_j3 CHILD analyze_j4
PARENT analyze_j4 CHILD stage_out_local_local_2_0
PARENT stage_in_local_local_0 CHILD preprocess_j1
PARENT stage_out_local_local_2_0 CHILD register_local_2_0
PARENT create_dir_blackdiamond_0_local CHILD analyze_j4
PARENT create_dir_blackdiamond_0_local CHILD findrange_j2
PARENT create_dir_blackdiamond_0_local CHILD preprocess_j1
PARENT create_dir_blackdiamond_0_local CHILD findrange_j3
PARENT create_dir_blackdiamond_0_local CHILD stage_in_local_local_0
######################################################################
# End of DAG
##################################################################</computeroutput></programlisting>
          </listitem>

          <listitem>
            <para>... and the dagman.out file.</para>

            <programlisting><emphasis role="bold"><command>$</command><computeroutput> cat $HOME/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0001/blackdiamond-0.dag.dagman.out </computeroutput></emphasis><computeroutput>

11/25 01:10:47 ******************************************************
11/25 01:10:47 ** condor_scheduniv_exec.339.0 (CONDOR_DAGMAN) STARTING UP
11/25 01:10:47 ** /opt/condor/7.4.2/bin/condor_dagman
11/25 01:10:47 ** SubsystemInfo: name=DAGMAN type=DAGMAN(10) class=DAEMON(1)
11/25 01:10:47 ** Configuration: subsystem:DAGMAN local:&lt;NONE&gt; class:DAEMON
11/25 01:10:47 ** $CondorVersion: 7.4.2 Mar 29 2010 BuildID: 227044 $
11/25 01:10:47 ** $CondorPlatform: X86_64-LINUX_RHEL5 $
11/25 01:10:47 ** PID = 7844
11/25 01:10:47 ** Log last touched time unavailable (No such file or directory)
11/25 01:10:47 ******************************************************
11/25 01:10:47 Using config source: /opt/condor/config/condor_config
11/25 01:10:47 Using local config sources: 
11/25 01:10:47    /opt/condor/config/condor_config.local
11/25 01:10:47 DaemonCore: Command Socket at &lt;172.16.80.129:40035&gt;
11/25 01:10:47 DAGMAN_DEBUG_CACHE_SIZE setting: 5242880
11/25 01:10:47 DAGMAN_DEBUG_CACHE_ENABLE setting: False
11/25 01:10:47 DAGMAN_SUBMIT_DELAY setting: 0
11/25 01:10:47 DAGMAN_MAX_SUBMIT_ATTEMPTS setting: 6
11/25 01:10:47 DAGMAN_STARTUP_CYCLE_DETECT setting: 0
11/25 01:10:47 DAGMAN_MAX_SUBMITS_PER_INTERVAL setting: 5
11/25 01:10:47 DAGMAN_USER_LOG_SCAN_INTERVAL setting: 5
11/25 01:10:47 allow_events (DAGMAN_IGNORE_DUPLICATE_JOB_EXECUTION, DAGMAN_ALLOW_EVENTS) setting: 114
11/25 01:10:47 DAGMAN_RETRY_SUBMIT_FIRST setting: 1
11/25 01:10:47 DAGMAN_RETRY_NODE_FIRST setting: 0
11/25 01:10:47 DAGMAN_MAX_JOBS_IDLE setting: 0
11/25 01:10:47 DAGMAN_MAX_JOBS_SUBMITTED setting: 0
11/25 01:10:47 DAGMAN_MUNGE_NODE_NAMES setting: 1
11/25 01:10:47 DAGMAN_PROHIBIT_MULTI_JOBS setting: 0
11/25 01:10:47 DAGMAN_SUBMIT_DEPTH_FIRST setting: 0
11/25 01:10:47 DAGMAN_ABORT_DUPLICATES setting: 1
11/25 01:10:47 DAGMAN_ABORT_ON_SCARY_SUBMIT setting: 1
11/25 01:10:47 DAGMAN_PENDING_REPORT_INTERVAL setting: 600
11/25 01:10:47 DAGMAN_AUTO_RESCUE setting: 1
11/25 01:10:47 DAGMAN_MAX_RESCUE_NUM setting: 100
11/25 01:10:47 DAGMAN_DEFAULT_NODE_LOG setting: null
11/25 01:10:47 ALL_DEBUG setting: 
11/25 01:10:47 DAGMAN_DEBUG setting: 
....
11/25 01:10:47 Default node log file is:
 &lt;/home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0001/blackdiamond-0.dag.nodes.log&gt;
11/25 01:10:47 DAG Lockfile will be written to blackdiamond-0.dag.lock
11/25 01:10:47 DAG Input file is blackdiamond-0.dag
11/25 01:10:47 Parsing 1 dagfiles
11/25 01:10:47 Parsing blackdiamond-0.dag ...
11/25 01:10:47 Dag contains 8 total jobs
11/25 01:10:47 Sleeping for 12 seconds to ensure ProcessId uniqueness
11/25 01:10:59 Bootstrapping...
11/25 01:10:59 Number of pre-completed nodes: 0
11/25 01:10:59 Registering condor_event_timer...
11/25 01:11:00 Sleeping for one second for log file consistency
11/25 01:11:01 Submitting Condor Node create_dir_blackdiamond_0_local job(s)...
11/25 01:11:01 submitting: condor_submit -a dag_node_name' '=' 'create_dir_blackdiamond_0_local -a 
+DAGManJobId' '=' '339 -a DAGManJobId' '=' '339 -a submit_event_notes' '=' 'DAG' 'Node:' '
create_dir_blackdiamond_0_local -a +DAGParentNodeNames' '=' '"" create_dir_blackdiamond_0_local.sub
11/25 01:11:01 From submit: Submitting job(s).
11/25 01:11:01 From submit: Logging submit event(s).
11/25 01:11:01 From submit: 1 job(s) submitted to cluster 340.
11/25 01:11:01  assigned Condor ID (340.0)
11/25 01:11:01 Just submitted 1 job this cycle...
11/25 01:11:01 Currently monitoring 1 Condor log file(s)
11/25 01:11:01 Event: ULOG_SUBMIT for Condor Node create_dir_blackdiamond_0_local (340.0)
11/25 01:11:01 Number of idle job procs: 1
11/25 01:11:01 Of 8 nodes total:
11/25 01:11:01  Done     Pre   Queued    Post   Ready   Un-Ready   Failed
11/25 01:11:01   ===     ===      ===     ===     ===        ===      ===
11/25 01:11:01     0       0        1       0       0          7        0
....
11/25 01:11:06 Currently monitoring 1 Condor log file(s)
11/25 01:11:06 Event: ULOG_EXECUTE for Condor Node create_dir_blackdiamond_0_local (340.0)
11/25 01:11:06 Number of idle job procs: 0
11/25 01:11:06 Event: ULOG_JOB_TERMINATED for Condor Node create_dir_blackdiamond_0_local (340.0)
11/25 01:11:06 Node create_dir_blackdiamond_0_local job proc (340.0) completed successfully.
11/25 01:11:06 Node create_dir_blackdiamond_0_local job completed
11/25 01:11:06 Running POST script of Node create_dir_blackdiamond_0_local...
11/25 01:11:06 Number of idle job procs: 0
11/25 01:11:06 Of 8 nodes total:
11/25 01:11:06  Done     Pre   Queued    Post   Ready   Un-Ready   Failed
11/25 01:11:06   ===     ===      ===     ===     ===        ===      ===
11/25 01:11:06     0       0        0       1       0          7        0
11/25 01:11:11 Currently monitoring 1 Condor log file(s)
11/25 01:11:11 Event: ULOG_POST_SCRIPT_TERMINATED for Condor Node create_dir_blackdiamond_0_local (340.0)
11/25 01:11:11 POST Script of Node create_dir_blackdiamond_0_local completed successfully.
11/25 01:11:11 Of 8 nodes total:
11/25 01:11:11  Done     Pre   Queued    Post   Ready   Un-Ready   Failed
11/25 01:11:11   ===     ===      ===     ===     ===        ===      ===
11/25 01:11:11     1       0        0       0       1          6        0
....
11/25 01:15:52 Event: ULOG_POST_SCRIPT_TERMINATED for Condor Node register_local_2_0 (347.0)
11/25 01:15:52 POST Script of Node register_local_2_0 completed successfully.
11/25 01:15:52 Of 8 nodes total:
11/25 01:15:52  Done     Pre   Queued    Post   Ready   Un-Ready   Failed
11/25 01:15:52   ===     ===      ===     ===     ===        ===      ===
11/25 01:15:52     8       0        0       0       0          0        0
11/25 01:15:52 All jobs Completed!
11/25 01:15:52 Note: 0 total job deferrals because of -MaxJobs limit (0)
11/25 01:15:52 Note: 0 total job deferrals because of -MaxIdle limit (0)
11/25 01:15:52 Note: 0 total job deferrals because of node category throttles
11/25 01:15:52 Note: 0 total PRE script deferrals because of -MaxPre limit (20)
11/25 01:15:52 Note: 0 total POST script deferrals because of -MaxPost limit (20)
11/25 01:15:52 **** condor_scheduniv_exec.339.0 (condor_DAGMAN) pid 7844 EXITING WITH STATUS 0
[p

</computeroutput></programlisting>
          </listitem>
        </itemizedlist>
      </section>
    </section>

    <section>
      <title>Removing a running workflow</title>

      <para>Sometimes you may want to halt the execution of the workflow or
      just permanently remove it. You can stop/halt a workflow by running the
      pegasus-remove command mentioned in the output of pegasus-run</para>

      <programlisting><command>$ pegasus-remove $HOME/pegasus-wms/dags/pegasus/pegasus/diamond/runXXXX</command><computeroutput>

Job 2788.0 marked for removal</computeroutput></programlisting>
    </section>

    <section>
      <title>Generating statistics and plots of a workflow run</title>

      <para>In this section, we will generate statistics and plots of the
      diamond workflow we ran using pegasus-statistics and
      pegasus-plots</para>

      <para></para>

      <section>
        <title>Generating Statistics Using pegasus-statistics</title>

        <para>pegasus-statistics generates workflow execution statistics. To
        generate statistics run the command as shown below</para>

        <programlisting><emphasis role="bold">$ cd $HOME/pegasus-wms


$ pegasus-statistics dags/pegasus/pegasus/blackdiamond/run0001/
</emphasis>

******************************************** SUMMARY ********************************************
Total workflow execution time      :          306 
Total workflow execution wall time :      240.843 
Total jobs                         :            8 
Total tasks                        :            8 
# jobs succeeded                   :            8 
# jobs failed                      :            0 
# jobs unsubmitted                 :            0 
# jobs unknown                     :            0 

Workflow execution statistics created at :
/home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0001/statistics/workflow

Workflow events with time starting with zero is created at :
/home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0001/statistics/out

Job statistics is created at : 
/home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0001/statistics/jobs

Logical transformation statistics is created at :
/home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0001/statistics/breakdown.txt
 </programlisting>

        <para><emphasis role="bold">Workflow statistics
        table</emphasis></para>

        <para>Workflow statistics table contains information about the
        workflow run like total execution time, job's failed etc.</para>

        <table>
          <title>Table Workflow Statistics</title>

          <tgroup cols="2">
            <tbody>
              <row>
                <entry>Total workflow execution time</entry>

                <entry>306</entry>
              </row>

              <row>
                <entry>Total workflow execution wall time</entry>

                <entry>240.843</entry>
              </row>

              <row>
                <entry>Total jobs</entry>

                <entry>8</entry>
              </row>

              <row>
                <entry>Total tasks</entry>

                <entry>8</entry>
              </row>

              <row>
                <entry># jobs succeeded</entry>

                <entry>8</entry>
              </row>

              <row>
                <entry># jobs failed</entry>

                <entry>0</entry>
              </row>

              <row>
                <entry># jobs unsubmitted</entry>

                <entry>0</entry>
              </row>

              <row>
                <entry># jobs unknown</entry>

                <entry>0</entry>
              </row>
            </tbody>
          </tgroup>
        </table>

        <para><emphasis role="bold">Job statistics table</emphasis></para>

        <para>Job statistics table contains the following details about the
        jobs in the workflow. A sample table is shown below.</para>

        <itemizedlist>
          <listitem>
            <para>Job - the name of the job</para>
          </listitem>

          <listitem>
            <para>Site - the site where the job ran</para>
          </listitem>

          <listitem>
            <para>Kickstart - the actual duration of the job in seconds on the
            remote compute node</para>
          </listitem>

          <listitem>
            <para>Post - the postscript time as reported by DAGMan</para>
          </listitem>

          <listitem>
            <para>DAGMan - the time between the last parent job of a job
            completes and the job gets submitted</para>
          </listitem>

          <listitem>
            <para>CondorQTime - the time between submission by DAGMan and the
            remote Grid submission. It is an estimate of the time spent in the
            condor q on the submit node</para>
          </listitem>

          <listitem>
            <para>Resource - the time between the remote Grid submission and
            start of remote execution . It is an estimate of the time job
            spent in the remote queue</para>
          </listitem>

          <listitem>
            <para>Runtime - the time spent on the resource as seen by Condor
            DAGMan . Is always &gt;=kickstart</para>
          </listitem>

          <listitem>
            <para>CondorQLen - the number of outstanding jobs in the queue
            when this job was released</para>
          </listitem>

          <listitem>
            <para>Seqexec - the time taken for the completion of a clustered
            job</para>
          </listitem>

          <listitem>
            <para>Seqexec-Delay- the time difference between the time for the
            completion of a clustered job and sum of all the individual tasks
            kickstart time</para>
          </listitem>
        </itemizedlist>

        <table>
          <title>Table Job Statistics</title>

          <tgroup align="center" cols="11">
            <thead>
              <row>
                <entry align="center">Job</entry>

                <entry align="center">Site</entry>

                <entry align="center">Kickstart</entry>

                <entry align="center">Post</entry>

                <entry align="center">DAGMan</entry>

                <entry align="center">CondorQTime</entry>

                <entry align="center">Resource</entry>

                <entry align="center">Runtime</entry>

                <entry align="center">CondorQLen</entry>

                <entry align="center">Seqexec</entry>

                <entry align="center">Seqexec-Delay</entry>
              </row>
            </thead>

            <tbody>
              <row>
                <entry>analyze_j4</entry>

                <entry>local</entry>

                <entry>60.03</entry>

                <entry>6.00</entry>

                <entry>6.00</entry>

                <entry>0.00</entry>

                <entry>0.00</entry>

                <entry>60.00</entry>

                <entry>0</entry>

                <entry>-</entry>

                <entry>-</entry>
              </row>

              <row>
                <entry>create_dir_blackdiamond_0_local</entry>

                <entry>local</entry>

                <entry>0.04</entry>

                <entry>5.00</entry>

                <entry>14.00</entry>

                <entry>0.00</entry>

                <entry>0.00</entry>

                <entry>0.06</entry>

                <entry>0</entry>

                <entry>-</entry>

                <entry>-</entry>
              </row>

              <row>
                <entry>findrange_j2</entry>

                <entry>local</entry>

                <entry>60.03</entry>

                <entry>5.00</entry>

                <entry>6.00</entry>

                <entry>0.00</entry>

                <entry>0.00</entry>

                <entry>65.00</entry>

                <entry>0</entry>

                <entry>-</entry>

                <entry>-</entry>
              </row>

              <row>
                <entry>findrange_j3</entry>

                <entry>local</entry>

                <entry>60.03</entry>

                <entry>5.00</entry>

                <entry>6.00</entry>

                <entry>0.00</entry>

                <entry>0.00</entry>

                <entry>60.00</entry>

                <entry>0</entry>

                <entry>-</entry>

                <entry>-</entry>
              </row>

              <row>
                <entry>preprocess_j1</entry>

                <entry>local</entry>

                <entry>60.03</entry>

                <entry>5.00</entry>

                <entry>6.00</entry>

                <entry>0.00</entry>

                <entry>0.00</entry>

                <entry>60.00</entry>

                <entry>0</entry>

                <entry>-</entry>

                <entry>-</entry>
              </row>

              <row>
                <entry>register_local_2_0</entry>

                <entry>local</entry>

                <entry>0.50</entry>

                <entry>5.00</entry>

                <entry>6.00</entry>

                <entry>0.00</entry>

                <entry>0.00</entry>

                <entry>0.05</entry>

                <entry>0</entry>

                <entry>-</entry>

                <entry>-</entry>
              </row>

              <row>
                <entry>stage_in_local_local_0</entry>

                <entry>local</entry>

                <entry>0.08</entry>

                <entry>6.00</entry>

                <entry>6.00</entry>

                <entry>0.00</entry>

                <entry>0.00</entry>

                <entry>0.04</entry>

                <entry>0</entry>

                <entry>-</entry>

                <entry>-</entry>
              </row>

              <row>
                <entry>stage_out_local_local_2_0</entry>

                <entry>local</entry>

                <entry>0.08</entry>

                <entry>5.00</entry>

                <entry>6.00</entry>

                <entry>0.00</entry>

                <entry>0.00</entry>

                <entry>0.03</entry>

                <entry>0</entry>

                <entry>-</entry>

                <entry>-</entry>
              </row>
            </tbody>
          </tgroup>
        </table>

        <para><emphasis role="bold">Logical transformation statistics
        table</emphasis></para>

        <para>Logical transformation statistics table contains information
        about each type of transformation in the workflow.</para>

        <table>
          <title>Table: Logical Transformation Statistics</title>

          <tgroup align="center" cols="7">
            <thead>
              <row>
                <entry align="center">Transformation</entry>

                <entry align="center">Count</entry>

                <entry align="center">Mean</entry>

                <entry align="center">Variance</entry>

                <entry align="center">Min</entry>

                <entry align="center">Max</entry>

                <entry align="center">Total</entry>
              </row>
            </thead>

            <tbody>
              <row>
                <entry>diamond::analyze:2.0</entry>

                <entry>1</entry>

                <entry>60.16</entry>

                <entry>0.00</entry>

                <entry>60.16</entry>

                <entry>60.16</entry>

                <entry>60.16</entry>
              </row>

              <row>
                <entry>diamond::findrange:2.0</entry>

                <entry>2</entry>

                <entry>60.31</entry>

                <entry>0.01</entry>

                <entry>60.25</entry>

                <entry>60.37</entry>

                <entry>120.62</entry>
              </row>

              <row>
                <entry>diamond::preprocess:2.0</entry>

                <entry>1</entry>

                <entry>60.48</entry>

                <entry>0.00</entry>

                <entry>60.48</entry>

                <entry>60.48</entry>

                <entry>60.48</entry>
              </row>
            </tbody>
          </tgroup>
        </table>
      </section>

      <section>
        <title>Generating plots using pegasus-plots</title>

        <para>pegasus-plots generates graphs and charts to visualize workflow
        execution. To generate graphs and charts run the command as shown
        below.</para>

        <programlisting><emphasis role="bold">$ cd $HOME/pegasus-wms 
$ pegasus-plots dags/pegasus/pegasus/blackdiamond/run0001/</emphasis>

******  show-job *****  
Please wait, this may take a few minutes ...
****** Finished executing show-job  ***** 
******  show-run *****  
Please wait, this may take a few minutes ...
****** Finished executing show-run  ***** 
******  dag2dot  ***** 
Please wait, this may take a few minutes ...
****** Finished executing dag2dot ***** 
******  dot  ***** 
****** Finished executing dot2png ***** 
******  dax2dot  ***** 
Please wait, this may take a few minutes ...
****** Finished executing dax2dot ***** 
******  dot  ***** 
****** Finished executing dot2png ***** 


******************************************** SUMMARY ********************************************
The workflow execution Gantt chart is created at -
png format :- /home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0001/graph/blackdiamond-2.png 
eps format :- /home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0001/graph/blackdiamond-2.eps 

The host over time chart is created at -
png format :-/home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0001/graph/blackdiamond-host.png 
eps format :-/home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0001/graph/blackdiamond-host.eps

PNG file corresponding to the dag is created at: 
/home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0001/graph/blackdiamond-dag.png 

PNG file corresponding to the dax is created at: 
/home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0001/graph/diamond-dax.png 
**************************************************************************************************
[pegasus@pegasus pegasus-wms]$ 
</programlisting>

        <para></para>

        <section>
          <title>Abstract Worfklow / DAX Image</title>

          <para>/home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0001/graph/diamond-dax.png</para>

          <figure>
            <title>Figure: Black Diamond DAX Image</title>

            <mediaobject>
              <imageobject>
                <imagedata fileref="images/tutorial-blackdiamond-dax.png" />
              </imageobject>
            </mediaobject>
          </figure>
        </section>

        <section>
          <title>Executable Workflow / DAG Image</title>

          <para>/home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0001/graph/blackdiamond-dag.png</para>

          <figure>
            <title>Figure: Black Diamond DAG Image</title>

            <mediaobject>
              <imageobject>
                <imagedata fileref="images/tutorial-blackdiamond-dag.png" />
              </imageobject>
            </mediaobject>
          </figure>
        </section>

        <section>
          <title>Gantt Chart of Workflow Execution</title>

          <para>/home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0001/graph/blackdiamond-2.png</para>

          <para><emphasis role="bold">X axis </emphasis>- time in seconds .
          Each tic is 60 seconds</para>

          <para><emphasis role="bold">Y axis</emphasis> - Job Number .</para>

          <figure>
            <title>Figure: Gantt Chart of Workflow Execution</title>

            <mediaobject>
              <imageobject>
                <imagedata fileref="images/tutorial-blackdiamond-gantt.png" />
              </imageobject>
            </mediaobject>
          </figure>
        </section>

        <section>
          <title>Host over time chart</title>

          <para>/home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0001/graph/blackdiamond-host.png</para>

          <para><emphasis role="bold">X axis </emphasis>- time in seconds .
          Each tic is 60 seconds</para>

          <para><emphasis role="bold">Y axis</emphasis> - Job Number .</para>

          <figure>
            <title>Figure: Gantt Chart of Workflow Execution</title>

            <mediaobject>
              <imageobject>
                <imagedata fileref="images/tutorial-blackdiamond-host.png" />
              </imageobject>
            </mediaobject>
          </figure>
        </section>
      </section>
    </section>
  </section>

  <section>
    <title>Planning and Executing Workflow against a Remote Resource</title>

    <para>In this exercise we are going to run pegasus-plan to generate a
    executable workflow from the abstract workflow (montage.dax). The
    Executable workflow generated, are condor submit files that are submitted
    to remote grid resources using pegasus-run</para>

    <para>The instructors have provided:</para>

    <itemizedlist>
      <listitem>
        <para>A dax (montage.dax) in the $HOME/pegasus-wms/dax/
        directory.</para>
      </listitem>
    </itemizedlist>

    <para>You will need to write some things yourself, by following the
    instructions below: <itemizedlist>
        <listitem>
          <para>Run pegasus-plan to generate the condor submit files out of
          the dax.</para>
        </listitem>
      </itemizedlist></para>

    <para>Instructions:</para>

    <itemizedlist>
      <listitem>
        <para>Let us run pegasus-plan on the montage dax on the tg_ncsa
        cluster. If multiple sites are available you could provide the sites
        using a comma "," separated list like tg_ncsa,viz etc.<programlisting><command>$ cd $HOME/pegasus-wms
$ pegasus-plan -Dpegasus.schema.dax=/opt/pegasus/default/etc/dax-2.1.xsd \
               --dir dags --sites cluster --output local --force \
               --nocleanup --dax `pwd`/dax/montage.dax --submit -v</command></programlisting>
        The above command says that we need to plan the montage dax on the
        <emphasis role="bold">cluster</emphasis> site. The cluster site in the
        VM is managed by SGE that is running in the VM. The jobs for this
        workflow will be submitted to <emphasis
        role="bold">jobmanager-sge</emphasis> in the VM. The output data needs
        to be transferred back to the local host. The condor submit files are
        to be generated in a directory structure whose base is dags. We also
        are requesting that no cleanup jobs be added as we require the
        intermediate data on the remote host. Here is the output of
        pegasus-plan. <programlisting><computeroutput>
2010.11.24 18:20:10.948 PST: [INFO] event.pegasus.parse.dax dax.id /home/pegasus/pegasus-wms/dax/montage.dax   
2010.11.24 18:20:11.309 PST: [INFO] event.pegasus.parse.dax dax.id /home/pegasus/pegasus-wms/dax/montage.dax 
2010.11.24 18:20:11.350 PST: [INFO] event.pegasus.refinement dax.id montage_0  - STARTED 
2010.11.24 18:20:11.360 PST: [INFO] event.pegasus.siteselection dax.id montage_0  - STARTED 
2010.11.24 18:20:11.416 PST: [INFO] event.pegasus.siteselection dax.id montage_0  - FINISHED 
2010.11.24 18:20:11.504 PST: [INFO]  Grafting transfer nodes in the workflow 
2010.11.24 18:20:11.505 PST: [INFO] event.pegasus.generate.transfer-nodes dax.id montage_0  - STARTED 
2010.11.24 18:20:11.655 PST: [INFO] event.pegasus.generate.transfer-nodes dax.id montage_0  - FINISHED 
2010.11.24 18:20:11.657 PST: [INFO] event.pegasus.generate.workdir-nodes dax.id montage_0  - STARTED 
2010.11.24 18:20:11.660 PST: [INFO] event.pegasus.generate.workdir-nodes dax.id montage_0  - FINISHED 
2010.11.24 18:20:11.660 PST: [INFO] event.pegasus.generate.cleanup-wf dax.id montage_0  - STARTED 
2010.11.24 18:20:11.661 PST: [INFO] event.pegasus.generate.cleanup-wf dax.id montage_0  - FINISHED 
2010.11.24 18:20:11.661 PST: [INFO] event.pegasus.refinement dax.id montage_0  - FINISHED 
2010.11.24 18:20:11.715 PST: [INFO]  Generating codes for the concrete workflow 
2010.11.24 18:20:12.406 PST: [INFO]  Generating codes for the concrete workflow -DONE 
2010.11.24 18:20:12.406 PST: [INFO]  Generating code for the cleanup workflow 
2010.11.24 18:20:12.528 PST: [INFO]  Generating code for the cleanup workflow -DONE 
2010.11.24 18:20:12.672 PST:    
2010.11.24 18:20:12.679 PST:   ----------------------------------------------------------------------- 
2010.11.24 18:20:12.685 PST:   File for submitting this DAG to Condor           : montage-0.dag.condor.sub 
2010.11.24 18:20:12.691 PST:   Log of DAGMan debugging messages                 : montage-0.dag.dagman.out 
2010.11.24 18:20:12.704 PST:   Log of Condor library output                     : montage-0.dag.lib.out 
2010.11.24 18:20:12.711 PST:   Log of Condor library error messages             : montage-0.dag.lib.err 
2010.11.24 18:20:12.726 PST:   Log of the life of condor_dagman itself          : montage-0.dag.dagman.log 
2010.11.24 18:20:12.731 PST:    
2010.11.24 18:20:12.762 PST:   -no_submit given, not submitting DAG to Condor.  You can do this with: 
2010.11.24 18:20:12.792 PST:   "condor_submit montage-0.dag.condor.sub" 
2010.11.24 18:20:12.798 PST:   ----------------------------------------------------------------------- 
2010.11.24 18:20:12.804 PST:   Submitting job(s). 
2010.11.24 18:20:12.815 PST:   Logging submit event(s). 
2010.11.24 18:20:12.821 PST:   1 job(s) submitted to cluster 275. 
2010.11.24 18:20:13.504 PST:    
2010.11.24 18:20:13.510 PST:   Your Workflow has been started and runs in base directory given below 
2010.11.24 18:20:13.519 PST:    
2010.11.24 18:20:13.530 PST:   cd /home/pegasus/pegasus-wms/dags/pegasus/pegasus/montage/run0001 
2010.11.24 18:20:13.535 PST:    
2010.11.24 18:20:13.542 PST:   *** To monitor the workflow you can run *** 
2010.11.24 18:20:13.555 PST:    
2010.11.24 18:20:13.562 PST:   pegasus-status -l /home/pegasus/pegasus-wms/dags/pegasus/pegasus/montage/run0001 
2010.11.24 18:20:13.570 PST:    
2010.11.24 18:20:13.578 PST:   *** To remove your workflow run *** 
2010.11.24 18:20:13.585 PST:   pegasus-remove -d 275.0 
2010.11.24 18:20:13.592 PST:   or 
2010.11.24 18:20:13.604 PST:   pegasus-remove /home/pegasus/pegasus-wms/dags/pegasus/pegasus/montage/run0001 
2010.11.24 18:20:13.610 PST:    
2010.11.24 18:20:13.617 PST:   Time taken to execute is 3.76 seconds 
2010.11.24 18:20:13.617 PST: [INFO] event.pegasus.planner planner.version 3.0.0  - FINISHED 
</computeroutput></programlisting></para>
      </listitem>

      <listitem>
        <para>If you get any errors above while running pegasus-plan you can
        add -vvvvv to enable maximum verbosity on pegasus-run.</para>
      </listitem>
    </itemizedlist>

    <para>The above command submits the workflow to Condor DAGMan/CondorG.
    After submitting it starts a monitoring daemon pegasus-monitord that
    parses the condor log files to update the status of the jobs and push it
    in a work database.</para>

    <para>Monitor the workflow using the commands provided in the output of
    the pegasus-run command and other commands explained earlier.</para>

    <para>The workflow generates a single output file montage.jpg that resides
    in the directory <emphasis
    role="bold">/home/pegasus/local-storage/storage/montage.jpg</emphasis> if
    it runs successfully</para>

    <para>The grid workflow will take time to execute on the VM. On the
    instructor's MAC Pro Desktop it took about<emphasis role="bold"> 30
    minutes </emphasis>to run.</para>
  </section>

  <section>
    <title>Advanced Exercises</title>

    <section>
      <title>Optimizing a workflow by clustering small jobs (To Be Done
      offline)</title>

      <para>Sometimes a workflow may have too many jobs whose execution time
      is a few seconds long. In such instances the overhead of scheduling each
      job on a grid is too large and the runtime of the entire workflow can be
      optimized by using Pegasus clustering techniques. One such technique is
      to cluster jobs horizontally on the same level into one or more
      sequential jobs.</para>

      <programlisting><command>$ cd $HOME/pegasus-wms
$ pegasus-plan -Dpegasus.schema.dax=/opt/pegasus/default/etc/dax-2.1.xsd \
            --dir `pwd`/dags --sites cluster --output local --nocleanup --force\
            --cluster horizontal --dax `pwd`/dax/montage.dax -v</command></programlisting>

      <para>After clustering the executable workflow will contain 26 jobs
      compared to 44 in the non clustered mode.</para>
    </section>

    <section>
      <title>Data Reuse</title>

      <para>In the DAX you can specify what output data products you want to
      track in the replica catalog. This is done by setting the register flags
      with the output files for a job. For our tutorial, we only register the
      final output data products. So if you were able to execute the diamond
      or the montage workflow successfully, we can do data reuse. Let us run
      <emphasis role="bold">pegasus-plan </emphasis>on the diamond workflow
      again. However, this time we will remove the <emphasis
      role="bold">--force</emphasis> option.</para>

      <programlisting><emphasis role="bold">$ cd $HOME/pegasus-wms
$ pegasus-plan --dax `pwd`/dax/diamond.dax --dir `pwd`/dags -s local -o local --nocleanup -v
</emphasis>
2010.11.25 01:35:11.186 PST: [INFO] event.pegasus.refinement dax.id blackdiamond_0  - STARTED 
2010.11.25 01:35:11.210 PST: [INFO] event.pegasus.reduce dax.id blackdiamond_0  - STARTED 
2010.11.25 01:35:11.211 PST: [INFO]  Nodes/Jobs Deleted from the Workflow during reduction  
2010.11.25 01:35:11.211 PST: [INFO]     analyze_j4 
2010.11.25 01:35:11.211 PST: [INFO]     findrange_j2 
2010.11.25 01:35:11.211 PST: [INFO]     findrange_j3 
2010.11.25 01:35:11.211 PST: [INFO]     preprocess_j1 
2010.11.25 01:35:11.211 PST: [INFO]  Nodes/Jobs Deleted from the Workflow during reduction  - DONE 
2010.11.25 01:35:11.212 PST: [INFO] event.pegasus.reduce dax.id blackdiamond_0  - FINISHED 
2010.11.25 01:35:11.212 PST: [INFO] event.pegasus.siteselection dax.id blackdiamond_0  - STARTED 
2010.11.25 01:35:11.219 PST: [INFO] event.pegasus.siteselection dax.id blackdiamond_0  - FINISHED 
2010.11.25 01:35:11.289 PST: [INFO]  Grafting transfer nodes in the workflow 
2010.11.25 01:35:11.290 PST: [INFO] event.pegasus.generate.transfer-nodes dax.id blackdiamond_0  - STARTED 
2010.11.25 01:35:11.370 PST: [INFO]  Adding stage out jobs for jobs deleted from the workflow 
2010.11.25 01:35:11.370 PST: [INFO]  The leaf file f.d is already at the output pool local 
2010.11.25 01:35:11.371 PST: [INFO] event.pegasus.generate.transfer-nodes dax.id blackdiamond_0  - FINISHED 
2010.11.25 01:35:11.372 PST: [INFO] event.pegasus.generate.workdir-nodes dax.id blackdiamond_0  - STARTED 
2010.11.25 01:35:11.374 PST: [INFO] event.pegasus.generate.workdir-nodes dax.id blackdiamond_0  - FINISHED 
2010.11.25 01:35:11.374 PST: [INFO] event.pegasus.generate.cleanup-wf dax.id blackdiamond_0  - STARTED 
2010.11.25 01:35:11.375 PST: [INFO] event.pegasus.generate.cleanup-wf dax.id blackdiamond_0  - FINISHED 
2010.11.25 01:35:11.375 PST: [INFO] event.pegasus.refinement dax.id blackdiamond_0  - FINISHED 
2010.11.25 01:35:11.426 PST: [INFO]  Generating codes for the concrete workflow 
2010.11.25 01:35:12.078 PST: [INFO]  Generating codes for the concrete workflow -DONE 
2010.11.25 01:35:12.083 PST:   


The executable workflow generated contains only a single NOOP job.
It seems that the output files are already at the output site. 
To regenerate the output data from scratch specify --force option.



pegasus-run -Dpegasus.user.properties=$HOME/.../blackdiamond/run0003/pegasus.4078026914028890643.properties\
 /home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0003

 
2010.11.25 01:35:12.083 PST:   Time taken to execute is 1.508 seconds 
2010.11.25 01:35:12.083 PST: [INFO] event.pegasus.planner planner.version 3.0.0  - FINISHED 
</programlisting>

      <para>You can increase the debug level to see how pegasus deletes the
      jobs bottom up of the workflow. Pass -vvvv to pegasus-plan
      command.</para>
    </section>

    <section>
      <title>Hierarchal Workflows</title>

      <para>Pegasus 3.0 allows you to create workflows of workflows i.e your
      workflow can contain dax jobs that refer to the sub-workflows. In this
      exercise, we will execute a workflow super-diamond that will execute two
      diamond workflows.</para>

      <para>Let us look at superdiamond.dax in the dax directory</para>

      <programlisting><emphasis role="bold">$ cat $HOME/pegasus-wms/dax/superdiamond.dax</emphasis>

&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;!-- generated on: 2010-11-25T08:42:30-08:00 --&gt;
&lt;!-- generated by: pegasus [ ?? ] --&gt;
&lt;adag xmlns="http://pegasus.isi.edu/schema/DAX" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://pegasus.isi.edu/schema/DAX http://pegasus.isi.edu/schema/dax-3.2.xsd" versi
on="3.2" name="superdiamond" index="0" count="1"&gt;

&lt;!-- Section 1: Files - Acts as a Replica Catalog (can be empty) --&gt;

   &lt;file name="f.a"&gt;
      &lt;pfn url="file:///scratch/tutorial/inputdata/diamond/f.a" site="local"/&gt;
   &lt;/file&gt;
   
   &lt;file name="black-1.dax"&gt;
      &lt;pfn url="/home/pegasus/pegasus-wms/dax/black-1.dax" site="local"/&gt;
   &lt;/file&gt;

   &lt;file name="black-2.dax"&gt;
      &lt;pfn url="/home/pegasus/pegasus-wms/dax/black-2.dax" site="local"/&gt;
   &lt;/file&gt;


&lt;!-- Section 2: Executables - Acts as a Transformaton Catalog (can be empty) --&gt;


&lt;!-- Section 3: Transformations - Aggregates executables and Files (can be empty) --&gt;


&lt;!-- Section 4: Job's, DAX's or Dag's - Defines a JOB or DAX or DAG (Atleast 1 required) --&gt;

   <emphasis role="bold">&lt;dax id="d1" file="black-1.dax"  &gt;
    &lt;argument&gt;-s local --force -o local&lt;/argument&gt;
   &lt;/dax&gt;</emphasis>

   &lt;dax id="d2" file="black-2.dax"  &gt;
    &lt;argument&gt;-s local --force -o local&lt;/argument&gt;
   &lt;/dax&gt;



&lt;!-- Section 5: Dependencies - Parent Child relationships (can be empty) --&gt;

   &lt;child ref="d2"&gt;
      &lt;parent ref="d1"/&gt;
   &lt;/child&gt;

&lt;/adag&gt;
</programlisting>

      <para>Now let us submit this super diamond workflow</para>

      <programlisting><emphasis role="bold">$ pegasus-plan --dax `pwd`/dax/superdiamond.dax --force --submit\
               --dir dags -s local -o local --nocleanup -v</emphasis>

2010.11.29 21:15:49.110 PST: [INFO] event.pegasus.refinement dax.id superdiamond_0  - STARTED 
2010.11.29 21:15:49.123 PST: [INFO] event.pegasus.siteselection dax.id superdiamond_0  - STARTED 
2010.11.29 21:15:49.142 PST: [INFO] event.pegasus.siteselection dax.id superdiamond_0  - FINISHED 
2010.11.29 21:15:49.220 PST: [INFO]  Grafting transfer nodes in the workflow 
2010.11.29 21:15:49.221 PST: [INFO] event.pegasus.generate.transfer-nodes dax.id superdiamond_0  - STARTED 
2010.11.29 21:15:49.305 PST: [INFO] event.pegasus.generate.transfer-nodes dax.id superdiamond_0  - FINISHED 
2010.11.29 21:15:49.307 PST: [INFO] event.pegasus.generate.workdir-nodes dax.id superdiamond_0  - STARTED 
2010.11.29 21:15:49.312 PST: [INFO] event.pegasus.generate.workdir-nodes dax.id superdiamond_0  - FINISHED 
2010.11.29 21:15:49.312 PST: [INFO] event.pegasus.generate.cleanup-wf dax.id superdiamond_0  - STARTED 
2010.11.29 21:15:49.314 PST: [INFO] event.pegasus.generate.cleanup-wf dax.id superdiamond_0  - FINISHED 
2010.11.29 21:15:49.314 PST: [INFO] event.pegasus.refinement dax.id superdiamond_0  - FINISHED 
2010.11.29 21:15:49.371 PST: [INFO]  Generating codes for the concrete workflow 
2010.11.29 21:15:50.200 PST: [INFO]  Generating codes for the concrete workflow -DONE 
2010.11.29 21:15:50.200 PST: [INFO]  Generating code for the cleanup workflow 
2010.11.29 21:15:50.323 PST: [INFO]  Generating code for the cleanup workflow -DONE 
2010.11.29 21:15:50.496 PST:    
2010.11.29 21:15:50.502 PST:   ----------------------------------------------------------------------- 
2010.11.29 21:15:50.508 PST:   File for submitting this DAG to Condor           : superdiamond-0.dag.condor.sub 
2010.11.29 21:15:50.514 PST:   Log of DAGMan debugging messages                 : superdiamond-0.dag.dagman.out 
2010.11.29 21:15:50.521 PST:   Log of Condor library output                     : superdiamond-0.dag.lib.out 
2010.11.29 21:15:50.528 PST:   Log of Condor library error messages             : superdiamond-0.dag.lib.err 
2010.11.29 21:15:50.559 PST:   Log of the life of condor_dagman itself          : superdiamond-0.dag.dagman.log 
2010.11.29 21:15:50.578 PST:    
2010.11.29 21:15:50.588 PST:   -no_submit given, not submitting DAG to Condor.  You can do this with: 
2010.11.29 21:15:50.601 PST:   "condor_submit superdiamond-0.dag.condor.sub" 
2010.11.29 21:15:50.618 PST:   ----------------------------------------------------------------------- 
2010.11.29 21:15:50.625 PST:   Submitting job(s). 
2010.11.29 21:15:50.637 PST:   Logging submit event(s). 
2010.11.29 21:15:50.642 PST:   1 job(s) submitted to cluster 1. 
2010.11.29 21:15:51.179 PST:    
2010.11.29 21:15:51.185 PST:   Your Workflow has been started and runs in base directory given below 
2010.11.29 21:15:51.191 PST:    
2010.11.29 21:15:51.197 PST:   cd /home/pegasus/pegasus-wms/dags/pegasus/pegasus/superdiamond/run0001 
2010.11.29 21:15:51.208 PST:    
2010.11.29 21:15:51.214 PST:   *** To monitor the workflow you can run *** 
2010.11.29 21:15:51.220 PST:    
2010.11.29 21:15:51.227 PST:   pegasus-status -l /home/pegasus/pegasus-wms/dags/pegasus/pegasus/superdiamond/run0001 
2010.11.29 21:15:51.234 PST:    
2010.11.29 21:15:51.240 PST:   *** To remove your workflow run *** 
2010.11.29 21:15:51.245 PST:   pegasus-remove -d 1.0 
2010.11.29 21:15:51.253 PST:   or 
2010.11.29 21:15:51.261 PST:   pegasus-remove /home/pegasus/pegasus-wms/dags/pegasus/pegasus/superdiamond/run0001 
2010.11.29 21:15:51.268 PST:    
2010.11.29 21:15:51.277 PST:   Time taken to execute is 2.745 seconds 
2010.11.29 21:15:51.277 PST: [INFO] event.pegasus.planner planner.version 3.0.0  - FINISHED 

</programlisting>

      <para>You can track the workflow using the pegasus-status command</para>

      <programlisting><emphasis role="bold">$ watch  pegasus-status -l /home/pegasus/pegasus-wms/dags/pegasus/pegasus/superdiamond/run0001</emphasis> 


</programlisting>

      <para>After the workflow has completed you will see the black-1-f.d and
      black-2-f.d in the storage directory</para>

      <programlisting><emphasis role="bold">$ ls -lh /home/pegasus/local-storage/storage/black-*
</emphasis>

-rw-r--r-- 1 pegasus pegasus 3.6K Nov 29 21:36 /home/pegasus/local-storage/storage/black-1-f.d
-rw-r--r-- 1 pegasus pegasus 3.6K Nov 29 21:41 /home/pegasus/local-storage/storage/black-2-f.d

</programlisting>

      <section>
        <title>Directory Structure For the Hierarchal Workflows</title>

        <para>Pegasus ensures that each of the workflows have their own submit
        directory and execution directories.</para>

        <para>The table below lists the submit directories for all the
        workflows in this exercise</para>

        <table>
          <title>Table: Submit Directory Structure for Hierarchal
          Workflows</title>

          <tgroup cols="2">
            <tbody>
              <row>
                <entry>superdiamond ( the outer level workflow )</entry>

                <entry>/home/pegasus/pegasus-wms/dags/pegasus/pegasus/superdiamond/run0001</entry>
              </row>

              <row>
                <entry>black-1 ( the first sub workflow )</entry>

                <entry>/home/pegasus/pegasus-wms/dags/pegasus/pegasus/superdiamond/run0001/black-1_d1</entry>
              </row>

              <row>
                <entry>black-2 ( the second sub workflow )</entry>

                <entry>/home/pegasus/pegasus-wms/dags/pegasus/pegasus/superdiamond/run0001/black-2_d2</entry>
              </row>
            </tbody>
          </tgroup>
        </table>

        <para>The table below lists the execution directories ( one per
        workflow ) in this exercise</para>

        <table>
          <title>Table: Execution Directory Structure for Hierarchal
          Workflows</title>

          <tgroup cols="2">
            <tbody>
              <row>
                <entry>superdiamond ( the outer level workflow )</entry>

                <entry>/home/pegasus/local-scratch
                /exec/pegasus/pegasus/superdiamond/run0001</entry>
              </row>

              <row>
                <entry>black-1 ( the first sub workflow )</entry>

                <entry>/home/pegasus/local-scratch/exec
                /pegasus/pegasus/superdiamond/run0001/black-1_d1</entry>
              </row>

              <row>
                <entry>black-2 ( the second sub workflow )</entry>

                <entry>/home/pegasus/local-scratch/exec
                /pegasus/pegasus/superdiamond/run0001/black-2_d2</entry>
              </row>
            </tbody>
          </tgroup>
        </table>
      </section>
    </section>
  </section>
</chapter>