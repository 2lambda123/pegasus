<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN"
"http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<chapter id="tutorial_vm">
  <title>Pegasus Tutorial Using Self-contained Virtual Machine</title>

  <note>
    <para>These tutorial notes refer to Pegasus 2.4. The VM Tutorial for
    Pegasus 3.0 will be available in the first week of December 2010.</para>
  </note>

  <para>These are the student notes for the Pegasus WMS tutorial on the
  Virtual Machine that can be downloaded from the Pegasus Website. They are
  designed to be used in conjunction with instructor presentation and
  support.</para>

  <para>You will see two styles of machine text here:</para>

  <programlisting><command>Text like this is input that you should type.</command><computeroutput>

Text like this is the output you should get.</computeroutput></programlisting>

  <para>For example:</para>

  <programlisting><command>$ date</command><computeroutput>
Wed Nov 24 14:47:59 PST 2010
</computeroutput></programlisting>

  <section>
    <title>Downloading and Running the VM</title>

    <para>You will need to install VMPlayer or Virtual Box to run the virtual
    machine on your computer. If you already have one of the tools installed,
    use that. Otherwise download and install one from:</para>

    <itemizedlist>
      <listitem>
        <para><ulink
        url="http://www.virtualbox.org/">http://www.virtualbox.org/</ulink></para>
      </listitem>

      <listitem>
        <para><ulink
        url="http://www.vmware.com/products/player/">http://www.vmware.com/products/player/</ulink></para>
      </listitem>
    </itemizedlist>

    <para>Download the corresponding disk image:</para>

    <itemizedlist>
      <listitem>
        <para><ulink
        url="http://pegasus.isi.edu/tutorial/virtual/PegasusVM.tar.bz2">VMWare
        Pegasus Image</ulink></para>
      </listitem>
    </itemizedlist>

    <para>When you start the virtual machine , it will ask for a password for
    a user called <emphasis role="bold">pegasus</emphasis>. The password is
    same as the username i.e. pegasus.</para>

    <para>After logging on, start a terminal</para>

    <programlisting><command>$ </command><emphasis role="bold">[pegasus@pegasus ~]$ pwd</emphasis>

/home/pegasus
</programlisting>

    <para>In general, to run workflows on the Grid you will need to obtain
    Grid Credentials. The VM already has a user certificate installed for the
    pegasus user. To generate the proxy ( grid credentials) run the <emphasis
    role="bold">grid-proxy-init</emphasis> command.</para>

    <programlisting><command>$ [pegasus@pegasus ~]$ grid-proxy-init 

</command><computeroutput>Your identity: /O=Grid/OU=GlobusTest/OU=simpleCA-seqware/CN=Pegasus User
Creating proxy ................................... Done
Your proxy is valid until: Thu Nov 25 02:48:18 2010

</computeroutput></programlisting>

    <para>Check your proxy using grid-proxy-info.</para>

    <programlisting><command>$ </command><emphasis role="bold">[pegasus@pegasus ~]$ grid-proxy-info</emphasis><computeroutput><computeroutput>

subject  : /O=Grid/OU=GlobusTest/OU=simpleCA-seqware/CN=Pegasus User/CN=1814822797
issuer   : /O=Grid/OU=GlobusTest/OU=simpleCA-seqware/CN=Pegasus User
identity : /O=Grid/OU=GlobusTest/OU=simpleCA-seqware/CN=Pegasus User
type     : RFC 3820 compliant impersonation proxy
strength : 512 bits
path     : /tmp/x509up_u501
timeleft : 11:59:29

</computeroutput></computeroutput></programlisting>
  </section>

  <section>
    <title>Mapping and Executing Workflows using Pegasus</title>

    <para>In this chapter you will be introduced to planning and executing a
    workflow through Pegasus WMS locally. You will then plan and execute a
    larger Montage workflow on the GRID.</para>

    <para>All the exercises in this Chapter will be run from the
    $HOME/pegasus-wms/ directory. All the files that are required reside in
    this directory</para>

    <programlisting><command>$ cd $HOME/pegasus-wms</command></programlisting>

    <para>Files for the exercise are stored in subdirectories:</para>

    <programlisting><command>$ ls</command><computeroutput><computeroutput>

config  dax</computeroutput></computeroutput></programlisting>

    <para>You may also see some other files here.</para>

    <section>
      <title>Creating a DIAMOND DAX</title>

      <para>We generate a 4 node diamond dax. There is a small piece of java
      code that uses the DAX API to generate the DAX. Open the file
      $HOME/pegasus-wms/dax/CreateDAX.java in a file editor:</para>

      <programlisting><command>$ emacs -nw dax/CreateDAX.java</command></programlisting>

      <para>There is a function constructDAX( String daxFile ) that constructs
      the DAX. Towards the end of the function there is some commented out
      code.</para>

      <programlisting> // Add analyze job
//To be uncommented for exercise 2.1
    
        Job j4 = new Job("j4", "pegasus", "analyze", "4.0");
        j4.addArgument("-a analyze -T 60 -i ").addArgument(fc1);
        j4.addArgument(" ").addArgument(fc2);
        j4.addArgument("-o ").addArgument(fd);
        j4.uses(fc1, File.LINK.INPUT);
        j4.uses(fc2, File.LINK.INPUT);
        j4.uses(fd, File.LINK.OUTPUT);
        
        //add job to the DAX
        dax.addJob(j4);

        //analyze job is a child to the findrange jobs
        dax.addDependency("j2", "j4");
        dax.addDependency("j3", "j4");
    
//End of commented out code for Exercise 2.1
</programlisting>

      <para>The above snippet of code, adds a job with the ID0000004 to the
      DAX. It illustrates how to specify</para>

      <orderedlist>
        <listitem>
          <para>the arguments for the job</para>
        </listitem>

        <listitem>
          <para>the logical files used by the job</para>
        </listitem>

        <listitem>
          <para>the dependencies to other jobs</para>
        </listitem>

        <listitem>
          <para>adding the job to the dax</para>
        </listitem>
      </orderedlist>

      <para>After uncommenting the code, compile and run the CreateDAX
      program.</para>

      <programlisting><emphasis role="bold">$ cd dax

$ javac -classpath .:/opt/pegasus/default/lib/pegasus.jar CreateDAX.java

$  java -classpath .:/opt/pegasus/default/lib/pegasus.jar CreateDAX local /opt/pegasus/default ./diamond.dax
</emphasis></programlisting>

      <para>Let us view the generated diamond.dax.</para>

      <programlisting><emphasis role="bold">$ cat diamond.dax</emphasis></programlisting>

      <para>Inside the DAX, you should see three sections.</para>

      <orderedlist>
        <listitem>
          <para>list of input file locations</para>
        </listitem>

        <listitem>
          <para>list of executable locations</para>
        </listitem>

        <listitem>
          <para>definition of all jobs - each job in the workflow. 4 jobs in
          total.</para>
        </listitem>

        <listitem>
          <para>list of control-flow dependencies - this section specifies a
          partial order in which jobs are to executed.</para>
        </listitem>
      </orderedlist>
    </section>

    <section>
      <title>Setting up the Replica Catalog</title>

      <para>First lets change to the tutorial base directory.<programlisting><command>$ cd $HOME/pegasus-wms</command></programlisting></para>

      <para>In this exercise you will insert entries into the Replica Catalog.
      The replica catalog that we will use today is a simple file based
      catalog. We also support and recommend GLOBUS RLS or a JDBC
      implementation for production runs.</para>

      <para>A Replica Catalog maintains the LFN to PFN mapping for the input
      files of your workflow. Pegasus queries it to determine the locations of
      the raw input data files required by the workflow. Additionally, all the
      materialized data is registered into Replica Catalog for data reuse
      later on.</para>

      <para>You can use the <emphasis role="bold">
      pegasus-rc-client</emphasis> command to insert , query and delete from
      the replica catalog.</para>

      <para>To execute the diamond dax created in <emphasis
      role="bold">exercise 2.1</emphasis>, we will need to register input file
      f.a in the replica catalog. The file f.a resides at
      /scratch/tutorial/inputdata/diamond/f.a . Let us insert a single entry
      into the replica catalog.</para>

      <programlisting><command>$  pegasus-rc-client -Dpegasus.user.properties=config/properties insert f.a \
          gsiftp://pegasus/scratch/tutorial/inputdata/diamond/f.a pool=local
</command></programlisting>

      <para>Let us know verify if f.a has been registered successfully by
      querying the replica catalog using rc-client</para>

      <programlisting><emphasis role="bold">$ pegasus-rc-client -Dpegasus.user.properties=config/properties lookup f.a</emphasis>

 f.a gsiftp://pegasus/scratch/tutorial/inputdata/diamond/f.a pool="local"
</programlisting>

      <para>The <emphasis role="bold">pegasus-rc-client</emphasis> also allows
      for bulk insertion of entries. We will be inserting the entries for
      montage workflow using the bulk mode. The input data to be used for the
      montage workflow resides in the /scratch/tutorial/inputdata/0.2degree
      directory. We are going to insert entries into the replica catalog that
      point to the files in this directory.</para>

      <para>The instructors have provided:</para>

      <itemizedlist>
        <listitem>
          <para>A file replicas.in, the input data file for the
          pegasus-rc-client that contains the mappings that need to be
          populated in the Replica Catalog. The file is inside the config
          directory</para>
        </listitem>
      </itemizedlist>

      <para>Instructions:</para>

      <itemizedlist>
        <listitem>
          <para>Let us see what the file looks like. <programlisting><command>$ cat config/rc.in</command><computeroutput>


statfile_20070529_153243_22618.tbl
     gsiftp://pegasus/scratch/tutorial/inputdata/0.2degree/statfile.tbl
          pool="local"
2mass-atlas-990502s-j1440198.fits
     gsiftp://pegasus/scratch/0.2degree/2mass-atlas-990502s-j1440198.fits
          pool="local"
2mass-atlas-990502s-j1440186.fits
     gsiftp://pegasus/scratch/0.2degree/2mass-atlas-990502s-j1440186.fits
          pool="local"
 2mass-atlas-990502s-j1430092.fits
     gsiftp://pegasus/scratch/0.2degree/2mass-atlas-990502s-j1430092.fits
          pool="local"
 2mass-atlas-990502s-j1420198.fits
     gsiftp://pegasus/scratch/0.2degree/2mass-atlas-990502s-j1420198.fits
          pool="local"
 2mass-atlas-990502s-j1420186.fits
     gsiftp://pegasus/scratch/0.2degree/2mass-atlas-990502s-j1420186.fits
          pool="local"
 cimages_20070529_153243_22618.tbl
     gsiftp://pegasus/scratch/0.2degree/cimages.tbl pool="local"
 pimages_20070529_153243_22618.tbl
     gsiftp://pegasus/scratch/0.2degree/pimages.tbl pool="local"
 region_20070529_153243_22618.hdr
     gsiftp://pegasus/scratch/0.2degree/region.hdr pool="local"
 2mass-atlas-990502s-j1430080.fits
     gsiftp://pegasus/scratch/0.2degree/2mass-atlas-990502s-j1430080.fits
          pool="local"</computeroutput></programlisting></para>
        </listitem>

        <listitem>
          <para>Now we are ready to run rc-client and populate the data. Since
          each of you have an individual file replica catalog, all the 10
          entries should be successfully registered. <programlisting><command>$ pegasus-rc-client -Dpegasus.user.properties=config/properties --insert config/rc.in</command><computeroutput><computeroutput>

#Successfully worked on : 12 lines</computeroutput>
#Worked on total number of : 12 lines.</computeroutput></programlisting></para>
        </listitem>

        <listitem>
          <para>Now the entries have been successfully inserted into the
          Replica Catalog. We should query the replica catalog for a
          particular lfn. <programlisting><command>$ pegasus-rc-client -Dpegasus.user.properties=config/properties \
                   lookup pimages_20080505_143233_14944.tbl</command><computeroutput><computeroutput>

pimages_20080505_143233_14944.tbl
         gsiftp://pegasus/scratch/tutorial/inputdata/0.2degree/pimages.tbl 
           pool="local"</computeroutput></computeroutput></programlisting></para>
        </listitem>
      </itemizedlist>

      <para>Congratulations!! You have the replica catalog setup correctly for
      use. This is the catalog which you will tinker with most, while running
      Pegasus.</para>
    </section>

    <section>
      <title>Setting up the Site Catalog and Transformation Catalog</title>

      <para>In this exercise you will setup your Site Catalog and the
      Transformation Catalog.</para>

      <para>The instructors have provided: <itemizedlist>
          <listitem>
            <para>A ready transformation catalog (tc.data.text) in the
            $HOME/pegasus-wms/config directory.</para>
          </listitem>

          <listitem>
            <para>A semi ready properties file in the $HOME/pegasus-wms/config
            directory.</para>
          </listitem>
        </itemizedlist></para>

      <itemizedlist>
        <listitem>
          <para>The site catalog contains information about the layout of your
          grid where you want to run your workflows. For each site information
          like workdirectory, jobmanagers to use, gridftp servers to use and
          other site wide information like environment variables to be set is
          maintained.</para>

          <para>Lets see the site catalog for the Pegasus VM. It refers to two
          sites <emphasis role="bold">local</emphasis> and <emphasis
          role="bold">cluster</emphasis> .</para>

          <para><programlisting><command>$ cat $HOME/pegasus-wms/config/sites.xml3</command><computeroutput>

&lt;sitecatalog xmlns="http://pegasus.isi.edu/schema/sitecatalog" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" \
  xsi:schemaLocation="http://pegasus.isi.edu/schema/sitecatalog http://pegasus.isi.edu/schema/sc-3.0.xsd" version="3.0"&gt;

&lt;site  handle="cluster" arch="x86" os="LINUX" osrelease="" osversion="" glibc=""&gt;
  &lt;grid  type="gt2" contact="pegasus/jobmanager-fork" scheduler="Fork" jobtype="auxillary"/&gt;
  &lt;grid  type="gt2" contact="pegasus/jobmanager-sge" scheduler="SGE" jobtype="compute"/&gt;
  &lt;head-fs&gt;
   &lt;scratch&gt;
    &lt;shared&gt;
     &lt;file-server protocol="gsiftp" url="gsiftp://pegasus" mount-point="/home/pegasus/cluster-scratch"/&gt;
     &lt;internal-mount-point mount-point="/home/pegasus/cluster-scratch"/&gt;
    &lt;/shared&gt;
   &lt;/scratch&gt;
   &lt;storage&gt;
    &lt;shared&gt;
     &lt;file-server protocol="gsiftp" url="gsiftp://pegasus" mount-point="/home/pegasus/cluster-storage"/&gt;
     &lt;internal-mount-point mount-point="/home/pegasus/cluster-storage"/&gt;
    &lt;/shared&gt;
   &lt;/storage&gt;
  &lt;/head-fs&gt;
  &lt;replica-catalog  type="LRC" url="rlsn://localhost"/&gt;
  &lt;profile namespace="env" key="GLOBUS_LOCATION" &gt;/usr/local/globus/gt-5.0.2&lt;/profile&gt;
  &lt;profile namespace="env" key="JAVA_HOME" &gt;/usr/java/default&lt;/profile&gt;
  &lt;profile namespace="env" key="LD_LIBRARY_PATH" &gt;/usr/local/globus/gt-5.0.2/lib&lt;/profile&gt;
  &lt;profile namespace="env" key="PEGASUS_HOME" &gt;/opt/pegasus/default&lt;/profile&gt;
  &lt;profile namespace="pegasus" key="bundle" &gt;1&lt;/profile&gt;
  &lt;profile namespace="pegasus" key="stagein.clusters" &gt;1&lt;/profile&gt;
 &lt;/site&gt;

 &lt;site  handle="local" arch="x86" os="LINUX" osrelease="" osversion="" glibc=""&gt;
  &lt;grid  type="gt2" contact="localhost/jobmanager-fork" scheduler="Fork" jobtype="auxillary"/&gt;
  &lt;grid  type="gt2" contact="localhost/jobmanager-fork" scheduler="Fork" jobtype="compute"/&gt;
  &lt;head-fs&gt;
   &lt;scratch&gt;
    &lt;shared&gt;
     &lt;file-server protocol="gsiftp" url="file://" mount-point="/home/pegasus/local-scratch"/&gt;
     &lt;internal-mount-point mount-point="/home/pegasus/local-scratch"/&gt;
    &lt;/shared&gt;
   &lt;/scratch&gt;
   &lt;storage&gt;
    &lt;shared&gt;
     &lt;file-server protocol="gsiftp" url="file://" mount-point="/home/pegasus/local-storage"/&gt;
     &lt;internal-mount-point mount-point="/home/pegasus/local-storage"/&gt;
    &lt;/shared&gt;
   &lt;/storage&gt;
  &lt;/head-fs&gt;
  &lt;replica-catalog  type="LRC" url="rlsn://localhost"/&gt;
  &lt;profile namespace="env" key="GLOBUS_LOCATION" &gt;/usr/local/globus/gt-5.0.2&lt;/profile&gt;
  &lt;profile namespace="env" key="JAVA_HOME" &gt;/usr/java/default&lt;/profile&gt;
  &lt;profile namespace="env" key="LD_LIBRARY_PATH" &gt;/usr/local/globus/gt-5.0.2/lib&lt;/profile&gt;
  &lt;profile namespace="env" key="PEGASUS_HOME" &gt;/opt/pegasus/default&lt;/profile&gt;
 &lt;/site&gt;

&lt;/sitecatalog&gt;
 
</computeroutput></programlisting></para>
        </listitem>
      </itemizedlist>

      <itemizedlist>
        <listitem>
          <para>The transformation catalog maintains information about where
          the application code resides on the grid. In our case, it contains
          the locations where the Diamond or Montage code is installed in the
          Pegasus VM. We will use the pegasus-<emphasis
          role="bold">tc-client</emphasis> to add the entry for the
          transformation analyze into the transformation catalog.</para>

          <programlisting><emphasis role="bold">$ pegasus-tc-client -Dpegasus.user.properties=config/properties -a -l diamond::analyze:2.0 \
      -p /opt/pegasus/default/bin/keg -r local -t INSTALLED -s INTEL32::LINUX 
<emphasis>
 2008.04.30 15:11:59.313 PDT: [INFO] Added tc entry sucessfully
</emphasis></emphasis></programlisting>

          <para>Let us try and query for the entry we inserted. </para>

          <programlisting><emphasis role="bold">$ pegasus-tc-client -Dpegasus.user.properties=config/properties -q -P -l diamond::analyze:2.0
</emphasis>
#RESID     LTX                     PFN                                           TYPE          SYSINFO

local    diamond::analyze:2.0    /cluster-software/pegasus/current/bin/keg    INSTALLED    INTEL32::LINUX

</programlisting>

          <para>Let us try and query the transformation catalog for all the
          entries in it. Let us see what our transformation catalog looks
          like</para>

          <programlisting><command>$ pegasus-tc-client -Dpegasus.user.properties=config/properties -q -B          </command><computeroutput>

local   mDiff       
             gsiftp://pegasus/scratch/tutorial/software/montage/3.0/x86/bin/mDiff       
                                STATIC_BINARY   INTEL32::LINUX  ENV::MONTAGE_HOME="."
local   mFitplane
             gsiftp://pegasus/scratch/tutorial/software/montage/3.0/x86/bin/mFitplane
                                STATIC_BINARY   INTEL32::LINUX  ENV::MONTAGE_HOME="."
local   mAdd:3.0  
             gsiftp://pegasus/scratch/tutorial/software/montage/3.0/x86/bin/mAdd
                                STATIC_BINARY   INTEL32::LINUX  ENV::MONTAGE_HOME="."
local   mBackground:3.0
             gsiftp://pegasus/scratch/tutorial/software/montage/3.0/x86/bin/mBackground
                                STATIC_BINARY   INTEL32::LINUX  ENV::MONTAGE_HOME="."
local   mBgModel:3.0
             gsiftp://pegasus/scratch/tutorial/software/montage/3.0/x86/bin/mBgModel
                                STATIC_BINARY   INTEL32::LINUX  ENV::MONTAGE_HOME="."
local   mConcatFit:3.0 
             gsiftp://pegasus/scratch/tutorial/software/montage/3.0/x86/bin/mConcatFit
                                STATIC_BINARY   INTEL32::LINUX  ENV::MONTAGE_HOME="."
local   mDiffFit:3.0
             gsiftp://pegasus/scratch/tutorial/software/montage/3.0/x86/bin/mDiffFit 
                                STATIC_BINARY   INTEL32::LINUX  ENV::MONTAGE_HOME="."
local   mImgtbl:3.0 
             gsiftp://pegasus/scratch/tutorial/software/montage/3.0/x86/bin/mImgtbl  
                                STATIC_BINARY   INTEL32::LINUX  ENV::MONTAGE_HOME="."
local   mJPEG:3.0       
             gsiftp://pegasus/scratch/tutorial/software/montage/3.0/x86/bin/mJPEG  
                                STATIC_BINARY   INTEL32::LINUX  ENV::MONTAGE_HOME="."
local   mProject:3.0 
             gsiftp://pegasus/scratch/tutorial/software/montage/3.0/x86/bin/mProjectPP 
                                STATIC_BINARY   INTEL32::LINUX  ENV::MONTAGE_HOME="."
local   mProjectPP:3.0
             gsiftp://pegasus/scratch/tutorial/software/montage/3.0/x86/bin/mProjectPP
                                STATIC_BINARY   INTEL32::LINUX  ENV::MONTAGE_HOME="."
local   mShrink:3.0
             gsiftp://pegasus/scratch/tutorial/software/montage/3.0/x86/bin/mShrink
                                STATIC_BINARY   INTEL32::LINUX  NULL</computeroutput></programlisting>
        </listitem>

        <listitem>
          <para>Open the properties file and check a few properties.</para>

          <programlisting><command>$ cat config/properties</command><computeroutput><computeroutput>
 
##########################
# PEGASUS USER PROPERTIES 
##########################

## SELECT THE REPLICAT CATALOG MODE AND URL
pegasus.catalog.replica = File
pegasus.catalog.replica.file = ${user.home}/pegasus-wms/config/rc.data


## SELECT THE SITE CATALOG MODE AND FILE
pegasus.catalog.site = XML3
pegasus.catalog.site.file = ${user.home}/pegasus-wms/config/sites.xml3


## SELECT THE TRANSFORMATION CATALOG MODE AND FILE
pegasus.catalog.transformation = Text
pegasus.catalog.transformation.file = ${user.home}/pegasus-wms/config/tc.data.text

## USE DAGMAN RETRY FEATURE FOR FAILURES
dagman.retry=2

## CHECK JOB EXIT CODES FOR FAILURE
dagman.post.scope=all

## STAGE ALL OUR EXECUTABLES OR USE INSTALLED ONES 
pegasus.catalog.transformation.mapper = All

## WORK AND STORAGE DIR  
pegasus.dir.storage = storage
pegasus.dir.exec = exec

#JOB CATEGORIES
dagman.projection.maxjobs 2
[pegasus@pegasus pegasus-wms</computeroutput></computeroutput></programlisting>
        </listitem>

        <listitem>
          <para>Also the client pegasus-sc-client can be used to generate a
          site catalog and transformation catalog for the Open Science
          Grid.</para>

          <programlisting><command>$ </command><emphasis role="bold"><computeroutput>[pegasus@pegasus pegasus-wms]$ pegasus-sc-client -Dpegasus.user.properties=`pwd`/config/properties --vo engage --sc ./engage-osg-sc.xml \
  --source OSGMM --grid OSG -vvvv 
</computeroutput></emphasis><computeroutput>
2010.11.24 18:00:46.410 PST: [INFO]  Skipping site CIT_CMS_T2 
2010.11.24 18:00:46.416 PST: [INFO]  Adding site RENCI-Engagement 
2010.11.24 18:00:46.475 PST: [INFO]  Adding site Nebraska 
2010.11.24 18:00:46.476 PST: [INFO]  Adding site Prairiefire 
2010.11.24 18:00:46.476 PST: [INFO]  Adding site BNL-ATLAS 
2010.11.24 18:00:46.477 PST: [INFO]  Adding site BNL-ATLAS__1 
2010.11.24 18:00:46.478 PST: [INFO]  Adding site UFlorida-PG 
2010.11.24 18:00:46.478 PST: [INFO]  Skipping site CIT_CMS_T2__1 
2010.11.24 18:00:46.478 PST: [INFO]  Adding site RENCI-Blueridge 
2010.11.24 18:00:46.479 PST: [INFO]  Adding site Nebraska__1 
2010.11.24 18:00:46.480 PST: [INFO]  Adding site UMissHEP 
2010.11.24 18:00:46.480 PST: [INFO]  Adding site UCR-HEP 
2010.11.24 18:00:46.481 PST: [INFO]  Adding site LIGO_UWM_NEMO 
2010.11.24 18:00:46.482 PST: [INFO]  Adding site FNAL_FERMIGRID 
2010.11.24 18:00:46.482 PST: [INFO]  Adding site USCMS-FNAL-WC1 
2010.11.24 18:00:46.483 PST: [INFO]  Adding site UConn-OSG 
2010.11.24 18:00:46.484 PST: [INFO]  Adding site UFlorida-HPC 
2010.11.24 18:00:46.484 PST: [INFO]  Adding site GridUNESP_CENTRAL 
2010.11.24 18:00:46.493 PST: [INFO]  Adding site NWICG_NotreDame 
2010.11.24 18:00:46.494 PST: [INFO]  Site LOCAL . Creating default entry 
2010.11.24 18:00:46.527 PST: [INFO]  Loaded 19 sites  
2010.11.24 18:00:46.527 PST:   Writing out site catalog to /home/pegasus/pegasus-wms/./engage-osg-sc.xml 
2010.11.24 18:00:46.959 PST:   Number of SRM Properties retrieved 14 
2010.11.24 18:00:46.970 PST:   Writing out properties to /home/pegasus/pegasus-wms/./pegasus.6475454308491531036.properties 
2010.11.24 18:00:46.972 PST: [INFO]  Time taken to execute is 1.101 seconds 
2010.11.24 18:00:46.972 PST: [INFO] event.pegasus.planner planner.version 3.0.0  - FINISHED 


</computeroutput></programlisting>
        </listitem>
      </itemizedlist>
    </section>

    <section>
      <title>Planning workflow using pegasus-plan and running locally using
      pegasus-run.</title>

      <para>In this exercise we are going to run pegasus-plan to generate a
      executable workflow from the abstract workflow (diamond.dax). The
      Executable workflow generated, are condor submit files that are
      submitted locally using pegasus-run</para>

      <para>The instructors have provided: <itemizedlist>
          <listitem>
            <para>A dax (diamond.dax) in the $HOME/pegasus-wms/dax
            directory.</para>
          </listitem>
        </itemizedlist></para>

      <para>You will need to write some things yourself, by following the
      instructions below: <itemizedlist>
          <listitem>
            <para>Run pegasus-plan to generate the condor submit files out of
            the dax.</para>
          </listitem>

          <listitem>
            <para>Run pegasus-run to submit the workflow locally.</para>
          </listitem>
        </itemizedlist></para>

      <para>Instructions:</para>

      <itemizedlist>
        <listitem>
          <para>Let us run pegasus-plan on the diamond dax. <programlisting><command>$ cd ~/pegasus-wms
$ pegasus-plan -Dpegasus.user.properties=`pwd`/config/properties \
               --dax `pwd`/dax/diamond.dax --force\
               --dir dags -s local -o local --nocleanup -v</command></programlisting>
          The above command says that we need to plan the diamond dax locally.
          The condor submit files are to be generated in a directory structure
          whose base is dags. We also are requesting that no cleanup jobs be
          added as we require the intermediate data to be saved. Here is the
          output of pegasus-plan. <programlisting><computeroutput>2010.07.19 16:47:05.276 PDT: [INFO] event.pegasus.planner planner.version 2.4.2  - STARTED 
2010.07.19 16:47:05.858 PDT: [INFO] event.pegasus.parse.dax dax.id /home/pegasus/pegasus-wms/dax/diamond.dax  - STARTED 
2010.07.19 16:47:05.901 PDT: [INFO] event.pegasus.parse.dax dax.id /home/pegasus/pegasus-wms/dax/diamond.dax  - FINISHED 
2010.07.19 16:47:05.944 PDT: [INFO] event.pegasus.refinement dax.id diamond_0  - STARTED 
2010.07.19 16:47:05.957 PDT: [INFO] event.pegasus.siteselection dax.id diamond_0  - STARTED 
2010.07.19 16:47:05.977 PDT: [INFO] event.pegasus.siteselection dax.id diamond_0  - FINISHED 
2010.07.19 16:47:05.986 PDT: [INFO]  Grafting transfer nodes in the workflow 
2010.07.19 16:47:05.987 PDT: [INFO] event.pegasus.generate.transfer-nodes dax.id diamond_0  - STARTED 
2010.07.19 16:47:06.031 PDT: [INFO] event.pegasus.generate.transfer-nodes dax.id diamond_0  - FINISHED 
2010.07.19 16:47:06.032 PDT: [INFO] event.pegasus.generate.workdir-nodes dax.id diamond_0  - STARTED 
2010.07.19 16:47:06.034 PDT: [INFO] event.pegasus.generate.workdir-nodes dax.id diamond_0  - FINISHED 
2010.07.19 16:47:06.034 PDT: [INFO] event.pegasus.generate.cleanup-wf dax.id diamond_0  - STARTED 
2010.07.19 16:47:06.035 PDT: [INFO] event.pegasus.generate.cleanup-wf dax.id diamond_0  - FINISHED 
2010.07.19 16:47:06.035 PDT: [INFO] event.pegasus.refinement dax.id diamond_0  - FINISHED 
2010.07.19 16:47:06.202 PDT: [INFO]  Generating codes for the concrete workflow 
2010.07.19 16:47:06.316 PDT: [INFO]  Generating codes for the concrete workflow -DONE 
2010.07.19 16:47:06.316 PDT: [INFO]  Generating code for the cleanup workflow 
2010.07.19 16:47:06.369 PDT: [INFO]  Generating code for the cleanup workflow -DONE 
2010.07.19 16:47:06.373 PDT: [INFO]  


I have concretized your abstract workflow. The workflow has been entered 
into the workflow database with a state of "planned". The next step is 
to start or execute your workflow. The invocation required is


pegasus-run -Dpegasus.user.properties=/home/pegasus/pegasus-wms/dags/pegasus/pegasus/diamond/run0001/pegasus.4459134667464687814.properties \
 --nodatabase /home/pegasus/pegasus-wms/dags/pegasus/pegasus/diamond/run0001

 
2010.07.19 16:47:06.373 PDT: [INFO]  Time taken to execute is 1.065 seconds 
2010.07.19 16:47:06.374 PDT: [INFO] event.pegasus.planner planner.version 2.4.2  - FINISHED 
</computeroutput></programlisting></para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Now run pegasus-run as mentioned in the
          output of pegasus-plan. Do not copy the command below it is just for
          illustration purpose.</emphasis><programlisting><emphasis
                role="bold"></emphasis><emphasis role="bold">[pegasus@pegasus pegasus-wms]$ pegasus-run \
      -Dpegasus.user.properties=/home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0001/pegasus.350356687577055673.properties \
       /home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0001</emphasis>

-----------------------------------------------------------------------
File for submitting this DAG to Condor           : blackdiamond-0.dag.condor.sub
Log of DAGMan debugging messages                 : blackdiamond-0.dag.dagman.out
Log of Condor library output                     : blackdiamond-0.dag.lib.out
Log of Condor library error messages             : blackdiamond-0.dag.lib.err
Log of the life of condor_dagman itself          : blackdiamond-0.dag.dagman.log

-no_submit given, not submitting DAG to Condor.  You can do this with:
"condor_submit blackdiamond-0.dag.condor.sub"
-----------------------------------------------------------------------
Submitting job(s).
Logging submit event(s).
1 job(s) submitted to cluster 320.

Your Workflow has been started and runs in base directory given below

cd /home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0001

*** To monitor the workflow you can run ***

pegasus-status -l /home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0001

*** To remove your workflow run ***
pegasus-remove -d 320.0
or
pegasus-remove /home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0001

[pegasus@pegasus pegasus-wms]$ 

</programlisting></para>
        </listitem>
      </itemizedlist>
    </section>

    <section>
      <title>Tracking the progress of the workflow and debugging the
      workflows.</title>

      <para>In this exercise we are going to list ways to track your workflow,
      and give some debugging hints when something goes wrong.</para>

      <para>We will change into the directory, that was mentioned by the
      output of pegasus-run command.</para>

      <programlisting><command>$ cd /home/pegasus/pegasus-wms/dags/pegasus/pegasus/diamond/run0001</command></programlisting>

      <para>In this directory you will see a whole lot of files. That should
      not scare you. Unless things go wrong, you need to look at just a very
      few number of files to track the progress of the workflow</para>

      <itemizedlist>
        <listitem>
          <para><emphasis role="bold">Run the command pegasus-status as
          mentioned by pegasus-run above to check the status of your jobs. Use
          the watch command to auto repeat the command every 2
          seconds.</emphasis><programlisting><command>$ watch pegasus-status /home/pegasus/pegasus-wms/dags/pegasus/pegasus/diamond/run0001</command><computeroutput>

-- Submitter: pegasus : &lt;172.16.80.128:40195&gt; : pegasus
 ID      OWNER/NODENAME   SUBMITTED     RUN_TIME ST PRI SIZE CMD               
  84.0   pegasus         7/19 16:59   0+00:01:17 R  0   7.3  condor_dagman -f -
  87.0    |-preprocess_  7/19 17:00   0+00:00:31 R  10  0.1  kickstart -n diamo
</computeroutput></programlisting> The above output shows that a couple of
          jobs are running under the main dagman process. Keep a lookout to
          track whether a workflow is running or not. If you do not see any of
          your job in the output for sometime (say 30 seconds), we know the
          workflow has finished. We need to wait, as there might be delay in
          Condor DAGMan releasing the next job into the queue after a job has
          finished successfully.</para>

          <para>If output of pegasus-status is empty, then either your
          workflow has - successfully completed - stopped midway due to non
          recoverable error. We can now run pegasus-analyzer to analyze the
          workflow.</para>
        </listitem>

        <listitem>
          <para>Using <emphasis role="bold">pegasus-analyzer</emphasis> to
          analyze the workflow</para>

          <programlisting><emphasis role="bold">[pegasus@pegasus run0001]$ pegasus-analyzer -q -i /home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0001</emphasis>

pegasus-analyzer: initializing...

************************************Summary*************************************

 Total jobs         :      8 (100.00%)
 # jobs succeeded   :      8 (100.00%)
 # jobs failed      :      0 (0.00%)
 # jobs unsubmitted :      0 (0.00%)

**************************************Done**************************************

pegasus-analyzer: end of status report


</programlisting>
        </listitem>

        <listitem>
          <para>Another way to monitor the workflow is to check the <emphasis
          role="bold">jobstate.log</emphasis> file. This is the output file of
          the monitoring daemon that is parsing all the condor log files to
          determine the status of the jobs. It logs the events seen by Condor
          into a more readable form for us. <programlisting><command>$ more jobstate.log</command><computeroutput>

1290676248 INTERNAL *** MONITORD_STARTED ***
1290676247 INTERNAL *** DAGMAN_STARTED 339.0 ***
[..]</computeroutput></programlisting> In the starting of the jobstate.log,
          when the workflow has just started running you will see a lot of
          entries with status UN_READY. That designates that DAGMan has just
          parsed in the .dag file and has not started working on any job as
          yet. Initially all the jobs in the workflow are listed as UN_READY.
          After sometime you will see entries in jobstate.log, that shows a
          job is being executed etc. <programlisting><computeroutput>
1290676261 create_dir_blackdiamond_0_local SUBMIT 340.0 local - 1
1290676266 create_dir_blackdiamond_0_local EXECUTE 340.0 local - 1
1290676266 create_dir_blackdiamond_0_local JOB_TERMINATED 340.0 local - 1
1290676266 create_dir_blackdiamond_0_local JOB_SUCCESS 0 local - 1
1290676266 create_dir_blackdiamond_0_local POST_SCRIPT_STARTED 340.0 local - 1
1290676271 create_dir_blackdiamond_0_local POST_SCRIPT_TERMINATED 340.0 local - 1
1290676271 create_dir_blackdiamond_0_local POST_SCRIPT_SUCCESS 0 local - 1</computeroutput></programlisting></para>

          <para>The above shows the being submitted and then executed on the
          grid. In addition it lists that job is being run on the grid site
          local (which is your submit machine). The various states of the job
          while it goes through submission to execution to post processing are
          in UPPERCASE.</para>
        </listitem>

        <listitem>
          <para>Successfully Completed : Let us again look at the
          jobstate.log. This time we need to look at the last few lines of
          jobstate.log <programlisting><command>$ tail jobstate.log</command><computeroutput>

1290676542 register_local_2_0 SUBMIT 347.0 local - 8
1290676547 register_local_2_0 EXECUTE 347.0 local - 8
1290676547 register_local_2_0 JOB_TERMINATED 347.0 local - 8
1290676547 register_local_2_0 JOB_SUCCESS 0 local - 8
1290676547 register_local_2_0 POST_SCRIPT_STARTED 347.0 local - 8
1290676552 register_local_2_0 POST_SCRIPT_TERMINATED 347.0 local - 8
1290676552 register_local_2_0 POST_SCRIPT_SUCCESS 0 local - 8
1290676552 INTERNAL *** DAGMAN_FINISHED 0 ***
1290676554 INTERNAL *** MONITORD_FINISHED 0 ***
</computeroutput></programlisting> Looking at the last two lines we see that
          DAGMan finished, and tailstatd finished successfully with a status
          0. This means workflow ran successfully. Congratulations you ran
          your workflow on the local site successfully. The workflow generates
          a final output file f.d that resides in the directory <emphasis
          role="bold">/home/pegasus/local-storage/storage/f.d</emphasis>
          .</para>

          <para>To view the file, you can do the following <programlisting><command>$ cat /home/pegasus/local-storage/storage/f.d
</command>

--- start f.c1 ----
  --- start f.b1 ----
    --- start f.a ----
      Input File for the Diamond Workflow.--- final f.a ----
    Timestamp Today: 20101125T011133.275-08:00 (1290676293.275;60.001)
    Applicationname: preprocess @ 172.16.80.129 (VPN)
    Current Workdir: /home/pegasus/local-scratch/exec/pegasus/pegasus/blackdiamond/run0001
    Systemenvironm.: x86_64-Linux 2.6.18-194.8.1.el5
    Processor Info.: 1 x Intel(R) Xeon(R) CPU           E5462  @ 2.80GHz @ 2794.306
    Load Averages  : 0.691 0.238 0.113
    Memory Usage MB: 1002 total, 350 free, 0 shared, 33 buffered
    Swap Usage   MB: 2015 total, 2015 free
    Filesystem Info: /                        ext3    17GB total,     9GB avail
    Output Filename: f.b1
    Input Filenames: f.a
  --- final f.b1 ----
  Timestamp Today: 20101125T011249.441-08:00 (1290676369.441;60.024)
  Applicationname: findrange @ 172.16.80.129 (VPN)
  Current Workdir: /home/pegasus/local-scratch/exec/pegasus/pegasus/blackdiamond/run0001
  Systemenvironm.: x86_64-Linux 2.6.18-194.8.1.el5
  Processor Info.: 1 x Intel(R) Xeon(R) CPU           E5462  @ 2.80GHz @ 2794.306
  Load Averages  : 1.413 0.530 0.219
  Memory Usage MB: 1002 total, 349 free, 0 shared, 33 buffered
  Swap Usage   MB: 2015 total, 2015 free
  Filesystem Info: /                        ext3    17GB total,     9GB avail
  Output Filename: f.c1
  Input Filenames: f.b1
--- final f.c1 ----
--- start f.c2 ----
  --- start f.b2 ----
    --- start f.a ----
      Input File for the Diamond Workflow.--- final f.a ----
    Timestamp Today: 20101125T011133.275-08:00 (1290676293.275;60.001)
    Applicationname: preprocess @ 172.16.80.129 (VPN)
    Current Workdir: /home/pegasus/local-scratch/exec/pegasus/pegasus/blackdiamond/run0001
    Systemenvironm.: x86_64-Linux 2.6.18-194.8.1.el5
    Processor Info.: 1 x Intel(R) Xeon(R) CPU           E5462  @ 2.80GHz @ 2794.306
    Load Averages  : 0.691 0.238 0.113
    Memory Usage MB: 1002 total, 350 free, 0 shared, 33 buffered
    Swap Usage   MB: 2015 total, 2015 free
    Filesystem Info: /                        ext3    17GB total,     9GB avail
    Output Filename: f.b2
    Input Filenames: f.a
  --- final f.b2 ----
  Timestamp Today: 20101125T011254.555-08:00 (1290676374.555;60.001)
  Applicationname: findrange @ 172.16.80.129 (VPN)
  Current Workdir: /home/pegasus/local-scratch/exec/pegasus/pegasus/blackdiamond/run0001
  Systemenvironm.: x86_64-Linux 2.6.18-194.8.1.el5
  Processor Info.: 1 x Intel(R) Xeon(R) CPU           E5462  @ 2.80GHz @ 2794.306
  Load Averages  : 1.380 0.538 0.223
  Memory Usage MB: 1002 total, 349 free, 0 shared, 33 buffered
  Swap Usage   MB: 2015 total, 2015 free
  Filesystem Info: /                        ext3    17GB total,     9GB avail
  Output Filename: f.c2
  Input Filenames: f.b2
--- final f.c2 ----
Timestamp Today: 20101125T011410.669-08:00 (1290676450.669;60.001)
Applicationname: analyze @ 172.16.80.129 (VPN)
Current Workdir: /home/pegasus/local-scratch/exec/pegasus/pegasus/blackdiamond/run0001
Systemenvironm.: x86_64-Linux 2.6.18-194.8.1.el5
Processor Info.: 1 x Intel(R) Xeon(R) CPU           E5462  @ 2.80GHz @ 2794.306
Load Averages  : 1.025 0.597 0.266
Memory Usage MB: 1002 total, 349 free, 0 shared, 34 buffered
Swap Usage   MB: 2015 total, 2015 free
Filesystem Info: /                        ext3    17GB total,     9GB avail
Output Filename: f.d
Input Filenames: f.c1 f.c2
</programlisting></para>
        </listitem>

        <listitem>
          <para>Unsuccessfully Completed (Workflow execution stopped midway) :
          Let us again look at the jobstate.log. Again we need to look at the
          last few lines of jobstate.log <programlisting><command>$ tail jobstate.log</command><computeroutput>

1290677127 stage_in_local_local_0 EXECUTE 352.0 local - 4
1290677127 stage_in_local_local_0 JOB_TERMINATED 352.0 local - 4
1290677127 stage_in_local_local_0 JOB_FAILURE 1 local - 4
1290677127 stage_in_local_local_0 POST_SCRIPT_STARTED 352.0 local - 4
1290677132 stage_in_local_local_0 POST_SCRIPT_TERMINATED 352.0 local - 4
1290677132 stage_in_local_local_0 POST_SCRIPT_FAILURE 1 local - 4
1290677132 INTERNAL *** DAGMAN_FINISHED 1 ***
1290677134 INTERNAL *** MONITORD_FINISHED 0 ***

</computeroutput></programlisting>Looking at the last two lines we see that
          DAGMan finished, and tailstatd finished unsuccessfully with a status
          1. We can easily determine which job failed. It is
          stage_in_local_local_0 in this case. To determine the reason for
          failure we need to look at it's kickstart output file which is
          JOBNAME.out.NNN. where NNN is 000 - NNN</para>
        </listitem>
      </itemizedlist>
    </section>

    <section>
      <title>Debugging a failed workflow using pegasus-analyzer</title>

      <para>In this section, we will run the diamond workflow but remove the
      input file so that the workflow fails during execution. This is to
      highlight how to use pegasus-analyzer to debug a failed workflow.</para>

      <para>First of all lets rename the input file f.a</para>

      <programlisting><emphasis role="bold"> mv /scratch/tutorial/inputdata/diamond/f.a /scratch/tutorial/inputdata/diamond/f.a.old
</emphasis></programlisting>

      <para>We will now repeat exercise <emphasis role="bold">2.4 and
      2.5</emphasis> and submit the workflow again.</para>

      <programlisting><emphasis role="bold">Plan and Submit the diamond workflow</emphasis> . Pass --submit to pegasus-plan to submit in case of successful planning

[pegasus@pegasus pegasus-wms]$  pegasus-plan -Dpegasus.user.properties=`pwd`/config/properties --dax `pwd`/dax/diamond.dax --force \
        --dir dags -s local -o local --nocleanup --submit -v

<emphasis role="bold">
Use pegasus-status to track the workflow and wait it to fail</emphasis>

[pegasus@pegasus pegasus-wms]$ pegasus-status  /home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0002


-- Submitter: pegasus : &lt;172.16.80.128:40195&gt; : pegasus
 ID      OWNER/NODENAME   SUBMITTED     RUN_TIME ST PRI SIZE CMD               
  96.0   pegasus         7/19 17:40   0+00:01:06 R  0   7.3  condor_dagman -f -

<emphasis role="bold">
The --long option to pegasus-status of a running workflow gives more detail
<emphasis>[pegasus@pegasus pegasus-wms]$ pegasus-status -l /home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0002
blackdiamond-0.dag is running.
11/25 01:25:06  Done     Pre   Queued    Post   Ready   Un-Ready   Failed
11/25 01:25:06   ===     ===      ===     ===     ===        ===      ===
11/25 01:25:06     1       0        1       0       0          6        0</emphasis><emphasis>

WORKFLOW STATUS : RUNNING | 1/8 ( 12% ) | (condor processing workflow)
</emphasis>


We can also use --long option to pegasus-status to see the FINAL status of the workflow</emphasis>

[pegasus@pegasus pegasus-wms]$ pegasus-status -l /home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0002
blackdiamond-0.dag FAILED (status 1)
11/25 01:25:32  Done     Pre   Queued    Post   Ready   Un-Ready   Failed
11/25 01:25:32   ===     ===      ===     ===     ===        ===      ===
11/25 01:25:32     1       0        0       0       0          6        1

WORKFLOW STATUS : FAILED | 1/8 ( 12% ) | (rescue needs to be submitted)


</programlisting>

      <para>We will now run pegasus-analyzer on the failed workflow submit
      directory to see what job failed.</para>

      <programlisting><emphasis role="bold">[pegasus@pegasus pegasus-wms]$ pegasus-analyzer -q -i /home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0002<emphasis>
pegasus-analyzer: initializing...
</emphasis></emphasis>
************************************Summary*************************************

 Total jobs         :      8 (100.00%)
 # jobs succeeded   :      1 (12.50%)
 # jobs failed      :      1 (12.50%)
 # jobs unsubmitted :      6 (75.00%)

******************************Failed jobs' details******************************

=============================stage_in_local_local_0=============================

 last state: POST_SCRIPT_FAILURE
       site: local
submit file: /home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0002/stage_in_local_local_0.sub
output file: /home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0002/stage_in_local_local_0.out.002
 error file: /home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0002/stage_in_local_local_0.err.002

**************************************Done**************************************

pegasus-analyzer: end of status report

[pegasus@pegasus pegasus-wms]$ pegasus-analyzer  -i /home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0002
pegasus-analyzer: initializing...

************************************Summary*************************************

 Total jobs         :      8 (100.00%)
 # jobs succeeded   :      1 (12.50%)
 # jobs failed      :      1 (12.50%)
 # jobs unsubmitted :      6 (75.00%)

******************************Failed jobs' details******************************

=============================stage_in_local_local_0=============================

 last state: POST_SCRIPT_FAILURE
       site: local
submit file: /home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0002/stage_in_local_local_0.sub
output file: /home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0002/stage_in_local_local_0.out.002
 error file: /home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0002/stage_in_local_local_0.err.002

-------------------------------Task #1 - Summary--------------------------------

site        : local
hostname    : pegasus
executable  : /opt/pegasus/default/bin/pegasus-transfer
arguments   : 
exitcode    : 1
working dir : /home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0002

--Task #1 - pegasus::pegasus-transfer - pegasus::pegasus-transfer:1.0 - stdout--

2010-11-25 01:25:22,320    INFO:  Reading URL pairs from stdin
2010-11-25 01:25:22,321    INFO:  PATH=/usr/local/globus/default/bin:/opt/pegasus/default/bin:/usr/bin:/bin
2010-11-25 01:25:22,321    INFO:  LD_LIBRARY_PATH=/usr/local/globus/default/lib:/usr/java/jdk1.6.0_20/jre/lib/amd64/server:/usr/java/jdk1.6.0_20/jre/lib/amd64:/usr/java/jdk1.6.0_20/jre/../lib/amd64
2010-11-25 01:25:22,321    INFO:  Executing cp commands
/bin/cp: cannot stat `/scratch/tutorial/inputdata/diamond/f.a': No such file or directory
2010-11-25 01:25:22,331 CRITICAL:  Command'/bin/cp -L"/scratch/tutorial/inputdata/diamond/f.a""/home/pegasus/local-scratch/exec/pegasus/pegasus/blackdiamond/run0002/f.a"'failed with error code 1

**************************************Done**************************************

pegasus-analyzer: end of status report

[pegasus@pegasus pegasus-wms]$ 

</programlisting>

      <para>The above tells us that the stage-in job for the workflow failed,
      and points us to the stdout of the job. By default, all jobs in Pegasus
      are launched via kickstart that captures runtime provenance of the job
      and helps in debugging. Hence, the stdout of the job is the kickstart
      stdout which is in XML.</para>

      <para>. the duration of the job the start time for the job the node on
      which the job ran the stdout/stderr of the job the arguments with which
      it launched the job the environment that was set for the job before it
      was launched. the machine information about the node that the job ran on
      Amongst the above information, the dagman.out file gives a coarser
      grained estimate of the job duration and start time</para>
    </section>

    <section>
      <title>Kickstart</title>

      <para>Kickstart is a light weight C executable that is shipped with the
      pegasus worker package. All jobs are launced via Kickstart on the remote
      end, unless explicitly disabled at the time of running
      pegasus-plan.</para>

      <para>Kickstart does not work with</para>

      <orderedlist>
        <listitem>
          <para>Condor Standard Universe Jobs</para>
        </listitem>

        <listitem>
          <para>MPI jobs</para>
        </listitem>
      </orderedlist>

      <para>Pegasus automatically disables kickstart for the above
      jobs.</para>

      <para>Kickstart captures useful runtime provenance information about the
      job launched by it on the remote note, and puts in an XML record that it
      writes to it's stdout. The stdout appears in the workflow submit
      directory as &lt;job&gt;.out.00n . Some useful information captured by
      kickstart and logged are as follows</para>

      <orderedlist>
        <listitem>
          <para>the exitcode with which the job it launched exited</para>
        </listitem>

        <listitem>
          <para>the duration of the job</para>
        </listitem>

        <listitem>
          <para>the start time for the job</para>
        </listitem>

        <listitem>
          <para>the node on which the job ran</para>
        </listitem>

        <listitem>
          <para>the directory in which the job ran</para>
        </listitem>

        <listitem>
          <para>the stdout/stderr of the job</para>
        </listitem>

        <listitem>
          <para>the arguments with which it launched the job</para>
        </listitem>

        <listitem>
          <para>the environment that was set for the job before it was
          launched.</para>
        </listitem>

        <listitem>
          <para>the machine information about the node that the job ran
          on</para>
        </listitem>
      </orderedlist>

      <section>
        <title>Reading a Kickstart Output File</title>

        <para>Lets look at the stdout of our failed job.</para>

        <programlisting><emphasis role="bold">[pegasus@pegasus pegasus-wms]$ cat  /home/pegasus/pegasus-wms/dags/pegasus/pegasus/diamond/run0002/stage_in_local_0.out.002</emphasis>

&lt;?xml version="1.0" encoding="ISO-8859-1"?&gt;
&lt;invocation xmlns="http://pegasus.isi.edu/schema/invocation" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" \
xsi:schemaLocation="http://pegasus.isi.edu/schema/invocation http://pegasus.isi.edu/schema/iv-2.0.xsd" version="2.0" \
start="2010-07-19T17:41:57.857-07:00" duration="0.091" <emphasis role="bold">transformation="globus::guc"</emphasis> derivation="globus::guc" <emphasis
            role="bold">resource="local"</emphasis> \
wf-label="diamond" wf-stamp="2010-07-19T14:36:28-07:00" <emphasis role="bold">hostaddr="172.16.80.128"</emphasis> <emphasis
            role="bold">hostname="pegasus"</emphasis> pid="12792" uid="501" \
user="pegasus" gid="501" group="pegasus" umask="0022"&gt;
  <emphasis role="bold">&lt;mainjob start="2010-07-19T17:41:57.870-07:00" duration="0.078" pid="12793"&gt;</emphasis>
    &lt;usage utime="0.012" stime="0.012" minflt="1115" majflt="0" nswap="0" nsignals="0" nvcsw="2" nivcsw="24"/&gt;
    <emphasis role="bold">&lt;status raw="256"&gt;&lt;regular exitcode="1"/&gt;&lt;/status&gt;</emphasis>
    &lt;statcall error="0"&gt;
      &lt;!-- deferred flag: 0 --&gt;
      &lt;file name="/usr/local/globus/default/bin/globus-url-copy"&gt;7F454C46020101000000000000000000&lt;/file&gt;
      &lt;statinfo mode="0100755" size="94699" inode="1109806" nlink="1" blksize="4096" blocks="208" mtime="2008-10-02T14:38:57-07:00" atime="2010-07-19T17:41:42-07:00" ctime="2010-05-24T15:17:51-07:00" uid="30101" gid="30101"/&gt;
    &lt;/statcall&gt;
    <emphasis role="bold">&lt;argument-vector&gt;
      &lt;arg nr="1"&gt;-p&lt;/arg&gt;
      &lt;arg nr="2"&gt;1&lt;/arg&gt;
      &lt;arg nr="3"&gt;-cd&lt;/arg&gt;
      &lt;arg nr="4"&gt;-vb&lt;/arg&gt;
      &lt;arg nr="5"&gt;-f&lt;/arg&gt;
      &lt;arg nr="6"&gt;stage_in_local_0.in&lt;/arg&gt;
    &lt;/argument-vector&gt;</emphasis>
  &lt;/mainjob&gt;
  <emphasis role="bold">&lt;cwd&gt;/home/pegasus/pegasus-wms/dags/pegasus/pegasus/diamond/run0002&lt;/cwd&gt;</emphasis>
  &lt;usage utime="0.002" stime="0.014" minflt="472" majflt="0" nswap="0" nsignals="0" nvcsw="1" nivcsw="6"/&gt;
  <emphasis role="bold">&lt;machine page-size="4096" provider="LINUX"&gt;
    &lt;stamp&gt;2010-07-19T17:41:57.857-07:00&lt;/stamp&gt;
    &lt;uname system="linux" nodename="pegasus" release="2.6.18-194.8.1.el5" machine="x86_64"&gt;#1 SMP Thu Jul 1 19:04:48 EDT 2010&lt;/uname&gt;
    &lt;ram total="1051533312" free="43859968" shared="0" buffer="102002688"/&gt;
    &lt;swap total="2113921024" free="2113921024"/&gt;
    &lt;boot idle="21305.050"&gt;2010-07-19T11:37:35.937-07:00&lt;/boot&gt;
    &lt;cpu count="1" speed="2794" vendor="GenuineIntel"&gt;Intel(R) Xeon(R) CPU E5462 @ 2.80GHz&lt;/cpu&gt;
    &lt;load min1="0.00" min5="0.00" min15="0.00"/&gt;
    &lt;proc total="113" running="1" sleeping="111" zombie="1" vmsize="7912247296" rss="401776640"/&gt;
    &lt;task total="140" running="1" sleeping="138" zombie="1"/&gt;
  &lt;/machine&gt;</emphasis>
  &lt;statcall error="0" id="stdin"&gt;
    &lt;!-- deferred flag: 0 --&gt;
    &lt;file name="/dev/null"/&gt;
    &lt;statinfo mode="020666" size="0" inode="1777" nlink="1" blksize="4096" blocks="0" mtime="2010-07-19T11:38:11-07:00" atime="2010-07-19T11:38:11-07:00" ctime="2010-07-19T11:38:11-07:00" uid="0" user="root" gid="0" group="root"/&gt;
  &lt;/statcall&gt;
  <emphasis role="bold">&lt;statcall error="0" id="stdout"&gt;
    &lt;temporary name="/opt/condor/local.pegasus/spool/local_univ_execute/dir_12791/gs.out.3qsk2X" descriptor="3"/&gt;
    &lt;statinfo mode="0100600" size="149" inode="2021925" nlink="1" blksize="4096" blocks="16" mtime="2010-07-19T17:41:57-07:00" atime="2010-07-19T17:41:57-07:00" ctime="2010-07-19T17:41:57-07:00" uid="501" user="pegasus" gid="501" group="pegasus"/&gt;
    &lt;data&gt;Source: gsiftp://pegasus/scratch/tutorial/inputdata/diamond/
Dest:   file:///home/pegasus/local-scratch/exec/pegasus/pegasus/diamond/run0002/
  f.a

&lt;/data&gt;
  &lt;/statcall&gt;
  &lt;statcall error="0" id="stderr"&gt;
    &lt;temporary name="/opt/condor/local.pegasus/spool/local_univ_execute/dir_12791/gs.err.kooIKM" descriptor="4"/&gt;
    &lt;statinfo mode="0100600" size="338" inode="2021926" nlink="1" blksize="4096" blocks="16" mtime="2010-07-19T17:41:57-07:00" atime="2010-07-19T17:41:57-07:00" ctime="2010-07-19T17:41:57-07:00" uid="501" user="pegasus" gid="501" group="pegasus"/&gt;
    &lt;data&gt;
error: globus_ftp_client: the server responded with an error
500 500-Command failed. : globus_l_gfs_file_open failed.
500-globus_xio: Unable to open file /scratch/tutorial/inputdata/diamond/f.a
500-globus_xio: System error in open: No such file or directory
500-globus_xio: A system call failed: No such file or directory
500 End.

&lt;/data&gt;</emphasis>
  &lt;/statcall&gt;
  &lt;statcall error="2" id="gridstart"&gt;
    &lt;!-- deferred flag: 0 --&gt;
    &lt;file name="condor_exec.exe"/&gt;
  &lt;/statcall&gt;
  &lt;statcall error="0" id="logfile"&gt;
    &lt;descriptor number="1"/&gt;
    &lt;statinfo mode="0100644" size="0" inode="2054616" nlink="1" blksize="4096" blocks="8" mtime="2010-07-19T17:41:57-07:00" atime="2010-07-19T17:41:57-07:00" ctime="2010-07-19T17:41:57-07:00" uid="501" user="pegasus" gid="501" group="pegasus"/&gt;
  &lt;/statcall&gt;
  &lt;statcall error="0" id="channel"&gt;
    &lt;fifo name="/opt/condor/local.pegasus/spool/local_univ_execute/dir_12791/gs.app.zYO6sB" descriptor="5" count="0" rsize="0" wsize="0"/&gt;
    &lt;statinfo mode="010640" size="0" inode="2021927" nlink="1" blksize="4096" blocks="8" mtime="2010-07-19T17:41:57-07:00" atime="2010-07-19T17:41:57-07:00" ctime="2010-07-19T17:41:57-07:00" uid="501" user="pegasus" gid="501" group="pegasus"/&gt;
  &lt;/statcall&gt;
  &lt;environment&gt;
    &lt;env key="GLOBUS_LOCATION"&gt;/usr/local/globus/default&lt;/env&gt;
    &lt;env key="GRIDSTART_CHANNEL"&gt;/opt/condor/local.pegasus/spool/local_univ_execute/dir_12791/gs.app.zYO6sB&lt;/env&gt;
    &lt;env key="JAVA_HOME"&gt;/usr/java/default&lt;/env&gt;
    &lt;env key="LD_LIBRARY_PATH"&gt;/usr/local/globus/default/lib&lt;/env&gt;
    &lt;env key="PEGASUS_HOME"&gt;/opt/pegasus/default&lt;/env&gt;
    &lt;env key="TEMP"&gt;/opt/condor/local.pegasus/spool/local_univ_execute/dir_12791&lt;/env&gt;
    &lt;env key="TMP"&gt;/opt/condor/local.pegasus/spool/local_univ_execute/dir_12791&lt;/env&gt;
    &lt;env key="TMPDIR"&gt;/opt/condor/local.pegasus/spool/local_univ_execute/dir_12791&lt;/env&gt;
    &lt;env key="_CONDOR_ANCESTOR_12791"&gt;12792:1279586517:2377531392&lt;/env&gt;
    &lt;env key="_CONDOR_ANCESTOR_3849"&gt;4088:1279564727:2726859010&lt;/env&gt;
    &lt;env key="_CONDOR_ANCESTOR_4088"&gt;12791:1279586517:3701537309&lt;/env&gt;
    &lt;env key="_CONDOR_HIGHPORT"&gt;41000&lt;/env&gt;
    &lt;env key="_CONDOR_JOB_AD"&gt;/opt/condor/local.pegasus/spool/local_univ_execute/dir_12791/.job.ad&lt;/env&gt;
    &lt;env key="_CONDOR_LOWPORT"&gt;40000&lt;/env&gt;
    &lt;env key="_CONDOR_MACHINE_AD"&gt;/opt/condor/local.pegasus/spool/local_univ_execute/dir_12791/.machine.ad&lt;/env&gt;
    &lt;env key="_CONDOR_SCRATCH_DIR"&gt;/opt/condor/local.pegasus/spool/local_univ_execute/dir_12791&lt;/env&gt;
    &lt;env key="_CONDOR_SLOT"&gt;1&lt;/env&gt;
  &lt;/environment&gt;
  &lt;resource&gt;
    &lt;soft id="RLIMIT_CPU"&gt;unlimited&lt;/soft&gt;
    &lt;hard id="RLIMIT_CPU"&gt;unlimited&lt;/hard&gt;
    &lt;soft id="RLIMIT_FSIZE"&gt;unlimited&lt;/soft&gt;
    &lt;hard id="RLIMIT_FSIZE"&gt;unlimited&lt;/hard&gt;
    &lt;soft id="RLIMIT_DATA"&gt;unlimited&lt;/soft&gt;
    &lt;hard id="RLIMIT_DATA"&gt;unlimited&lt;/hard&gt;
    &lt;soft id="RLIMIT_STACK"&gt;unlimited&lt;/soft&gt;
    &lt;hard id="RLIMIT_STACK"&gt;unlimited&lt;/hard&gt;
    &lt;soft id="RLIMIT_CORE"&gt;0&lt;/soft&gt;
    &lt;hard id="RLIMIT_CORE"&gt;0&lt;/hard&gt;
    &lt;soft id="RESOURCE_5"&gt;unlimited&lt;/soft&gt;
    &lt;hard id="RESOURCE_5"&gt;unlimited&lt;/hard&gt;
    &lt;soft id="RLIMIT_NPROC"&gt;8192&lt;/soft&gt;
    &lt;hard id="RLIMIT_NPROC"&gt;8192&lt;/hard&gt;
    &lt;soft id="RLIMIT_NOFILE"&gt;1024&lt;/soft&gt;
    &lt;hard id="RLIMIT_NOFILE"&gt;1024&lt;/hard&gt;
    &lt;soft id="RLIMIT_MEMLOCK"&gt;32768&lt;/soft&gt;
    &lt;hard id="RLIMIT_MEMLOCK"&gt;32768&lt;/hard&gt;
    &lt;soft id="RLIMIT_AS"&gt;unlimited&lt;/soft&gt;
    &lt;hard id="RLIMIT_AS"&gt;unlimited&lt;/hard&gt;
    &lt;soft id="RLIMIT_LOCKS"&gt;unlimited&lt;/soft&gt;
    &lt;hard id="RLIMIT_LOCKS"&gt;unlimited&lt;/hard&gt;
    &lt;soft id="RLIMIT_SIGPENDING"&gt;8192&lt;/soft&gt;
    &lt;hard id="RLIMIT_SIGPENDING"&gt;8192&lt;/hard&gt;
    &lt;soft id="RLIMIT_MSGQUEUE"&gt;819200&lt;/soft&gt;
    &lt;hard id="RLIMIT_MSGQUEUE"&gt;819200&lt;/hard&gt;
    &lt;soft id="RLIMIT_NICE"&gt;0&lt;/soft&gt;
    &lt;hard id="RLIMIT_NICE"&gt;0&lt;/hard&gt;
    &lt;soft id="RLIMIT_RTPRIO"&gt;0&lt;/soft&gt;
    &lt;hard id="RLIMIT_RTPRIO"&gt;0&lt;/hard&gt;
  &lt;/resource&gt;
&lt;/invocation&gt;
[pegasus@pegasus pegasus-wms]$ 

</programlisting>
      </section>
    </section>

    <section>
      <title>Condor DAGMan format and log files etc.</title>

      <para>In this exercise we will learn about the DAG file format and some
      of the log files generated when the DAG runs.</para>

      <itemizedlist>
        <listitem>
          <para>Now take a look at the DAG file...</para>

          <programlisting><command>$ cat $HOME/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0001/blackdiamond-0.dag</command><computeroutput>


######################################################################
# PEGASUS WMS GENERATED DAG FILE
# DAG blackdiamond
# Index = 0, Count = 1
######################################################################
MAXJOBS projection 2

JOB create_dir_blackdiamond_0_local create_dir_blackdiamond_0_local.sub
SCRIPT POST create_dir_blackdiamond_0_local /opt/pegasus/default/bin/pegasus-exitcode \
  /home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0001/create_dir_blackdiamond_0_local.out
RETRY create_dir_blackdiamond_0_local 2

JOB stage_in_local_local_0 stage_in_local_local_0.sub
SCRIPT POST stage_in_local_local_0 /opt/pegasus/default/bin/pegasus-exitcode  \
 /home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0001/stage_in_local_local_0.out
RETRY stage_in_local_local_0 2

JOB preprocess_j1 preprocess_j1.sub
SCRIPT POST preprocess_j1 /opt/pegasus/default/bin/pegasus-exitcode   \
/home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0001/preprocess_j1.out
RETRY preprocess_j1 2

JOB findrange_j2 findrange_j2.sub
SCRIPT POST findrange_j2 /opt/pegasus/default/bin/pegasus-exitcode   \
/home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0001/findrange_j2.out
RETRY findrange_j2 2

JOB findrange_j3 findrange_j3.sub
SCRIPT POST findrange_j3 /opt/pegasus/default/bin/pegasus-exitcode   \
/home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0001/findrange_j3.out
RETRY findrange_j3 2

JOB analyze_j4 analyze_j4.sub
SCRIPT POST analyze_j4 /opt/pegasus/default/bin/pegasus-exitcode   \
/home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0001/analyze_j4.out
RETRY analyze_j4 2

JOB stage_out_local_local_2_0 stage_out_local_local_2_0.sub
SCRIPT POST stage_out_local_local_2_0 /opt/pegasus/default/bin/pegasus-exitcode   \
/home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0001/stage_out_local_local_2_0.out
RETRY stage_out_local_local_2_0 2

JOB register_local_2_0 register_local_2_0.sub
SCRIPT POST register_local_2_0 /opt/pegasus/default/bin/pegasus-exitcode   \
/home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0001/register_local_2_0.out
RETRY register_local_2_0 2

PARENT findrange_j2 CHILD analyze_j4
PARENT preprocess_j1 CHILD findrange_j2
PARENT preprocess_j1 CHILD findrange_j3
PARENT findrange_j3 CHILD analyze_j4
PARENT analyze_j4 CHILD stage_out_local_local_2_0
PARENT stage_in_local_local_0 CHILD preprocess_j1
PARENT stage_out_local_local_2_0 CHILD register_local_2_0
PARENT create_dir_blackdiamond_0_local CHILD analyze_j4
PARENT create_dir_blackdiamond_0_local CHILD findrange_j2
PARENT create_dir_blackdiamond_0_local CHILD preprocess_j1
PARENT create_dir_blackdiamond_0_local CHILD findrange_j3
PARENT create_dir_blackdiamond_0_local CHILD stage_in_local_local_0
######################################################################
# End of DAG
##################################################################</computeroutput></programlisting>
        </listitem>

        <listitem>
          <para>... and the dagman.out file.</para>

          <programlisting><emphasis role="bold"><command>$</command><computeroutput> cat $HOME/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0001/blackdiamond-0.dag.dagman.out </computeroutput></emphasis><computeroutput>

11/25 01:10:47 ******************************************************
11/25 01:10:47 ** condor_scheduniv_exec.339.0 (CONDOR_DAGMAN) STARTING UP
11/25 01:10:47 ** /opt/condor/7.4.2/bin/condor_dagman
11/25 01:10:47 ** SubsystemInfo: name=DAGMAN type=DAGMAN(10) class=DAEMON(1)
11/25 01:10:47 ** Configuration: subsystem:DAGMAN local:&lt;NONE&gt; class:DAEMON
11/25 01:10:47 ** $CondorVersion: 7.4.2 Mar 29 2010 BuildID: 227044 $
11/25 01:10:47 ** $CondorPlatform: X86_64-LINUX_RHEL5 $
11/25 01:10:47 ** PID = 7844
11/25 01:10:47 ** Log last touched time unavailable (No such file or directory)
11/25 01:10:47 ******************************************************
11/25 01:10:47 Using config source: /opt/condor/config/condor_config
11/25 01:10:47 Using local config sources: 
11/25 01:10:47    /opt/condor/config/condor_config.local
11/25 01:10:47 DaemonCore: Command Socket at &lt;172.16.80.129:40035&gt;
11/25 01:10:47 DAGMAN_DEBUG_CACHE_SIZE setting: 5242880
11/25 01:10:47 DAGMAN_DEBUG_CACHE_ENABLE setting: False
11/25 01:10:47 DAGMAN_SUBMIT_DELAY setting: 0
11/25 01:10:47 DAGMAN_MAX_SUBMIT_ATTEMPTS setting: 6
11/25 01:10:47 DAGMAN_STARTUP_CYCLE_DETECT setting: 0
11/25 01:10:47 DAGMAN_MAX_SUBMITS_PER_INTERVAL setting: 5
11/25 01:10:47 DAGMAN_USER_LOG_SCAN_INTERVAL setting: 5
11/25 01:10:47 allow_events (DAGMAN_IGNORE_DUPLICATE_JOB_EXECUTION, DAGMAN_ALLOW_EVENTS) setting: 114
11/25 01:10:47 DAGMAN_RETRY_SUBMIT_FIRST setting: 1
11/25 01:10:47 DAGMAN_RETRY_NODE_FIRST setting: 0
11/25 01:10:47 DAGMAN_MAX_JOBS_IDLE setting: 0
11/25 01:10:47 DAGMAN_MAX_JOBS_SUBMITTED setting: 0
11/25 01:10:47 DAGMAN_MUNGE_NODE_NAMES setting: 1
11/25 01:10:47 DAGMAN_PROHIBIT_MULTI_JOBS setting: 0
11/25 01:10:47 DAGMAN_SUBMIT_DEPTH_FIRST setting: 0
11/25 01:10:47 DAGMAN_ABORT_DUPLICATES setting: 1
11/25 01:10:47 DAGMAN_ABORT_ON_SCARY_SUBMIT setting: 1
11/25 01:10:47 DAGMAN_PENDING_REPORT_INTERVAL setting: 600
11/25 01:10:47 DAGMAN_AUTO_RESCUE setting: 1
11/25 01:10:47 DAGMAN_MAX_RESCUE_NUM setting: 100
11/25 01:10:47 DAGMAN_DEFAULT_NODE_LOG setting: null
11/25 01:10:47 ALL_DEBUG setting: 
11/25 01:10:47 DAGMAN_DEBUG setting: 
....
11/25 01:10:47 Default node log file is: &lt;/home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0001/blackdiamond-0.dag.nodes.log&gt;
11/25 01:10:47 DAG Lockfile will be written to blackdiamond-0.dag.lock
11/25 01:10:47 DAG Input file is blackdiamond-0.dag
11/25 01:10:47 Parsing 1 dagfiles
11/25 01:10:47 Parsing blackdiamond-0.dag ...
11/25 01:10:47 Dag contains 8 total jobs
11/25 01:10:47 Sleeping for 12 seconds to ensure ProcessId uniqueness
11/25 01:10:59 Bootstrapping...
11/25 01:10:59 Number of pre-completed nodes: 0
11/25 01:10:59 Registering condor_event_timer...
11/25 01:11:00 Sleeping for one second for log file consistency
11/25 01:11:01 Submitting Condor Node create_dir_blackdiamond_0_local job(s)...
11/25 01:11:01 submitting: condor_submit -a dag_node_name' '=' 'create_dir_blackdiamond_0_local -a +DAGManJobId' '=' '339 -a DAGManJobId' '=' '339 -a submit_event_notes' '=' 'DAG' 'Node:' 'create_dir_blackdiamond_0_local -a +DAGParentNodeNames' '=' '"" create_dir_blackdiamond_0_local.sub
11/25 01:11:01 From submit: Submitting job(s).
11/25 01:11:01 From submit: Logging submit event(s).
11/25 01:11:01 From submit: 1 job(s) submitted to cluster 340.
11/25 01:11:01  assigned Condor ID (340.0)
11/25 01:11:01 Just submitted 1 job this cycle...
11/25 01:11:01 Currently monitoring 1 Condor log file(s)
11/25 01:11:01 Event: ULOG_SUBMIT for Condor Node create_dir_blackdiamond_0_local (340.0)
11/25 01:11:01 Number of idle job procs: 1
11/25 01:11:01 Of 8 nodes total:
11/25 01:11:01  Done     Pre   Queued    Post   Ready   Un-Ready   Failed
11/25 01:11:01   ===     ===      ===     ===     ===        ===      ===
11/25 01:11:01     0       0        1       0       0          7        0
....
11/25 01:11:06 Currently monitoring 1 Condor log file(s)
11/25 01:11:06 Event: ULOG_EXECUTE for Condor Node create_dir_blackdiamond_0_local (340.0)
11/25 01:11:06 Number of idle job procs: 0
11/25 01:11:06 Event: ULOG_JOB_TERMINATED for Condor Node create_dir_blackdiamond_0_local (340.0)
11/25 01:11:06 Node create_dir_blackdiamond_0_local job proc (340.0) completed successfully.
11/25 01:11:06 Node create_dir_blackdiamond_0_local job completed
11/25 01:11:06 Running POST script of Node create_dir_blackdiamond_0_local...
11/25 01:11:06 Number of idle job procs: 0
11/25 01:11:06 Of 8 nodes total:
11/25 01:11:06  Done     Pre   Queued    Post   Ready   Un-Ready   Failed
11/25 01:11:06   ===     ===      ===     ===     ===        ===      ===
11/25 01:11:06     0       0        0       1       0          7        0
11/25 01:11:11 Currently monitoring 1 Condor log file(s)
11/25 01:11:11 Event: ULOG_POST_SCRIPT_TERMINATED for Condor Node create_dir_blackdiamond_0_local (340.0)
11/25 01:11:11 POST Script of Node create_dir_blackdiamond_0_local completed successfully.
11/25 01:11:11 Of 8 nodes total:
11/25 01:11:11  Done     Pre   Queued    Post   Ready   Un-Ready   Failed
11/25 01:11:11   ===     ===      ===     ===     ===        ===      ===
11/25 01:11:11     1       0        0       0       1          6        0
....
11/25 01:15:52 Event: ULOG_POST_SCRIPT_TERMINATED for Condor Node register_local_2_0 (347.0)
11/25 01:15:52 POST Script of Node register_local_2_0 completed successfully.
11/25 01:15:52 Of 8 nodes total:
11/25 01:15:52  Done     Pre   Queued    Post   Ready   Un-Ready   Failed
11/25 01:15:52   ===     ===      ===     ===     ===        ===      ===
11/25 01:15:52     8       0        0       0       0          0        0
11/25 01:15:52 All jobs Completed!
11/25 01:15:52 Note: 0 total job deferrals because of -MaxJobs limit (0)
11/25 01:15:52 Note: 0 total job deferrals because of -MaxIdle limit (0)
11/25 01:15:52 Note: 0 total job deferrals because of node category throttles
11/25 01:15:52 Note: 0 total PRE script deferrals because of -MaxPre limit (20)
11/25 01:15:52 Note: 0 total POST script deferrals because of -MaxPost limit (20)
11/25 01:15:52 **** condor_scheduniv_exec.339.0 (condor_DAGMAN) pid 7844 EXITING WITH STATUS 0
[p

</computeroutput></programlisting>
        </listitem>
      </itemizedlist>
    </section>

    <section>
      <title>Removing a running workflow</title>

      <para>Sometimes you may want to halt the execution of the workflow or
      just permanently remove it. You can stop/halt a workflow by running the
      pegasus-remove command mentioned in the output of pegasus-run</para>

      <programlisting><command>$ pegasus-remove $HOME/pegasus-wms/dags/pegasus/pegasus/diamond/runXXXX</command><computeroutput>

Job 2788.0 marked for removal</computeroutput></programlisting>
    </section>

    <section>
      <title>Planning workflow using pegasus-plan and Running pegasus-run to
      submit the workflow to a grid resource.</title>

      <para>In this exercise we are going to run pegasus-plan to generate a
      executable workflow from the abstract workflow (montage.dax). The
      Executable workflow generated, are condor submit files that are
      submitted to remote grid resources using pegasus-run</para>

      <para>The instructors have provided:</para>

      <itemizedlist>
        <listitem>
          <para>A dax (montage.dax) in the $HOME/pegasus-wms/dax/
          directory.</para>
        </listitem>
      </itemizedlist>

      <para>You will need to write some things yourself, by following the
      instructions below: <itemizedlist>
          <listitem>
            <para>Run pegasus-plan to generate the condor submit files out of
            the dax.</para>
          </listitem>
        </itemizedlist></para>

      <para>Instructions:</para>

      <itemizedlist>
        <listitem>
          <para>Let us run pegasus-plan on the montage dax on the tg_ncsa
          cluster. If multiple sites are available you could provide the sites
          using a comma "," separated list like tg_ncsa,viz
          etc.<programlisting><command>$ cd $HOME/pegasus-wms
$ pegasus-plan -Dpegasus.user.properties=`pwd`/config/properties \
               -Dpegasus.schema.dax=/opt/pegasus/default/etc/dax-2.1.xsd \
               --dir dags --sites cluster --output local --force \
               --nocleanup --dax `pwd`/dax/montage.dax --submit -v</command></programlisting>
          The above command says that we need to plan the montage dax on the
          <emphasis role="bold">cluster</emphasis> site. The cluster site in
          the VM is managed by SGE that is running in the VM. The jobs for
          this workflow will be submitted to <emphasis
          role="bold">jobmanager-sge</emphasis> in the VM. The output data
          needs to be transferred back to the local host. The condor submit
          files are to be generated in a directory structure whose base is
          dags. We also are requesting that no cleanup jobs be added as we
          require the intermediate data on the remote host. Here is the output
          of pegasus-plan. <programlisting><computeroutput>
2010.11.24 18:20:10.948 PST: [INFO] event.pegasus.parse.dax dax.id /home/pegasus/pegasus-wms/dax/montage.dax  - STARTED 
2010.11.24 18:20:11.309 PST: [INFO] event.pegasus.parse.dax dax.id /home/pegasus/pegasus-wms/dax/montage.dax  - FINISHED 
2010.11.24 18:20:11.350 PST: [INFO] event.pegasus.refinement dax.id montage_0  - STARTED 
2010.11.24 18:20:11.360 PST: [INFO] event.pegasus.siteselection dax.id montage_0  - STARTED 
2010.11.24 18:20:11.416 PST: [INFO] event.pegasus.siteselection dax.id montage_0  - FINISHED 
2010.11.24 18:20:11.504 PST: [INFO]  Grafting transfer nodes in the workflow 
2010.11.24 18:20:11.505 PST: [INFO] event.pegasus.generate.transfer-nodes dax.id montage_0  - STARTED 
2010.11.24 18:20:11.655 PST: [INFO] event.pegasus.generate.transfer-nodes dax.id montage_0  - FINISHED 
2010.11.24 18:20:11.657 PST: [INFO] event.pegasus.generate.workdir-nodes dax.id montage_0  - STARTED 
2010.11.24 18:20:11.660 PST: [INFO] event.pegasus.generate.workdir-nodes dax.id montage_0  - FINISHED 
2010.11.24 18:20:11.660 PST: [INFO] event.pegasus.generate.cleanup-wf dax.id montage_0  - STARTED 
2010.11.24 18:20:11.661 PST: [INFO] event.pegasus.generate.cleanup-wf dax.id montage_0  - FINISHED 
2010.11.24 18:20:11.661 PST: [INFO] event.pegasus.refinement dax.id montage_0  - FINISHED 
2010.11.24 18:20:11.715 PST: [INFO]  Generating codes for the concrete workflow 
2010.11.24 18:20:12.406 PST: [INFO]  Generating codes for the concrete workflow -DONE 
2010.11.24 18:20:12.406 PST: [INFO]  Generating code for the cleanup workflow 
2010.11.24 18:20:12.528 PST: [INFO]  Generating code for the cleanup workflow -DONE 
2010.11.24 18:20:12.672 PST:    
2010.11.24 18:20:12.679 PST:   ----------------------------------------------------------------------- 
2010.11.24 18:20:12.685 PST:   File for submitting this DAG to Condor           : montage-0.dag.condor.sub 
2010.11.24 18:20:12.691 PST:   Log of DAGMan debugging messages                 : montage-0.dag.dagman.out 
2010.11.24 18:20:12.704 PST:   Log of Condor library output                     : montage-0.dag.lib.out 
2010.11.24 18:20:12.711 PST:   Log of Condor library error messages             : montage-0.dag.lib.err 
2010.11.24 18:20:12.726 PST:   Log of the life of condor_dagman itself          : montage-0.dag.dagman.log 
2010.11.24 18:20:12.731 PST:    
2010.11.24 18:20:12.762 PST:   -no_submit given, not submitting DAG to Condor.  You can do this with: 
2010.11.24 18:20:12.792 PST:   "condor_submit montage-0.dag.condor.sub" 
2010.11.24 18:20:12.798 PST:   ----------------------------------------------------------------------- 
2010.11.24 18:20:12.804 PST:   Submitting job(s). 
2010.11.24 18:20:12.815 PST:   Logging submit event(s). 
2010.11.24 18:20:12.821 PST:   1 job(s) submitted to cluster 275. 
2010.11.24 18:20:13.504 PST:    
2010.11.24 18:20:13.510 PST:   Your Workflow has been started and runs in base directory given below 
2010.11.24 18:20:13.519 PST:    
2010.11.24 18:20:13.530 PST:   cd /home/pegasus/pegasus-wms/dags/pegasus/pegasus/montage/run0001 
2010.11.24 18:20:13.535 PST:    
2010.11.24 18:20:13.542 PST:   *** To monitor the workflow you can run *** 
2010.11.24 18:20:13.555 PST:    
2010.11.24 18:20:13.562 PST:   pegasus-status -l /home/pegasus/pegasus-wms/dags/pegasus/pegasus/montage/run0001 
2010.11.24 18:20:13.570 PST:    
2010.11.24 18:20:13.578 PST:   *** To remove your workflow run *** 
2010.11.24 18:20:13.585 PST:   pegasus-remove -d 275.0 
2010.11.24 18:20:13.592 PST:   or 
2010.11.24 18:20:13.604 PST:   pegasus-remove /home/pegasus/pegasus-wms/dags/pegasus/pegasus/montage/run0001 
2010.11.24 18:20:13.610 PST:    
2010.11.24 18:20:13.617 PST:   Time taken to execute is 3.76 seconds 
2010.11.24 18:20:13.617 PST: [INFO] event.pegasus.planner planner.version 3.0.0  - FINISHED 
</computeroutput></programlisting></para>
        </listitem>

        <listitem>
          <para>If you get any errors above while running pegasus-plan you can
          add -vvvvv to enable maximum verbosity on pegasus-run.</para>
        </listitem>
      </itemizedlist>

      <para>The above command submits the workflow to Condor DAGMan/CondorG.
      After submitting it starts a monitoring daemon pegasus-monitord that
      parses the condor log files to update the status of the jobs and push it
      in a work database.</para>

      <para>Monitor the workflow using the commands provided in the output of
      the pegasus-run command and other commands explained earlier.</para>

      <para>The workflow generates a single output file montage.jpg that
      resides in the directory <emphasis
      role="bold">/home/pegasus/local-storage/storage/montage.jpg</emphasis>
      if it runs successfully</para>

      <para>The grid workflow will take time to execute on the VM. On the
      instructor's MAC Pro Desktop it took about<emphasis role="bold"> 30
      minutes </emphasis>to run.</para>
    </section>
  </section>

  <section>
    <title>Advanced Exercises</title>

    <section>
      <title>Optimizing a workflow by clustering small jobs (To Be Done
      offline)</title>

      <para>Sometimes a workflow may have too many jobs whose execution time
      is a few seconds long. In such instances the overhead of scheduling each
      job on a grid is too large and the runtime of the entire workflow can be
      optimized by using Pegasus clustering techniques. One such technique is
      to cluster jobs horizontally on the same level into one or more
      sequential jobs.</para>

      <programlisting><command>$ cd $HOME/pegasus-wms
$ pegasus-plan -Dpegasus.user.properties=`pwd`/config/properties \
            -Dpegasus.schema.dax=/opt/pegasus/default/etc/dax-2.1.xsd \
            --dir `pwd`/dags --sites cluster --output local --nocleanup --force\
            --cluster horizontal --dax `pwd`/dax/montage.dax -v</command></programlisting>

      <para>After clustering the executable workflow will contain 26 jobs
      compared to 44 in the non clustered mode.</para>
    </section>

    <section>
      <title>Data Reuse</title>

      <para>In the DAX you can specify what output data products you want to
      track in the replica catalog. This is done by setting the register flags
      with the output files for a job. For our tutorial, we only register the
      final output data products. So if you were able to execute the diamond
      or the montage workflow successfully, we can do data reuse. Let us run
      <emphasis role="bold">pegasus-plan </emphasis>on the diamond workflow
      again. However, this time we will remove the <emphasis
      role="bold">--force</emphasis> option.</para>

      <programlisting><emphasis role="bold">$ cd $HOME/pegasus-wms
$ pegasus-plan -Dpegasus.user.properties=`pwd`/config/properties --dax `pwd`/dax/diamond.dax --dir `pwd`/dags -s local \
                   -o local --nocleanup -v
</emphasis>
2010.11.25 01:35:11.186 PST: [INFO] event.pegasus.refinement dax.id blackdiamond_0  - STARTED 
2010.11.25 01:35:11.210 PST: [INFO] event.pegasus.reduce dax.id blackdiamond_0  - STARTED 
2010.11.25 01:35:11.211 PST: [INFO]  Nodes/Jobs Deleted from the Workflow during reduction  
2010.11.25 01:35:11.211 PST: [INFO]     analyze_j4 
2010.11.25 01:35:11.211 PST: [INFO]     findrange_j2 
2010.11.25 01:35:11.211 PST: [INFO]     findrange_j3 
2010.11.25 01:35:11.211 PST: [INFO]     preprocess_j1 
2010.11.25 01:35:11.211 PST: [INFO]  Nodes/Jobs Deleted from the Workflow during reduction  - DONE 
2010.11.25 01:35:11.212 PST: [INFO] event.pegasus.reduce dax.id blackdiamond_0  - FINISHED 
2010.11.25 01:35:11.212 PST: [INFO] event.pegasus.siteselection dax.id blackdiamond_0  - STARTED 
2010.11.25 01:35:11.219 PST: [INFO] event.pegasus.siteselection dax.id blackdiamond_0  - FINISHED 
2010.11.25 01:35:11.289 PST: [INFO]  Grafting transfer nodes in the workflow 
2010.11.25 01:35:11.290 PST: [INFO] event.pegasus.generate.transfer-nodes dax.id blackdiamond_0  - STARTED 
2010.11.25 01:35:11.370 PST: [INFO]  Adding stage out jobs for jobs deleted from the workflow 
2010.11.25 01:35:11.370 PST: [INFO]  The leaf file f.d is already at the output pool local 
2010.11.25 01:35:11.371 PST: [INFO] event.pegasus.generate.transfer-nodes dax.id blackdiamond_0  - FINISHED 
2010.11.25 01:35:11.372 PST: [INFO] event.pegasus.generate.workdir-nodes dax.id blackdiamond_0  - STARTED 
2010.11.25 01:35:11.374 PST: [INFO] event.pegasus.generate.workdir-nodes dax.id blackdiamond_0  - FINISHED 
2010.11.25 01:35:11.374 PST: [INFO] event.pegasus.generate.cleanup-wf dax.id blackdiamond_0  - STARTED 
2010.11.25 01:35:11.375 PST: [INFO] event.pegasus.generate.cleanup-wf dax.id blackdiamond_0  - FINISHED 
2010.11.25 01:35:11.375 PST: [INFO] event.pegasus.refinement dax.id blackdiamond_0  - FINISHED 
2010.11.25 01:35:11.426 PST: [INFO]  Generating codes for the concrete workflow 
2010.11.25 01:35:12.078 PST: [INFO]  Generating codes for the concrete workflow -DONE 
2010.11.25 01:35:12.083 PST:   


The executable workflow generated contains only a single NOOP job.
It seems that the output files are already at the output site. 
To regenerate the output data from scratch specify --force option.



pegasus-run -Dpegasus.user.properties=/home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0003/pegasus.4078026914028890643.properties /home/pegasus/pegasus-wms/dags/pegasus/pegasus/blackdiamond/run0003

 
2010.11.25 01:35:12.083 PST:   Time taken to execute is 1.508 seconds 
2010.11.25 01:35:12.083 PST: [INFO] event.pegasus.planner planner.version 3.0.0  - FINISHED 
</programlisting>

      <para>You can increase the debug level to see how pegasus deletes the
      jobs bottom up of the workflow. Pass -vvvvv to pegasus-plan
      command.</para>
    </section>
  </section>
</chapter>